{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ee9736",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas tqdm pdfplumber python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621f9871",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install kaggle kagglehub pandas-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f290ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SchemaDriftError(Exception):\n",
    "    \"\"\"Raised when schema drift is detected.\"\"\"\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "471907c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "def authenticate_kaggle():\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "    return api\n",
    "\n",
    "def download_kaggle_dataset(api, dataset_slug: str, download_dir: str):\n",
    "    os.makedirs(download_dir, exist_ok=True)\n",
    "    dataset_name = dataset_slug.split(\"/\")[-1]\n",
    "    zip_path = os.path.join(download_dir, f\"{dataset_name}.zip\")\n",
    "    extract_path = os.path.join(download_dir, dataset_name)\n",
    "\n",
    "    if os.path.exists(extract_path):\n",
    "        print(f\"✅ Dataset already extracted at '{extract_path}', skipping download.\")\n",
    "        return extract_path\n",
    "\n",
    "    print(f\"⬇️ Downloading dataset '{dataset_slug}'...\")\n",
    "    api.dataset_download_files(dataset_slug, path=download_dir, quiet=False)\n",
    "\n",
    "    import zipfile\n",
    "    print(f\"📦 Extracting dataset '{zip_path}'...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_path)\n",
    "    print(f\"✅ Extraction completed at '{extract_path}'.\")\n",
    "    \n",
    "    return extract_path\n",
    "\n",
    "def find_csv_file(root_path: str, target_csv: str) -> str:\n",
    "    for dirpath, _, filenames in os.walk(root_path):\n",
    "        if target_csv in filenames:\n",
    "            full_path = os.path.join(dirpath, target_csv)\n",
    "            print(f\"✅ Found CSV at: {full_path}\")\n",
    "            return full_path\n",
    "    raise FileNotFoundError(f\"❌ CSV file '{target_csv}' not found inside '{root_path}'.\")\n",
    "\n",
    "def load_csv_to_dataframe(csv_path: str) -> pd.DataFrame:\n",
    "    print(f\"📖 Loading CSV file '{csv_path}'...\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"✅ CSV loaded successfully. Shape: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "def infer_schema(df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    \"\"\"Infer schema as {column_name: expected_type}\"\"\"\n",
    "    schema = {}\n",
    "    for col in df.columns:\n",
    "        dtype = df[col].dropna().infer_objects().dtype\n",
    "        if pd.api.types.is_string_dtype(dtype):\n",
    "            schema[col] = str\n",
    "        elif pd.api.types.is_integer_dtype(dtype):\n",
    "            schema[col] = int\n",
    "        elif pd.api.types.is_float_dtype(dtype):\n",
    "            schema[col] = float\n",
    "        elif pd.api.types.is_bool_dtype(dtype):\n",
    "            schema[col] = bool\n",
    "        else:\n",
    "            schema[col] = str  # fallback: treat unknown types as str\n",
    "    print(f\"🔍 Inferred Schema: {schema}\")\n",
    "    return schema\n",
    "\n",
    "def dataframe_to_json_records(df: pd.DataFrame) -> List[dict]:\n",
    "    df = df.fillna('')\n",
    "    records = df.to_dict(orient='records')\n",
    "    print(f\"✅ Converted DataFrame to {len(records)} JSON records.\")\n",
    "    return records\n",
    "\n",
    "def validate_json_against_schema(records: List[dict], schema: Dict[str, Any]):\n",
    "    \"\"\"Validate each record against inferred schema.\"\"\"\n",
    "    for idx, record in enumerate(records):\n",
    "        for field, expected_type in schema.items():\n",
    "            if field not in record:\n",
    "                raise ValueError(f\"❌ Record {idx}: Missing field '{field}'.\")\n",
    "\n",
    "            value = record[field]\n",
    "\n",
    "            # Allow empty string as null (acceptable for optional fields)\n",
    "            if value == '' or value is None:\n",
    "                continue\n",
    "\n",
    "            # Some values from CSV come as wrong type (e.g., numbers as strings)\n",
    "            if not isinstance(value, expected_type):\n",
    "                try:\n",
    "                    # Try to cast\n",
    "                    expected_type(value)\n",
    "                except (ValueError, TypeError):\n",
    "                    raise TypeError(f\"❌ Record {idx}: Field '{field}' expected {expected_type.__name__}, got {type(value).__name__} with value '{value}'.\")\n",
    "    print(\"✅ Schema validation passed for all records.\")\n",
    "\n",
    "def save_json(data: List[dict], filepath: str):\n",
    "    if os.path.exists(filepath):\n",
    "        os.remove(filepath)\n",
    "        print(f\"🧹 Old JSON file '{filepath}' deleted.\")\n",
    "    \n",
    "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"✅ New JSON file saved to '{filepath}'.\")\n",
    "\n",
    "def download_and_load_kaggle_dataset(api, dataset_slug: str, download_dir: str = \"datasets\", csv_filename: str = None) -> pd.DataFrame:\n",
    "    if csv_filename is None:\n",
    "        raise ValueError(\"You must specify the CSV filename to load.\")\n",
    "\n",
    "    extract_path = download_kaggle_dataset(api, dataset_slug, download_dir)\n",
    "    csv_path = find_csv_file(extract_path, csv_filename)\n",
    "    df = load_csv_to_dataframe(csv_path)\n",
    "    return df\n",
    "\n",
    "def download_and_load_multiple_datasets(dataset_info: list, download_dir: str = \"datasets\") -> dict:\n",
    "    api = authenticate_kaggle()\n",
    "    loaded_datasets = {}\n",
    "\n",
    "    for item in dataset_info:\n",
    "        print(f\"\\n🚀 Processing dataset: {item['slug']}\")\n",
    "        df = download_and_load_kaggle_dataset(api, item['slug'], download_dir, item['csv'])\n",
    "        dataset_key = item.get('key', item['slug'].split('/')[-1])\n",
    "        loaded_datasets[dataset_key] = df\n",
    "    \n",
    "    print(\"\\n✅ All datasets loaded successfully.\")\n",
    "    return loaded_datasets\n",
    "\n",
    "def convert_and_save_datasets_with_schema(datasets: dict, output_dir: str = \"json_outputs\") -> dict:\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    all_json_data = {}\n",
    "\n",
    "    for key, df in datasets.items():\n",
    "        print(f\"\\n🚀 Converting dataset '{key}' to JSON with schema validation...\")\n",
    "        schema = infer_schema(df)\n",
    "        json_data = dataframe_to_json_records(df)\n",
    "        validate_json_against_schema(json_data, schema)\n",
    "\n",
    "        json_path = os.path.join(output_dir, f\"{key}.json\")\n",
    "        save_json(json_data, json_path)\n",
    "        \n",
    "        all_json_data[key] = json_data\n",
    "    \n",
    "    print(\"\\n✅ All datasets converted, validated with schema inference, and saved.\")\n",
    "    return all_json_data\n",
    "\n",
    "def save_schema(schema: Dict[str, Any], schema_path: str):\n",
    "    os.makedirs(os.path.dirname(schema_path), exist_ok=True)\n",
    "    schema_serializable = {k: v.__name__ for k, v in schema.items()}\n",
    "    with open(schema_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(schema_serializable, f, indent=4)\n",
    "    print(f\"✅ Baseline schema saved at '{schema_path}'.\")\n",
    "\n",
    "def load_schema(schema_path: str) -> Dict[str, Any]:\n",
    "    with open(schema_path, 'r', encoding='utf-8') as f:\n",
    "        schema_data = json.load(f)\n",
    "    schema = {k: eval(v) for k, v in schema_data.items()}\n",
    "    return schema\n",
    "\n",
    "def compare_schemas(baseline: Dict[str, Any], current: Dict[str, Any]) -> List[str]:\n",
    "    \"\"\"Return list of drift messages.\"\"\"\n",
    "    drift_messages = []\n",
    "\n",
    "    baseline_keys = set(baseline.keys())\n",
    "    current_keys = set(current.keys())\n",
    "\n",
    "    missing_keys = baseline_keys - current_keys\n",
    "    extra_keys = current_keys - baseline_keys\n",
    "\n",
    "    if missing_keys:\n",
    "        drift_messages.append(f\"❌ Missing fields in new data: {missing_keys}\")\n",
    "    if extra_keys:\n",
    "        drift_messages.append(f\"⚠️ Extra new fields in new data: {extra_keys}\")\n",
    "\n",
    "    for field in baseline_keys & current_keys:\n",
    "        if baseline[field] != current[field]:\n",
    "            drift_messages.append(f\"❌ Field '{field}' type mismatch: baseline {baseline[field].__name__}, current {current[field].__name__}\")\n",
    "\n",
    "    return drift_messages\n",
    "\n",
    "\n",
    "def convert_and_save_datasets_with_schema_and_drift_detection(\n",
    "    datasets: dict, \n",
    "    output_dir: str = \"json_outputs\", \n",
    "    schema_dir: str = \"schemas\",\n",
    "    allow_fields: dict = None  # <-- 🔥 Now allowlist\n",
    ") -> dict:\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    os.makedirs(schema_dir, exist_ok=True)\n",
    "    all_json_data = {}\n",
    "\n",
    "    for key, df in datasets.items():\n",
    "        print(f\"\\n🚀 Processing dataset '{key}' with column allowlist, schema validation, and drift detection...\")\n",
    "\n",
    "        # 🔥 Allow only specified fields if allowlist is given\n",
    "        if allow_fields and key in allow_fields:\n",
    "            fields_to_allow = allow_fields[key]\n",
    "            existing_fields = df.columns.tolist()\n",
    "            missing_fields = [field for field in fields_to_allow if field not in existing_fields]\n",
    "            if missing_fields:\n",
    "                print(f\"⚠️ Warning: These allowlisted fields are missing in '{key}': {missing_fields}\")\n",
    "            df = df[[col for col in fields_to_allow if col in df.columns]]\n",
    "            print(f\"🧹 Retained fields for '{key}': {df.columns.tolist()}\")\n",
    "\n",
    "        current_schema = infer_schema(df)\n",
    "        json_data = dataframe_to_json_records(df)\n",
    "\n",
    "        schema_path = os.path.join(schema_dir, f\"{key}_schema.json\")\n",
    "        \n",
    "        if os.path.exists(schema_path):\n",
    "            baseline_schema = load_schema(schema_path)\n",
    "            drift_messages = compare_schemas(baseline_schema, current_schema)\n",
    "            if drift_messages:\n",
    "                for msg in drift_messages:\n",
    "                    print(msg)\n",
    "                raise SchemaDriftError(f\"❌ Schema drift detected in dataset '{key}'. Please review changes.\")\n",
    "            else:\n",
    "                print(f\"✅ No schema drift detected for dataset '{key}'.\")\n",
    "        else:\n",
    "            print(f\"🆕 No baseline schema found for '{key}'. Saving new baseline.\")\n",
    "            save_schema(current_schema, schema_path)\n",
    "\n",
    "        validate_json_against_schema(json_data, current_schema)\n",
    "\n",
    "        json_path = os.path.join(output_dir, f\"{key}.json\")\n",
    "        save_json(json_data, json_path)\n",
    "        \n",
    "        all_json_data[key] = json_data\n",
    "    \n",
    "    print(\"\\n✅ All datasets processed, validated, drift-checked, and filtered by column allowlist.\")\n",
    "    return all_json_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4614bf88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Processing dataset: snehaanbhawal/resume-dataset\n",
      "✅ Dataset already extracted at 'datasets\\resume-dataset', skipping download.\n",
      "✅ Found CSV at: datasets\\resume-dataset\\Resume\\Resume.csv\n",
      "📖 Loading CSV file 'datasets\\resume-dataset\\Resume\\Resume.csv'...\n",
      "✅ CSV loaded successfully. Shape: (2484, 4)\n",
      "\n",
      "🚀 Processing dataset: arshkon/linkedin-job-postings\n",
      "✅ Dataset already extracted at 'datasets\\linkedin-job-postings', skipping download.\n",
      "✅ Found CSV at: datasets\\linkedin-job-postings\\postings.csv\n",
      "📖 Loading CSV file 'datasets\\linkedin-job-postings\\postings.csv'...\n",
      "✅ CSV loaded successfully. Shape: (123849, 31)\n",
      "\n",
      "✅ All datasets loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "dataset_info = [\n",
    "    {\n",
    "        \"slug\": \"snehaanbhawal/resume-dataset\",\n",
    "        \"csv\": \"Resume.csv\",\n",
    "        \"key\": \"resume_data\"\n",
    "    },\n",
    "    {\n",
    "        \"slug\": \"arshkon/linkedin-job-postings\",\n",
    "        \"csv\": \"postings.csv\",\n",
    "        \"key\": \"job_postings\"\n",
    "    }\n",
    "]\n",
    "#datasets = download_and_load_multiple_datasets(dataset_info)\n",
    "datasets = download_and_load_multiple_datasets(dataset_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06d590eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Processing dataset 'resume_data' with column allowlist, schema validation, and drift detection...\n",
      "🧹 Retained fields for 'resume_data': ['Category', 'Resume_str']\n",
      "🔍 Inferred Schema: {'Category': <class 'str'>, 'Resume_str': <class 'str'>}\n",
      "✅ Converted DataFrame to 2484 JSON records.\n",
      "🆕 No baseline schema found for 'resume_data'. Saving new baseline.\n",
      "✅ Baseline schema saved at 'schemas\\resume_data_schema.json'.\n",
      "✅ Schema validation passed for all records.\n",
      "✅ New JSON file saved to 'json_outputs\\resume_data.json'.\n",
      "\n",
      "🚀 Processing dataset 'job_postings' with column allowlist, schema validation, and drift detection...\n",
      "⚠️ Warning: These allowlisted fields are missing in 'job_postings': ['posted_date', 'job_function']\n",
      "🧹 Retained fields for 'job_postings': ['title', 'company_name', 'location', 'description', 'skills_desc', 'job_id', 'formatted_experience_level', 'formatted_work_type']\n",
      "🔍 Inferred Schema: {'title': <class 'str'>, 'company_name': <class 'str'>, 'location': <class 'str'>, 'description': <class 'str'>, 'skills_desc': <class 'str'>, 'job_id': <class 'int'>, 'formatted_experience_level': <class 'str'>, 'formatted_work_type': <class 'str'>}\n",
      "✅ Converted DataFrame to 123849 JSON records.\n",
      "🆕 No baseline schema found for 'job_postings'. Saving new baseline.\n",
      "✅ Baseline schema saved at 'schemas\\job_postings_schema.json'.\n",
      "✅ Schema validation passed for all records.\n",
      "✅ New JSON file saved to 'json_outputs\\job_postings.json'.\n",
      "\n",
      "✅ All datasets processed, validated, drift-checked, and filtered by column allowlist.\n"
     ]
    }
   ],
   "source": [
    "#json_structures = convert_and_save_datasets_with_schema(datasets)\n",
    "allow_fields = {\n",
    "    \"resume_data\": [\"Category\", \"Resume_str\"],  # Only keep these two fields for resumes\n",
    "    \"job_postings\": [\"title\", \"company_name\", \"location\", \"description\", \"skills_desc\", \"posted_date\", \"job_id\" , \"formatted_experience_level\", \"formatted_work_type\", \"job_function\"]  # Example allowlist for job postings\n",
    "}\n",
    "json_structures = convert_and_save_datasets_with_schema_and_drift_detection(datasets, allow_fields=allow_fields)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
