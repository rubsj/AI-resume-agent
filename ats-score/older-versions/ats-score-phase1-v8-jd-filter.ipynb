{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f630d895",
   "metadata": {},
   "source": [
    "# Global setup and package installation used in most phases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969fcea6",
   "metadata": {},
   "source": [
    "## Colab + GPU Detection Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cf79fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def is_running_in_colab():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "def get_available_gpu_memory_gb():\n",
    "    try:\n",
    "        output = subprocess.check_output(\n",
    "            [\"nvidia-smi\", \"--query-gpu=memory.free\", \"--format=csv,nounits,noheader\"],\n",
    "            encoding=\"utf-8\"\n",
    "        )\n",
    "        free_mem_mb = int(output.strip().split(\"\\n\")[0])\n",
    "        return free_mem_mb / 1024\n",
    "    except Exception:\n",
    "        return 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66be54d4",
   "metadata": {},
   "source": [
    "## install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942fba10",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_running_in_colab():\n",
    "    # Install the required packages\n",
    "    !pip install kagglehub pandas\n",
    "    !pip install -q transformers accelerate bitsandbytes sentencepiece pydantic huggingface_hub xformers\n",
    "else:\n",
    "    %pip install kagglehub pandas\n",
    "    %pip install -q transformers accelerate sentencepiece pydantic huggingface_hub xformers\n",
    "    #%pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu121\n",
    "    #%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "    %pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128\n",
    "    %pip install -U bitsandbytes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4a9f61",
   "metadata": {},
   "source": [
    "## Login to huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1235aef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# Set your token here securely or prompt for it in Colab\n",
    "# Recommended: store in Colab secrets or environment variable\n",
    "HF_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "\n",
    "if not HF_TOKEN:\n",
    "    if is_running_in_colab():\n",
    "        # If running in Colab, use the Colab secrets\n",
    "        try:\n",
    "            from google.colab import userdata\n",
    "            HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "            if not HF_TOKEN:\n",
    "                raise ValueError(\"‚ö†Ô∏è Hugging Face token not found in Colab secrets.\")\n",
    "            print(\"üîë Hugging Face token found in Colab secrets.\")\n",
    "        except ImportError:\n",
    "            print(\"‚ö†Ô∏è Unable to authenticate in Colab. Please set your Hugging Face token manually.\")\n",
    "    else:\n",
    "        # Prompt for token if not set in environment\n",
    "        print(\"üîë Please enter your Hugging Face token:\")\n",
    "        # For Colab or local prompt input\n",
    "        HF_TOKEN = input(\"üîë Enter your Hugging Face token: \").strip()\n",
    "\n",
    "login(token=HF_TOKEN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b423735e",
   "metadata": {},
   "source": [
    "## Setup Kaggle Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409818e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def setup_kaggle_credentials():\n",
    "    kaggle_path = os.path.expanduser('~/.kaggle/kaggle.json')\n",
    "    if not os.path.exists(kaggle_path):\n",
    "        from google.colab import files\n",
    "        print(\"üìÇ Upload kaggle.json file...\")\n",
    "        uploaded = files.upload()\n",
    "        os.makedirs(os.path.dirname(kaggle_path), exist_ok=True)\n",
    "        for filename in uploaded.keys():\n",
    "            shutil.move(filename, kaggle_path)\n",
    "        os.chmod(kaggle_path, 0o600)\n",
    "        print(f\"‚úÖ Kaggle credentials setup at {kaggle_path}\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Kaggle credentials already exist at {kaggle_path}\")\n",
    "\n",
    "setup_kaggle_credentials()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d384b2f",
   "metadata": {},
   "source": [
    "## Mount Google Drive (Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1517ea98",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_running_in_colab():\n",
    "   from google.colab import drive\n",
    "   drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9964af3",
   "metadata": {},
   "source": [
    "##  Load Nous-Hermes-mistral-Instruct with Fallback to Quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3882e7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "import torch\n",
    "\n",
    "def load_model_pipeline(model_name: str, hf_token: str):\n",
    "    has_cuda = torch.cuda.is_available()\n",
    "    free_mem = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3) if has_cuda else 0\n",
    "    print(f\"üíª CUDA: {has_cuda} | GPU Memory: {free_mem:.2f} GB\")\n",
    "\n",
    "    device_map = {\"\": 0} if has_cuda else \"cpu\"\n",
    "    use_4bit = has_cuda and free_mem < 24\n",
    "\n",
    "    # Set quantization config\n",
    "    quant_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True if use_4bit else False,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    ) if use_4bit else None\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token, trust_remote_code=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # ‚úÖ Fix warning about pad_token\n",
    "\n",
    "    # Load model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device_map,\n",
    "        quantization_config=quant_config,\n",
    "        torch_dtype=torch.float16 if not quant_config else None,\n",
    "        trust_remote_code=True,\n",
    "        token=hf_token\n",
    "    )\n",
    "\n",
    "    print(f\"‚úÖ Model loaded on {next(model.parameters()).device}\")\n",
    "    return pipeline(\"text-generation\", model=model, tokenizer=tokenizer, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d505348b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_pipeline = load_model_pipeline(\n",
    "    model_name=\"NousResearch/Nous-Hermes-2-Mistral-7B-DPO\",\n",
    "    hf_token=HF_TOKEN\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb849be",
   "metadata": {},
   "source": [
    "# Phase 1: First Steps Notebook ‚Äî Data Ingestion + Minimal Parsing\n",
    "1. Load Resume and JD datasets\n",
    "2. Minimal Parsing into JSON Structure\n",
    "3. Save structured JSON for Phase 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb756b8f",
   "metadata": {},
   "source": [
    "## Util Classes and methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43abc8ac",
   "metadata": {},
   "source": [
    "### Configurations  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e827a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# üõ† CONFIGURATION\n",
    "# ==============================\n",
    "\n",
    "class Config:\n",
    "    DATASET_DOWNLOAD_DIR = \"datasets\"\n",
    "    JSON_OUTPUT_DIR = \"json_outputs_phase1_run2\"\n",
    "    AUTO_CLEANUP = True\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a537a5",
   "metadata": {},
   "source": [
    "### Downloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89480102",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================\n",
    "# DOWNLOADER\n",
    "# ==============================\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "class DatasetDownloader:\n",
    "    @staticmethod\n",
    "    def download_and_extract(dataset_path: str) -> tuple[str, str]:\n",
    "        os.makedirs(Config.DATASET_DOWNLOAD_DIR, exist_ok=True)\n",
    "        dataset_slug = dataset_path.split(\"/\")[-1]\n",
    "        extract_folder_path = os.path.join(Config.DATASET_DOWNLOAD_DIR, dataset_slug)\n",
    "        zip_filename = f\"{dataset_slug}.zip\"\n",
    "        zip_path = os.path.join(Config.DATASET_DOWNLOAD_DIR, zip_filename)\n",
    "\n",
    "        if os.path.exists(extract_folder_path) and any(Path(extract_folder_path).rglob(\"*.csv\")):\n",
    "            print(f\"‚ö° Dataset folder already exists at '{extract_folder_path}', skipping download and extraction.\")\n",
    "            return extract_folder_path, zip_filename\n",
    "\n",
    "        print(f\"‚¨áÔ∏è Downloading dataset: {dataset_path} ...\")\n",
    "        !kaggle datasets download -d {dataset_path} -p {Config.DATASET_DOWNLOAD_DIR}\n",
    "\n",
    "        if not os.path.exists(zip_path):\n",
    "            raise FileNotFoundError(f\"‚ùå Zip file '{zip_filename}' not found after download!\")\n",
    "\n",
    "        os.makedirs(extract_folder_path, exist_ok=True)\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_folder_path)\n",
    "\n",
    "        print(f\"‚úÖ Downloaded and extracted to '{extract_folder_path}'.\")\n",
    "        return extract_folder_path, zip_filename\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1170df00",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c06119",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================\n",
    "# LOADER\n",
    "# ==============================\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class DatasetLoader:\n",
    "    @staticmethod\n",
    "    def load_csv(dataset_folder: str, target_csv_name: str) -> pd.DataFrame:\n",
    "        print(f\"üîç Searching for '{target_csv_name}' inside {dataset_folder}...\")\n",
    "        if not os.path.exists(dataset_folder):\n",
    "            raise FileNotFoundError(f\"‚ùå Dataset folder '{dataset_folder}' does not exist!\")\n",
    "\n",
    "        for root, _, files in os.walk(dataset_folder):\n",
    "            for file in files:\n",
    "                if file.lower() == target_csv_name.lower():\n",
    "                    csv_path = os.path.join(root, file)\n",
    "                    df = pd.read_csv(csv_path)\n",
    "                    print(f\"‚úÖ Loaded CSV with shape {df.shape}\")\n",
    "                    return df\n",
    "\n",
    "        raise FileNotFoundError(f\"‚ùå CSV file '{target_csv_name}' not found inside extracted dataset!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a80a5a",
   "metadata": {},
   "source": [
    "### Data Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08247c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================\n",
    "# PROCESSOR\n",
    "# ==============================\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class DatasetProcessor:\n",
    "    @staticmethod\n",
    "    def filter_fields(df: pd.DataFrame, allowed_fields: List[str]) -> pd.DataFrame:\n",
    "        missing_fields = [field for field in allowed_fields if field not in df.columns]\n",
    "        if missing_fields:\n",
    "            raise ValueError(f\"‚ùå Fields {missing_fields} not found in dataset!\")\n",
    "\n",
    "        filtered_df = df[allowed_fields]\n",
    "        print(f\"‚úÖ Filtered columns: {list(filtered_df.columns)}\")\n",
    "        return filtered_df\n",
    "\n",
    "    @staticmethod\n",
    "    def save_to_json(df: pd.DataFrame, output_json_name: str):\n",
    "        os.makedirs(Config.JSON_OUTPUT_DIR, exist_ok=True)\n",
    "        output_path = os.path.join(Config.JSON_OUTPUT_DIR, output_json_name)\n",
    "\n",
    "        if os.path.exists(output_path):\n",
    "            os.remove(output_path)\n",
    "            print(f\"üóëÔ∏è Existing JSON '{output_path}' deleted.\")\n",
    "\n",
    "        df.to_json(output_path, orient='records', lines=True, force_ascii=False)\n",
    "        print(f\"‚úÖ Data saved to JSON at '{output_path}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fc4fb6",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7245255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# CLEANER\n",
    "# ==============================\n",
    "class Cleaner:\n",
    "    @staticmethod\n",
    "    def cleanup_dataset_artifacts(extracted_folder_path: str, zip_filename: str):\n",
    "        if os.path.exists(extracted_folder_path):\n",
    "            shutil.rmtree(extracted_folder_path)\n",
    "            print(f\"üßπ Folder '{extracted_folder_path}' has been deleted successfully.\")\n",
    "\n",
    "        zip_path = os.path.join(Config.DATASET_DOWNLOAD_DIR, zip_filename)\n",
    "        if os.path.exists(zip_path):\n",
    "            os.remove(zip_path)\n",
    "            print(f\"üóëÔ∏è Zip file '{zip_path}' has been deleted successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a2525b",
   "metadata": {},
   "source": [
    "### Hybrid Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d85253",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================\n",
    "# HYBRID LOADER\n",
    "# ==============================\n",
    "try:\n",
    "    import kagglehub\n",
    "    from kagglehub import KaggleDatasetAdapter\n",
    "except ImportError:\n",
    "    kagglehub = None\n",
    "\n",
    "class HybridDatasetLoader:\n",
    "    @staticmethod\n",
    "    def load_dataset(dataset_path: str, file_name: str) -> pd.DataFrame:\n",
    "        if kagglehub:\n",
    "            try:\n",
    "                print(f\"üì• Trying KaggleHub for {dataset_path}...\")\n",
    "                df = kagglehub.dataset_load(KaggleDatasetAdapter.PANDAS, dataset_path, file_name)\n",
    "                print(f\"‚úÖ Loaded using KaggleHub: shape = {df.shape}\")\n",
    "                return df\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è KaggleHub failed: {e}\\nFalling back to ZIP-based loader.\")\n",
    "\n",
    "        extracted_folder, _ = DatasetDownloader.download_and_extract(dataset_path)\n",
    "        return DatasetLoader.load_csv(extracted_folder, file_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bace5dc8",
   "metadata": {},
   "source": [
    "### Infer JD Domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cfe6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_keywords_dict = {\n",
    "    'advocate': ['advocate'],\n",
    "    'agriculture': ['agriculture'],\n",
    "    'apparel': ['apparel'],\n",
    "    'arts': ['arts'],\n",
    "    'automobile': ['automobile'],\n",
    "    'aviation': ['aviation'],\n",
    "    'banking': ['banking'],\n",
    "    'bpo': ['bpo'],\n",
    "    'business development': ['business', 'development', 'business development', 'business-development'],\n",
    "    'chef': ['chef'],\n",
    "    'construction': ['construction'],\n",
    "    'consultant': ['consultant'],\n",
    "    'data scientist': ['data', 'data analyst', 'data scientist', 'scientist'],\n",
    "    'designing': ['designing', 'designer'],\n",
    "    'digital media': ['digital', 'digital marketing executive', 'media', 'digital media', 'digital-media'],\n",
    "    'engineering': ['engineering'],\n",
    "    'finance': ['finance', 'financial analyst'],\n",
    "    'healthcare': ['healthcare'],\n",
    "    'hr': ['hr'],\n",
    "    'information technology': ['information', 'technology', 'information technology', 'information-technology'],\n",
    "    'public relations': ['public', 'relations', 'public relations', 'public-relations'],\n",
    "    'marketing': ['marketing'],\n",
    "    'sales': ['sales', 'sales executive'],\n",
    "    'teacher': ['teacher'],\n",
    "    'technician': ['technician'],\n",
    "    'training': ['training'],\n",
    "    'web designing': ['web', 'designing'],\n",
    "    'fitness': ['fitness'],\n",
    "    'accountant': ['accountant', 'accounting']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f1837f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_domain_from_title(title):\n",
    "    title_lower = title.lower()\n",
    "    for domain, keywords in domain_keywords_dict.items():\n",
    "        if any(kw in title_lower for kw in keywords):\n",
    "            return domain\n",
    "    return \"unknown\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7419f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Efficient LLM Inference in Batches for JD Domains\n",
    "# ==============================\n",
    "from tqdm import tqdm\n",
    "\n",
    "def infer_domains_in_batches(texts, batch_size=8):\n",
    "    results = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"üß† Inferring JD domains\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        prompts = [\n",
    "            f\"Given this job description:\\n\\n{desc}\\n\\nWhat is the most likely job function or domain?\" for desc in batch\n",
    "        ]\n",
    "        try:\n",
    "            responses = llm_pipeline(prompts)\n",
    "            for r in responses:\n",
    "                results.append(r[0]['generated_text'].strip().split(\"\\n\")[-1])\n",
    "        except Exception:\n",
    "            results.extend([\"unknown\"] * len(batch))\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18dbb61",
   "metadata": {},
   "source": [
    "### Filter and Rank JDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e07d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# JD Filtering and Ranking (with batched domain inference)\n",
    "# ==============================\n",
    "def filter_and_rank_jds(jd_df, resume_domains, max_total=1000, top_n_per_domain=10):\n",
    "    # Ensure necessary columns exist\n",
    "    for col in ['title', 'description']:\n",
    "        if col not in jd_df.columns:\n",
    "            raise ValueError(f\"‚ùå Column '{col}' not found in JD dataset\")\n",
    "        jd_df[col] = jd_df[col].fillna('').astype(str)\n",
    "\n",
    "    # Infer domains \n",
    "    print(\"üß† Inferring JD domains from title using keyword matcing...\")\n",
    "    #jd_df['inferred_domain'] = infer_domains_in_batches(jd_df['description'].tolist())\n",
    "    jd_df['inferred_domain'] = jd_df['title'].fillna(\"\").apply(infer_domain_from_title)\n",
    "\n",
    "\n",
    "    all_ranked = []\n",
    "\n",
    "    for domain in resume_domains:\n",
    "        matches = jd_df[\n",
    "            jd_df['title'].str.contains(domain, na=False, case=False) |\n",
    "            jd_df['inferred_domain'].str.contains(domain, na=False, case=False)\n",
    "        ].copy()\n",
    "\n",
    "        if matches.empty:\n",
    "            print(f\"‚ö†Ô∏è No JDs matched domain: '{domain}'\")\n",
    "            continue\n",
    "\n",
    "        matches['richness_score'] = matches['description'].str.len()\n",
    "        top = matches.sort_values(by='richness_score', ascending=False).head(top_n_per_domain)\n",
    "        all_ranked.append(top)\n",
    "\n",
    "    if not all_ranked:\n",
    "        raise ValueError(\"‚ùå No job descriptions matched any resume domains.\")\n",
    "\n",
    "    final_jds_df = pd.concat(all_ranked, ignore_index=True)\n",
    "    final_jds_df = final_jds_df.drop_duplicates().sort_values(by='richness_score', ascending=False).head(max_total)\n",
    "\n",
    "    print(f\"‚úÖ Filtered and ranked {len(final_jds_df)} job descriptions across {len(resume_domains)} domains.\")\n",
    "    return final_jds_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4ed1ea",
   "metadata": {},
   "source": [
    "### Load Resume and JD datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc2aa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Resume Dataset Loader (with caching)\n",
    "# ==============================\n",
    "def load_resume_dataset(dataset_path: str = \"snehaanbhawal/resume-dataset\", target_csv_name: str = \"Resume.csv\") -> pd.DataFrame:\n",
    "    if not hasattr(load_resume_dataset, \"_cache\"):\n",
    "        print(\"üì• Loading resume dataset for the first time...\")\n",
    "        load_resume_dataset._cache = HybridDatasetLoader.load_dataset(dataset_path, target_csv_name)\n",
    "    else:\n",
    "        print(\"‚úÖ Using cached resume dataset.\")\n",
    "    \n",
    "    return load_resume_dataset._cache\n",
    "\n",
    "# ==============================\n",
    "# Job Description Dataset Loader (with caching)\n",
    "# ==============================\n",
    "def load_job_description_dataset(dataset_path: str = \"arshkon/linkedin-job-postings\", target_csv_name: str = \"postings.csv\") -> pd.DataFrame:\n",
    "    if not hasattr(load_job_description_dataset, \"_cache\"):\n",
    "        print(\"üì• Loading job description dataset for the first time...\")\n",
    "        load_job_description_dataset._cache = HybridDatasetLoader.load_dataset(dataset_path, target_csv_name)\n",
    "    else:\n",
    "        print(\"‚úÖ Using cached job description dataset.\")\n",
    "    \n",
    "    return load_job_description_dataset._cache\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff68a05",
   "metadata": {},
   "source": [
    "### JD Dataset Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212dbc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# JD Processing Function\n",
    "# ==============================\n",
    "def process_dataset_jd(dataset_path: str, target_csv_name: str, allowed_fields: List[str], output_json_name: str):\n",
    "    jd_df = load_job_description_dataset(dataset_path, target_csv_name)\n",
    "    resume_df = load_resume_dataset()\n",
    "    resume_domains = resume_df['Category'].dropna().str.lower().unique().tolist()\n",
    "    ranked_jds_df = filter_and_rank_jds(jd_df, resume_domains)\n",
    "    filtered_df = DatasetProcessor.filter_fields(ranked_jds_df, allowed_fields)\n",
    "    DatasetProcessor.save_to_json(filtered_df, output_json_name)\n",
    "    \n",
    "    # cleanup dataset\n",
    "    dataset_slug = dataset_path.split(\"/\")[-1]\n",
    "    extracted_folder = os.path.join(Config.DATASET_DOWNLOAD_DIR, dataset_slug)\n",
    "    zip_filename = f\"{dataset_slug}.zip\"\n",
    "    if Config.AUTO_CLEANUP:\n",
    "        Cleaner.cleanup_dataset_artifacts(extracted_folder, zip_filename)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e917e9d6",
   "metadata": {},
   "source": [
    "### Resume Dataset Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ea4be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Resume Filtering (5 per category)\n",
    "# ==============================\n",
    "def filter_resumes_by_category(resume_df: pd.DataFrame, top_n: int = 5) -> pd.DataFrame:\n",
    "    if 'Category' not in resume_df.columns:\n",
    "        raise ValueError(\"‚ùå Resume dataset does not contain 'Category' column.\")\n",
    "\n",
    "    filtered_resumes = (\n",
    "        resume_df\n",
    "        .dropna(subset=['Category'])\n",
    "        .groupby('Category', group_keys=False)\n",
    "        .apply(lambda group: group.head(top_n))\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    print(f\"‚úÖ Filtered {len(filtered_resumes)} resumes (top {top_n} from each category).\")\n",
    "    return filtered_resumes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766a42bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================\n",
    "# MAIN FLOW\n",
    "# ==============================\n",
    "\n",
    "def process_dataset_resume(dataset_path: str, target_csv_name: str, allowed_fields: List[str], output_json_name: str):\n",
    "    df = load_resume_dataset(dataset_path, target_csv_name)\n",
    "    filtered_df = DatasetProcessor.filter_fields(df, allowed_fields)\n",
    "    DatasetProcessor.save_to_json(filtered_df, output_json_name)\n",
    "\n",
    "    dataset_slug = dataset_path.split(\"/\")[-1]\n",
    "    extracted_folder = os.path.join(Config.DATASET_DOWNLOAD_DIR, dataset_slug)\n",
    "    zip_filename = f\"{dataset_slug}.zip\"\n",
    "    if Config.AUTO_CLEANUP:\n",
    "        Cleaner.cleanup_dataset_artifacts(extracted_folder, zip_filename)\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Save Filtered Resumes\n",
    "# ==============================\n",
    "def process_and_save_filtered_resumes(dataset_path: str, target_csv_name: str, allowed_fields: List[str], output_json_name: str):\n",
    "    resume_df = load_resume_dataset(dataset_path, target_csv_name)\n",
    "    df = filter_resumes_by_category(resume_df)\n",
    "    filtered_df = DatasetProcessor.filter_fields(df, allowed_fields)\n",
    "    DatasetProcessor.save_to_json(filtered_df, output_json_name)\n",
    "    \n",
    "    dataset_slug = dataset_path.split(\"/\")[-1]\n",
    "    extracted_folder = os.path.join(Config.DATASET_DOWNLOAD_DIR, dataset_slug)\n",
    "    zip_filename = f\"{dataset_slug}.zip\"\n",
    "    if Config.AUTO_CLEANUP:\n",
    "        Cleaner.cleanup_dataset_artifacts(extracted_folder, zip_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60577088",
   "metadata": {},
   "source": [
    "## Login and do the processing of Resume and JD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491107cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_and_save_filtered_resumes(\n",
    "    dataset_path=\"snehaanbhawal/resume-dataset\",\n",
    "    target_csv_name=\"Resume.csv\",\n",
    "    allowed_fields=[\"Category\", \"Resume_str\"],\n",
    "    output_json_name=\"parsed_resumes.json\"\n",
    ")\n",
    "\n",
    "process_dataset_jd(\n",
    "    dataset_path=\"arshkon/linkedin-job-postings\",\n",
    "    target_csv_name=\"postings.csv\",\n",
    "    allowed_fields=[\"title\", \"company_name\", \"location\",  \"skills_desc\", \"job_id\" , \"formatted_experience_level\", \"formatted_work_type\", \"description\"], #\"description\",\n",
    "    output_json_name=\"parsed_jds.json\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bb5022",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Process Resume Dataset\n",
    "process_dataset_resume(\n",
    "    dataset_path=\"snehaanbhawal/resume-dataset\",\n",
    "    target_csv_name=\"Resume.csv\",\n",
    "    allowed_fields=[\"Category\", \"Resume_str\"],\n",
    "    output_json_name=\"parsed_resumes.json\"\n",
    ")\n",
    "\n",
    "\n",
    "# Process Job Postings Dataset\n",
    "process_dataset_jd(\n",
    "    dataset_path=\"arshkon/linkedin-job-postings\",\n",
    "    target_csv_name=\"postings.csv\",\n",
    "    allowed_fields=[\"title\", \"company_name\", \"location\",  \"skills_desc\", \"job_id\" , \"formatted_experience_level\", \"formatted_work_type\", \"description\"], #\"description\",\n",
    "    output_json_name=\"parsed_jds.json\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
