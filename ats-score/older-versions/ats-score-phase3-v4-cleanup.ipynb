{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f630d895",
   "metadata": {},
   "source": [
    "# Global setup and package installation used in most phases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969fcea6",
   "metadata": {},
   "source": [
    "## Colab + GPU Detection Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cf79fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def is_running_in_colab():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "def get_available_gpu_memory_gb():\n",
    "    try:\n",
    "        output = subprocess.check_output(\n",
    "            [\"nvidia-smi\", \"--query-gpu=memory.free\", \"--format=csv,nounits,noheader\"],\n",
    "            encoding=\"utf-8\"\n",
    "        )\n",
    "        free_mem_mb = int(output.strip().split(\"\\n\")[0])\n",
    "        return free_mem_mb / 1024\n",
    "    except Exception:\n",
    "        return 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66be54d4",
   "metadata": {},
   "source": [
    "## install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942fba10",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_running_in_colab():\n",
    "    # Install the required packages\n",
    "    !pip install kagglehub pandas\n",
    "    !pip install -q transformers accelerate bitsandbytes sentencepiece pydantic huggingface_hub xformers\n",
    "    !pip install regex json5\n",
    "    !pip install sentence-transformers scikit-learn\n",
    "\n",
    "else:\n",
    "    %pip install kagglehub pandas\n",
    "    %pip install -q transformers accelerate sentencepiece pydantic huggingface_hub xformers\n",
    "    #%pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu121\n",
    "    #%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "    %pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128\n",
    "    %pip install -U bitsandbytes\n",
    "    %pip install regex json5\n",
    "    %pip install sentence-transformers scikit-learn\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4a9f61",
   "metadata": {},
   "source": [
    "## Login to huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1235aef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# Set your token here securely or prompt for it in Colab\n",
    "# Recommended: store in Colab secrets or environment variable\n",
    "HF_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "\n",
    "if not HF_TOKEN:\n",
    "    if is_running_in_colab():\n",
    "        # If running in Colab, use the Colab secrets\n",
    "        try:\n",
    "            from google.colab import userdata\n",
    "            HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "            if not HF_TOKEN:\n",
    "                raise ValueError(\"‚ö†Ô∏è Hugging Face token not found in Colab secrets.\")\n",
    "            print(\"üîë Hugging Face token found in Colab secrets.\")\n",
    "        except ImportError:\n",
    "            print(\"‚ö†Ô∏è Unable to authenticate in Colab. Please set your Hugging Face token manually.\")\n",
    "    else:\n",
    "        # Prompt for token if not set in environment\n",
    "        print(\"üîë Please enter your Hugging Face token:\")\n",
    "        # For Colab or local prompt input\n",
    "        HF_TOKEN = input(\"üîë Enter your Hugging Face token: \").strip()\n",
    "\n",
    "login(token=HF_TOKEN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b423735e",
   "metadata": {},
   "source": [
    "## Setup Kaggle Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409818e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def setup_kaggle_credentials():\n",
    "    kaggle_path = os.path.expanduser('~/.kaggle/kaggle.json')\n",
    "    if not os.path.exists(kaggle_path):\n",
    "        from google.colab import files\n",
    "        print(\"üìÇ Upload kaggle.json file...\")\n",
    "        uploaded = files.upload()\n",
    "        os.makedirs(os.path.dirname(kaggle_path), exist_ok=True)\n",
    "        for filename in uploaded.keys():\n",
    "            shutil.move(filename, kaggle_path)\n",
    "        os.chmod(kaggle_path, 0o600)\n",
    "        print(f\"‚úÖ Kaggle credentials setup at {kaggle_path}\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Kaggle credentials already exist at {kaggle_path}\")\n",
    "\n",
    "setup_kaggle_credentials()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d384b2f",
   "metadata": {},
   "source": [
    "## Mount Google Drive (Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1517ea98",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_running_in_colab():\n",
    "   from google.colab import drive\n",
    "   drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9964af3",
   "metadata": {},
   "source": [
    "##  Load Nous-Hermes-mistral-Instruct with Fallback to Quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3882e7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "import torch\n",
    "\n",
    "def load_model_pipeline(model_name: str, hf_token: str):\n",
    "    has_cuda = torch.cuda.is_available()\n",
    "    free_mem = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3) if has_cuda else 0\n",
    "    print(f\"üíª CUDA: {has_cuda} | GPU Memory: {free_mem:.2f} GB\")\n",
    "\n",
    "    device_map = {\"\": 0} if has_cuda else \"cpu\"\n",
    "    use_4bit = has_cuda and free_mem < 24\n",
    "\n",
    "    # Set quantization config\n",
    "    quant_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True if use_4bit else False,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    ) if use_4bit else None\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token, trust_remote_code=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # ‚úÖ Fix warning about pad_token\n",
    "\n",
    "    # Load model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device_map,\n",
    "        quantization_config=quant_config,\n",
    "        torch_dtype=torch.float16 if not quant_config else None,\n",
    "        trust_remote_code=True,\n",
    "        token=hf_token\n",
    "    )\n",
    "\n",
    "    print(f\"‚úÖ Model loaded on {next(model.parameters()).device}\")\n",
    "    return pipeline(\"text-generation\", model=model, tokenizer=tokenizer, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d505348b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_pipeline = load_model_pipeline(\n",
    "    model_name=\"NousResearch/Nous-Hermes-2-Mistral-7B-DPO\",\n",
    "    hf_token=HF_TOKEN\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739b4783",
   "metadata": {},
   "source": [
    "# Global utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10779c22",
   "metadata": {},
   "source": [
    "### Utility to merge json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a472fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "def merge_json_files(\n",
    "    source_dir: Path,\n",
    "    output_file: Path,\n",
    "    pattern: str,\n",
    "    merged_dir: Path\n",
    "):\n",
    "    source_dir.mkdir(parents=True, exist_ok=True)\n",
    "    merged_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    merged_data = []\n",
    "\n",
    "    # Load existing output if it exists\n",
    "    if output_file.exists():\n",
    "        with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                merged_data = json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"‚ö†Ô∏è Could not decode {output_file}, starting from scratch.\")\n",
    "\n",
    "    # Identify matching files\n",
    "    files_to_merge = sorted(source_dir.glob(pattern))\n",
    "\n",
    "    for file_path in files_to_merge:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "                if isinstance(data, list):\n",
    "                    merged_data.extend(data)\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è Skipping {file_path.name}: not a list.\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Failed to parse {file_path.name}: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Move to merged folder\n",
    "        shutil.move(str(file_path), merged_dir / file_path.name)\n",
    "        print(f\"‚úÖ Merged and moved: {file_path.name}\")\n",
    "\n",
    "    # Write combined output\n",
    "    if merged_data:\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(merged_data, f, indent=2)\n",
    "        print(f\"üíæ Saved to: {output_file}\")\n",
    "    else:\n",
    "        print(\"üì≠ No valid data to merge.\")\n",
    "\n",
    "# === Usage ===\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63df9fc3",
   "metadata": {},
   "source": [
    "### Utility to save json to a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727b44eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "# üì¶ Save JSON Output with Safety\n",
    "def save_json_output(data, output_path: str, indent: int = 4, overwrite: bool = True):\n",
    "    output_dir = os.path.dirname(output_path)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    if os.path.exists(output_path):\n",
    "        if overwrite:\n",
    "            os.remove(output_path)\n",
    "        else:\n",
    "            raise FileExistsError(f\"File {output_path} already exists and overwrite=False.\")\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=indent, ensure_ascii=False)\n",
    "\n",
    "    print(f\"‚úÖ Saved output to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc498258",
   "metadata": {},
   "source": [
    "### Utility to load file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf31583",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import json\n",
    "\n",
    "# üìÇ Load normalized JSON data\n",
    "def load_json_file(file_path: str) -> Any:\n",
    "    if not os.path.isfile(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1447c72a",
   "metadata": {},
   "source": [
    "### truncate text util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15df20b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_text(text: str, max_chars=1500) -> str:\n",
    "    \"\"\"Trims long resumes/JDs to prevent LLM overload.\"\"\"\n",
    "    return text.strip()[:max_chars]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b329b93",
   "metadata": {},
   "source": [
    "### Configurations  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4308e95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# üõ† CONFIGURATION\n",
    "# ==============================\n",
    "\n",
    "class Config:\n",
    "    DATASET_DOWNLOAD_DIR = \"datasets\"\n",
    "    JSON_OUTPUT_DIR = \"json_outputs_run1\"\n",
    "    JSON_OUTPUT_NORMALIZED_DIR = \"json_outputs_run1/normalized\"\n",
    "    JSON_OUTPUT_SCORING_DIR = \"json_outputs_run1/scoring\"\n",
    "    AUTO_CLEANUP = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e89760a",
   "metadata": {},
   "source": [
    "# Phase 3 Rubric-Based Scoring Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e31b1a",
   "metadata": {},
   "source": [
    "## Rule-Based Scoring Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a195e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "\n",
    "def rule_based_scoring(resume: Dict, jd: Dict) -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Rule-based scoring for each section. Each section returns:\n",
    "    {\n",
    "        \"score\": float (0.0 to 1.0),\n",
    "        \"details\": str\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    def skills_match():\n",
    "        r = set(resume.get(\"skills\", []))\n",
    "        j = set(jd.get(\"required_skills\", []))\n",
    "        score = len(r & j) / len(j) if j else 1.0\n",
    "        return {\"score\": round(score, 2), \"details\": f\"Matched skills: {list(r & j)}\"}\n",
    "\n",
    "    def experience_alignment():\n",
    "        r = resume.get(\"total_experience_years\", 0.0)\n",
    "        j = jd.get(\"required_experience_years\", 0.0)\n",
    "        score = min(r / j, 1.0) if j else 1.0\n",
    "        return {\"score\": round(score, 2), \"details\": f\"{r} years vs required {j}\"}\n",
    "\n",
    "    def education_alignment():\n",
    "        resume_degrees = [edu[\"degree\"].lower() for edu in resume.get(\"education\", [])]\n",
    "        jd_degrees = [deg.lower() for deg in jd.get(\"preferred_degrees\", [])]\n",
    "        match = [deg for deg in jd_degrees if any(deg in r for r in resume_degrees)]\n",
    "        score = len(match) / len(jd_degrees) if jd_degrees else 1.0\n",
    "        return {\"score\": round(score, 2), \"details\": f\"Matched degrees: {match}\"}\n",
    "\n",
    "    def certs_projects():\n",
    "        certs_r = set(resume.get(\"certifications\", []))\n",
    "        certs_j = set(jd.get(\"certifications\", []))\n",
    "        projects = resume.get(\"projects\", [])\n",
    "        has_projects = len(projects) > 0\n",
    "\n",
    "        cert_score = len(certs_r & certs_j) / len(certs_j) if certs_j else 1.0\n",
    "        project_bonus = 0.1 if has_projects else 0.0\n",
    "        final_score = min(cert_score + project_bonus, 1.0)\n",
    "\n",
    "        details = f\"Certs matched: {list(certs_r & certs_j)}; Projects: {'Yes' if has_projects else 'No'}\"\n",
    "        return {\"score\": round(final_score, 2), \"details\": details}\n",
    "\n",
    "    return {\n",
    "        \"skills_match\": skills_match(),\n",
    "        \"experience_alignment\": experience_alignment(),\n",
    "        \"education_alignment\": education_alignment(),\n",
    "        \"certs_projects\": certs_projects()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b948ff22",
   "metadata": {},
   "source": [
    "## LLM-Based Scoring Functions (Structured Prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3f753b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_text_list(field):\n",
    "    \"\"\"Flattens a string or list of strings into a clean single string.\"\"\"\n",
    "    if isinstance(field, str):\n",
    "        return field\n",
    "    elif isinstance(field, list):\n",
    "        return \" \".join(str(f) for f in field if isinstance(f, str))\n",
    "    return str(field)\n",
    "\n",
    "def format_experience_descriptions(resume: Dict) -> str:\n",
    "    descs = []\n",
    "    for e in resume.get(\"experience\", []):\n",
    "        desc = flatten_text_list(e.get(\"description\", \"\"))\n",
    "        descs.append(desc)\n",
    "    return \"; \".join(descs)\n",
    "\n",
    "def format_projects(resume: Dict) -> str:\n",
    "    return \"; \".join(flatten_text_list(p) for p in resume.get(\"projects\", []))\n",
    "\n",
    "def format_education(resume: Dict) -> str:\n",
    "    return \"; \".join(f\"{edu.get('degree', '')} from {edu.get('institution', '')}\" for edu in resume.get(\"education\", []))\n",
    "\n",
    "def format_certifications(resume: Dict) -> str:\n",
    "    return \", \".join(flatten_text_list(c) for c in resume.get(\"certifications\", []))\n",
    "\n",
    "def create_llm_prompt(resume: Dict, jd: Dict) -> str:\n",
    "    resume_summary = f\"\"\"\n",
    "Resume:\n",
    "- Title: {resume.get(\"basics\", {}).get(\"current_title\", \"\")}\n",
    "- Skills: {', '.join(resume.get(\"skills\", []))}\n",
    "- Certifications: {format_certifications(resume)}\n",
    "- Projects: {format_projects(resume)}\n",
    "- Experience: {format_experience_descriptions(resume)}\n",
    "- Education: {format_education(resume)}\n",
    "\"\"\"\n",
    "\n",
    "    jd_summary = f\"\"\"\n",
    "Job Description:\n",
    "- Title: {jd.get(\"title\", \"\")}\n",
    "- Required Skills: {', '.join(jd.get(\"required_skills\", []))}\n",
    "- Required Certifications: {', '.join(jd.get(\"certifications\", []))}\n",
    "- Preferred Degrees: {', '.join(jd.get(\"preferred_degrees\", []))}\n",
    "- Soft Skills Required: {', '.join(jd.get(\"soft_skills\", []))}\n",
    "\"\"\"\n",
    "\n",
    "    return f\"\"\"\n",
    "You are an ATS scoring expert.\n",
    "\n",
    "Evaluate the following resume against the job description.\n",
    "\n",
    "Respond ONLY with valid JSON using this exact structure:\n",
    "\n",
    "{{\n",
    "  \"skills_match\": {{\n",
    "    \"score\": float between 0 and 1,\n",
    "    \"details\": \"reasoning\"\n",
    "  }},\n",
    "  \"experience_alignment\": {{\n",
    "    \"score\": float between 0 and 1,\n",
    "    \"details\": \"reasoning\"\n",
    "  }},\n",
    "  \"education_alignment\": {{\n",
    "    \"score\": float between 0 and 1,\n",
    "    \"details\": \"reasoning\"\n",
    "  }},\n",
    "  \"certs_projects\": {{\n",
    "    \"score\": float between 0 and 1,\n",
    "    \"details\": \"reasoning\"\n",
    "  }},\n",
    "  \"soft_skills\": {{\n",
    "    \"score\": float between 0 and 1,\n",
    "    \"details\": \"reasoning\"\n",
    "  }},\n",
    "  \"transferable_skills\": {{\n",
    "    \"score\": float between 0 and 1,\n",
    "    \"details\": \"reasoning\"\n",
    "  }},\n",
    "  \"grammar_and_cleanliness\": {{\n",
    "    \"score\": float between 0 and 1,\n",
    "    \"details\": \"reasoning\"\n",
    "  }},\n",
    "  \"leadership\": {{\n",
    "    \"score\": float between 0 and 1,\n",
    "    \"details\": \"only if leadership inferred or requested by JD\"\n",
    "  }}\n",
    "}}\n",
    "\n",
    "{resume_summary}\n",
    "{jd_summary}\n",
    "\n",
    "Assistant:\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef9a5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_last_json_block_with_regex(text: str) -> Dict:\n",
    "    import regex\n",
    "    import json5\n",
    "    text = text.replace(\"‚Äú\", \"\\\"\").replace(\"‚Äù\", \"\\\"\").replace(\"‚Äò\", \"'\").replace(\"‚Äô\", \"'\")\n",
    "    matches = regex.findall(r\"\\{(?:[^{}]|(?R))*\\}\", text, flags=regex.DOTALL)\n",
    "\n",
    "    for block in reversed(matches):\n",
    "        try:\n",
    "            parsed = json5.loads(block)\n",
    "            if \"skills_match\" in parsed:\n",
    "                return parsed\n",
    "        except Exception:\n",
    "            continue\n",
    "    raise ValueError(\"No valid JSON block found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b9943b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_based_scoring(resume, jd, resume_id=\"resume\", jd_id=\"jd\", retries=2):\n",
    "    prompt = create_llm_prompt(resume, jd)\n",
    "\n",
    "    for attempt in range(retries + 1):\n",
    "        try:\n",
    "            response = llm_pipeline(prompt, max_new_tokens=512)[0]['generated_text']\n",
    "            parsed = extract_last_json_block_with_regex(response)\n",
    "            return parsed\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è LLM attempt {attempt + 1} failed for {resume_id} x {jd_id}: {e}\")\n",
    "            print(\"üß™ Raw output preview:\\n\", response[:512])\n",
    "\n",
    "    # Fallback structure\n",
    "    return {\n",
    "        section: {\"score\": 0.5, \"details\": \"LLM fallback\"}\n",
    "        for section in [\n",
    "            \"skills_match\", \"experience_alignment\", \"education_alignment\", \"certs_projects\",\n",
    "            \"soft_skills\", \"transferable_skills\", \"grammar_and_cleanliness\", \"leadership\"\n",
    "        ]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e71a89",
   "metadata": {},
   "source": [
    "## Combine Section Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d277dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_section_scores(\n",
    "    rule_scores: Dict[str, Dict],\n",
    "    llm_scores: Dict[str, Dict],\n",
    "    rule_weight: float = 0.5,\n",
    "    llm_weight: float = 0.5\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Merge rule-based and LLM-based section scores using weighted average.\n",
    "    Each section includes:\n",
    "    - score (merged)\n",
    "    - details: { \"rule\": ..., \"llm\": ... }\n",
    "    Returns:\n",
    "    {\n",
    "      \"final_section_scores\": {\n",
    "         section_name: {\n",
    "            \"score\": float,\n",
    "            \"details\": { \"rule\": str, \"llm\": str }\n",
    "         }\n",
    "      },\n",
    "      \"final_ats_score\": float,\n",
    "      \"weights_used\": { section: { \"rule\": float, \"llm\": float } }\n",
    "    }\n",
    "    \"\"\"\n",
    "    merged_scores = {}\n",
    "    section_weights_used = {}\n",
    "    all_sections = set(rule_scores.keys()) | set(llm_scores.keys())\n",
    "\n",
    "    for section in all_sections:\n",
    "        rule_score = rule_scores.get(section, {}).get(\"score\")\n",
    "        rule_details = rule_scores.get(section, {}).get(\"details\")\n",
    "        llm_score = llm_scores.get(section, {}).get(\"score\")\n",
    "        llm_details = llm_scores.get(section, {}).get(\"details\")\n",
    "\n",
    "        if rule_score is not None and llm_score is not None:\n",
    "            score = (rule_score * rule_weight) + (llm_score * llm_weight)\n",
    "            weight_info = {\"rule\": rule_weight, \"llm\": llm_weight}\n",
    "        elif rule_score is not None:\n",
    "            score = rule_score\n",
    "            weight_info = {\"rule\": 1.0, \"llm\": 0.0}\n",
    "        elif llm_score is not None:\n",
    "            score = llm_score\n",
    "            weight_info = {\"rule\": 0.0, \"llm\": 1.0}\n",
    "        else:\n",
    "            score = 0.0\n",
    "            weight_info = {\"rule\": 0.0, \"llm\": 0.0}\n",
    "\n",
    "        merged_scores[section] = {\n",
    "            \"score\": round(score, 4),\n",
    "            \"details\": {\n",
    "                \"rule\": rule_details or \"N/A\",\n",
    "                \"llm\": llm_details or \"N/A\"\n",
    "            }\n",
    "        }\n",
    "        section_weights_used[section] = weight_info\n",
    "\n",
    "    final_ats_score = round(\n",
    "        sum(sec[\"score\"] for sec in merged_scores.values()) / len(merged_scores),\n",
    "        4\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"final_section_scores\": merged_scores,\n",
    "        \"final_ats_score\": final_ats_score,\n",
    "        \"weights_used\": section_weights_used\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b465428d",
   "metadata": {},
   "source": [
    "## Test phase 3 - scoring logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dd2aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìÇ Mini Test Resumes\n",
    "test_resumes = [\n",
    "    {\n",
    "        \"resume_id\": \"61e6bdda-548c-4d24-87f3-5d97fdef032b\",\n",
    "        \"basics\": {\n",
    "            \"name\": \"Alice Smith\",\n",
    "            \"email\": \"alice@example.com\",\n",
    "            \"phone\": \"123-456-7890\",\n",
    "            \"location\": \"New York, NY\",\n",
    "            \"current_title\": \"Software Engineer\",\n",
    "            \"linkedin_url\": \"\"\n",
    "        },\n",
    "        \"education\": [\n",
    "            {\"degree\": \"B.Sc. Computer Science\", \"field\": \"Computer Science\", \"institution\": \"NYU\", \"year\": \"2018\", \"gpa\": \"3.7\"}\n",
    "        ],\n",
    "        \"experience\": [\n",
    "            {\"job_title\": \"Software Developer\", \"company\": \"ABC Corp\", \"start_date\": \"06/2018\", \"end_date\": \"08/2021\", \"duration_in_months\": 38, \"description\": \"Developed web applications.\"}\n",
    "        ],\n",
    "        \"skills\": [\"Python\", \"Django\", \"SQL\"],\n",
    "        \"certifications\": [\"AWS Certified Developer\"],\n",
    "        \"projects\": [\"E-commerce platform\"],\n",
    "        \"languages\": [\"English\"],\n",
    "        \"total_experience_years\": 3.2\n",
    "    },\n",
    "    {\n",
    "        \"resume_id\": \"f14f29c5-8ed9-493a-975d-c210655ff0aa\",\n",
    "        \"basics\": {\n",
    "            \"name\": \"Bob Johnson\",\n",
    "            \"email\": \"bob@example.com\",\n",
    "            \"phone\": \"987-654-3210\",\n",
    "            \"location\": \"San Francisco, CA\",\n",
    "            \"current_title\": \"Data Analyst\",\n",
    "            \"linkedin_url\": \"\"\n",
    "        },\n",
    "        \"education\": [\n",
    "            {\"degree\": \"B.A. Statistics\", \"field\": \"Statistics\", \"institution\": \"UCLA\", \"year\": \"2017\", \"gpa\": \"3.5\"}\n",
    "        ],\n",
    "        \"experience\": [\n",
    "            {\"job_title\": \"Data Analyst\", \"company\": \"XYZ Inc\", \"start_date\": \"01/2018\", \"end_date\": \"12/2020\", \"duration_in_months\": 36, \"description\": \"Analyzed data trends.\"}\n",
    "        ],\n",
    "        \"skills\": [\"SQL\", \"Tableau\", \"Python\"],\n",
    "        \"certifications\": [],\n",
    "        \"projects\": [\"Sales analytics dashboard\"],\n",
    "        \"languages\": [\"English\"],\n",
    "        \"total_experience_years\": 3.0\n",
    "    }\n",
    "]\n",
    "\n",
    "# üìÇ Mini Test JDs\n",
    "test_jds = [\n",
    "    {\n",
    "        \"jd_id\": \"9a62f845-94f2-40fe-a63b-e6f5cbd765c5\",\n",
    "        \"title\": \"Backend Engineer\",\n",
    "        \"summary\": \"Looking for a backend engineer with 3+ years experience in Python and SQL. AWS certification preferred.\",\n",
    "        \"required_experience_years\": 3.0,\n",
    "        \"preferred_degrees\": [\"B.Sc. Computer Science\"],\n",
    "        \"required_skills\": [\"Python\", \"SQL\"],\n",
    "        \"optional_skills\": [\"Django\"],\n",
    "        \"certifications\": [\"AWS Certified Developer\"],\n",
    "        \"soft_skills\": [\"Teamwork\", \"Communication\"],\n",
    "        \"job_location\": \"New York, NY\",\n",
    "        \"remote_option\": True,\n",
    "        \"employment_type\": \"Full-time\",\n",
    "        \"inferred_domain\": \"engineering\"\n",
    "    },\n",
    "    {\n",
    "        \"jd_id\": \"c890e8d6-9f04-429b-9a27-c4f5fcb59ce5\",\n",
    "        \"title\": \"Business Data Analyst\",\n",
    "        \"summary\": \"Seeking a Data Analyst with 2+ years experience in SQL, Excel, and data visualization tools.\",\n",
    "        \"required_experience_years\": 2.0,\n",
    "        \"preferred_degrees\": [\"B.A. Statistics\"],\n",
    "        \"required_skills\": [\"SQL\", \"Excel\"],\n",
    "        \"optional_skills\": [\"Tableau\"],\n",
    "        \"certifications\": [],\n",
    "        \"soft_skills\": [\"Analytical thinking\", \"Attention to detail\"],\n",
    "        \"job_location\": \"San Francisco, CA\",\n",
    "        \"remote_option\": False,\n",
    "        \"employment_type\": \"Full-time\",\n",
    "        \"inferred_domain\": \"technology\"\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb31be41",
   "metadata": {},
   "source": [
    "#### Scoring Loop (Test Batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6ee026",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "# Replace with your test resumes and JDs\n",
    "#test_resumes = resumes[:2]  # or load from a separate test dataset\n",
    "#test_jds = jds[:2]\n",
    "\n",
    "test_results = []\n",
    "\n",
    "for resume_idx, resume in tqdm(enumerate(test_resumes), desc=\"Scoring Test Batch\", total=len(test_resumes)):\n",
    "    resume_id = resume.get(\"resume_id\", f\"resume_{resume_idx}\")\n",
    "    \n",
    "    for jd_idx, jd in enumerate(test_jds):\n",
    "        jd_id = jd.get(\"jd_id\", f\"jd_{jd_idx}\")\n",
    "\n",
    "        # Run both scoring engines\n",
    "        rule_scores = rule_based_scoring(resume, jd)\n",
    "        llm_scores = llm_based_scoring(resume, jd, resume_id, jd_id)\n",
    "\n",
    "        # Merge section scores and compute final ATS score\n",
    "        merged_result = merge_section_scores(rule_scores, llm_scores, rule_weight=0.5, llm_weight=0.5)\n",
    "\n",
    "        test_results.append({\n",
    "            \"resume_id\": resume_id,\n",
    "            \"job_id\": jd_id,\n",
    "            \"rule_based\": rule_scores,\n",
    "            \"llm_based\": llm_scores,\n",
    "            \"final_section_scores\": merged_result[\"final_section_scores\"],\n",
    "            \"final_ats_score\": merged_result[\"final_ats_score\"],\n",
    "            \"weights_used\": merged_result[\"weights_used\"],\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"model_used\": \"Nous-Hermes-2-Mistral-7B-DPO\"\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1420d4eb",
   "metadata": {},
   "source": [
    "#### Save Mini Test Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4ba4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üíæ Save Mini Test Output\n",
    "import os\n",
    "test_file = os.path.join(Config.JSON_OUTPUT_SCORING_DIR, 'test_phase3_hybrid_parallel_scoring.json')\n",
    "\n",
    "save_json_output(test_results, test_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1515d8d8",
   "metadata": {},
   "source": [
    "## Embedding-Based Relevance Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aec5ca0",
   "metadata": {},
   "source": [
    "### Text Construction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9c2e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resume_to_text(resume: Dict) -> str:\n",
    "    def safe_join(items):\n",
    "        return \", \".join(str(i) for i in items if i)\n",
    "\n",
    "    def extract_certifications(cert_list):\n",
    "        return [\n",
    "            c.get(\"name\", \"\") if isinstance(c, dict) else str(c)\n",
    "            for c in cert_list\n",
    "        ]\n",
    "\n",
    "    def extract_education(edus):\n",
    "        return [\n",
    "            f\"{e.get('degree', '')} from {e.get('institution', '')} ({e.get('year', '')})\"\n",
    "            for e in edus if isinstance(e, dict)\n",
    "        ]\n",
    "\n",
    "    def extract_experience(exps):\n",
    "        results = []\n",
    "        for exp in exps:\n",
    "            parts = [\n",
    "                exp.get(\"title\", \"\"),\n",
    "                exp.get(\"company\", \"\"),\n",
    "                exp.get(\"location\", \"\"),\n",
    "                f\"{exp.get('start_date', '')} to {exp.get('end_date', '')}\"\n",
    "            ]\n",
    "            results.append(\" | \".join(filter(None, parts)))\n",
    "        return results\n",
    "\n",
    "    basics = resume.get(\"basics\", {})\n",
    "    title = basics.get(\"current_title\", \"\")\n",
    "    location = basics.get(\"location\", \"\")\n",
    "\n",
    "    skills = safe_join(resume.get(\"skills\", []))\n",
    "    certs = safe_join(extract_certifications(resume.get(\"certifications\", [])))\n",
    "    education = safe_join(extract_education(resume.get(\"education\", [])))\n",
    "    experience = safe_join(extract_experience(resume.get(\"experience\", [])))\n",
    "    projects = safe_join(resume.get(\"projects\", []))\n",
    "    languages = safe_join(resume.get(\"languages\", []))\n",
    "\n",
    "    summary = f\"\"\"\n",
    "Title: {title}\n",
    "Location: {location}\n",
    "Skills: {skills}\n",
    "Certifications: {certs}\n",
    "Education: {education}\n",
    "Experience: {experience}\n",
    "Projects: {projects}\n",
    "Languages: {languages}\n",
    "\"\"\"\n",
    "    return summary.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5095a12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jd_to_text(jd: Dict) -> str:\n",
    "    def safe_join(items):\n",
    "        return \", \".join(str(i) for i in items if i)\n",
    "\n",
    "    title = jd.get(\"title\", \"\")\n",
    "    summary = jd.get(\"summary\", jd.get(\"description\", \"\"))\n",
    "    required_skills = safe_join(jd.get(\"required_skills\", []))\n",
    "    optional_skills = safe_join(jd.get(\"optional_skills\", []))\n",
    "    soft_skills = safe_join(jd.get(\"soft_skills\", []))\n",
    "    certifications = safe_join(jd.get(\"certifications\", []))\n",
    "    degrees = safe_join(jd.get(\"preferred_degrees\", []))\n",
    "    domain = jd.get(\"inferred_domain\", \"unknown\")\n",
    "    location = jd.get(\"job_location\", \"\")\n",
    "    emp_type = jd.get(\"employment_type\", \"\")\n",
    "    remote = \"Remote\" if jd.get(\"remote_option\") else \"Onsite\"\n",
    "\n",
    "    jd_text = f\"\"\"\n",
    "Title: {title}\n",
    "Domain: {domain}\n",
    "Location: {location} ({remote})\n",
    "Employment Type: {emp_type}\n",
    "Summary: {summary}\n",
    "Required Skills: {required_skills}\n",
    "Optional Skills: {optional_skills}\n",
    "Soft Skills: {soft_skills}\n",
    "Preferred Degrees: {degrees}\n",
    "Certifications: {certifications}\n",
    "\"\"\"\n",
    "    return jd_text.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b0b182",
   "metadata": {},
   "source": [
    "### Embedding + Relevance Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10b7fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def generate_semantic_relevance_map(\n",
    "    resumes: List[Dict],\n",
    "    jds: List[Dict],\n",
    "    top_n: int = 10,\n",
    "    model_name: str = \"all-MiniLM-L6-v2\"\n",
    ") -> Dict[str, List[Dict]]:\n",
    "    \"\"\"\n",
    "    Generates a relevance map using cosine similarity of sentence embeddings\n",
    "    between each resume and all job descriptions.\n",
    "\n",
    "    Returns:\n",
    "        Dict[resume_id, List[{jd_id, score}]]\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer(model_name)\n",
    "\n",
    "    print(\"üß† Encoding job descriptions...\")\n",
    "    jd_ids, jd_texts = [], []\n",
    "    for jd in jds:\n",
    "        try:\n",
    "            if not jd.get(\"jd_id\"):\n",
    "                continue\n",
    "            text = jd_to_text(jd)\n",
    "            if text.strip():\n",
    "                jd_ids.append(jd[\"jd_id\"])\n",
    "                jd_texts.append(text)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Skipping JD due to error: {e}\")\n",
    "            continue\n",
    "\n",
    "    jd_embeddings = model.encode(jd_texts, show_progress_bar=True, batch_size=32)\n",
    "\n",
    "    relevance_map = {}\n",
    "\n",
    "    print(\"üìÑ Encoding resumes and computing similarities...\")\n",
    "    for resume in tqdm(resumes, desc=\"Generating semantic relevance map\"):\n",
    "        try:\n",
    "            resume_id = resume.get(\"resume_id\")\n",
    "            if not resume_id:\n",
    "                continue\n",
    "\n",
    "            resume_text = resume_to_text(resume)\n",
    "            if not resume_text.strip():\n",
    "                continue\n",
    "\n",
    "            resume_emb = model.encode([resume_text])[0]\n",
    "            sim_scores = cosine_similarity([resume_emb], jd_embeddings)[0]\n",
    "            top_indices = np.argsort(sim_scores)[::-1][:top_n]\n",
    "\n",
    "            top_matches = [\n",
    "                {\"jd_id\": jd_ids[i], \"score\": round(float(sim_scores[i]), 4)}\n",
    "                for i in top_indices\n",
    "            ]\n",
    "\n",
    "            relevance_map[resume_id] = top_matches\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error processing resume {resume.get('resume_id', 'unknown')}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return relevance_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7b0994",
   "metadata": {},
   "source": [
    "## Phase 3: Scoring Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1d129a",
   "metadata": {},
   "source": [
    "### Checkpoint Handling (JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef277819",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "\n",
    "def load_resume_checkpoint(path: str) -> int:\n",
    "    if not os.path.exists(path):\n",
    "        return 0\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return json.load(file).get(\"last_index\", 0)\n",
    "\n",
    "def save_resume_checkpoint(path: str, index: int):\n",
    "    data = {\n",
    "        \"last_index\": index,\n",
    "        \"timestamp\": datetime.now(timezone.utc).isoformat()\n",
    "    }\n",
    "    save_json_output(data, path)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae98c44",
   "metadata": {},
   "source": [
    "### Get Relevant JDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec779031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_jds(\n",
    "    resume_id: str,\n",
    "    relevance_map: Dict[str, List[Dict]],\n",
    "    jd_lookup: Dict[str, Dict],\n",
    "    threshold: float = 0.5\n",
    ") -> List[Dict]:\n",
    "    matches = relevance_map.get(resume_id, [])\n",
    "    return [\n",
    "        jd_lookup[m[\"jd_id\"]]\n",
    "        for m in matches\n",
    "        if m[\"score\"] >= threshold and m[\"jd_id\"] in jd_lookup\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5c063c",
   "metadata": {},
   "source": [
    "### Scoring a Single Resume-JD Pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1bfb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_resume_jd_pair(\n",
    "    resume: Dict,\n",
    "    jd: Dict,\n",
    "    resume_id: str,\n",
    "    jd_id: str,\n",
    "    rule_weight: float,\n",
    "    llm_weight: float,\n",
    "    relevance_score: float,\n",
    ") -> Dict:\n",
    "    rule_scores = rule_based_scoring(resume, jd)\n",
    "    llm_scores = llm_based_scoring(resume, jd, resume_id, jd_id)\n",
    "    merged = merge_section_scores(rule_scores, llm_scores, rule_weight, llm_weight)\n",
    "\n",
    "    merged.update({\n",
    "        \"resume_id\": resume_id,\n",
    "        \"job_id\": jd_id,\n",
    "        \"timestamp\": datetime.now(timezone.utc).isoformat(),\n",
    "        \"inferred_domain\": jd.get(\"inferred_domain\", \"unknown\"),\n",
    "        \"relevance_score\": relevance_score,\n",
    "        \"match_quality\": (\n",
    "            \"strong\" if merged[\"final_ats_score\"] >= 0.75 else\n",
    "            \"medium\" if merged[\"final_ats_score\"] >= 0.5 else\n",
    "            \"weak\"\n",
    "        )\n",
    "    })\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d1fabc",
   "metadata": {},
   "source": [
    "### Main Modular Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b50913",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from typing import List, Dict\n",
    "import os\n",
    "import time\n",
    "\n",
    "def score_and_save_in_batches(\n",
    "    resumes: List[Dict],\n",
    "    jd_lookup: Dict[str, Dict],\n",
    "    relevance_map: Dict[str, List[Dict]],\n",
    "    output_dir: str = Config.JSON_OUTPUT_SCORING_DIR,\n",
    "    save_every: int = 5,\n",
    "    limit: int = 20,\n",
    "    relevance_threshold: float = 0.4,\n",
    "    rule_weight: float = 0.5,\n",
    "    llm_weight: float = 0.5,\n",
    "    resume_from_checkpoint: bool = True\n",
    "):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    checkpoint_file = os.path.join(output_dir, \"checkpoint.json\")\n",
    "\n",
    "    # Determine starting index\n",
    "    start = load_resume_checkpoint(checkpoint_file) if resume_from_checkpoint else 0\n",
    "    end = min(start + limit, len(resumes))\n",
    "\n",
    "    successes, failures = [], []\n",
    "    timestamp = int(time.time())\n",
    "\n",
    "    for idx in tqdm(range(start, end), desc=\"Scoring resumes\"):\n",
    "        resume = resumes[idx]\n",
    "        resume_id = resume.get(\"resume_id\", f\"resume_{idx}\")\n",
    " \n",
    "\n",
    "        relevant_jds = get_relevant_jds(resume_id, relevance_map, jd_lookup, threshold=relevance_threshold)\n",
    "        if not relevant_jds:\n",
    "            print(f\"‚ö†Ô∏è No relevant JDs found for {resume_id}\")\n",
    "            continue\n",
    "\n",
    "        for jd in relevant_jds:\n",
    "            jd_id = jd.get(\"jd_id\", \"\")\n",
    "            try:\n",
    "                relevance_score = next((m[\"score\"] for m in relevance_map[resume_id] if m[\"jd_id\"] == jd_id), 0.0)\n",
    "                result = score_resume_jd_pair(\n",
    "                    resume, jd, resume_id, jd_id,\n",
    "                    rule_weight, llm_weight,\n",
    "                    relevance_score\n",
    "                )\n",
    "                successes.append(result)\n",
    "            except Exception as e:\n",
    "                failures.append({\n",
    "                    \"resume_id\": resume_id,\n",
    "                    \"jd_id\": jd_id,\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "\n",
    "        if (idx - start + 1) % save_every == 0:\n",
    "            partial_success_file = f\"{output_dir}/scored_part_{start}_{idx}_{timestamp}.json\"\n",
    "            partial_fail_file = f\"{output_dir}/failed_part_{start}_{idx}_{timestamp}.json\"\n",
    "            if successes:\n",
    "                save_json_output(successes, partial_success_file)\n",
    "            if failures:\n",
    "                save_json_output(failures, partial_fail_file)\n",
    "            save_resume_checkpoint(checkpoint_file, idx + 1)\n",
    "\n",
    "    # Final save\n",
    "    success_file = f\"{output_dir}/scored_final_{start}_{end}_{timestamp}.json\"\n",
    "    fail_file = f\"{output_dir}/failed_final_{start}_{end}_{timestamp}.json\"\n",
    "    if successes:\n",
    "        save_json_output(successes, success_file)\n",
    "    if failures:\n",
    "        save_json_output(failures, fail_file)\n",
    "    save_resume_checkpoint(checkpoint_file, end)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa81952",
   "metadata": {},
   "source": [
    "## Load Normalized Resumes and JDs and relevance score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e79d4e",
   "metadata": {},
   "source": [
    "### Load Resumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e992097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "resumes_path = os.path.join(Config.JSON_OUTPUT_NORMALIZED_DIR, 'normalized_resumes.json')\n",
    "resumes = load_json_file(resumes_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2379d41f",
   "metadata": {},
   "source": [
    "### Load JDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3d0e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "jds_path = os.path.join(Config.JSON_OUTPUT_NORMALIZED_DIR, 'normalized_jds.json')\n",
    "jds = load_json_file(jds_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8555c133",
   "metadata": {},
   "source": [
    "### create and save relevance map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c247df62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "relevance_map = generate_semantic_relevance_map(resumes, jds, top_n=10)\n",
    "\n",
    "relevance_map_file = os.path.join(Config.JSON_OUTPUT_SCORING_DIR, 'semantic_relevance_scores.json')\n",
    "save_json_output(relevance_map, relevance_map_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f67dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loaded {len(resumes)} resumes and {len(jds)} job descriptions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4fd7f6",
   "metadata": {},
   "source": [
    "## Execute Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998fc6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "jd_lookup = {jd[\"jd_id\"]: jd for jd in jds}\n",
    "\n",
    "score_and_save_in_batches(\n",
    "    resumes=resumes,\n",
    "    jd_lookup=jd_lookup,\n",
    "    relevance_map=relevance_map,\n",
    "    output_dir=Config.JSON_OUTPUT_SCORING_DIR,\n",
    "    save_every=5,\n",
    "    limit=1,\n",
    "    relevance_threshold=0.45,\n",
    "    rule_weight=0.5,\n",
    "    llm_weight=0.5,\n",
    "    resume_from_checkpoint=True  # set True to resume, False to start fresh\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
