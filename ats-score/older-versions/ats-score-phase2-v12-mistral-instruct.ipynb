{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f630d895",
   "metadata": {},
   "source": [
    "# Global setup and package installation used in most phases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969fcea6",
   "metadata": {},
   "source": [
    "## Colab + GPU Detection Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cf79fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def is_running_in_colab():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "def get_available_gpu_memory_gb():\n",
    "    try:\n",
    "        output = subprocess.check_output(\n",
    "            [\"nvidia-smi\", \"--query-gpu=memory.free\", \"--format=csv,nounits,noheader\"],\n",
    "            encoding=\"utf-8\"\n",
    "        )\n",
    "        free_mem_mb = int(output.strip().split(\"\\n\")[0])\n",
    "        return free_mem_mb / 1024\n",
    "    except Exception:\n",
    "        return 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66be54d4",
   "metadata": {},
   "source": [
    "## install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942fba10",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_running_in_colab():\n",
    "    # Install the required packages\n",
    "    !pip install kagglehub pandas\n",
    "    !pip install -q transformers accelerate bitsandbytes sentencepiece pydantic huggingface_hub xformers\n",
    "    !pip install regex json5\n",
    "else:\n",
    "    %pip install kagglehub pandas\n",
    "    %pip install -q transformers accelerate sentencepiece pydantic huggingface_hub xformers\n",
    "    #%pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu121\n",
    "    #%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "    %pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128\n",
    "    %pip install -U bitsandbytes\n",
    "    %pip install regex json5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4a9f61",
   "metadata": {},
   "source": [
    "## Login to huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1235aef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# Set your token here securely or prompt for it in Colab\n",
    "# Recommended: store in Colab secrets or environment variable\n",
    "HF_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "\n",
    "if not HF_TOKEN:\n",
    "    if is_running_in_colab():\n",
    "        # If running in Colab, use the Colab secrets\n",
    "        try:\n",
    "            from google.colab import userdata\n",
    "            HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "            if not HF_TOKEN:\n",
    "                raise ValueError(\"‚ö†Ô∏è Hugging Face token not found in Colab secrets.\")\n",
    "            print(\"üîë Hugging Face token found in Colab secrets.\")\n",
    "        except ImportError:\n",
    "            print(\"‚ö†Ô∏è Unable to authenticate in Colab. Please set your Hugging Face token manually.\")\n",
    "    else:\n",
    "        # Prompt for token if not set in environment\n",
    "        print(\"üîë Please enter your Hugging Face token:\")\n",
    "        # For Colab or local prompt input\n",
    "        HF_TOKEN = input(\"üîë Enter your Hugging Face token: \").strip()\n",
    "\n",
    "login(token=HF_TOKEN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b423735e",
   "metadata": {},
   "source": [
    "## Setup Kaggle Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409818e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def setup_kaggle_credentials():\n",
    "    kaggle_path = os.path.expanduser('~/.kaggle/kaggle.json')\n",
    "    if not os.path.exists(kaggle_path):\n",
    "        from google.colab import files\n",
    "        print(\"üìÇ Upload kaggle.json file...\")\n",
    "        uploaded = files.upload()\n",
    "        os.makedirs(os.path.dirname(kaggle_path), exist_ok=True)\n",
    "        for filename in uploaded.keys():\n",
    "            shutil.move(filename, kaggle_path)\n",
    "        os.chmod(kaggle_path, 0o600)\n",
    "        print(f\"‚úÖ Kaggle credentials setup at {kaggle_path}\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Kaggle credentials already exist at {kaggle_path}\")\n",
    "\n",
    "setup_kaggle_credentials()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d384b2f",
   "metadata": {},
   "source": [
    "## Mount Google Drive (Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1517ea98",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_running_in_colab():\n",
    "   from google.colab import drive\n",
    "   drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9964af3",
   "metadata": {},
   "source": [
    "##  Load Mistral-Instruct with Fallback to Quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3882e7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "import torch\n",
    "\n",
    "def load_model_pipeline(model_name: str, hf_token: str):\n",
    "    has_cuda = torch.cuda.is_available()\n",
    "    free_mem = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3) if has_cuda else 0\n",
    "    print(f\"üíª CUDA: {has_cuda} | GPU Memory: {free_mem:.2f} GB\")\n",
    "\n",
    "    device_map = {\"\": 0} if has_cuda else \"cpu\"\n",
    "    use_4bit = has_cuda and free_mem < 24\n",
    "\n",
    "    # Set quantization config\n",
    "    quant_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True if use_4bit else False,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    ) if use_4bit else None\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token, trust_remote_code=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # ‚úÖ Fix warning about pad_token\n",
    "\n",
    "    # Load model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device_map,\n",
    "        quantization_config=quant_config,\n",
    "        torch_dtype=torch.float16 if not quant_config else None,\n",
    "        trust_remote_code=True,\n",
    "        token=hf_token\n",
    "    )\n",
    "\n",
    "    print(f\"‚úÖ Model loaded on {next(model.parameters()).device}\")\n",
    "    return pipeline(\"text-generation\", model=model, tokenizer=tokenizer, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d505348b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_pipeline = load_model_pipeline(\n",
    "    model_name=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    hf_token=HF_TOKEN\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739b4783",
   "metadata": {},
   "source": [
    "# Global utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10779c22",
   "metadata": {},
   "source": [
    "### Utility to merge normalized json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a472fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "def merge_json_files(\n",
    "    source_dir: Path,\n",
    "    output_file: Path,\n",
    "    pattern: str,\n",
    "    merged_dir: Path\n",
    "):\n",
    "    source_dir.mkdir(parents=True, exist_ok=True)\n",
    "    merged_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    merged_data = []\n",
    "\n",
    "    # Load existing output if it exists\n",
    "    if output_file.exists():\n",
    "        with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                merged_data = json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"‚ö†Ô∏è Could not decode {output_file}, starting from scratch.\")\n",
    "\n",
    "    # Identify matching files\n",
    "    files_to_merge = sorted(source_dir.glob(pattern))\n",
    "\n",
    "    for file_path in files_to_merge:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "                if isinstance(data, list):\n",
    "                    merged_data.extend(data)\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è Skipping {file_path.name}: not a list.\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Failed to parse {file_path.name}: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Move to merged folder\n",
    "        shutil.move(str(file_path), merged_dir / file_path.name)\n",
    "        print(f\"‚úÖ Merged and moved: {file_path.name}\")\n",
    "\n",
    "    # Write combined output\n",
    "    if merged_data:\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(merged_data, f, indent=2)\n",
    "        print(f\"üíæ Saved to: {output_file}\")\n",
    "    else:\n",
    "        print(\"üì≠ No valid data to merge.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d169a9",
   "metadata": {},
   "source": [
    "### Infer JD Domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eac8105",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_keywords_dict = {\n",
    "    'advocate': ['advocate'],\n",
    "    'agriculture': ['agriculture'],\n",
    "    'apparel': ['apparel'],\n",
    "    'arts': ['arts'],\n",
    "    'automobile': ['automobile'],\n",
    "    'aviation': ['aviation'],\n",
    "    'banking': ['banking'],\n",
    "    'bpo': ['bpo'],\n",
    "    'business development': ['business', 'development', 'business development', 'business-development'],\n",
    "    'chef': ['chef'],\n",
    "    'construction': ['construction'],\n",
    "    'consultant': ['consultant'],\n",
    "    'data scientist': ['data', 'data analyst', 'data scientist', 'scientist'],\n",
    "    'designing': ['designing', 'designer'],\n",
    "    'digital media': ['digital', 'digital marketing executive', 'media', 'digital media', 'digital-media'],\n",
    "    'engineering': ['engineering'],\n",
    "    'finance': ['finance', 'financial analyst'],\n",
    "    'healthcare': ['healthcare'],\n",
    "    'hr': ['hr'],\n",
    "    'information technology': ['information', 'technology', 'information technology', 'information-technology'],\n",
    "    'public relations': ['public', 'relations', 'public relations', 'public-relations'],\n",
    "    'marketing': ['marketing'],\n",
    "    'sales': ['sales', 'sales executive'],\n",
    "    'teacher': ['teacher'],\n",
    "    'technician': ['technician'],\n",
    "    'training': ['training'],\n",
    "    'web designing': ['web', 'designing'],\n",
    "    'fitness': ['fitness'],\n",
    "    'accountant': ['accountant', 'accounting']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb59d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_domain_from_title(title):\n",
    "    title_lower = title.lower()\n",
    "    for domain, keywords in domain_keywords_dict.items():\n",
    "        if any(kw in title_lower for kw in keywords):\n",
    "            return domain\n",
    "    return \"unknown\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8712f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# no longer being used\n",
    "def infer_job_domain_llm(description: str) -> str:\n",
    "    print(\"üîç Inferring job domain using LLM...\")\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Given a job description, return only the most likely domain of the job as a one-word or short noun phrase. Do not include explanation, punctuation, or label. Just return the domain.\n",
    "\n",
    "Job Description:\n",
    "{description.strip()}\n",
    "\n",
    "Domain:\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = llm_pipeline(prompt)[0]['generated_text']\n",
    "\n",
    "        # Extract only the part after the final \"Domain:\" (handles echo case)\n",
    "        if \"Domain:\" in response:\n",
    "            response = response.split(\"Domain:\")[-1]\n",
    "\n",
    "        # Get the first non-empty word/line\n",
    "        domain = next((line.strip().lower() for line in response.strip().splitlines() if line.strip()), \"unknown\")\n",
    "\n",
    "        # Clean up unwanted characters\n",
    "        domain = re.sub(r\"[^a-zA-Z &\\-]\", \"\", domain)\n",
    "\n",
    "        return domain if domain else \"unknown\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è LLM inference failed: {e}\")\n",
    "        return \"unknown\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a17bfc2",
   "metadata": {},
   "source": [
    "### Configurations  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b50e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# üõ† CONFIGURATION\n",
    "# ==============================\n",
    "\n",
    "class Config:\n",
    "    DATASET_DOWNLOAD_DIR = \"datasets\"\n",
    "    JSON_OUTPUT_DIR = \"json_outputs_run3\"\n",
    "    JSON_OUTPUT_NORMALIZED_DIR = \"json_outputs_run3/normalized\"\n",
    "    AUTO_CLEANUP = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f7f202",
   "metadata": {},
   "source": [
    "## Utility to save json to a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85be866c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "# üì¶ Save JSON Output with Safety\n",
    "def save_json_output(data, output_path: str, indent: int = 4, overwrite: bool = True):\n",
    "    output_dir = os.path.dirname(output_path)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    if os.path.exists(output_path):\n",
    "        if overwrite:\n",
    "            os.remove(output_path)\n",
    "        else:\n",
    "            raise FileExistsError(f\"File {output_path} already exists and overwrite=False.\")\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=indent, ensure_ascii=False)\n",
    "\n",
    "    print(f\"‚úÖ Saved output to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461b99a9",
   "metadata": {},
   "source": [
    "## Utility to load file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f349cd",
   "metadata": {},
   "source": [
    "### load_ndjson_file() (for resume/jd input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9445d7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "def load_ndjson_file(file_path: Path) -> List[dict]:\n",
    "    if not os.path.isfile(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return [json.loads(line) for line in file if line.strip()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1a0736",
   "metadata": {},
   "source": [
    "### load_json_file() (for checkpoint & metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b796868",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_file(file_path: Path) -> dict:\n",
    "    if not os.path.isfile(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a1b769",
   "metadata": {},
   "source": [
    "# Phase 2 -\tParse resume/JD into JSON structured scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7917da",
   "metadata": {},
   "source": [
    "## Define Pydantic Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d92205",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Education(BaseModel):\n",
    "    degree: Optional[str] = None\n",
    "    field: Optional[str] = None\n",
    "    institution: Optional[str] = None\n",
    "    year: Optional[str] = None\n",
    "    gpa: Optional[float] = None\n",
    "\n",
    "class Experience(BaseModel):\n",
    "    job_title: Optional[str] = None\n",
    "    company: Optional[str] = None\n",
    "    start_date: Optional[str] = None\n",
    "    end_date: Optional[str] = None\n",
    "    duration_in_months: Optional[int] = None\n",
    "    description: Optional[List[str]] = None\n",
    "    \n",
    "class Certification(BaseModel):\n",
    "    certification: Optional[str] = None\n",
    "    date_issued: Optional[str] = None\n",
    "\n",
    "class Project(BaseModel):\n",
    "    project_title: Optional[str] = None\n",
    "    description: Optional[str] = None\n",
    "    start_date: Optional[str] = None\n",
    "    end_date: Optional[str] = None\n",
    "    url: Optional[str] = None\n",
    "    \n",
    "class Language(BaseModel):\n",
    "    language: Optional[str] = None\n",
    "    proficiency: Optional[str] = None\n",
    "    \n",
    "class Other(BaseModel):\n",
    "    section_name: Optional[str] = None\n",
    "    content: Optional[str] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10276bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from pydantic import BaseModel\n",
    "import re\n",
    "\n",
    "\n",
    "class ResumeSchema(BaseModel):\n",
    "    resume_id: Optional[str] = None\n",
    "    summary: Optional[str] = None\n",
    "    education: Optional[List[Education]] = None\n",
    "    experience: Optional[List[Experience]] = None\n",
    "    skills: Optional[List[str]] = None\n",
    "    certifications: Optional[List[Certification]] = None\n",
    "    projects: Optional[List[Project]] = None\n",
    "    languages: Optional[List[Language]] = None\n",
    "    other: Optional[List[Other]] = None\n",
    "    total_experience_years: Optional[float] = None\n",
    "\n",
    "    @classmethod\n",
    "    def normalize(cls, resume_dict: dict) -> dict:\n",
    "        resume_dict = dict(resume_dict)\n",
    "\n",
    "        # Normalize sections\n",
    "        for key in [\"skills\", \"certifications\", \"projects\", \"languages\"]:\n",
    "            if not isinstance(resume_dict.get(key), list):\n",
    "                resume_dict[key] = []\n",
    "\n",
    "        # Normalize Experience\n",
    "        normalized_exp = []\n",
    "        for item in resume_dict.get(\"experience\", []):\n",
    "            if not isinstance(item, dict):\n",
    "                continue\n",
    "            normalized_exp.append({\n",
    "                \"job_title\": item.get(\"job_title\", item.get(\"title\", \"\")),\n",
    "                \"company\": item.get(\"company\", \"\"),\n",
    "                \"start_date\": item.get(\"start_date\", \"\"),\n",
    "                \"end_date\": item.get(\"end_date\", \"\"),\n",
    "                \"duration_in_months\": item.get(\"duration_in_months\", None),\n",
    "                \"description\": item.get(\"description\", \"\")\n",
    "            })\n",
    "        resume_dict[\"experience\"] = normalized_exp\n",
    "\n",
    "        # Normalize Education\n",
    "        normalized_edu = []\n",
    "        for item in resume_dict.get(\"education\", []):\n",
    "            if not isinstance(item, dict):\n",
    "                continue\n",
    "            degree = item.get(\"degree\", \"\")\n",
    "            field = item.get(\"field\", \"\")\n",
    "            if not field:\n",
    "                match = re.search(r\"in\\\\s+(.+)\", degree, flags=re.IGNORECASE)\n",
    "                field = match.group(1).strip() if match else \"\"\n",
    "            year = str(item.get(\"year\", \"\")) if item.get(\"year\") else \"\"\n",
    "            gpa = item.get(\"gpa\", None)\n",
    "            normalized_edu.append({\n",
    "                \"degree\": degree,\n",
    "                \"field\": field,\n",
    "                \"institution\": item.get(\"institution\", \"\"),\n",
    "                \"year\": year,\n",
    "                \"gpa\": gpa\n",
    "            })\n",
    "        resume_dict[\"education\"] = normalized_edu\n",
    "\n",
    "        # Total Experience fallback\n",
    "        if \"total_experience_years\" not in resume_dict:\n",
    "            resume_dict[\"total_experience_years\"] = 0.0\n",
    "\n",
    "        return resume_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d198213",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JobDescriptionSchema(BaseModel):\n",
    "    jd_id: str\n",
    "    title: str\n",
    "    summary: str\n",
    "    required_experience_years: float\n",
    "    preferred_degrees: List[str]\n",
    "    required_skills: List[str]\n",
    "    optional_skills: List[str]\n",
    "    certifications: List[str]\n",
    "    soft_skills: List[str]\n",
    "    job_location: str\n",
    "    remote_option: Optional[bool] = False\n",
    "    employment_type: Optional[str] = None\n",
    "    inferred_domain: str = \"unknown\"\n",
    "\n",
    "    @classmethod\n",
    "    def normalize(cls, jd_dict: dict) -> dict:\n",
    "        jd_dict = dict(jd_dict)\n",
    "\n",
    "        aliases = {\n",
    "            \"years_required\": \"required_experience_years\",\n",
    "            \"requirements\": \"required_skills\",\n",
    "            \"degree_preferences\": \"preferred_degrees\",\n",
    "            \"certs\": \"certifications\",\n",
    "            \"skills_soft\": \"soft_skills\",\n",
    "            \"job_summary\": \"summary\"\n",
    "        }\n",
    "        for old, new in aliases.items():\n",
    "            if old in jd_dict and new not in jd_dict:\n",
    "                jd_dict[new] = jd_dict.pop(old)\n",
    "\n",
    "        # Required Experience Extraction\n",
    "        def extract_experience_years(text: str) -> float:\n",
    "            if not isinstance(text, str):\n",
    "                return 0.0\n",
    "            match = re.search(r'(\\\\d+(\\\\.\\\\d+)?)\\\\s*\\\\+?\\\\s*(years?|yrs?)', text.lower())\n",
    "            return float(match.group(1)) if match else 0.0\n",
    "\n",
    "        try:\n",
    "            val = jd_dict.get(\"required_experience_years\")\n",
    "            if val is None:\n",
    "                jd_dict[\"required_experience_years\"] = extract_experience_years(jd_dict.get(\"summary\", \"\"))\n",
    "            elif isinstance(val, str):\n",
    "                jd_dict[\"required_experience_years\"] = float(val.split()[0])\n",
    "            else:\n",
    "                jd_dict[\"required_experience_years\"] = float(val)\n",
    "        except Exception:\n",
    "            jd_dict[\"required_experience_years\"] = 0.0\n",
    "\n",
    "        # Normalize fields\n",
    "        for field in [\"preferred_degrees\", \"required_skills\", \"optional_skills\", \"certifications\", \"soft_skills\"]:\n",
    "            if not isinstance(jd_dict.get(field), list):\n",
    "                jd_dict[field] = []\n",
    "\n",
    "        for field in [\"title\", \"summary\", \"job_location\", \"employment_type\"]:\n",
    "            jd_dict[field] = jd_dict.get(field, \"\") or \"\"\n",
    "\n",
    "        # Remote Option\n",
    "        remote_flag = jd_dict.get(\"remote_option\", None)\n",
    "        if remote_flag is None:\n",
    "            remote_flag = \"remote\" in jd_dict.get(\"summary\", \"\").lower()\n",
    "        jd_dict[\"remote_option\"] = bool(remote_flag)\n",
    "\n",
    "        return jd_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7a6c15",
   "metadata": {},
   "source": [
    "### Generate schema string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3699f84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import get_origin, get_args, Union\n",
    "from pydantic import BaseModel\n",
    "\n",
    "def generate_example_structure(model_class) -> dict:\n",
    "    def default_for_type(field_type):\n",
    "        origin = get_origin(field_type)\n",
    "        args = get_args(field_type)\n",
    "\n",
    "        if origin is list and args:\n",
    "            inner_type = args[0]\n",
    "            return [default_for_type(inner_type)]\n",
    "        elif origin is Union and type(None) in args:\n",
    "            non_none_types = [arg for arg in args if arg is not type(None)]\n",
    "            return default_for_type(non_none_types[0]) if non_none_types else \"\"\n",
    "        elif field_type is str:\n",
    "            return \"\"\n",
    "        elif field_type in [float, int]:\n",
    "            return 0.0\n",
    "        elif isinstance(field_type, type) and issubclass(field_type, BaseModel):\n",
    "            return generate_example_structure(field_type)\n",
    "        else:\n",
    "            return \"\"\n",
    "\n",
    "    return {\n",
    "        field_name: default_for_type(field.annotation)\n",
    "        for field_name, field in model_class.model_fields.items()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80341360",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "import json\n",
    "from typing import Optional, Type\n",
    "\n",
    "@lru_cache(maxsize=4)\n",
    "def get_schema_str(schema_model: Optional[Type]) -> str:\n",
    "    \"\"\"\n",
    "    Returns a cleaned JSON schema string (as example structure) for a given Pydantic model.\n",
    "    Filters out fields not needed for the prompt like 'resume_id', 'duration_in_months', etc.\n",
    "    \"\"\"\n",
    "    if schema_model is None:\n",
    "        return \"{}\"\n",
    "    \n",
    "    example = generate_example_structure(schema_model)\n",
    "\n",
    "    # ‚õîÔ∏è Fields to remove before rendering into the prompt\n",
    "    exclude_fields = {\"resume_id\", \"duration_in_months\", \"total_experience_years\"}\n",
    "\n",
    "    def recursive_filter(obj):\n",
    "        if isinstance(obj, dict):\n",
    "            return {\n",
    "                k: recursive_filter(v)\n",
    "                for k, v in obj.items()\n",
    "                if k not in exclude_fields\n",
    "            }\n",
    "        elif isinstance(obj, list):\n",
    "            return [recursive_filter(v) for v in obj]\n",
    "        return obj\n",
    "\n",
    "    filtered = recursive_filter(example)\n",
    "\n",
    "    return json.dumps(filtered, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4debc085",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ‚úÖ Step 2: Generate schema string from your updated ResumeSchema\n",
    "schema_str = get_schema_str(ResumeSchema)\n",
    "\n",
    "# ‚úÖ Step 3: Print and inspect\n",
    "print(schema_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ae6d1f",
   "metadata": {},
   "source": [
    "### generate mock sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1394e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "import json\n",
    "\n",
    "@lru_cache(maxsize=4)\n",
    "def get_mock_resume_json() -> str:\n",
    "    mock_data = {\n",
    "        \"summary\": \"Certified accountant with 5+ years in financial operations and reporting.\",\n",
    "        \"education\": [\n",
    "            {\n",
    "            \"degree\": \"BBA\",\n",
    "            \"field\": \"Accounting\",\n",
    "            \"institution\": \"State University\",\n",
    "            \"year\": \"2012\",\n",
    "            \"gpa\": 0.0\n",
    "            }\n",
    "        ],\n",
    "        \"experience\": [\n",
    "            {\n",
    "            \"job_title\": \"Accountant\",\n",
    "            \"company\": \"ABC Corp\",\n",
    "            \"start_date\": \"01/2018\",\n",
    "            \"end_date\": \"12/2020\",\n",
    "            \"description\": [\n",
    "                \"Prepared financial reports and managed audits.\",\n",
    "                \"Assisted in budget forecasting and expense tracking.\"\n",
    "            ]\n",
    "            }\n",
    "        ],\n",
    "        \"skills\": [\n",
    "            \"Excel\",\n",
    "            \"SAP\",\n",
    "            \"Financial Reporting\"\n",
    "        ],\n",
    "        \"certifications\": [\n",
    "            {\n",
    "            \"certification\": \"CPA\",\n",
    "            \"date_issued\": \"\"\n",
    "            }\n",
    "        ],\n",
    "        \"projects\": [\n",
    "            {\n",
    "            \"project_title\": \"ERP Migration\",\n",
    "            \"description\": \"Led the migration from legacy system to SAP.\",\n",
    "            \"start_date\": \"\",\n",
    "            \"end_date\": \"11/2020\",\n",
    "            \"url\": \"\"\n",
    "            }\n",
    "        ],\n",
    "        \"languages\": [\n",
    "            {\n",
    "            \"language\": \"Spanish\",\n",
    "            \"proficiency\": \"\"\n",
    "            }\n",
    "        ],\n",
    "        \"other\": [\n",
    "            {\n",
    "            \"section_name\": \"Affiliations\",\n",
    "            \"content\": \"Member of National Association of Accountants\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    return json.dumps(mock_data, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274f774b",
   "metadata": {},
   "source": [
    "##  Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c7edce",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESUME_PROMPT_TEMPLATE = \"\"\"\n",
    "[INST]\n",
    "You are a strict JSON resume parser.\n",
    "\n",
    "Return only a valid JSON object ‚Äî no markdown, comments, or extra text.\n",
    "\n",
    "Follow this example structure (use the same key names and nesting, but do not copy the example values):\n",
    "\n",
    "{mock_json}\n",
    "\n",
    "Use this schema (all keys required; do not reorder or remove fields):\n",
    "{schema}\n",
    "\n",
    "Rules:\n",
    "- Use only resume content ‚Äî do not summarize, infer, or rewrite.\n",
    "- Copy text as-is with minor cleanup (punctuation/spacing).\n",
    "- Leave fields empty (\"\", []) if content is not present in the resume.\n",
    "- Do not paraphrase or fabricate.\n",
    "- Limit job descriptions to 10 bullet points max per job, in original order.\n",
    "- Do not repeat responsibilities across roles.\n",
    "- Include **all** fields from the schema, even if empty.\n",
    "- Output must start with `{{` and end with `}}` ‚Äî nothing before or after.\n",
    "\n",
    "Resume:\n",
    "<<<\n",
    "{text}\n",
    ">>>\n",
    "[/INST]\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ce3a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "JD_PROMPT_TEMPLATE = \"\"\"\n",
    "You are a JSON job description parser and experience extractor.\n",
    "\n",
    "Given the following job description text, extract a structured JSON following this schema:\n",
    "\n",
    "{schema}\n",
    "\n",
    "Instructions:\n",
    "- Parse title, summary, skills, certifications, and other fields exactly as shown.\n",
    "- Pay special attention to \"required_experience_years\":\n",
    "    - If experience years are explicitly listed, extract that number.\n",
    "    - Accept formats like \"5+ years\", \"3-5 years\", \"8 years required\", etc.\n",
    "    - If multiple ranges are mentioned (e.g., \"3-5 years\"), use the lower value (3 years).\n",
    "    - If no years are mentioned explicitly, infer from job title level:\n",
    "        - \"Senior\", \"Lead\" ‚Üí Assume 5+ years\n",
    "        - \"Mid-level\", \"Experienced\" ‚Üí Assume 3 years\n",
    "        - \"Entry level\", \"Junior\" ‚Üí Assume 0-1 years\n",
    "    - If still ambiguous, default to 0 years.\n",
    "- Handle remote/hybrid jobs:\n",
    "    - Set \"remote_option\" = true if remote keywords are present (remote, work from home, hybrid, WFH).\n",
    "- Infer the **inferred_domain** from the job description:\n",
    "    - Return a short domain noun (e.g., \"software\", \"marketing\", \"data science\", \"finance\", \"healthcare\").\n",
    "    - Use the title and summary to guide inference.\n",
    "    - If uncertain, use \"unknown\".\n",
    "- If a field is missing, leave it empty (\"\") or as an empty list [] depending on the field type.\n",
    "- Return ONLY a valid JSON object. No extra text, no explanations, no markdown formatting.\n",
    "- Your output MUST start with a {{.\n",
    "\n",
    "Job Description Text:\n",
    "--------------------\n",
    "{text}\n",
    "--------------------\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeb4f67",
   "metadata": {},
   "source": [
    "##  Inference + Validation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d521772",
   "metadata": {},
   "source": [
    "### Generate Raw LLM Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195bd924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_llm_output(prompt: str, max_new_tokens: int = 4096) -> str:\n",
    "    \"\"\"Run LLM and return the generated text with token count logging.\"\"\"\n",
    "    try:\n",
    "        # üî¢ Print input token count\n",
    "        input_tokens = llm_pipeline.tokenizer.encode(prompt, add_special_tokens=False)\n",
    "        print(f\"üßÆ Prompt token count: {len(input_tokens)} | Max new tokens: {max_new_tokens} | Estimated total: {len(input_tokens) + max_new_tokens}\")\n",
    "\n",
    "        # üîÅ Generate\n",
    "        outputs = llm_pipeline(prompt, max_new_tokens=max_new_tokens, do_sample=False)\n",
    "        \n",
    "        output_tokens = llm_pipeline.tokenizer.encode(outputs[0][\"generated_text\"], add_special_tokens=False)\n",
    "        print(f\"üìù Output token count: {len(output_tokens)}\")\n",
    "\n",
    "        return outputs[0][\"generated_text\"]\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"LLM generation failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94efd107",
   "metadata": {},
   "source": [
    "### Sanitize Output: Strip Prompt, Fix Cutoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa85362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_llm_output(response: str, prompt: str) -> str:\n",
    "    raw = response.replace(prompt, \"\").strip()\n",
    "\n",
    "    # Truncate garbage after the last closing brace\n",
    "    raw = re.sub(r'}[^}]*$', '}', raw)\n",
    "\n",
    "    # Remove markdown bullets or --- headers at end\n",
    "    raw = re.sub(r'(---|‚Ä¢|‚Äì|-)\\s*$', '', raw, flags=re.MULTILINE)\n",
    "\n",
    "    return raw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8d3f67",
   "metadata": {},
   "source": [
    "### Regex-based JSON Block Extractor and raw data processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb189464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "import json5\n",
    "\n",
    "def extract_json_block(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts the first valid JSON object from a text using the `regex` module and parses with `json5`.\n",
    "    This is more robust than standard `json` and can handle trailing commas, comments, etc.\n",
    "    \"\"\"\n",
    "    # Recursive regex pattern to find balanced curly braces (non-greedy)\n",
    "    pattern = r'(\\{(?:[^{}]|(?R))*\\})'\n",
    "\n",
    "    for match in regex.finditer(pattern, text, flags=regex.DOTALL):\n",
    "        json_candidate = match.group(1)\n",
    "        try:\n",
    "            return json5.loads(json_candidate)\n",
    "        except json5.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "    raise ValueError(\"‚ùå No valid JSON object found using regex and json5.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f7f103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_raw_data(raw: str) -> str:\n",
    "    import regex\n",
    "    raw = raw.strip().strip(\"`\")\n",
    "\n",
    "    # Remove code fences\n",
    "    raw = regex.sub(r\"^```(?:json)?\\s*|\\s*```$\", \"\", raw, flags=regex.IGNORECASE)\n",
    "\n",
    "    # Extract content between first '{' and last '}'\n",
    "    start_idx = raw.find('{')\n",
    "    end_idx = raw.rfind('}')\n",
    "    if start_idx == -1 or end_idx == -1 or end_idx <= start_idx:\n",
    "        raise ValueError(\"Cannot locate complete JSON object in the output\")\n",
    "\n",
    "    json_body = raw[start_idx:end_idx + 1]\n",
    "\n",
    "    # Fix common issues\n",
    "    json_body = regex.sub(r\",\\s*([\\]}])\", r\"\\1\", json_body)        # remove trailing commas\n",
    "    json_body = regex.sub(r\",\\s*,\", \",\", json_body)                # remove double commas\n",
    "    json_body = regex.sub(r'\"\\s*:\\s*:', '\":', json_body)           # fix \"::\"\n",
    "    json_body = regex.sub(r'(\"\\s*:[^,}\\]]+\")\\s*(\")', r'\\1,\\2', json_body)  # fix missing commas\n",
    "\n",
    "    # Final fallback: force valid quote usage\n",
    "    json_body = regex.sub(r\"‚Äò|‚Äô\", '\"', json_body)\n",
    "    json_body = regex.sub(r\"‚Äú|‚Äù\", '\"', json_body)\n",
    "\n",
    "    return json_body\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6789c18b",
   "metadata": {},
   "source": [
    "### Format Resume string for LLM processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf7d4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "CANONICAL_HEADER_PATTERNS = [\n",
    "    r'\\b(?:work|professional)?\\s*experience\\b',\n",
    "    r'\\b(?:education(?:\\s+(?:and|&)\\s+training)?|training(?:\\s+(?:and|&)\\s+(?:development|certifications?|programs?)|s)?|specialized\\s+training)\\b',\n",
    "    r'\\b(?:technical\\s+)?skills?\\b',\n",
    "    r'\\b(?:certifications?|licenses?|certifications?\\s*(?:and|&)\\s*licenses?)\\b',\n",
    "    r'\\b(?:projects?|key\\s+projects|project\\s+highlights)\\b',\n",
    "    r'\\blanguages?\\b',\n",
    "    r'\\b(?:executive\\s+)?summary\\b',\n",
    "    r'\\b(?:professional\\s+)?affiliations?\\b',\n",
    "    r'\\b(?:awards?|accomplishments?|honors|achievements)\\b',\n",
    "    r'\\bpublications?\\b',\n",
    "    r'\\b(?:interests|hobbies|extracurricular\\s+activities)\\b',\n",
    "    r'\\b(?:volunteer\\s+experience|community\\s+involvement|volunteering)\\b',\n",
    "    r'\\bobjective\\b',\n",
    "    r'\\breferences\\b',\n",
    "    r'\\bprofile\\b'\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "def is_known_section_header(line):\n",
    "    stripped = line.strip()\n",
    "    return any(\n",
    "        re.search(rf'^\\s*{pattern}\\s*[:\\-]?\\s*$', stripped, re.IGNORECASE)\n",
    "        for pattern in CANONICAL_HEADER_PATTERNS\n",
    "    )\n",
    "\n",
    "\n",
    "def is_date_like(text):\n",
    "    return bool(re.match(r'(?i)^\\d{1,2}/\\d{4}$|^[A-Za-z]{3,9} \\d{4}$|^\\d{4}$', text.strip()))\n",
    "\n",
    "def is_location_like(text):\n",
    "    return bool(re.fullmatch(r'(City|State|[A-Z][a-z]{1,15})', text.strip()))\n",
    "\n",
    "def is_degree_fragment(text):\n",
    "    return bool(re.fullmatch(r'[A-Za-z]{2,5}', text.strip()))  # BBA, MSc, etc.\n",
    "\n",
    "\n",
    "def detect_real_headers(lines):\n",
    "    section_lines = []\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        stripped = line.strip()\n",
    "        if not stripped:\n",
    "            continue\n",
    "\n",
    "        # ‚úÖ Canonical match: skip heuristics & filtering\n",
    "        if is_known_section_header(stripped):\n",
    "            section_lines.append((i, stripped))\n",
    "            continue\n",
    "\n",
    "        # ‚ùå Filtering for heuristic-only (to avoid false positives)\n",
    "        if is_date_like(stripped) or is_location_like(stripped) or is_degree_fragment(stripped):\n",
    "            continue\n",
    "\n",
    "    return section_lines\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277fbc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "def preprocess_resume_text(text: str) -> str:\n",
    "    # Normalize escaped newlines and tabs into actual characters\n",
    "    text = text.replace('\\\\n', '\\n').replace('\\\\t', '\\t')\n",
    "\n",
    "    # Clean actual tabs and escaped slashes\n",
    "    text = text.replace('\\\\/', '/').replace('\\t', ' ')\n",
    "    # Clean tabs and escaped slashes\n",
    "    text = text.replace('\\\\/', '/').replace('\\t', ' ')\n",
    "\n",
    "    # ‚úÖ Normalize: fix spaced slashes in dates (e.g. '08 / 2014' ‚Üí '08/2014')\n",
    "    text = re.sub(r'(?<=\\d)\\s*/\\s*(?=\\d{4})', '/', text)\n",
    "\n",
    "    # ‚úÖ Protect date ranges by marking them before splitting on 3+ spaces\n",
    "    text = re.sub(\n",
    "        r'(?i)(\\d{1,2}/\\d{4})\\s*(to|-)\\s*(\\d{1,2}/\\d{4}|current|present)',\n",
    "        r'__DATERANGE__\\1 to \\3__ENDDATE__',\n",
    "        text\n",
    "    )\n",
    "\n",
    "    # ‚úÖ Collapse \"Company ‚Äî City, State\" formatting\n",
    "    text = re.sub(\n",
    "        r'(?i)([A-Za-z0-9&.,()\\-\\' ]{2,})\\s*[-Ôºç‚Äì‚Äî]{1,2}\\s*([A-Z][a-z]+)\\s*,\\s*([A-Z][a-z]+)',\n",
    "        r'\\1 ‚Äî \\2 , \\3',\n",
    "        text\n",
    "    )\n",
    "\n",
    "    # ‚úÖ Convert 3+ spaces into newlines\n",
    "    text = re.sub(r' {3,}', '\\n', text)\n",
    "\n",
    "    # Normalize excessive newlines and spaces\n",
    "    text = re.sub(r'\\n{2,}', '\\n', text)\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)\n",
    "\n",
    "    # ‚úÖ Restore protected date ranges\n",
    "    text = text.replace('__DATERANGE__', '\\n').replace('__ENDDATE__', '\\n')\n",
    "\n",
    "    # Add breaks around known section headers\n",
    "    def insert_header_breaks(line):\n",
    "        stripped = line.strip()\n",
    "        if is_known_section_header(stripped):\n",
    "            return f\"\\n{stripped}\\n\"\n",
    "        return stripped\n",
    "\n",
    "    lines = text.splitlines()\n",
    "    lines = [insert_header_breaks(line) for line in lines]\n",
    "    text = \"\\n\".join(lines)\n",
    "\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a70056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "def split_run_on_sentences(text):\n",
    "    \"\"\"Split long lines into sentence-like segments while preserving periods.\"\"\"\n",
    "    lines = text.splitlines()\n",
    "    split_lines = []\n",
    "\n",
    "    for line in lines:\n",
    "        stripped = line.strip()\n",
    "\n",
    "        # Skip short or bullet lines\n",
    "        if not stripped or stripped.startswith('-') or len(stripped) < 50:\n",
    "            split_lines.append(stripped)\n",
    "            continue\n",
    "\n",
    "        # Split after period only if followed by space and capital letter\n",
    "        if stripped.count('.') >= 2:\n",
    "            # This keeps the period with the sentence\n",
    "            segments = re.split(r'(?<=\\.) (?=[A-Z])', stripped)\n",
    "            split_lines.extend([seg.strip() for seg in segments])\n",
    "        else:\n",
    "            split_lines.append(stripped)\n",
    "\n",
    "    return split_lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9e9ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_resume_for_llm(text: str) -> str:\n",
    "    text = preprocess_resume_text(text)\n",
    " \n",
    "    lines = text.splitlines()\n",
    "     # Sentence splitting\n",
    "    lines = split_run_on_sentences(\"\\n\".join(lines))\n",
    "  \n",
    "    headers = detect_real_headers(lines)\n",
    "    print(\"Detected Headers:\", headers)\n",
    "\n",
    "    header_indices = {idx for idx, _ in headers}\n",
    "\n",
    "    output_lines = []\n",
    "    for i, line in enumerate(lines):\n",
    "        stripped = line.strip()\n",
    "\n",
    "        if not stripped:\n",
    "            continue\n",
    "\n",
    "        # Skip lines that are isolated numbers (e.g. caused by bad splitting)\n",
    "        if stripped.isdigit():\n",
    "            continue\n",
    "\n",
    "        # Insert a line break before date ranges (if not already done)\n",
    "        if re.match(r'(?i)^\\d{1,2}/\\d{4} to (\\d{1,2}/\\d{4}|current|present)$', stripped):\n",
    "            output_lines.append(\"\")  # add a break before\n",
    "            output_lines.append(stripped)\n",
    "            continue\n",
    "\n",
    "        # Header detection\n",
    "        if i in header_indices:\n",
    "            output_lines.append(\"\")  # add a blank line before header\n",
    "            output_lines.append(stripped.upper())\n",
    "            output_lines.append(\"\")  # and after\n",
    "        else:\n",
    "            output_lines.append(stripped)\n",
    "\n",
    "    formatted = \"\\n\".join(output_lines)\n",
    "\n",
    "    # Normalize inline date ranges (if still embedded)\n",
    "    formatted = re.sub(\n",
    "        r'(?i)(\\d{1,2}/\\d{4})\\s*(to|-)\\s*(\\d{1,2}/\\d{4}|current|present)',\n",
    "        r'\\1 to \\3',\n",
    "        formatted\n",
    "    )\n",
    "\n",
    "    # Format SKILLS section as bullets\n",
    "    formatted = re.sub(\n",
    "        r'(?i)(\\nSKILLS\\n)([^\\n]+)',\n",
    "        lambda m: m.group(1) + \"\\n\" + \"\\n\".join(f\"- {s.strip()}\" for s in m.group(2).split(',')),\n",
    "        formatted\n",
    "    )\n",
    "\n",
    "    return formatted.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0de79d7",
   "metadata": {},
   "source": [
    "### Final Orchestrator: Fault-Tolerant Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69d7930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from typing import Dict, Optional, Type\n",
    "\n",
    "def inject_ids(parsed: Dict, schema_model: Optional[Type], record_id: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Injects a UUID as `resume_id` or `jd_id` based on the schema model name.\n",
    "    \"\"\"\n",
    "    if not schema_model:\n",
    "        print(\"‚ö†Ô∏è No schema model provided for ID injection.\")\n",
    "        return parsed\n",
    "    schema_name = schema_model.__name__.lower()\n",
    "    if schema_name.startswith(\"resume\"):\n",
    "        parsed[\"resume_id\"] = record_id\n",
    "    elif schema_name.startswith(\"jobdescription\"):\n",
    "        parsed[\"jd_id\"] = record_id\n",
    "    return parsed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc83f2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "def extract_structured_json(\n",
    "    text: str,\n",
    "    prompt_template: str,\n",
    "    schema_model: Union[None, type] = None,\n",
    "    max_new_tokens: int = 4096,\n",
    "    retries: int = 0,\n",
    "    validate: bool = True,\n",
    "    record_id: str = \"\",\n",
    "    mock_json: str = \"\"\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Runs LLM to extract structured JSON and validates against schema.\n",
    "    Includes: prompt sanitization, retry, echo detection, brace parser fallback, schema validation.\n",
    "    \"\"\"\n",
    "    schema_str = get_schema_str(schema_model)\n",
    "    prompt = prompt_template.format(text=text, schema=schema_str, mock_json=mock_json)\n",
    "    raw_output = \"\"\n",
    "    attempt = 0\n",
    "\n",
    "    while attempt <= retries:\n",
    "        try:\n",
    "            # Step 1: Get LLM output\n",
    "            response = generate_llm_output(prompt, max_new_tokens)\n",
    "            print(\"üß™ LLM output was:\\n\", response)  # Preview first 300 chars\n",
    "            raw_output = sanitize_llm_output(response, prompt)\n",
    "            #print(\"üß™ Raw output was:\\n\", raw_output)\n",
    "\n",
    "            # Step 2: Detect schema echo or instruction echo\n",
    "            if \"$schema\" in raw_output or \"Ensure these rules\" in raw_output:\n",
    "                raise ValueError(\"LLM echoed schema or instruction block instead of generating JSON.\")\n",
    "\n",
    "            # Step 3: Try JSON load directly\n",
    "            json_start = raw_output.find(\"{\")\n",
    "            if json_start == -1:\n",
    "                raise ValueError(\"No opening '{' found in LLM output.\")\n",
    "            \n",
    "            cleaned_output = raw_output[json_start:]\n",
    "            cleaned_output = clean_raw_data(cleaned_output)\n",
    "\n",
    "            parsed = json.loads(cleaned_output)\n",
    "            parsed = inject_ids(parsed, schema_model, record_id)\n",
    "            \n",
    "            print(\"üß™ Parsed output was:\\n\", parsed)  # Preview first 300 chars\n",
    "            \n",
    "\n",
    "            # Step 4: Optional schema validation\n",
    "            if validate and schema_model:\n",
    "                if hasattr(schema_model, \"normalize\"):\n",
    "                    parsed = schema_model.normalize(parsed)\n",
    "                schema_model.model_validate(parsed)\n",
    "\n",
    "            return parsed\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Attempt {attempt + 1} failed: {e}\")\n",
    "            #print(\"üß™ Raw output was:\\n\", raw_output)  # Preview first 300 chars\n",
    "            attempt += 1\n",
    "\n",
    "    # Step 5: Fallback using brace matching\n",
    "    try:\n",
    "        parsed = extract_json_block(raw_output)\n",
    "        parsed = inject_ids(parsed, schema_model, record_id)\n",
    "            \n",
    "        if validate and schema_model:\n",
    "            if hasattr(schema_model, \"normalize\"):\n",
    "                parsed = schema_model.normalize(parsed)\n",
    "            schema_model.model_validate(parsed)\n",
    "        return parsed\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"raw_output\": raw_output.strip(),\n",
    "            \"error\": f\"Regex fallback failed: {e}\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f8e0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import ValidationError\n",
    "\n",
    "def pydantic_validate(model_class, data):\n",
    "    \"\"\"\n",
    "    Version-safe validator that supports both Pydantic v1 and v2.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Pydantic v2\n",
    "        return model_class.model_validate(data)\n",
    "    except AttributeError:\n",
    "        # Fallback to Pydantic v1\n",
    "        return model_class.parse_obj(data)\n",
    "\n",
    "\n",
    "def validate_entry(entry, is_resume):\n",
    "    try:\n",
    "        model = ResumeSchema if is_resume else JobDescriptionSchema\n",
    "        if hasattr(model, \"normalize\"):\n",
    "            normalized = model.normalize(entry)\n",
    "        else:\n",
    "            normalized = entry\n",
    "        pydantic_validate(model, normalized)\n",
    "        return True, None\n",
    "    except ValidationError as ve:\n",
    "        return False, str(ve)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143847db",
   "metadata": {},
   "source": [
    "##  Normalize in Batches with Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7325133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_metadata_summary(\n",
    "    output_dir: Path,\n",
    "    is_resume: bool,\n",
    "    input_file: str,\n",
    "    total_records: int,\n",
    "    total_valid: int,\n",
    "    total_invalid: int,\n",
    "    start_index: int,\n",
    "    end_index: int,\n",
    "    timestamp: str,\n",
    "    batch_id: str\n",
    "):\n",
    "    summary = {\n",
    "        \"batch_id\": batch_id,\n",
    "        \"timestamp\": timestamp,\n",
    "        \"input_file\": input_file,\n",
    "        \"input_type\": \"resume\" if is_resume else \"job_description\",\n",
    "        \"records_start_index\": start_index,\n",
    "        \"records_end_index\": end_index,\n",
    "        \"records_total\": total_records,\n",
    "        \"records_valid\": total_valid,\n",
    "        \"records_invalid\": total_invalid,\n",
    "        \"output_dir\": str(output_dir)\n",
    "    }\n",
    "    summary_file = output_dir / f\"meta_{'resumes' if is_resume else 'jds'}_{start_index}_{end_index}_{timestamp}_{batch_id}.json\"\n",
    "    save_json_output(summary, str(summary_file), overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72001714",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_jd_text(record: dict) -> str:\n",
    "    \"\"\"\n",
    "    Constructs a rich text string from all non-empty fields of a JD record.\n",
    "    \"\"\"\n",
    "    jd_parts = []\n",
    "    for k, v in record.items():\n",
    "        label = k.replace('_', ' ').title()\n",
    "\n",
    "        if v is None:\n",
    "            continue\n",
    "\n",
    "        if isinstance(v, list):\n",
    "            if v:\n",
    "                value_str = \", \".join(str(i) for i in v if i)\n",
    "                jd_parts.append(f\"{label}: {value_str}\")\n",
    "        elif isinstance(v, (str, int, float)):\n",
    "            value_str = str(v).strip()\n",
    "            if value_str:\n",
    "                jd_parts.append(f\"{label}: {value_str}\")\n",
    "\n",
    "    return \"\\n\".join(jd_parts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1d8c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from datetime import datetime\n",
    "from typing import List\n",
    "\n",
    "def normalize_batch(\n",
    "    records: List[dict],\n",
    "    start_idx: int,\n",
    "    end_idx: int,\n",
    "    is_resume: bool,\n",
    "    output_dir: Path,\n",
    "    prompt_template,\n",
    "    schema_model\n",
    "):\n",
    "    results, invalids = [], []\n",
    "\n",
    "    for record in records:\n",
    "        if is_resume:\n",
    "            raw_resume_text  = record.get(\"Resume_str\", \"\")\n",
    "            # Step 1: Restore structure\n",
    "            text = format_resume_for_llm(raw_resume_text)\n",
    "            #print(f\"üìù Normalized input resume text:\\n{text} \\n\")\n",
    "            max_new_tokens =4096\n",
    "            record_id = record.get(\"ID\", str(uuid.uuid4()))\n",
    "            mock_json = get_mock_resume_json()\n",
    "        else:\n",
    "            text = render_jd_text(record)\n",
    "            max_new_tokens = 2048\n",
    "            record_id = record.get(\"job_id\", str(uuid.uuid4()))\n",
    "            #mock_json = get_mock_resume_json()\n",
    "\n",
    "        parsed = extract_structured_json(\n",
    "            text=text,\n",
    "            prompt_template=prompt_template,\n",
    "            schema_model=schema_model,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            record_id = record_id,\n",
    "            mock_json=mock_json,\n",
    "            validate=False\n",
    "        )\n",
    "\n",
    "        if \"raw_output\" in parsed or \"error\" in parsed:\n",
    "            invalids.append({\n",
    "                \"input\": text,\n",
    "                \"output\": parsed,\n",
    "                \"error\": parsed.get(\"error\", \"Malformed or unstructured output\")\n",
    "            })\n",
    "        else:\n",
    "            results.append(parsed)\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "    batch_id = uuid.uuid4().hex[:6]\n",
    "    prefix = \"resumes\" if is_resume else \"jds\"\n",
    "\n",
    "    if results:\n",
    "        save_json_output(\n",
    "            results,\n",
    "            output_path=output_dir / f\"{prefix}_valid_{start_idx}_{end_idx}_{timestamp}_{batch_id}.json\"\n",
    "        )\n",
    "    if invalids:\n",
    "        save_json_output(\n",
    "            invalids,\n",
    "            output_path=output_dir / f\"{prefix}_invalid_{start_idx}_{end_idx}_{timestamp}_{batch_id}.json\"\n",
    "        )\n",
    "\n",
    "    return results, invalids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca36dd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def normalize_file_in_batches(\n",
    "    input_filename: str,\n",
    "    output_dir: Path,\n",
    "    is_resume: bool = True,\n",
    "    input_dir: Path = Path(\"json_outputs\"),\n",
    "    save_every: int = 5,\n",
    "    limit: int = None\n",
    "):\n",
    "    input_path = input_dir / input_filename\n",
    "    data = load_ndjson_file(input_path)\n",
    "    \n",
    "\n",
    "    checkpoint_file = output_dir / f\"checkpoint_{'resumes' if is_resume else 'jds'}.json\"\n",
    "    start_index = 0\n",
    "    if checkpoint_file.exists():\n",
    "        checkpoint = load_json_file(checkpoint_file)\n",
    "        start_index = checkpoint.get(\"last_index\", 0)\n",
    "        print(f\"üîÅ Resuming from index {start_index}\")\n",
    "\n",
    "    # Apply limit\n",
    "    data_to_process = data[start_index:]\n",
    "    if limit is not None:\n",
    "        data_to_process = data_to_process[:limit]\n",
    "\n",
    "    prompt_template = RESUME_PROMPT_TEMPLATE if is_resume else JD_PROMPT_TEMPLATE\n",
    "    schema_model = ResumeSchema if is_resume else JobDescriptionSchema\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    total_valid, total_invalid = 0, 0\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "    batch_id = uuid.uuid4().hex[:6]\n",
    "    actual_start = start_index\n",
    "    actual_end = start_index + len(data_to_process)\n",
    "\n",
    "    for i in tqdm(range(0, len(data_to_process), save_every)):\n",
    "        batch = data_to_process[i:i + save_every]\n",
    "        batch_start = start_index + i\n",
    "        batch_end = batch_start + len(batch)  # ‚úÖ Accurate\n",
    "\n",
    "        results, invalids = normalize_batch(\n",
    "            records=batch,\n",
    "            start_idx=batch_start,\n",
    "            end_idx=batch_end,\n",
    "            is_resume=is_resume,\n",
    "            output_dir=output_dir,\n",
    "            prompt_template=prompt_template,\n",
    "            schema_model=schema_model\n",
    "        )\n",
    "\n",
    "        total_valid += len(results)\n",
    "        total_invalid += len(invalids)\n",
    "\n",
    "        save_json_output({\"last_index\": batch_end}, str(checkpoint_file), overwrite=True)\n",
    "\n",
    "    # ‚úÖ Save metadata summary\n",
    "    \"\"\"\n",
    "    save_metadata_summary(\n",
    "        output_dir=output_dir,\n",
    "        is_resume=is_resume,\n",
    "        input_file=input_filename,\n",
    "        total_records=len(data_to_process),\n",
    "        total_valid=total_valid,\n",
    "        total_invalid=total_invalid,\n",
    "        start_index=actual_start,\n",
    "        end_index=actual_end,\n",
    "        timestamp=timestamp,\n",
    "        batch_id=batch_id\n",
    "    )\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34057ff1",
   "metadata": {},
   "source": [
    "## Run Phase 2 End-to-End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf23348",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_file_in_batches(\n",
    "    input_filename=\"parsed_jds.json\",\n",
    "    input_dir=Path(Config.JSON_OUTPUT_DIR),\n",
    "    output_dir=Path(Config.JSON_OUTPUT_NORMALIZED_DIR),\n",
    "    is_resume=False,\n",
    "    save_every=5,\n",
    "    limit=60  # ‚úÖ Process only 20 records max\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b33fc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_file_in_batches(\n",
    "    input_filename=\"parsed_resumes.json\",\n",
    "    input_dir=Path(Config.JSON_OUTPUT_DIR),\n",
    "    output_dir=Path(Config.JSON_OUTPUT_NORMALIZED_DIR),\n",
    "    is_resume=True,\n",
    "    save_every=1,\n",
    "    limit=1  # ‚úÖ Process only 20 records max\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7409286b",
   "metadata": {},
   "source": [
    "## Merge normalized files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3940c601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "normalized_dir = Path(Config.JSON_OUTPUT_NORMALIZED_DIR)\n",
    "merged_dir = normalized_dir / \"merged\"\n",
    "\n",
    "merge_json_files(\n",
    "    source_dir=normalized_dir,\n",
    "    output_file=normalized_dir / \"normalized_jds.json\",\n",
    "    pattern=\"jds_valid*.json\",\n",
    "    merged_dir=merged_dir\n",
    ")\n",
    "\n",
    "merge_json_files(\n",
    "    source_dir=normalized_dir,\n",
    "    output_file=normalized_dir / \"normalized_resumes.json\",\n",
    "    pattern=\"resumes_valid*.json\",\n",
    "    merged_dir=merged_dir\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
