{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcb849be",
   "metadata": {},
   "source": [
    "# Phase 1: First Steps Notebook ‚Äî Data Ingestion + Minimal Parsing\n",
    "1. Setup and Install Dependencies\n",
    "2. Load Resume and JD datasets\n",
    "3. Minimal Parsing into JSON Structure\n",
    "4. Save structured JSON for Phase 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dd9696",
   "metadata": {},
   "source": [
    "## Setup and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4abca3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install kaggle kagglehub pandas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb756b8f",
   "metadata": {},
   "source": [
    "## Util Classes and methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43abc8ac",
   "metadata": {},
   "source": [
    "### Configurations  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1e827a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# üõ† CONFIGURATION\n",
    "# ==============================\n",
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "class Config:\n",
    "    DATASET_DOWNLOAD_DIR = \"datasets\"\n",
    "    JSON_OUTPUT_DIR = \"json_outputs\"\n",
    "    AUTO_CLEANUP = True\n",
    "\n",
    "    @staticmethod\n",
    "    def setup_kaggle_credentials():\n",
    "        kaggle_path = os.path.expanduser('~/.kaggle/kaggle.json')\n",
    "        if not os.path.exists(kaggle_path):\n",
    "            from google.colab import files\n",
    "            print(\"üìÇ Upload kaggle.json file...\")\n",
    "            uploaded = files.upload()\n",
    "            os.makedirs(os.path.dirname(kaggle_path), exist_ok=True)\n",
    "            for filename in uploaded.keys():\n",
    "                shutil.move(filename, kaggle_path)\n",
    "            os.chmod(kaggle_path, 0o600)\n",
    "            print(f\"‚úÖ Kaggle credentials setup at {kaggle_path}\")\n",
    "        else:\n",
    "            print(f\"‚úÖ Kaggle credentials already exist at {kaggle_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a537a5",
   "metadata": {},
   "source": [
    "### Downloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89480102",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================\n",
    "# DOWNLOADER\n",
    "# ==============================\n",
    "class DatasetDownloader:\n",
    "    @staticmethod\n",
    "    def download_and_extract(dataset_path: str) -> tuple[str, str]:\n",
    "        os.makedirs(Config.DATASET_DOWNLOAD_DIR, exist_ok=True)\n",
    "        dataset_slug = dataset_path.split(\"/\")[-1]\n",
    "        extract_folder_path = os.path.join(Config.DATASET_DOWNLOAD_DIR, dataset_slug)\n",
    "        zip_filename = f\"{dataset_slug}.zip\"\n",
    "        zip_path = os.path.join(Config.DATASET_DOWNLOAD_DIR, zip_filename)\n",
    "\n",
    "        if os.path.exists(extract_folder_path) and any(Path(extract_folder_path).rglob(\"*.csv\")):\n",
    "            print(f\"‚ö° Dataset folder already exists at '{extract_folder_path}', skipping download and extraction.\")\n",
    "            return extract_folder_path, zip_filename\n",
    "\n",
    "        print(f\"‚¨áÔ∏è Downloading dataset: {dataset_path} ...\")\n",
    "        !kaggle datasets download -d {dataset_path} -p {Config.DATASET_DOWNLOAD_DIR}\n",
    "\n",
    "        if not os.path.exists(zip_path):\n",
    "            raise FileNotFoundError(f\"‚ùå Zip file '{zip_filename}' not found after download!\")\n",
    "\n",
    "        os.makedirs(extract_folder_path, exist_ok=True)\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_folder_path)\n",
    "\n",
    "        print(f\"‚úÖ Downloaded and extracted to '{extract_folder_path}'.\")\n",
    "        return extract_folder_path, zip_filename\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1170df00",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2c06119",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================\n",
    "# LOADER\n",
    "# ==============================\n",
    "class DatasetLoader:\n",
    "    @staticmethod\n",
    "    def load_csv(dataset_folder: str, target_csv_name: str) -> pd.DataFrame:\n",
    "        print(f\"üîç Searching for '{target_csv_name}' inside {dataset_folder}...\")\n",
    "        if not os.path.exists(dataset_folder):\n",
    "            raise FileNotFoundError(f\"‚ùå Dataset folder '{dataset_folder}' does not exist!\")\n",
    "\n",
    "        for root, _, files in os.walk(dataset_folder):\n",
    "            for file in files:\n",
    "                if file.lower() == target_csv_name.lower():\n",
    "                    csv_path = os.path.join(root, file)\n",
    "                    df = pd.read_csv(csv_path)\n",
    "                    print(f\"‚úÖ Loaded CSV with shape {df.shape}\")\n",
    "                    return df\n",
    "\n",
    "        raise FileNotFoundError(f\"‚ùå CSV file '{target_csv_name}' not found inside extracted dataset!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a80a5a",
   "metadata": {},
   "source": [
    "### Data Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08247c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================\n",
    "# PROCESSOR\n",
    "# ==============================\n",
    "class DatasetProcessor:\n",
    "    @staticmethod\n",
    "    def filter_fields(df: pd.DataFrame, allowed_fields: List[str]) -> pd.DataFrame:\n",
    "        missing_fields = [field for field in allowed_fields if field not in df.columns]\n",
    "        if missing_fields:\n",
    "            raise ValueError(f\"‚ùå Fields {missing_fields} not found in dataset!\")\n",
    "\n",
    "        filtered_df = df[allowed_fields]\n",
    "        print(f\"‚úÖ Filtered columns: {list(filtered_df.columns)}\")\n",
    "        return filtered_df\n",
    "\n",
    "    @staticmethod\n",
    "    def save_to_json(df: pd.DataFrame, output_json_name: str):\n",
    "        os.makedirs(Config.JSON_OUTPUT_DIR, exist_ok=True)\n",
    "        output_path = os.path.join(Config.JSON_OUTPUT_DIR, output_json_name)\n",
    "\n",
    "        if os.path.exists(output_path):\n",
    "            os.remove(output_path)\n",
    "            print(f\"üóëÔ∏è Existing JSON '{output_path}' deleted.\")\n",
    "\n",
    "        df.to_json(output_path, orient='records', lines=True, force_ascii=False)\n",
    "        print(f\"‚úÖ Data saved to JSON at '{output_path}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fc4fb6",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7245255",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ==============================\n",
    "# CLEANER\n",
    "# ==============================\n",
    "class Cleaner:\n",
    "    @staticmethod\n",
    "    def cleanup_dataset_artifacts(extracted_folder_path: str, zip_filename: str):\n",
    "        if os.path.exists(extracted_folder_path):\n",
    "            shutil.rmtree(extracted_folder_path)\n",
    "            print(f\"üßπ Folder '{extracted_folder_path}' has been deleted successfully.\")\n",
    "\n",
    "        zip_path = os.path.join(Config.DATASET_DOWNLOAD_DIR, zip_filename)\n",
    "        if os.path.exists(zip_path):\n",
    "            os.remove(zip_path)\n",
    "            print(f\"üóëÔ∏è Zip file '{zip_path}' has been deleted successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a2525b",
   "metadata": {},
   "source": [
    "### Hybrid Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8d85253",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ==============================\n",
    "# HYBRID LOADER\n",
    "# ==============================\n",
    "try:\n",
    "    import kagglehub\n",
    "    from kagglehub import KaggleDatasetAdapter\n",
    "except ImportError:\n",
    "    kagglehub = None\n",
    "\n",
    "class HybridDatasetLoader:\n",
    "    @staticmethod\n",
    "    def load_dataset(dataset_path: str, file_name: str) -> pd.DataFrame:\n",
    "        if kagglehub:\n",
    "            try:\n",
    "                print(f\"üì• Trying KaggleHub for {dataset_path}...\")\n",
    "                df = kagglehub.dataset_load(KaggleDatasetAdapter.PANDAS, dataset_path, file_name)\n",
    "                print(f\"‚úÖ Loaded using KaggleHub: shape = {df.shape}\")\n",
    "                return df\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è KaggleHub failed: {e}\\nFalling back to ZIP-based loader.\")\n",
    "\n",
    "        extracted_folder, _ = DatasetDownloader.download_and_extract(dataset_path)\n",
    "        return DatasetLoader.load_csv(extracted_folder, file_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e917e9d6",
   "metadata": {},
   "source": [
    "### Main flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "766a42bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================\n",
    "# MAIN FLOW\n",
    "# ==============================\n",
    "def process_dataset(dataset_path: str, target_csv_name: str, allowed_fields: List[str], output_json_name: str):\n",
    "    df = HybridDatasetLoader.load_dataset(dataset_path, target_csv_name)\n",
    "    filtered_df = DatasetProcessor.filter_fields(df, allowed_fields)\n",
    "    DatasetProcessor.save_to_json(filtered_df, output_json_name)\n",
    "\n",
    "    dataset_slug = dataset_path.split(\"/\")[-1]\n",
    "    extracted_folder = os.path.join(Config.DATASET_DOWNLOAD_DIR, dataset_slug)\n",
    "    zip_filename = f\"{dataset_slug}.zip\"\n",
    "    if Config.AUTO_CLEANUP:\n",
    "        Cleaner.cleanup_dataset_artifacts(extracted_folder, zip_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60577088",
   "metadata": {},
   "source": [
    "## Login and do the processing of Resume and JD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3bb5022",
   "metadata": {},
   "outputs": [],
   "source": [
    "Config.setup_kaggle_credentials()\n",
    "# Process Resume Dataset\n",
    "process_dataset(\n",
    "    dataset_path=\"snehaanbhawal/resume-dataset\",\n",
    "    target_csv_name=\"Resume.csv\",\n",
    "    allowed_fields=[\"Category\", \"Resume_str\"],\n",
    "    output_json_name=\"parsed_resumes.json\"\n",
    ")\n",
    "\n",
    "# Process Job Postings Dataset\n",
    "process_dataset(\n",
    "    dataset_path=\"arshkon/linkedin-job-postings\",\n",
    "    target_csv_name=\"postings.csv\",\n",
    "    allowed_fields=[\"title\", \"company_name\", \"location\", \"description\", \"skills_desc\", \"job_id\" , \"formatted_experience_level\", \"formatted_work_type\"],\n",
    "    output_json_name=\"parsed_jds.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74195950",
   "metadata": {},
   "source": [
    "# Phase 2 -\tParse resume/JD into JSON structured scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc89812",
   "metadata": {},
   "source": [
    "## Colab + GPU Detection Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc1596fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_running_in_colab():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "def get_available_gpu_memory_gb():\n",
    "    try:\n",
    "        output = subprocess.check_output(\n",
    "            [\"nvidia-smi\", \"--query-gpu=memory.free\", \"--format=csv,nounits,noheader\"],\n",
    "            encoding=\"utf-8\"\n",
    "        )\n",
    "        free_mem_mb = int(output.strip().split(\"\\n\")[0])\n",
    "        return free_mem_mb / 1024\n",
    "    except Exception:\n",
    "        return 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27168f09",
   "metadata": {},
   "source": [
    "##  Install Dependencies  & Login to Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "381adace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://download.pytorch.org/whl/nightly/cu128\n",
      "Requirement already satisfied: torch in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (2.8.0.dev20250420+cu128)\n",
      "Requirement already satisfied: torchvision in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (0.22.0.dev20250421+cu128)\n",
      "Requirement already satisfied: torchaudio in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (2.6.0.dev20250421+cu128)\n",
      "Requirement already satisfied: filelock in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: setuptools in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from torch) (70.2.0)\n",
      "Requirement already satisfied: numpy in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from torchvision) (2.1.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: bitsandbytes in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (0.45.5)\n",
      "Requirement already satisfied: torch<3,>=2.0 in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from bitsandbytes) (2.8.0.dev20250420+cu128)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from bitsandbytes) (2.1.2)\n",
      "Requirement already satisfied: filelock in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (2024.10.0)\n",
      "Requirement already satisfied: setuptools in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (70.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (2.1.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "if is_running_in_colab():\n",
    "    # Install the required packages\n",
    "    !pip install -q transformers accelerate bitsandbytes sentencepiece pydantic huggingface_hub\n",
    "else:\n",
    "    %pip install -q transformers accelerate sentencepiece pydantic huggingface_hub\n",
    "    #%pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu121\n",
    "    #%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "    %pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128\n",
    "    %pip install -U bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d970ff92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# Set your token here securely or prompt for it in Colab\n",
    "# Recommended: store in Colab secrets or environment variable\n",
    "HF_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "\n",
    "if not HF_TOKEN:\n",
    "    if is_running_in_colab():\n",
    "        # If running in Colab, use the Colab secrets\n",
    "        try:\n",
    "            from google.colab import userdata\n",
    "            HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "            if not HF_TOKEN:\n",
    "                raise ValueError(\"‚ö†Ô∏è Hugging Face token not found in Colab secrets.\")\n",
    "            print(\"üîë Hugging Face token found in Colab secrets.\")\n",
    "        except ImportError:\n",
    "            print(\"‚ö†Ô∏è Unable to authenticate in Colab. Please set your Hugging Face token manually.\")\n",
    "    else:\n",
    "        # Prompt for token if not set in environment\n",
    "        print(\"üîë Please enter your Hugging Face token:\")\n",
    "        # For Colab or local prompt input\n",
    "        HF_TOKEN = input(\"üîë Enter your Hugging Face token: \").strip()\n",
    "\n",
    "login(token=HF_TOKEN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cae340",
   "metadata": {},
   "source": [
    "## Mount Google Drive (Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f485a41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_running_in_colab():\n",
    "   from google.colab import drive\n",
    "   drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e277a8c7",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd88e640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, uuid, subprocess, torch\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "from pydantic import BaseModel, ValidationError\n",
    "from huggingface_hub import login\n",
    "import re\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95271ea9",
   "metadata": {},
   "source": [
    "##  Load mistral-Instruct with Fallback to Quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af05c12a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6594cc1c857f486296dd40df6d90b4ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Model loaded on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "\n",
    "def load_mistral_pipeline_dynamic(model_name=\"mistralai/Mistral-7B-Instruct-v0.1\", hf_token=None):\n",
    "    has_cuda = torch.cuda.is_available()\n",
    "    free_vram_gb = get_available_gpu_memory_gb() if has_cuda else 0\n",
    "    device_map = {\"\": 0} if has_cuda else \"cpu\"\n",
    "    load_quantized = not has_cuda or free_vram_gb < 14\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token, trust_remote_code=True)\n",
    "\n",
    "    if load_quantized:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            token=hf_token,\n",
    "            device_map=device_map,\n",
    "            load_in_8bit=True,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            token=hf_token,\n",
    "            device_map=device_map,\n",
    "            torch_dtype=torch.float16,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "\n",
    "    print(\"üéØ Model loaded on:\", next(model.parameters()).device)\n",
    "    return pipeline(\"text-generation\", model=model, tokenizer=tokenizer, batch_size=2)\n",
    "\n",
    "llm_pipeline = load_mistral_pipeline_dynamic(hf_token=HF_TOKEN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4157bab3",
   "metadata": {},
   "source": [
    "## Define Pydantic Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c460a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "class Education(BaseModel):\n",
    "    degree: str\n",
    "    field: str\n",
    "    institution: str\n",
    "    year: str\n",
    "\n",
    "class Experience(BaseModel):\n",
    "    job_title: str\n",
    "    company: str\n",
    "    duration: str\n",
    "    description: str\n",
    "\n",
    "class ResumeSchema(BaseModel):\n",
    "    basics: dict\n",
    "    education: List[Education]\n",
    "    experience: List[Experience]\n",
    "    skills: List[str]\n",
    "    certifications: List[str]\n",
    "    projects: List[str]\n",
    "\n",
    "    @classmethod\n",
    "    def normalize(cls, resume_dict: dict) -> dict:\n",
    "        resume_dict = dict(resume_dict)\n",
    "\n",
    "        # Ensure keys are present\n",
    "        for field in [\"education\", \"experience\", \"skills\", \"certifications\", \"projects\"]:\n",
    "            if not isinstance(resume_dict.get(field), list):\n",
    "                resume_dict[field] = []\n",
    "\n",
    "        if not isinstance(resume_dict.get(\"basics\"), dict):\n",
    "            resume_dict[\"basics\"] = {}\n",
    "\n",
    "        return resume_dict\n",
    "\n",
    "\n",
    "class JobDescriptionSchema(BaseModel):\n",
    "    title: str\n",
    "    summary: str\n",
    "    required_experience_years: Union[float, int, str] = 0\n",
    "    preferred_degrees: List[str] = []\n",
    "    required_skills: List[str] = []\n",
    "    certifications: List[str] = []\n",
    "    soft_skills: List[str] = []\n",
    "\n",
    "    @classmethod\n",
    "    def normalize(cls, jd_dict: dict) -> dict:\n",
    "        \"\"\"Normalize types to avoid validation crashes.\"\"\"\n",
    "        jd_dict = dict(jd_dict)  # copy\n",
    "        try:\n",
    "            if isinstance(jd_dict.get(\"required_experience_years\"), str):\n",
    "                jd_dict[\"required_experience_years\"] = float(\n",
    "                    jd_dict[\"required_experience_years\"].split()[0]\n",
    "                )\n",
    "        except Exception:\n",
    "            jd_dict[\"required_experience_years\"] = 0.0\n",
    "\n",
    "        for field in [\"preferred_degrees\", \"required_skills\", \"certifications\", \"soft_skills\"]:\n",
    "            if not isinstance(jd_dict.get(field), list):\n",
    "                jd_dict[field] = []\n",
    "\n",
    "        return jd_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e36c60ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_example_structure(model_class) -> dict:\n",
    "    \"\"\"Generate an example JSON structure from a Pydantic model using placeholder values.\"\"\"\n",
    "    from typing import get_args, get_origin\n",
    "    from pydantic import BaseModel\n",
    "\n",
    "    def default_for_type(field_type):\n",
    "        origin = get_origin(field_type)\n",
    "        if origin is list or origin is List:\n",
    "            return []\n",
    "        elif field_type == str:\n",
    "            return \"\"\n",
    "        elif field_type in (float, int):\n",
    "            return 0.0\n",
    "        elif origin is dict or field_type == dict:\n",
    "            return {}\n",
    "        elif issubclass(field_type, BaseModel):\n",
    "            return generate_example_structure(field_type)\n",
    "        return \"\"\n",
    "\n",
    "    structure = {}\n",
    "    for field_name, field in model_class.model_fields.items():\n",
    "        try:\n",
    "            structure[field_name] = default_for_type(field.annotation)\n",
    "        except Exception:\n",
    "            structure[field_name] = \"\"\n",
    "    return structure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc3114a",
   "metadata": {},
   "source": [
    "##  Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be1de13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESUME_PROMPT_TEMPLATE = \"\"\"\n",
    "You are a resume-to-JSON parser.\n",
    "\n",
    "Given the raw resume text below, return a single valid JSON object following this structure:\n",
    "\n",
    "{schema}\n",
    "\n",
    "Begin your response with {{\n",
    "Use \"\" for strings, [] for lists, and 0.0 for missing numbers.\n",
    "\n",
    "---\n",
    "Resume Text:\n",
    "{text}\n",
    "---\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa1090c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "JD_PROMPT_TEMPLATE = \"\"\"\n",
    "You are a job description parser.\n",
    "\n",
    "Given the raw job description below, return a structured JSON object that follows this structure:\n",
    "\n",
    "{schema}\n",
    "\n",
    "Begin your response with {{\n",
    "Use \"\" for strings, [] for missing lists, and 0.0 for missing numbers.\n",
    "\n",
    "---\n",
    "Job Description:\n",
    "{text}\n",
    "---\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec7d2c6",
   "metadata": {},
   "source": [
    "##  Inference + Validation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86890d4c",
   "metadata": {},
   "source": [
    "### Generate Raw LLM Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "784055a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_llm_output(prompt: str, max_new_tokens: int = 1024) -> str:\n",
    "    return llm_pipeline(prompt, max_new_tokens=max_new_tokens, do_sample=False)[0][\"generated_text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b177af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_llm_output_old(prompt: str, max_new_tokens: int = 1024) -> str:\n",
    "    \"\"\"Run LLM and return the generated text.\"\"\"\n",
    "    try:\n",
    "        return llm_pipeline(prompt, max_new_tokens=max_new_tokens, do_sample=False)[0][\"generated_text\"]\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"LLM generation failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927b35d6",
   "metadata": {},
   "source": [
    "### Sanitize Output: Strip Prompt, Fix Cutoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5f7a9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_llm_output(response: str, prompt: str) -> str:\n",
    "    raw = response.replace(prompt, \"\").strip()\n",
    "\n",
    "    # Truncate garbage after the last closing brace\n",
    "    raw = re.sub(r'}[^}]*$', '}', raw)\n",
    "\n",
    "    # Remove markdown bullets or --- headers at end\n",
    "    raw = re.sub(r'(---|‚Ä¢|‚Äì|-)\\s*$', '', raw, flags=re.MULTILINE)\n",
    "\n",
    "    return raw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8df602",
   "metadata": {},
   "source": [
    "### Regex-based JSON Block Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e577818f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json_block(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Regex-free fallback JSON block extractor using brace balance.\n",
    "    Finds first balanced {} block.\n",
    "    \"\"\"\n",
    "    stack = []\n",
    "    start = None\n",
    "    for i, char in enumerate(text):\n",
    "        if char == '{':\n",
    "            if not stack:\n",
    "                start = i\n",
    "            stack.append(char)\n",
    "        elif char == '}':\n",
    "            if stack:\n",
    "                stack.pop()\n",
    "                if not stack:\n",
    "                    try:\n",
    "                        return json.loads(text[start:i+1])\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "    raise ValueError(\"No valid JSON object found in fallback.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72316749",
   "metadata": {},
   "source": [
    "### Final Orchestrator: Fault-Tolerant Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9354392e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_text(text: str, max_chars=1500) -> str:\n",
    "    \"\"\"Trims long resumes/JDs to prevent LLM overload.\"\"\"\n",
    "    return text.strip()[:max_chars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "050ce4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_structured_json(\n",
    "    text: str,\n",
    "    prompt_template: str,\n",
    "    schema_model: Union[None, type] = None,\n",
    "    max_new_tokens: int = 1024,\n",
    "    retries: int = 2,\n",
    "    validate: bool = True,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Runs LLM to extract structured JSON and validates against schema.\n",
    "    Includes: prompt sanitization, retry, echo detection, brace parser fallback, schema validation.\n",
    "    \"\"\"\n",
    "    example_schema = generate_example_structure(schema_model)\n",
    "    schema_str = json.dumps(example_schema, indent=2)\n",
    "    prompt = prompt_template.format(text=truncate_text(text), schema=schema_str)\n",
    "    raw_output = \"\"\n",
    "    attempt = 0\n",
    "\n",
    "    while attempt <= retries:\n",
    "        try:\n",
    "            # Step 1: Get LLM output\n",
    "            response = generate_llm_output(prompt, max_new_tokens)\n",
    "            raw_output = sanitize_llm_output(response, prompt)\n",
    "\n",
    "            # Step 2: Detect schema echo or instruction echo\n",
    "            if \"$schema\" in raw_output or \"Ensure these rules\" in raw_output:\n",
    "                raise ValueError(\"LLM echoed schema or instruction block instead of generating JSON.\")\n",
    "\n",
    "            # Step 3: Try JSON load directly\n",
    "            json_start = raw_output.find(\"{\")\n",
    "            if json_start == -1:\n",
    "                raise ValueError(\"No opening '{' found in LLM output.\")\n",
    "\n",
    "            parsed = json.loads(raw_output[json_start:])\n",
    "\n",
    "            # Step 4: Optional schema validation\n",
    "            if validate and schema_model:\n",
    "                if hasattr(schema_model, \"normalize\"):\n",
    "                    parsed = schema_model.normalize(parsed)\n",
    "                schema_model.model_validate(parsed)\n",
    "\n",
    "            return parsed\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Attempt {attempt + 1} failed: {e}\")\n",
    "            print(\"üß™ Raw output was:\\n\", raw_output[:300])  # Preview first 300 chars\n",
    "            attempt += 1\n",
    "\n",
    "    # Step 5: Fallback using brace matching\n",
    "    try:\n",
    "        parsed = extract_json_block(raw_output)\n",
    "        if validate and schema_model:\n",
    "            if hasattr(schema_model, \"normalize\"):\n",
    "                parsed = schema_model.normalize(parsed)\n",
    "            schema_model.model_validate(parsed)\n",
    "        return parsed\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"raw_output\": raw_output.strip(),\n",
    "            \"error\": f\"Regex fallback failed: {e}\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34b1aea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pydantic_validate(model_class, data):\n",
    "    \"\"\"\n",
    "    Version-safe validator that supports both Pydantic v1 and v2.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Pydantic v2\n",
    "        return model_class.model_validate(data)\n",
    "    except AttributeError:\n",
    "        # Fallback to Pydantic v1\n",
    "        return model_class.parse_obj(data)\n",
    "\n",
    "\n",
    "def validate_entry(entry, is_resume):\n",
    "    try:\n",
    "        model = ResumeSchema if is_resume else JobDescriptionSchema\n",
    "        if hasattr(model, \"normalize\"):\n",
    "            normalized = model.normalize(entry)\n",
    "        else:\n",
    "            normalized = entry\n",
    "        pydantic_validate(model, normalized)\n",
    "        return True, None\n",
    "    except ValidationError as ve:\n",
    "        return False, str(ve)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a69c78",
   "metadata": {},
   "source": [
    "##  Normalize in Batches with Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c947fced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_and_save(\n",
    "    input_filename,\n",
    "    output_filename_prefix,\n",
    "    is_resume=True,\n",
    "    output_dir=Path(\"json_outputs\"),\n",
    "    google_drive_sync=True,\n",
    "    drive_subdir=\"AI-Resume-Agent\",\n",
    "    limit: int = None,\n",
    "    resume: bool = True,\n",
    "    save_every: int = 5,\n",
    "    STRICT: bool = True,\n",
    "):\n",
    "\n",
    "    batch_id = uuid.uuid4().hex[:6]\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "    valid_file = f\"{output_filename_prefix}_{timestamp}_{batch_id}.json\"\n",
    "    invalid_file = f\"invalid_{output_filename_prefix}_{timestamp}_{batch_id}.json\"\n",
    "    meta_file = f\"run_metadata_{output_filename_prefix}_{timestamp}_{batch_id}.json\"\n",
    "    checkpoint_file = output_dir / f\"checkpoint_{output_filename_prefix}.json\"\n",
    "\n",
    "    input_path = output_dir / input_filename\n",
    "    valid_output_path = output_dir / valid_file\n",
    "    invalid_output_path = output_dir / invalid_file\n",
    "    metadata_path = output_dir / meta_file\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Load input\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw_data = [json.loads(line) for line in f.readlines() if line.strip()]\n",
    "\n",
    "    # Resume support\n",
    "    start_index = 0\n",
    "    results, invalids = [], []\n",
    "    if resume and checkpoint_file.exists():\n",
    "        with open(checkpoint_file, \"r\") as ckpt:\n",
    "            checkpoint = json.load(ckpt)\n",
    "            start_index = checkpoint.get(\"last_index\", 0)\n",
    "            print(f\"üîÅ Resuming from record {start_index}\")\n",
    "\n",
    "    raw_data = raw_data[start_index:]\n",
    "    if limit is not None:\n",
    "        raw_data = raw_data[:limit]\n",
    "\n",
    "    prompt_template = RESUME_PROMPT_TEMPLATE if is_resume else JD_PROMPT_TEMPLATE\n",
    "    schema_model = ResumeSchema if is_resume else JobDescriptionSchema\n",
    "\n",
    "    for idx, record in enumerate(tqdm(raw_data), start=start_index):\n",
    "        text = record.get(\"Resume_str\" if is_resume else \"description\", \"\")\n",
    "        parsed = extract_structured_json(\n",
    "            text=text,\n",
    "            prompt_template=prompt_template,\n",
    "            schema_model=schema_model,\n",
    "            validate=True\n",
    "        )\n",
    "        \n",
    "        # Skip any malformed or echoed results\n",
    "        if \"raw_output\" in parsed or \"error\" in parsed:\n",
    "            invalids.append({\n",
    "                \"input\": text,\n",
    "                \"output\": parsed,\n",
    "                \"error\": parsed.get(\"error\", \"Malformed or unstructured output\")\n",
    "            })\n",
    "            continue\n",
    "\n",
    "\n",
    "        # Normalize + validate\n",
    "        if STRICT:\n",
    "            is_valid, error_msg = validate_entry(parsed, is_resume)\n",
    "        else:\n",
    "            is_valid, error_msg = True, None\n",
    "\n",
    "        if is_valid:\n",
    "            results.append(parsed)\n",
    "        else:\n",
    "            invalids.append({\"input\": text, \"output\": parsed, \"error\": error_msg})\n",
    "\n",
    "\n",
    "        # Periodic save for durability\n",
    "        if (idx + 1 - start_index) % save_every == 0:\n",
    "            with open(valid_output_path, \"w\") as f: \n",
    "                json.dump(results, f, indent=2)\n",
    "            with open(invalid_output_path, \"w\") as f: \n",
    "                json.dump(invalids, f, indent=2)\n",
    "            with open(checkpoint_file, \"w\") as f: \n",
    "                json.dump({\"last_index\": idx + 1}, f)\n",
    "\n",
    "    # Final save\n",
    "    with open(valid_output_path, \"w\") as f: \n",
    "        json.dump(results, f, indent=2)\n",
    "    with open(invalid_output_path, \"w\") as f: \n",
    "        json.dump(invalids, f, indent=2)\n",
    "    with open(checkpoint_file, \"w\") as f: \n",
    "        json.dump({\"last_index\": start_index + len(raw_data)}, f)\n",
    "\n",
    "    # Metadata\n",
    "    meta = {\n",
    "        \"batch_id\": batch_id,\n",
    "        \"timestamp\": timestamp,\n",
    "        \"input_file\": input_filename,\n",
    "        \"output_valid_file\": valid_file,\n",
    "        \"output_invalid_file\": invalid_file,\n",
    "        \"record_count\": len(raw_data),\n",
    "        \"valid_count\": len(results),\n",
    "        \"invalid_count\": len(invalids),\n",
    "        \"model\": \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "        \"quantized\": not any(p.dtype == torch.float16 for p in llm_pipeline.model.parameters()),\n",
    "        \"device\": str(next(llm_pipeline.model.parameters()).device)\n",
    "    }\n",
    "\n",
    "    with open(metadata_path, \"w\") as f: \n",
    "        json.dump(meta, f, indent=2)\n",
    "\n",
    "    if google_drive_sync and is_running_in_colab():\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        drive_base = Path(\"/content/drive/MyDrive\") / drive_subdir\n",
    "        drive_base.mkdir(parents=True, exist_ok=True)\n",
    "        for file in [valid_output_path, invalid_output_path, metadata_path, checkpoint_file]:\n",
    "            if file.exists():\n",
    "                file.replace(drive_base / file.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48f65d6",
   "metadata": {},
   "source": [
    "## Run Phase 2 End-to-End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ecddd939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÅ Resuming from record 75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Attempt 1 failed: Expecting ',' delimiter: line 7 column 4 (char 87)\n",
      "üß™ Raw output was:\n",
      " {\n",
      "  \"basics\": {\n",
      "    \"name\": \"\",\n",
      "    \"phone\": \"\",\n",
      "    \"email\": \"\",\n",
      "    \"address\": \"\"\n",
      "  }\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Attempt 2 failed: Expecting ',' delimiter: line 7 column 4 (char 87)\n",
      "üß™ Raw output was:\n",
      " {\n",
      "  \"basics\": {\n",
      "    \"name\": \"\",\n",
      "    \"phone\": \"\",\n",
      "    \"email\": \"\",\n",
      "    \"address\": \"\"\n",
      "  }\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|‚ñà‚ñà        | 1/5 [01:24<05:37, 84.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Attempt 3 failed: Expecting ',' delimiter: line 7 column 4 (char 87)\n",
      "üß™ Raw output was:\n",
      " {\n",
      "  \"basics\": {\n",
      "    \"name\": \"\",\n",
      "    \"phone\": \"\",\n",
      "    \"email\": \"\",\n",
      "    \"address\": \"\"\n",
      "  }\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Attempt 1 failed: 3 validation errors for ResumeSchema\n",
      "experience.0.job_title\n",
      "  Field required [type=missing, input_value={'position': 'DIRECTV HR ... team of 10 employees']}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "experience.0.duration\n",
      "  Field required [type=missing, input_value={'position': 'DIRECTV HR ... team of 10 employees']}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "experience.0.description\n",
      "  Field required [type=missing, input_value={'position': 'DIRECTV HR ... team of 10 employees']}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "üß™ Raw output was:\n",
      " {\n",
      "  \"basics\": {\n",
      "    \"name\": \"\",\n",
      "    \"phone\": \"\",\n",
      "    \"email\": \"\",\n",
      "    \"address\": \"\"\n",
      "  },\n",
      "  \"education\": [],\n",
      "  \"experience\": [\n",
      "    {\n",
      "      \"position\": \"DIRECTV HR RECRUITER/ Administration\",\n",
      "      \"company\": \"Company Name\",\n",
      "      \"location\": \"City, State\",\n",
      "      \"start_date\": \"11/2014\",\n",
      "      \"end_da\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Attempt 2 failed: 3 validation errors for ResumeSchema\n",
      "experience.0.job_title\n",
      "  Field required [type=missing, input_value={'position': 'DIRECTV HR ... team of 10 employees']}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "experience.0.duration\n",
      "  Field required [type=missing, input_value={'position': 'DIRECTV HR ... team of 10 employees']}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "experience.0.description\n",
      "  Field required [type=missing, input_value={'position': 'DIRECTV HR ... team of 10 employees']}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "üß™ Raw output was:\n",
      " {\n",
      "  \"basics\": {\n",
      "    \"name\": \"\",\n",
      "    \"phone\": \"\",\n",
      "    \"email\": \"\",\n",
      "    \"address\": \"\"\n",
      "  },\n",
      "  \"education\": [],\n",
      "  \"experience\": [\n",
      "    {\n",
      "      \"position\": \"DIRECTV HR RECRUITER/ Administration\",\n",
      "      \"company\": \"Company Name\",\n",
      "      \"location\": \"City, State\",\n",
      "      \"start_date\": \"11/2014\",\n",
      "      \"end_da\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [01:49<02:28, 49.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Attempt 3 failed: 3 validation errors for ResumeSchema\n",
      "experience.0.job_title\n",
      "  Field required [type=missing, input_value={'position': 'DIRECTV HR ... team of 10 employees']}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "experience.0.duration\n",
      "  Field required [type=missing, input_value={'position': 'DIRECTV HR ... team of 10 employees']}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "experience.0.description\n",
      "  Field required [type=missing, input_value={'position': 'DIRECTV HR ... team of 10 employees']}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "üß™ Raw output was:\n",
      " {\n",
      "  \"basics\": {\n",
      "    \"name\": \"\",\n",
      "    \"phone\": \"\",\n",
      "    \"email\": \"\",\n",
      "    \"address\": \"\"\n",
      "  },\n",
      "  \"education\": [],\n",
      "  \"experience\": [\n",
      "    {\n",
      "      \"position\": \"DIRECTV HR RECRUITER/ Administration\",\n",
      "      \"company\": \"Company Name\",\n",
      "      \"location\": \"City, State\",\n",
      "      \"start_date\": \"11/2014\",\n",
      "      \"end_da\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Attempt 1 failed: 3 validation errors for ResumeSchema\n",
      "experience.0.job_title\n",
      "  Field required [type=missing, input_value={'position': 'HR Business..., 'end_date': '01/2021'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "experience.0.duration\n",
      "  Field required [type=missing, input_value={'position': 'HR Business..., 'end_date': '01/2021'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "experience.0.description\n",
      "  Field required [type=missing, input_value={'position': 'HR Business..., 'end_date': '01/2021'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "üß™ Raw output was:\n",
      " {\n",
      "  \"basics\": {\n",
      "    \"name\": \"HR BUSINESS PARTNER\",\n",
      "    \"summary\": \"Proactive Human Resources Business Partner guiding performance management, talent planning and benefits. Decisive with proven success providing employee relations support and oversight for efficient operations. Extensive knowledge an\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Attempt 2 failed: 3 validation errors for ResumeSchema\n",
      "experience.0.job_title\n",
      "  Field required [type=missing, input_value={'position': 'HR Business..., 'end_date': '01/2021'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "experience.0.duration\n",
      "  Field required [type=missing, input_value={'position': 'HR Business..., 'end_date': '01/2021'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "experience.0.description\n",
      "  Field required [type=missing, input_value={'position': 'HR Business..., 'end_date': '01/2021'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "üß™ Raw output was:\n",
      " {\n",
      "  \"basics\": {\n",
      "    \"name\": \"HR BUSINESS PARTNER\",\n",
      "    \"summary\": \"Proactive Human Resources Business Partner guiding performance management, talent planning and benefits. Decisive with proven success providing employee relations support and oversight for efficient operations. Extensive knowledge an\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [02:10<01:13, 36.67s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Attempt 3 failed: 3 validation errors for ResumeSchema\n",
      "experience.0.job_title\n",
      "  Field required [type=missing, input_value={'position': 'HR Business..., 'end_date': '01/2021'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "experience.0.duration\n",
      "  Field required [type=missing, input_value={'position': 'HR Business..., 'end_date': '01/2021'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "experience.0.description\n",
      "  Field required [type=missing, input_value={'position': 'HR Business..., 'end_date': '01/2021'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "üß™ Raw output was:\n",
      " {\n",
      "  \"basics\": {\n",
      "    \"name\": \"HR BUSINESS PARTNER\",\n",
      "    \"summary\": \"Proactive Human Resources Business Partner guiding performance management, talent planning and benefits. Decisive with proven success providing employee relations support and oversight for efficient operations. Extensive knowledge an\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [02:16<00:24, 24.26s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Attempt 1 failed: 3 validation errors for ResumeSchema\n",
      "experience.0.job_title\n",
      "  Field required [type=missing, input_value={'company': 'Company Name...tion, additional paym']}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "experience.0.duration\n",
      "  Field required [type=missing, input_value={'company': 'Company Name...tion, additional paym']}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "experience.0.description\n",
      "  Field required [type=missing, input_value={'company': 'Company Name...tion, additional paym']}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "üß™ Raw output was:\n",
      " {\n",
      "  \"basics\": {\n",
      "    \"name\": \"\",\n",
      "    \"phone\": \"\",\n",
      "    \"email\": \"\",\n",
      "    \"address\": \"\"\n",
      "  },\n",
      "  \"education\": [],\n",
      "  \"experience\": [\n",
      "    {\n",
      "      \"company\": \"Company Name\",\n",
      "      \"location\": \"City, State\",\n",
      "      \"position\": \"HR Generalist\",\n",
      "      \"start_date\": \"01/2009\",\n",
      "      \"end_date\": \"Current\",\n",
      "      \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Attempt 2 failed: 3 validation errors for ResumeSchema\n",
      "experience.0.job_title\n",
      "  Field required [type=missing, input_value={'company': 'Company Name...tion, additional paym']}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "experience.0.duration\n",
      "  Field required [type=missing, input_value={'company': 'Company Name...tion, additional paym']}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "experience.0.description\n",
      "  Field required [type=missing, input_value={'company': 'Company Name...tion, additional paym']}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "üß™ Raw output was:\n",
      " {\n",
      "  \"basics\": {\n",
      "    \"name\": \"\",\n",
      "    \"phone\": \"\",\n",
      "    \"email\": \"\",\n",
      "    \"address\": \"\"\n",
      "  },\n",
      "  \"education\": [],\n",
      "  \"experience\": [\n",
      "    {\n",
      "      \"company\": \"Company Name\",\n",
      "      \"location\": \"City, State\",\n",
      "      \"position\": \"HR Generalist\",\n",
      "      \"start_date\": \"01/2009\",\n",
      "      \"end_date\": \"Current\",\n",
      "      \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [02:43<00:00, 32.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Attempt 3 failed: 3 validation errors for ResumeSchema\n",
      "experience.0.job_title\n",
      "  Field required [type=missing, input_value={'company': 'Company Name...tion, additional paym']}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "experience.0.duration\n",
      "  Field required [type=missing, input_value={'company': 'Company Name...tion, additional paym']}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "experience.0.description\n",
      "  Field required [type=missing, input_value={'company': 'Company Name...tion, additional paym']}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "üß™ Raw output was:\n",
      " {\n",
      "  \"basics\": {\n",
      "    \"name\": \"\",\n",
      "    \"phone\": \"\",\n",
      "    \"email\": \"\",\n",
      "    \"address\": \"\"\n",
      "  },\n",
      "  \"education\": [],\n",
      "  \"experience\": [\n",
      "    {\n",
      "      \"company\": \"Company Name\",\n",
      "      \"location\": \"City, State\",\n",
      "      \"position\": \"HR Generalist\",\n",
      "      \"start_date\": \"01/2009\",\n",
      "      \"end_date\": \"Current\",\n",
      "      \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÅ Resuming from record 75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 20%|‚ñà‚ñà        | 1/5 [00:09<00:39,  9.88s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:15<00:22,  7.45s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [00:23<00:14,  7.50s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [00:29<00:06,  6.98s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:34<00:00,  6.92s/it]\n"
     ]
    }
   ],
   "source": [
    "def run_phase2_structured_normalization():\n",
    "    normalize_and_save(\"parsed_resumes.json\", \"normalized_resumes\", is_resume=True, limit=5, resume=True, STRICT=True)\n",
    "    normalize_and_save(\"parsed_jds.json\", \"normalized_jds\", is_resume=False, limit=5, resume=True, STRICT=True)\n",
    "\n",
    "run_phase2_structured_normalization()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
