{
  "best_global_step": 4500,
  "best_metric": 1.297846794128418,
  "best_model_checkpoint": "json_outputs_all_data/fine-tune/optuna_output/optuna_trial_10/checkpoint-4500",
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 4500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.006667777962993832,
      "grad_norm": 0.33749300241470337,
      "learning_rate": 0.0004956949856956416,
      "loss": 2.5962,
      "step": 10
    },
    {
      "epoch": 0.013335555925987664,
      "grad_norm": 0.30209285020828247,
      "learning_rate": 0.0004945912337791517,
      "loss": 2.4156,
      "step": 20
    },
    {
      "epoch": 0.020003333888981498,
      "grad_norm": 0.3049316108226776,
      "learning_rate": 0.0004934874818626618,
      "loss": 2.3761,
      "step": 30
    },
    {
      "epoch": 0.026671111851975328,
      "grad_norm": 0.276999294757843,
      "learning_rate": 0.0004923837299461717,
      "loss": 2.3978,
      "step": 40
    },
    {
      "epoch": 0.03333888981496916,
      "grad_norm": 0.3033742606639862,
      "learning_rate": 0.0004912799780296818,
      "loss": 2.3536,
      "step": 50
    },
    {
      "epoch": 0.040006667777962995,
      "grad_norm": 0.3354288935661316,
      "learning_rate": 0.0004901762261131919,
      "loss": 2.3449,
      "step": 60
    },
    {
      "epoch": 0.046674445740956826,
      "grad_norm": 0.30778393149375916,
      "learning_rate": 0.0004890724741967019,
      "loss": 2.3098,
      "step": 70
    },
    {
      "epoch": 0.053342223703950656,
      "grad_norm": 0.30267223715782166,
      "learning_rate": 0.00048796872228021196,
      "loss": 2.3679,
      "step": 80
    },
    {
      "epoch": 0.06001000166694449,
      "grad_norm": 0.3291720449924469,
      "learning_rate": 0.000486864970363722,
      "loss": 2.3324,
      "step": 90
    },
    {
      "epoch": 0.06667777962993832,
      "grad_norm": 0.375013142824173,
      "learning_rate": 0.000485761218447232,
      "loss": 2.3104,
      "step": 100
    },
    {
      "epoch": 0.07334555759293215,
      "grad_norm": 0.3572469651699066,
      "learning_rate": 0.00048465746653074204,
      "loss": 2.3197,
      "step": 110
    },
    {
      "epoch": 0.08001333555592599,
      "grad_norm": 0.37648409605026245,
      "learning_rate": 0.00048355371461425206,
      "loss": 2.3241,
      "step": 120
    },
    {
      "epoch": 0.08668111351891981,
      "grad_norm": 0.363831102848053,
      "learning_rate": 0.00048244996269776214,
      "loss": 2.2681,
      "step": 130
    },
    {
      "epoch": 0.09334889148191365,
      "grad_norm": 0.3490746319293976,
      "learning_rate": 0.00048134621078127217,
      "loss": 2.2953,
      "step": 140
    },
    {
      "epoch": 0.10001666944490749,
      "grad_norm": 0.3801558017730713,
      "learning_rate": 0.00048024245886478225,
      "loss": 2.2824,
      "step": 150
    },
    {
      "epoch": 0.10668444740790131,
      "grad_norm": 0.41457346081733704,
      "learning_rate": 0.00047913870694829227,
      "loss": 2.2437,
      "step": 160
    },
    {
      "epoch": 0.11335222537089515,
      "grad_norm": 0.40101251006126404,
      "learning_rate": 0.0004780349550318023,
      "loss": 2.1887,
      "step": 170
    },
    {
      "epoch": 0.12002000333388899,
      "grad_norm": 0.4046325385570526,
      "learning_rate": 0.0004769312031153123,
      "loss": 2.2301,
      "step": 180
    },
    {
      "epoch": 0.12668778129688282,
      "grad_norm": 0.4163368046283722,
      "learning_rate": 0.00047582745119882235,
      "loss": 2.2126,
      "step": 190
    },
    {
      "epoch": 0.13335555925987663,
      "grad_norm": 0.4491545259952545,
      "learning_rate": 0.00047472369928233237,
      "loss": 2.1764,
      "step": 200
    },
    {
      "epoch": 0.14002333722287047,
      "grad_norm": 0.5436486005783081,
      "learning_rate": 0.00047361994736584245,
      "loss": 2.1962,
      "step": 210
    },
    {
      "epoch": 0.1466911151858643,
      "grad_norm": 0.48802322149276733,
      "learning_rate": 0.0004725161954493525,
      "loss": 2.2234,
      "step": 220
    },
    {
      "epoch": 0.15335889314885814,
      "grad_norm": 0.5077023506164551,
      "learning_rate": 0.0004714124435328625,
      "loss": 2.1669,
      "step": 230
    },
    {
      "epoch": 0.16002667111185198,
      "grad_norm": 0.4407700300216675,
      "learning_rate": 0.0004703086916163726,
      "loss": 2.1421,
      "step": 240
    },
    {
      "epoch": 0.16669444907484582,
      "grad_norm": 0.4613328278064728,
      "learning_rate": 0.0004692049396998826,
      "loss": 2.1357,
      "step": 250
    },
    {
      "epoch": 0.17336222703783963,
      "grad_norm": 0.51104336977005,
      "learning_rate": 0.00046810118778339263,
      "loss": 2.1233,
      "step": 260
    },
    {
      "epoch": 0.18003000500083347,
      "grad_norm": 0.4934702515602112,
      "learning_rate": 0.00046699743586690266,
      "loss": 2.2101,
      "step": 270
    },
    {
      "epoch": 0.1866977829638273,
      "grad_norm": 0.5120450258255005,
      "learning_rate": 0.0004658936839504127,
      "loss": 2.2412,
      "step": 280
    },
    {
      "epoch": 0.19336556092682114,
      "grad_norm": 0.6074406504631042,
      "learning_rate": 0.00046478993203392276,
      "loss": 2.1533,
      "step": 290
    },
    {
      "epoch": 0.20003333888981498,
      "grad_norm": 0.5773027539253235,
      "learning_rate": 0.0004636861801174328,
      "loss": 2.1261,
      "step": 300
    },
    {
      "epoch": 0.2067011168528088,
      "grad_norm": 0.5557341575622559,
      "learning_rate": 0.0004625824282009428,
      "loss": 2.0991,
      "step": 310
    },
    {
      "epoch": 0.21336889481580262,
      "grad_norm": 0.6656454205513,
      "learning_rate": 0.00046147867628445284,
      "loss": 2.1175,
      "step": 320
    },
    {
      "epoch": 0.22003667277879646,
      "grad_norm": 0.6141592264175415,
      "learning_rate": 0.0004603749243679629,
      "loss": 2.0749,
      "step": 330
    },
    {
      "epoch": 0.2267044507417903,
      "grad_norm": 0.7051264643669128,
      "learning_rate": 0.00045927117245147294,
      "loss": 2.0686,
      "step": 340
    },
    {
      "epoch": 0.23337222870478413,
      "grad_norm": 0.6634563207626343,
      "learning_rate": 0.00045816742053498297,
      "loss": 2.053,
      "step": 350
    },
    {
      "epoch": 0.24004000666777797,
      "grad_norm": 0.6022384166717529,
      "learning_rate": 0.00045706366861849305,
      "loss": 2.1324,
      "step": 360
    },
    {
      "epoch": 0.24670778463077178,
      "grad_norm": 0.7179747223854065,
      "learning_rate": 0.0004559599167020031,
      "loss": 2.1128,
      "step": 370
    },
    {
      "epoch": 0.25337556259376565,
      "grad_norm": 0.8377892971038818,
      "learning_rate": 0.0004548561647855131,
      "loss": 2.101,
      "step": 380
    },
    {
      "epoch": 0.2600433405567595,
      "grad_norm": 0.5686925053596497,
      "learning_rate": 0.0004537524128690231,
      "loss": 2.0247,
      "step": 390
    },
    {
      "epoch": 0.26671111851975327,
      "grad_norm": 0.8702573180198669,
      "learning_rate": 0.00045264866095253315,
      "loss": 2.0586,
      "step": 400
    },
    {
      "epoch": 0.2733788964827471,
      "grad_norm": 0.752709686756134,
      "learning_rate": 0.00045154490903604323,
      "loss": 1.9923,
      "step": 410
    },
    {
      "epoch": 0.28004667444574094,
      "grad_norm": 0.7712222933769226,
      "learning_rate": 0.00045044115711955325,
      "loss": 2.0052,
      "step": 420
    },
    {
      "epoch": 0.2867144524087348,
      "grad_norm": 1.2213423252105713,
      "learning_rate": 0.0004493374052030633,
      "loss": 2.0068,
      "step": 430
    },
    {
      "epoch": 0.2933822303717286,
      "grad_norm": 0.8068612217903137,
      "learning_rate": 0.00044823365328657336,
      "loss": 2.0162,
      "step": 440
    },
    {
      "epoch": 0.30005000833472245,
      "grad_norm": 0.7674230337142944,
      "learning_rate": 0.0004471299013700834,
      "loss": 2.0727,
      "step": 450
    },
    {
      "epoch": 0.3067177862977163,
      "grad_norm": 0.7355404496192932,
      "learning_rate": 0.0004460261494535934,
      "loss": 2.0758,
      "step": 460
    },
    {
      "epoch": 0.3133855642607101,
      "grad_norm": 0.729027509689331,
      "learning_rate": 0.00044492239753710344,
      "loss": 2.0691,
      "step": 470
    },
    {
      "epoch": 0.32005334222370396,
      "grad_norm": 0.7061858773231506,
      "learning_rate": 0.00044381864562061346,
      "loss": 1.9477,
      "step": 480
    },
    {
      "epoch": 0.3267211201866978,
      "grad_norm": 1.3534601926803589,
      "learning_rate": 0.0004427148937041235,
      "loss": 1.9939,
      "step": 490
    },
    {
      "epoch": 0.33338889814969164,
      "grad_norm": 1.018064022064209,
      "learning_rate": 0.00044161114178763357,
      "loss": 2.0054,
      "step": 500
    },
    {
      "epoch": 0.3400566761126855,
      "grad_norm": 0.6604948043823242,
      "learning_rate": 0.0004405073898711436,
      "loss": 2.0191,
      "step": 510
    },
    {
      "epoch": 0.34672445407567926,
      "grad_norm": 0.7789267301559448,
      "learning_rate": 0.00043940363795465367,
      "loss": 1.9264,
      "step": 520
    },
    {
      "epoch": 0.3533922320386731,
      "grad_norm": 0.7828433513641357,
      "learning_rate": 0.0004382998860381637,
      "loss": 1.9416,
      "step": 530
    },
    {
      "epoch": 0.36006001000166693,
      "grad_norm": 0.8587573766708374,
      "learning_rate": 0.0004371961341216737,
      "loss": 1.8801,
      "step": 540
    },
    {
      "epoch": 0.36672778796466077,
      "grad_norm": 0.7142714262008667,
      "learning_rate": 0.00043609238220518375,
      "loss": 1.9295,
      "step": 550
    },
    {
      "epoch": 0.3733955659276546,
      "grad_norm": 0.7834444642066956,
      "learning_rate": 0.00043498863028869377,
      "loss": 1.7971,
      "step": 560
    },
    {
      "epoch": 0.38006334389064844,
      "grad_norm": 1.0773651599884033,
      "learning_rate": 0.0004338848783722038,
      "loss": 1.9608,
      "step": 570
    },
    {
      "epoch": 0.3867311218536423,
      "grad_norm": 0.6663276553153992,
      "learning_rate": 0.0004327811264557138,
      "loss": 1.9764,
      "step": 580
    },
    {
      "epoch": 0.3933988998166361,
      "grad_norm": 0.6108376979827881,
      "learning_rate": 0.0004316773745392239,
      "loss": 1.9635,
      "step": 590
    },
    {
      "epoch": 0.40006667777962995,
      "grad_norm": 0.9020128846168518,
      "learning_rate": 0.000430573622622734,
      "loss": 1.8201,
      "step": 600
    },
    {
      "epoch": 0.4067344557426238,
      "grad_norm": 0.6975194215774536,
      "learning_rate": 0.000429469870706244,
      "loss": 1.9557,
      "step": 610
    },
    {
      "epoch": 0.4134022337056176,
      "grad_norm": 0.8565027713775635,
      "learning_rate": 0.00042836611878975403,
      "loss": 1.8345,
      "step": 620
    },
    {
      "epoch": 0.4200700116686114,
      "grad_norm": 0.9498487114906311,
      "learning_rate": 0.00042726236687326406,
      "loss": 1.9147,
      "step": 630
    },
    {
      "epoch": 0.42673778963160525,
      "grad_norm": 1.1041120290756226,
      "learning_rate": 0.0004261586149567741,
      "loss": 1.8499,
      "step": 640
    },
    {
      "epoch": 0.4334055675945991,
      "grad_norm": 0.9846899509429932,
      "learning_rate": 0.0004250548630402841,
      "loss": 1.8739,
      "step": 650
    },
    {
      "epoch": 0.4400733455575929,
      "grad_norm": 0.7740341424942017,
      "learning_rate": 0.00042395111112379413,
      "loss": 1.9281,
      "step": 660
    },
    {
      "epoch": 0.44674112352058676,
      "grad_norm": 1.1843382120132446,
      "learning_rate": 0.00042284735920730427,
      "loss": 1.8287,
      "step": 670
    },
    {
      "epoch": 0.4534089014835806,
      "grad_norm": 0.7381002306938171,
      "learning_rate": 0.0004217436072908143,
      "loss": 1.9103,
      "step": 680
    },
    {
      "epoch": 0.46007667944657443,
      "grad_norm": 0.9984520077705383,
      "learning_rate": 0.0004206398553743243,
      "loss": 1.801,
      "step": 690
    },
    {
      "epoch": 0.46674445740956827,
      "grad_norm": 1.22793710231781,
      "learning_rate": 0.00041953610345783434,
      "loss": 1.8013,
      "step": 700
    },
    {
      "epoch": 0.4734122353725621,
      "grad_norm": 1.1463775634765625,
      "learning_rate": 0.00041843235154134437,
      "loss": 1.8356,
      "step": 710
    },
    {
      "epoch": 0.48008001333555594,
      "grad_norm": 0.9491240382194519,
      "learning_rate": 0.0004173285996248544,
      "loss": 1.7671,
      "step": 720
    },
    {
      "epoch": 0.4867477912985498,
      "grad_norm": 1.2519596815109253,
      "learning_rate": 0.0004162248477083644,
      "loss": 1.9252,
      "step": 730
    },
    {
      "epoch": 0.49341556926154356,
      "grad_norm": 1.2942570447921753,
      "learning_rate": 0.00041512109579187444,
      "loss": 1.8699,
      "step": 740
    },
    {
      "epoch": 0.5000833472245374,
      "grad_norm": 0.8312822580337524,
      "learning_rate": 0.0004140173438753845,
      "loss": 1.7944,
      "step": 750
    },
    {
      "epoch": 0.5067511251875313,
      "grad_norm": 0.8401555418968201,
      "learning_rate": 0.0004129135919588946,
      "loss": 1.8792,
      "step": 760
    },
    {
      "epoch": 0.5134189031505251,
      "grad_norm": 0.8980400562286377,
      "learning_rate": 0.00041180984004240463,
      "loss": 1.7827,
      "step": 770
    },
    {
      "epoch": 0.520086681113519,
      "grad_norm": 0.9674390554428101,
      "learning_rate": 0.00041070608812591465,
      "loss": 1.8156,
      "step": 780
    },
    {
      "epoch": 0.5267544590765127,
      "grad_norm": 0.8404424786567688,
      "learning_rate": 0.0004096023362094247,
      "loss": 1.8289,
      "step": 790
    },
    {
      "epoch": 0.5334222370395065,
      "grad_norm": 0.8858083486557007,
      "learning_rate": 0.0004084985842929347,
      "loss": 1.8356,
      "step": 800
    },
    {
      "epoch": 0.5400900150025004,
      "grad_norm": 0.7147239446640015,
      "learning_rate": 0.00040739483237644473,
      "loss": 1.9514,
      "step": 810
    },
    {
      "epoch": 0.5467577929654942,
      "grad_norm": 0.8631763458251953,
      "learning_rate": 0.00040629108045995476,
      "loss": 1.7131,
      "step": 820
    },
    {
      "epoch": 0.5534255709284881,
      "grad_norm": 0.7395562529563904,
      "learning_rate": 0.00040518732854346484,
      "loss": 1.7904,
      "step": 830
    },
    {
      "epoch": 0.5600933488914819,
      "grad_norm": 1.0722081661224365,
      "learning_rate": 0.0004040835766269749,
      "loss": 1.8227,
      "step": 840
    },
    {
      "epoch": 0.5667611268544758,
      "grad_norm": 0.937628984451294,
      "learning_rate": 0.00040297982471048494,
      "loss": 1.7807,
      "step": 850
    },
    {
      "epoch": 0.5734289048174696,
      "grad_norm": 1.3065000772476196,
      "learning_rate": 0.00040187607279399497,
      "loss": 1.7885,
      "step": 860
    },
    {
      "epoch": 0.5800966827804634,
      "grad_norm": 0.8097478747367859,
      "learning_rate": 0.000400772320877505,
      "loss": 1.8716,
      "step": 870
    },
    {
      "epoch": 0.5867644607434572,
      "grad_norm": 1.2368093729019165,
      "learning_rate": 0.000399668568961015,
      "loss": 1.5951,
      "step": 880
    },
    {
      "epoch": 0.5934322387064511,
      "grad_norm": 0.9164105653762817,
      "learning_rate": 0.00039856481704452504,
      "loss": 1.7899,
      "step": 890
    },
    {
      "epoch": 0.6001000166694449,
      "grad_norm": 0.8113130331039429,
      "learning_rate": 0.00039746106512803507,
      "loss": 1.7813,
      "step": 900
    },
    {
      "epoch": 0.6067677946324388,
      "grad_norm": 0.9050865173339844,
      "learning_rate": 0.00039635731321154515,
      "loss": 1.7732,
      "step": 910
    },
    {
      "epoch": 0.6134355725954326,
      "grad_norm": 1.1895756721496582,
      "learning_rate": 0.00039525356129505517,
      "loss": 1.8314,
      "step": 920
    },
    {
      "epoch": 0.6201033505584264,
      "grad_norm": 0.9630951285362244,
      "learning_rate": 0.00039414980937856525,
      "loss": 1.8531,
      "step": 930
    },
    {
      "epoch": 0.6267711285214203,
      "grad_norm": 1.0646036863327026,
      "learning_rate": 0.0003930460574620753,
      "loss": 1.7798,
      "step": 940
    },
    {
      "epoch": 0.633438906484414,
      "grad_norm": 1.190650224685669,
      "learning_rate": 0.0003919423055455853,
      "loss": 1.7829,
      "step": 950
    },
    {
      "epoch": 0.6401066844474079,
      "grad_norm": 1.0140583515167236,
      "learning_rate": 0.00039083855362909533,
      "loss": 1.7839,
      "step": 960
    },
    {
      "epoch": 0.6467744624104017,
      "grad_norm": 1.0035662651062012,
      "learning_rate": 0.00038973480171260535,
      "loss": 1.7684,
      "step": 970
    },
    {
      "epoch": 0.6534422403733956,
      "grad_norm": 1.1440268754959106,
      "learning_rate": 0.00038863104979611543,
      "loss": 1.7059,
      "step": 980
    },
    {
      "epoch": 0.6601100183363894,
      "grad_norm": 0.9033438563346863,
      "learning_rate": 0.00038752729787962546,
      "loss": 1.7092,
      "step": 990
    },
    {
      "epoch": 0.6667777962993833,
      "grad_norm": 1.2422869205474854,
      "learning_rate": 0.0003864235459631355,
      "loss": 1.8333,
      "step": 1000
    },
    {
      "epoch": 0.6734455742623771,
      "grad_norm": 1.195799469947815,
      "learning_rate": 0.0003853197940466455,
      "loss": 1.5966,
      "step": 1010
    },
    {
      "epoch": 0.680113352225371,
      "grad_norm": 1.0837039947509766,
      "learning_rate": 0.0003842160421301556,
      "loss": 1.7283,
      "step": 1020
    },
    {
      "epoch": 0.6867811301883647,
      "grad_norm": 1.2833117246627808,
      "learning_rate": 0.0003831122902136656,
      "loss": 1.7621,
      "step": 1030
    },
    {
      "epoch": 0.6934489081513585,
      "grad_norm": 0.8453885912895203,
      "learning_rate": 0.00038200853829717564,
      "loss": 1.6927,
      "step": 1040
    },
    {
      "epoch": 0.7001166861143524,
      "grad_norm": 1.1434754133224487,
      "learning_rate": 0.00038090478638068566,
      "loss": 1.6393,
      "step": 1050
    },
    {
      "epoch": 0.7067844640773462,
      "grad_norm": 1.170704960823059,
      "learning_rate": 0.00037980103446419574,
      "loss": 1.6075,
      "step": 1060
    },
    {
      "epoch": 0.7134522420403401,
      "grad_norm": 0.930669903755188,
      "learning_rate": 0.00037869728254770577,
      "loss": 1.7789,
      "step": 1070
    },
    {
      "epoch": 0.7201200200033339,
      "grad_norm": 1.3200575113296509,
      "learning_rate": 0.0003775935306312158,
      "loss": 1.6895,
      "step": 1080
    },
    {
      "epoch": 0.7267877979663278,
      "grad_norm": 0.7914977669715881,
      "learning_rate": 0.0003764897787147258,
      "loss": 1.7338,
      "step": 1090
    },
    {
      "epoch": 0.7334555759293215,
      "grad_norm": 1.087486982345581,
      "learning_rate": 0.00037538602679823584,
      "loss": 1.689,
      "step": 1100
    },
    {
      "epoch": 0.7401233538923154,
      "grad_norm": 1.1175594329833984,
      "learning_rate": 0.0003742822748817459,
      "loss": 1.6476,
      "step": 1110
    },
    {
      "epoch": 0.7467911318553092,
      "grad_norm": 1.1805223226547241,
      "learning_rate": 0.00037317852296525595,
      "loss": 1.8011,
      "step": 1120
    },
    {
      "epoch": 0.7534589098183031,
      "grad_norm": 1.1874735355377197,
      "learning_rate": 0.000372074771048766,
      "loss": 1.6946,
      "step": 1130
    },
    {
      "epoch": 0.7601266877812969,
      "grad_norm": 1.3603451251983643,
      "learning_rate": 0.00037097101913227605,
      "loss": 1.6608,
      "step": 1140
    },
    {
      "epoch": 0.7667944657442907,
      "grad_norm": 1.0343505144119263,
      "learning_rate": 0.0003698672672157861,
      "loss": 1.6537,
      "step": 1150
    },
    {
      "epoch": 0.7734622437072846,
      "grad_norm": 1.009771704673767,
      "learning_rate": 0.0003687635152992961,
      "loss": 1.7156,
      "step": 1160
    },
    {
      "epoch": 0.7801300216702783,
      "grad_norm": 1.2431782484054565,
      "learning_rate": 0.00036765976338280613,
      "loss": 1.6393,
      "step": 1170
    },
    {
      "epoch": 0.7867977996332722,
      "grad_norm": 0.8554706573486328,
      "learning_rate": 0.00036655601146631616,
      "loss": 1.7213,
      "step": 1180
    },
    {
      "epoch": 0.793465577596266,
      "grad_norm": 1.181684970855713,
      "learning_rate": 0.00036545225954982624,
      "loss": 1.7232,
      "step": 1190
    },
    {
      "epoch": 0.8001333555592599,
      "grad_norm": 1.4261826276779175,
      "learning_rate": 0.00036434850763333626,
      "loss": 1.6627,
      "step": 1200
    },
    {
      "epoch": 0.8068011335222537,
      "grad_norm": 1.2761592864990234,
      "learning_rate": 0.0003632447557168463,
      "loss": 1.7665,
      "step": 1210
    },
    {
      "epoch": 0.8134689114852476,
      "grad_norm": 1.1856061220169067,
      "learning_rate": 0.00036214100380035637,
      "loss": 1.5741,
      "step": 1220
    },
    {
      "epoch": 0.8201366894482414,
      "grad_norm": 2.168689727783203,
      "learning_rate": 0.0003610372518838664,
      "loss": 1.7555,
      "step": 1230
    },
    {
      "epoch": 0.8268044674112353,
      "grad_norm": 1.2866963148117065,
      "learning_rate": 0.0003599334999673764,
      "loss": 1.6357,
      "step": 1240
    },
    {
      "epoch": 0.833472245374229,
      "grad_norm": 1.1019960641860962,
      "learning_rate": 0.00035882974805088644,
      "loss": 1.734,
      "step": 1250
    },
    {
      "epoch": 0.8401400233372228,
      "grad_norm": 1.2381579875946045,
      "learning_rate": 0.00035772599613439647,
      "loss": 1.7083,
      "step": 1260
    },
    {
      "epoch": 0.8468078013002167,
      "grad_norm": 1.2782669067382812,
      "learning_rate": 0.0003566222442179065,
      "loss": 1.5589,
      "step": 1270
    },
    {
      "epoch": 0.8534755792632105,
      "grad_norm": 1.4414244890213013,
      "learning_rate": 0.00035551849230141657,
      "loss": 1.6856,
      "step": 1280
    },
    {
      "epoch": 0.8601433572262044,
      "grad_norm": 1.265418291091919,
      "learning_rate": 0.00035441474038492665,
      "loss": 1.6974,
      "step": 1290
    },
    {
      "epoch": 0.8668111351891982,
      "grad_norm": 1.1418180465698242,
      "learning_rate": 0.0003533109884684367,
      "loss": 1.6147,
      "step": 1300
    },
    {
      "epoch": 0.8734789131521921,
      "grad_norm": 1.1747113466262817,
      "learning_rate": 0.0003522072365519467,
      "loss": 1.6454,
      "step": 1310
    },
    {
      "epoch": 0.8801466911151858,
      "grad_norm": 1.3638173341751099,
      "learning_rate": 0.00035110348463545673,
      "loss": 1.6506,
      "step": 1320
    },
    {
      "epoch": 0.8868144690781797,
      "grad_norm": 1.449527621269226,
      "learning_rate": 0.00034999973271896675,
      "loss": 1.6299,
      "step": 1330
    },
    {
      "epoch": 0.8934822470411735,
      "grad_norm": 0.8228742480278015,
      "learning_rate": 0.0003488959808024768,
      "loss": 1.6681,
      "step": 1340
    },
    {
      "epoch": 0.9001500250041674,
      "grad_norm": 1.1635380983352661,
      "learning_rate": 0.0003477922288859868,
      "loss": 1.614,
      "step": 1350
    },
    {
      "epoch": 0.9068178029671612,
      "grad_norm": 0.944756805896759,
      "learning_rate": 0.00034668847696949683,
      "loss": 1.6658,
      "step": 1360
    },
    {
      "epoch": 0.913485580930155,
      "grad_norm": 0.8690886497497559,
      "learning_rate": 0.00034558472505300696,
      "loss": 1.5427,
      "step": 1370
    },
    {
      "epoch": 0.9201533588931489,
      "grad_norm": 1.0065807104110718,
      "learning_rate": 0.000344480973136517,
      "loss": 1.6729,
      "step": 1380
    },
    {
      "epoch": 0.9268211368561426,
      "grad_norm": 1.1034810543060303,
      "learning_rate": 0.000343377221220027,
      "loss": 1.7947,
      "step": 1390
    },
    {
      "epoch": 0.9334889148191365,
      "grad_norm": 1.2048641443252563,
      "learning_rate": 0.00034227346930353704,
      "loss": 1.6801,
      "step": 1400
    },
    {
      "epoch": 0.9401566927821303,
      "grad_norm": 1.3118748664855957,
      "learning_rate": 0.00034116971738704706,
      "loss": 1.7159,
      "step": 1410
    },
    {
      "epoch": 0.9468244707451242,
      "grad_norm": 1.1553479433059692,
      "learning_rate": 0.0003400659654705571,
      "loss": 1.6451,
      "step": 1420
    },
    {
      "epoch": 0.953492248708118,
      "grad_norm": 1.0561102628707886,
      "learning_rate": 0.0003389622135540671,
      "loss": 1.7756,
      "step": 1430
    },
    {
      "epoch": 0.9601600266711119,
      "grad_norm": 1.457319974899292,
      "learning_rate": 0.00033785846163757714,
      "loss": 1.6479,
      "step": 1440
    },
    {
      "epoch": 0.9668278046341057,
      "grad_norm": 1.4760850667953491,
      "learning_rate": 0.0003367547097210873,
      "loss": 1.5835,
      "step": 1450
    },
    {
      "epoch": 0.9734955825970996,
      "grad_norm": 1.1784261465072632,
      "learning_rate": 0.0003356509578045973,
      "loss": 1.545,
      "step": 1460
    },
    {
      "epoch": 0.9801633605600933,
      "grad_norm": 1.266062617301941,
      "learning_rate": 0.0003345472058881073,
      "loss": 1.614,
      "step": 1470
    },
    {
      "epoch": 0.9868311385230871,
      "grad_norm": 1.1486141681671143,
      "learning_rate": 0.00033344345397161735,
      "loss": 1.6318,
      "step": 1480
    },
    {
      "epoch": 0.993498916486081,
      "grad_norm": 1.3418781757354736,
      "learning_rate": 0.0003323397020551274,
      "loss": 1.5895,
      "step": 1490
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.3654195070266724,
      "learning_rate": 0.0003312359501386374,
      "loss": 1.6888,
      "step": 1500
    },
    {
      "epoch": 1.0,
      "eval_loss": 1.5930211544036865,
      "eval_runtime": 938.1989,
      "eval_samples_per_second": 6.394,
      "eval_steps_per_second": 3.198,
      "step": 1500
    },
    {
      "epoch": 1.0066677779629938,
      "grad_norm": 1.212441086769104,
      "learning_rate": 0.0003301321982221474,
      "loss": 1.5128,
      "step": 1510
    },
    {
      "epoch": 1.0133355559259876,
      "grad_norm": 1.0178881883621216,
      "learning_rate": 0.00032902844630565745,
      "loss": 1.5924,
      "step": 1520
    },
    {
      "epoch": 1.0200033338889816,
      "grad_norm": 1.2381683588027954,
      "learning_rate": 0.00032792469438916753,
      "loss": 1.6478,
      "step": 1530
    },
    {
      "epoch": 1.0266711118519753,
      "grad_norm": 1.0418988466262817,
      "learning_rate": 0.0003268209424726776,
      "loss": 1.6691,
      "step": 1540
    },
    {
      "epoch": 1.0333388898149691,
      "grad_norm": 1.0978997945785522,
      "learning_rate": 0.00032571719055618764,
      "loss": 1.6705,
      "step": 1550
    },
    {
      "epoch": 1.040006667777963,
      "grad_norm": 1.1583325862884521,
      "learning_rate": 0.00032461343863969766,
      "loss": 1.5875,
      "step": 1560
    },
    {
      "epoch": 1.046674445740957,
      "grad_norm": 0.9735357165336609,
      "learning_rate": 0.0003235096867232077,
      "loss": 1.6549,
      "step": 1570
    },
    {
      "epoch": 1.0533422237039507,
      "grad_norm": 1.2380609512329102,
      "learning_rate": 0.0003224059348067177,
      "loss": 1.5199,
      "step": 1580
    },
    {
      "epoch": 1.0600100016669445,
      "grad_norm": 1.2327306270599365,
      "learning_rate": 0.00032130218289022774,
      "loss": 1.4891,
      "step": 1590
    },
    {
      "epoch": 1.0666777796299383,
      "grad_norm": 0.9845584630966187,
      "learning_rate": 0.0003201984309737378,
      "loss": 1.6435,
      "step": 1600
    },
    {
      "epoch": 1.073345557592932,
      "grad_norm": 1.0881918668746948,
      "learning_rate": 0.00031909467905724784,
      "loss": 1.4872,
      "step": 1610
    },
    {
      "epoch": 1.080013335555926,
      "grad_norm": 1.196982502937317,
      "learning_rate": 0.0003179909271407579,
      "loss": 1.6429,
      "step": 1620
    },
    {
      "epoch": 1.0866811135189198,
      "grad_norm": 1.0455881357192993,
      "learning_rate": 0.00031688717522426795,
      "loss": 1.6108,
      "step": 1630
    },
    {
      "epoch": 1.0933488914819136,
      "grad_norm": 1.1447169780731201,
      "learning_rate": 0.00031578342330777797,
      "loss": 1.541,
      "step": 1640
    },
    {
      "epoch": 1.1000166694449074,
      "grad_norm": 1.4076390266418457,
      "learning_rate": 0.000314679671391288,
      "loss": 1.5962,
      "step": 1650
    },
    {
      "epoch": 1.1066844474079014,
      "grad_norm": 1.3391822576522827,
      "learning_rate": 0.000313575919474798,
      "loss": 1.5355,
      "step": 1660
    },
    {
      "epoch": 1.1133522253708952,
      "grad_norm": 1.0729671716690063,
      "learning_rate": 0.00031247216755830805,
      "loss": 1.5014,
      "step": 1670
    },
    {
      "epoch": 1.120020003333889,
      "grad_norm": 1.1348241567611694,
      "learning_rate": 0.00031136841564181813,
      "loss": 1.571,
      "step": 1680
    },
    {
      "epoch": 1.1266877812968827,
      "grad_norm": 1.1546714305877686,
      "learning_rate": 0.00031026466372532815,
      "loss": 1.5306,
      "step": 1690
    },
    {
      "epoch": 1.1333555592598765,
      "grad_norm": 1.580174207687378,
      "learning_rate": 0.0003091609118088382,
      "loss": 1.463,
      "step": 1700
    },
    {
      "epoch": 1.1400233372228705,
      "grad_norm": 1.4207639694213867,
      "learning_rate": 0.00030805715989234826,
      "loss": 1.4998,
      "step": 1710
    },
    {
      "epoch": 1.1466911151858643,
      "grad_norm": 1.071132779121399,
      "learning_rate": 0.0003069534079758583,
      "loss": 1.5512,
      "step": 1720
    },
    {
      "epoch": 1.153358893148858,
      "grad_norm": 1.0608623027801514,
      "learning_rate": 0.0003058496560593683,
      "loss": 1.4313,
      "step": 1730
    },
    {
      "epoch": 1.160026671111852,
      "grad_norm": 1.3176823854446411,
      "learning_rate": 0.00030474590414287833,
      "loss": 1.6169,
      "step": 1740
    },
    {
      "epoch": 1.1666944490748459,
      "grad_norm": 1.2820123434066772,
      "learning_rate": 0.00030364215222638836,
      "loss": 1.4095,
      "step": 1750
    },
    {
      "epoch": 1.1733622270378397,
      "grad_norm": 1.0559353828430176,
      "learning_rate": 0.00030253840030989844,
      "loss": 1.4982,
      "step": 1760
    },
    {
      "epoch": 1.1800300050008334,
      "grad_norm": 1.2750744819641113,
      "learning_rate": 0.00030143464839340846,
      "loss": 1.5402,
      "step": 1770
    },
    {
      "epoch": 1.1866977829638272,
      "grad_norm": 1.401488184928894,
      "learning_rate": 0.0003003308964769185,
      "loss": 1.4763,
      "step": 1780
    },
    {
      "epoch": 1.1933655609268212,
      "grad_norm": 1.4198291301727295,
      "learning_rate": 0.0002992271445604285,
      "loss": 1.4482,
      "step": 1790
    },
    {
      "epoch": 1.200033338889815,
      "grad_norm": 0.9789420366287231,
      "learning_rate": 0.0002981233926439386,
      "loss": 1.556,
      "step": 1800
    },
    {
      "epoch": 1.2067011168528088,
      "grad_norm": 1.0822995901107788,
      "learning_rate": 0.0002970196407274486,
      "loss": 1.5644,
      "step": 1810
    },
    {
      "epoch": 1.2133688948158026,
      "grad_norm": 1.0520963668823242,
      "learning_rate": 0.00029591588881095865,
      "loss": 1.4432,
      "step": 1820
    },
    {
      "epoch": 1.2200366727787966,
      "grad_norm": 1.0085232257843018,
      "learning_rate": 0.00029481213689446867,
      "loss": 1.5587,
      "step": 1830
    },
    {
      "epoch": 1.2267044507417904,
      "grad_norm": 1.4205974340438843,
      "learning_rate": 0.00029370838497797875,
      "loss": 1.5469,
      "step": 1840
    },
    {
      "epoch": 1.2333722287047841,
      "grad_norm": 1.2861886024475098,
      "learning_rate": 0.0002926046330614888,
      "loss": 1.5153,
      "step": 1850
    },
    {
      "epoch": 1.240040006667778,
      "grad_norm": 1.7731550931930542,
      "learning_rate": 0.0002915008811449988,
      "loss": 1.5311,
      "step": 1860
    },
    {
      "epoch": 1.2467077846307717,
      "grad_norm": 1.2095922231674194,
      "learning_rate": 0.0002903971292285088,
      "loss": 1.5138,
      "step": 1870
    },
    {
      "epoch": 1.2533755625937657,
      "grad_norm": 1.2214069366455078,
      "learning_rate": 0.0002892933773120189,
      "loss": 1.5662,
      "step": 1880
    },
    {
      "epoch": 1.2600433405567595,
      "grad_norm": 1.236879587173462,
      "learning_rate": 0.00028818962539552893,
      "loss": 1.5466,
      "step": 1890
    },
    {
      "epoch": 1.2667111185197533,
      "grad_norm": 1.2235287427902222,
      "learning_rate": 0.00028708587347903896,
      "loss": 1.6167,
      "step": 1900
    },
    {
      "epoch": 1.273378896482747,
      "grad_norm": 1.3805121183395386,
      "learning_rate": 0.00028598212156254904,
      "loss": 1.5474,
      "step": 1910
    },
    {
      "epoch": 1.280046674445741,
      "grad_norm": 1.2381101846694946,
      "learning_rate": 0.00028487836964605906,
      "loss": 1.6037,
      "step": 1920
    },
    {
      "epoch": 1.2867144524087348,
      "grad_norm": 1.3824387788772583,
      "learning_rate": 0.0002837746177295691,
      "loss": 1.6041,
      "step": 1930
    },
    {
      "epoch": 1.2933822303717286,
      "grad_norm": 1.3052667379379272,
      "learning_rate": 0.0002826708658130791,
      "loss": 1.3805,
      "step": 1940
    },
    {
      "epoch": 1.3000500083347224,
      "grad_norm": 1.5646350383758545,
      "learning_rate": 0.00028156711389658914,
      "loss": 1.4454,
      "step": 1950
    },
    {
      "epoch": 1.3067177862977162,
      "grad_norm": 1.4699522256851196,
      "learning_rate": 0.00028046336198009916,
      "loss": 1.5261,
      "step": 1960
    },
    {
      "epoch": 1.3133855642607102,
      "grad_norm": 1.5416187047958374,
      "learning_rate": 0.00027935961006360924,
      "loss": 1.4347,
      "step": 1970
    },
    {
      "epoch": 1.320053342223704,
      "grad_norm": 1.109609842300415,
      "learning_rate": 0.00027825585814711927,
      "loss": 1.5851,
      "step": 1980
    },
    {
      "epoch": 1.3267211201866977,
      "grad_norm": 1.0119777917861938,
      "learning_rate": 0.00027715210623062935,
      "loss": 1.4075,
      "step": 1990
    },
    {
      "epoch": 1.3333888981496917,
      "grad_norm": 1.4908361434936523,
      "learning_rate": 0.00027604835431413937,
      "loss": 1.4974,
      "step": 2000
    },
    {
      "epoch": 1.3400566761126855,
      "grad_norm": 1.348185420036316,
      "learning_rate": 0.0002749446023976494,
      "loss": 1.5077,
      "step": 2010
    },
    {
      "epoch": 1.3467244540756793,
      "grad_norm": 1.263907551765442,
      "learning_rate": 0.0002738408504811594,
      "loss": 1.3956,
      "step": 2020
    },
    {
      "epoch": 1.353392232038673,
      "grad_norm": 1.074822187423706,
      "learning_rate": 0.00027273709856466945,
      "loss": 1.4633,
      "step": 2030
    },
    {
      "epoch": 1.3600600100016669,
      "grad_norm": 0.9264746308326721,
      "learning_rate": 0.0002716333466481795,
      "loss": 1.5408,
      "step": 2040
    },
    {
      "epoch": 1.3667277879646607,
      "grad_norm": 1.425510048866272,
      "learning_rate": 0.0002705295947316895,
      "loss": 1.3325,
      "step": 2050
    },
    {
      "epoch": 1.3733955659276547,
      "grad_norm": 1.5894010066986084,
      "learning_rate": 0.0002694258428151996,
      "loss": 1.443,
      "step": 2060
    },
    {
      "epoch": 1.3800633438906484,
      "grad_norm": 1.4949331283569336,
      "learning_rate": 0.00026832209089870966,
      "loss": 1.5252,
      "step": 2070
    },
    {
      "epoch": 1.3867311218536422,
      "grad_norm": 1.2945579290390015,
      "learning_rate": 0.0002672183389822197,
      "loss": 1.5325,
      "step": 2080
    },
    {
      "epoch": 1.3933988998166362,
      "grad_norm": 1.7050824165344238,
      "learning_rate": 0.0002661145870657297,
      "loss": 1.4145,
      "step": 2090
    },
    {
      "epoch": 1.40006667777963,
      "grad_norm": 1.2154841423034668,
      "learning_rate": 0.00026501083514923973,
      "loss": 1.58,
      "step": 2100
    },
    {
      "epoch": 1.4067344557426238,
      "grad_norm": 1.3871607780456543,
      "learning_rate": 0.00026390708323274976,
      "loss": 1.5199,
      "step": 2110
    },
    {
      "epoch": 1.4134022337056176,
      "grad_norm": 1.5203598737716675,
      "learning_rate": 0.0002628033313162598,
      "loss": 1.4064,
      "step": 2120
    },
    {
      "epoch": 1.4200700116686114,
      "grad_norm": 1.3564361333847046,
      "learning_rate": 0.0002616995793997698,
      "loss": 1.5553,
      "step": 2130
    },
    {
      "epoch": 1.4267377896316051,
      "grad_norm": 1.58589768409729,
      "learning_rate": 0.0002605958274832799,
      "loss": 1.4486,
      "step": 2140
    },
    {
      "epoch": 1.4334055675945991,
      "grad_norm": 1.193699836730957,
      "learning_rate": 0.00025949207556678997,
      "loss": 1.5325,
      "step": 2150
    },
    {
      "epoch": 1.440073345557593,
      "grad_norm": 1.3817960023880005,
      "learning_rate": 0.0002583883236503,
      "loss": 1.4924,
      "step": 2160
    },
    {
      "epoch": 1.4467411235205867,
      "grad_norm": 1.1539111137390137,
      "learning_rate": 0.00025728457173381,
      "loss": 1.5976,
      "step": 2170
    },
    {
      "epoch": 1.4534089014835807,
      "grad_norm": 1.2973233461380005,
      "learning_rate": 0.00025618081981732005,
      "loss": 1.3865,
      "step": 2180
    },
    {
      "epoch": 1.4600766794465745,
      "grad_norm": 1.5662312507629395,
      "learning_rate": 0.00025507706790083007,
      "loss": 1.4248,
      "step": 2190
    },
    {
      "epoch": 1.4667444574095683,
      "grad_norm": 1.2459412813186646,
      "learning_rate": 0.0002539733159843401,
      "loss": 1.6448,
      "step": 2200
    },
    {
      "epoch": 1.473412235372562,
      "grad_norm": 1.6254868507385254,
      "learning_rate": 0.0002528695640678501,
      "loss": 1.5264,
      "step": 2210
    },
    {
      "epoch": 1.4800800133355558,
      "grad_norm": 1.3158515691757202,
      "learning_rate": 0.0002517658121513602,
      "loss": 1.4238,
      "step": 2220
    },
    {
      "epoch": 1.4867477912985498,
      "grad_norm": 1.0877175331115723,
      "learning_rate": 0.0002506620602348703,
      "loss": 1.5153,
      "step": 2230
    },
    {
      "epoch": 1.4934155692615436,
      "grad_norm": 1.0539546012878418,
      "learning_rate": 0.0002495583083183803,
      "loss": 1.4639,
      "step": 2240
    },
    {
      "epoch": 1.5000833472245374,
      "grad_norm": 1.1903051137924194,
      "learning_rate": 0.00024845455640189033,
      "loss": 1.5264,
      "step": 2250
    },
    {
      "epoch": 1.5067511251875314,
      "grad_norm": 1.1364223957061768,
      "learning_rate": 0.00024735080448540036,
      "loss": 1.4561,
      "step": 2260
    },
    {
      "epoch": 1.5134189031505252,
      "grad_norm": 1.596967339515686,
      "learning_rate": 0.0002462470525689104,
      "loss": 1.5703,
      "step": 2270
    },
    {
      "epoch": 1.520086681113519,
      "grad_norm": 1.5043662786483765,
      "learning_rate": 0.00024514330065242046,
      "loss": 1.4308,
      "step": 2280
    },
    {
      "epoch": 1.5267544590765127,
      "grad_norm": 1.5515702962875366,
      "learning_rate": 0.0002440395487359305,
      "loss": 1.4989,
      "step": 2290
    },
    {
      "epoch": 1.5334222370395065,
      "grad_norm": 1.1743919849395752,
      "learning_rate": 0.0002429357968194405,
      "loss": 1.4894,
      "step": 2300
    },
    {
      "epoch": 1.5400900150025003,
      "grad_norm": 1.45949387550354,
      "learning_rate": 0.00024183204490295054,
      "loss": 1.5917,
      "step": 2310
    },
    {
      "epoch": 1.546757792965494,
      "grad_norm": 1.2117105722427368,
      "learning_rate": 0.0002407282929864606,
      "loss": 1.6046,
      "step": 2320
    },
    {
      "epoch": 1.553425570928488,
      "grad_norm": 1.2004117965698242,
      "learning_rate": 0.00023962454106997061,
      "loss": 1.5785,
      "step": 2330
    },
    {
      "epoch": 1.5600933488914819,
      "grad_norm": 1.3682056665420532,
      "learning_rate": 0.00023852078915348067,
      "loss": 1.5222,
      "step": 2340
    },
    {
      "epoch": 1.5667611268544759,
      "grad_norm": 1.1752504110336304,
      "learning_rate": 0.0002374170372369907,
      "loss": 1.3221,
      "step": 2350
    },
    {
      "epoch": 1.5734289048174697,
      "grad_norm": 1.6234010457992554,
      "learning_rate": 0.00023631328532050075,
      "loss": 1.4113,
      "step": 2360
    },
    {
      "epoch": 1.5800966827804634,
      "grad_norm": 1.026540994644165,
      "learning_rate": 0.00023520953340401077,
      "loss": 1.3794,
      "step": 2370
    },
    {
      "epoch": 1.5867644607434572,
      "grad_norm": 1.4109708070755005,
      "learning_rate": 0.00023410578148752082,
      "loss": 1.376,
      "step": 2380
    },
    {
      "epoch": 1.593432238706451,
      "grad_norm": 1.49703848361969,
      "learning_rate": 0.00023300202957103085,
      "loss": 1.4967,
      "step": 2390
    },
    {
      "epoch": 1.6001000166694448,
      "grad_norm": 1.233249306678772,
      "learning_rate": 0.0002318982776545409,
      "loss": 1.4557,
      "step": 2400
    },
    {
      "epoch": 1.6067677946324388,
      "grad_norm": 1.1645150184631348,
      "learning_rate": 0.00023079452573805093,
      "loss": 1.4025,
      "step": 2410
    },
    {
      "epoch": 1.6134355725954326,
      "grad_norm": 1.3556365966796875,
      "learning_rate": 0.00022969077382156098,
      "loss": 1.3911,
      "step": 2420
    },
    {
      "epoch": 1.6201033505584264,
      "grad_norm": 1.2182546854019165,
      "learning_rate": 0.000228587021905071,
      "loss": 1.52,
      "step": 2430
    },
    {
      "epoch": 1.6267711285214204,
      "grad_norm": 1.0946223735809326,
      "learning_rate": 0.00022748326998858106,
      "loss": 1.5387,
      "step": 2440
    },
    {
      "epoch": 1.6334389064844141,
      "grad_norm": 1.0442957878112793,
      "learning_rate": 0.00022637951807209108,
      "loss": 1.5846,
      "step": 2450
    },
    {
      "epoch": 1.640106684447408,
      "grad_norm": 0.9864723682403564,
      "learning_rate": 0.0002252757661556011,
      "loss": 1.4438,
      "step": 2460
    },
    {
      "epoch": 1.6467744624104017,
      "grad_norm": 1.3001149892807007,
      "learning_rate": 0.00022417201423911116,
      "loss": 1.4921,
      "step": 2470
    },
    {
      "epoch": 1.6534422403733955,
      "grad_norm": 1.0687713623046875,
      "learning_rate": 0.0002230682623226212,
      "loss": 1.5174,
      "step": 2480
    },
    {
      "epoch": 1.6601100183363893,
      "grad_norm": 1.1226575374603271,
      "learning_rate": 0.00022196451040613124,
      "loss": 1.3397,
      "step": 2490
    },
    {
      "epoch": 1.6667777962993833,
      "grad_norm": 1.230857491493225,
      "learning_rate": 0.00022086075848964126,
      "loss": 1.5368,
      "step": 2500
    },
    {
      "epoch": 1.673445574262377,
      "grad_norm": 0.9778367280960083,
      "learning_rate": 0.00021975700657315131,
      "loss": 1.5178,
      "step": 2510
    },
    {
      "epoch": 1.680113352225371,
      "grad_norm": 1.2892787456512451,
      "learning_rate": 0.00021865325465666137,
      "loss": 1.5041,
      "step": 2520
    },
    {
      "epoch": 1.6867811301883648,
      "grad_norm": 1.4404971599578857,
      "learning_rate": 0.0002175495027401714,
      "loss": 1.4163,
      "step": 2530
    },
    {
      "epoch": 1.6934489081513586,
      "grad_norm": 1.1828399896621704,
      "learning_rate": 0.00021644575082368142,
      "loss": 1.4281,
      "step": 2540
    },
    {
      "epoch": 1.7001166861143524,
      "grad_norm": 1.301343321800232,
      "learning_rate": 0.0002153419989071915,
      "loss": 1.594,
      "step": 2550
    },
    {
      "epoch": 1.7067844640773462,
      "grad_norm": 1.3544124364852905,
      "learning_rate": 0.00021423824699070152,
      "loss": 1.3276,
      "step": 2560
    },
    {
      "epoch": 1.71345224204034,
      "grad_norm": 1.3493671417236328,
      "learning_rate": 0.00021313449507421155,
      "loss": 1.4393,
      "step": 2570
    },
    {
      "epoch": 1.7201200200033337,
      "grad_norm": 1.3402365446090698,
      "learning_rate": 0.00021203074315772157,
      "loss": 1.495,
      "step": 2580
    },
    {
      "epoch": 1.7267877979663278,
      "grad_norm": 1.5583744049072266,
      "learning_rate": 0.00021092699124123163,
      "loss": 1.5281,
      "step": 2590
    },
    {
      "epoch": 1.7334555759293215,
      "grad_norm": 1.1286903619766235,
      "learning_rate": 0.00020982323932474168,
      "loss": 1.4234,
      "step": 2600
    },
    {
      "epoch": 1.7401233538923155,
      "grad_norm": 1.1605712175369263,
      "learning_rate": 0.0002087194874082517,
      "loss": 1.4225,
      "step": 2610
    },
    {
      "epoch": 1.7467911318553093,
      "grad_norm": 1.4042507410049438,
      "learning_rate": 0.00020761573549176173,
      "loss": 1.497,
      "step": 2620
    },
    {
      "epoch": 1.753458909818303,
      "grad_norm": 1.1212955713272095,
      "learning_rate": 0.00020651198357527178,
      "loss": 1.4092,
      "step": 2630
    },
    {
      "epoch": 1.7601266877812969,
      "grad_norm": 1.0401514768600464,
      "learning_rate": 0.00020540823165878183,
      "loss": 1.3994,
      "step": 2640
    },
    {
      "epoch": 1.7667944657442907,
      "grad_norm": 1.308742642402649,
      "learning_rate": 0.00020430447974229186,
      "loss": 1.3681,
      "step": 2650
    },
    {
      "epoch": 1.7734622437072844,
      "grad_norm": 1.318424940109253,
      "learning_rate": 0.00020320072782580188,
      "loss": 1.4399,
      "step": 2660
    },
    {
      "epoch": 1.7801300216702782,
      "grad_norm": 1.3308637142181396,
      "learning_rate": 0.00020209697590931194,
      "loss": 1.5838,
      "step": 2670
    },
    {
      "epoch": 1.7867977996332722,
      "grad_norm": 1.0802357196807861,
      "learning_rate": 0.000200993223992822,
      "loss": 1.3114,
      "step": 2680
    },
    {
      "epoch": 1.793465577596266,
      "grad_norm": 1.723601222038269,
      "learning_rate": 0.00019988947207633202,
      "loss": 1.4121,
      "step": 2690
    },
    {
      "epoch": 1.80013335555926,
      "grad_norm": 1.263047695159912,
      "learning_rate": 0.00019878572015984204,
      "loss": 1.4355,
      "step": 2700
    },
    {
      "epoch": 1.8068011335222538,
      "grad_norm": 1.314353585243225,
      "learning_rate": 0.0001976819682433521,
      "loss": 1.4878,
      "step": 2710
    },
    {
      "epoch": 1.8134689114852476,
      "grad_norm": 1.3250447511672974,
      "learning_rate": 0.00019657821632686212,
      "loss": 1.4498,
      "step": 2720
    },
    {
      "epoch": 1.8201366894482414,
      "grad_norm": 1.498772382736206,
      "learning_rate": 0.00019547446441037217,
      "loss": 1.3353,
      "step": 2730
    },
    {
      "epoch": 1.8268044674112351,
      "grad_norm": 1.1898800134658813,
      "learning_rate": 0.0001943707124938822,
      "loss": 1.4201,
      "step": 2740
    },
    {
      "epoch": 1.833472245374229,
      "grad_norm": 1.5568311214447021,
      "learning_rate": 0.00019326696057739225,
      "loss": 1.4337,
      "step": 2750
    },
    {
      "epoch": 1.8401400233372227,
      "grad_norm": 1.0211470127105713,
      "learning_rate": 0.00019216320866090227,
      "loss": 1.4806,
      "step": 2760
    },
    {
      "epoch": 1.8468078013002167,
      "grad_norm": 1.4414533376693726,
      "learning_rate": 0.00019105945674441233,
      "loss": 1.4621,
      "step": 2770
    },
    {
      "epoch": 1.8534755792632105,
      "grad_norm": 1.2982195615768433,
      "learning_rate": 0.00018995570482792235,
      "loss": 1.3977,
      "step": 2780
    },
    {
      "epoch": 1.8601433572262045,
      "grad_norm": 1.2107536792755127,
      "learning_rate": 0.0001888519529114324,
      "loss": 1.2201,
      "step": 2790
    },
    {
      "epoch": 1.8668111351891983,
      "grad_norm": 1.3804160356521606,
      "learning_rate": 0.00018774820099494243,
      "loss": 1.4392,
      "step": 2800
    },
    {
      "epoch": 1.873478913152192,
      "grad_norm": 1.406408667564392,
      "learning_rate": 0.00018664444907845248,
      "loss": 1.4983,
      "step": 2810
    },
    {
      "epoch": 1.8801466911151858,
      "grad_norm": 1.5637788772583008,
      "learning_rate": 0.0001855406971619625,
      "loss": 1.4235,
      "step": 2820
    },
    {
      "epoch": 1.8868144690781796,
      "grad_norm": 1.4636718034744263,
      "learning_rate": 0.00018443694524547256,
      "loss": 1.426,
      "step": 2830
    },
    {
      "epoch": 1.8934822470411734,
      "grad_norm": 1.5728213787078857,
      "learning_rate": 0.00018333319332898258,
      "loss": 1.4138,
      "step": 2840
    },
    {
      "epoch": 1.9001500250041674,
      "grad_norm": 1.3571182489395142,
      "learning_rate": 0.0001822294414124926,
      "loss": 1.3596,
      "step": 2850
    },
    {
      "epoch": 1.9068178029671612,
      "grad_norm": 1.0569891929626465,
      "learning_rate": 0.00018112568949600266,
      "loss": 1.4507,
      "step": 2860
    },
    {
      "epoch": 1.913485580930155,
      "grad_norm": 1.1688873767852783,
      "learning_rate": 0.00018002193757951272,
      "loss": 1.4397,
      "step": 2870
    },
    {
      "epoch": 1.920153358893149,
      "grad_norm": 1.15237557888031,
      "learning_rate": 0.00017891818566302274,
      "loss": 1.3925,
      "step": 2880
    },
    {
      "epoch": 1.9268211368561428,
      "grad_norm": 1.2290595769882202,
      "learning_rate": 0.00017781443374653277,
      "loss": 1.359,
      "step": 2890
    },
    {
      "epoch": 1.9334889148191365,
      "grad_norm": 1.4199540615081787,
      "learning_rate": 0.00017671068183004285,
      "loss": 1.3968,
      "step": 2900
    },
    {
      "epoch": 1.9401566927821303,
      "grad_norm": 1.220352053642273,
      "learning_rate": 0.00017560692991355287,
      "loss": 1.4847,
      "step": 2910
    },
    {
      "epoch": 1.946824470745124,
      "grad_norm": 1.1521666049957275,
      "learning_rate": 0.0001745031779970629,
      "loss": 1.3064,
      "step": 2920
    },
    {
      "epoch": 1.9534922487081179,
      "grad_norm": 1.2795710563659668,
      "learning_rate": 0.00017339942608057292,
      "loss": 1.5053,
      "step": 2930
    },
    {
      "epoch": 1.9601600266711119,
      "grad_norm": 1.4859169721603394,
      "learning_rate": 0.000172295674164083,
      "loss": 1.3772,
      "step": 2940
    },
    {
      "epoch": 1.9668278046341057,
      "grad_norm": 1.2753515243530273,
      "learning_rate": 0.00017119192224759303,
      "loss": 1.4544,
      "step": 2950
    },
    {
      "epoch": 1.9734955825970997,
      "grad_norm": 1.4977675676345825,
      "learning_rate": 0.00017008817033110305,
      "loss": 1.3167,
      "step": 2960
    },
    {
      "epoch": 1.9801633605600935,
      "grad_norm": 1.2912111282348633,
      "learning_rate": 0.00016898441841461308,
      "loss": 1.4432,
      "step": 2970
    },
    {
      "epoch": 1.9868311385230872,
      "grad_norm": 1.2353144884109497,
      "learning_rate": 0.00016788066649812313,
      "loss": 1.2898,
      "step": 2980
    },
    {
      "epoch": 1.993498916486081,
      "grad_norm": 1.1845791339874268,
      "learning_rate": 0.00016677691458163318,
      "loss": 1.3353,
      "step": 2990
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.1205716133117676,
      "learning_rate": 0.0001656731626651432,
      "loss": 1.3902,
      "step": 3000
    },
    {
      "epoch": 2.0,
      "eval_loss": 1.3886362314224243,
      "eval_runtime": 938.3705,
      "eval_samples_per_second": 6.393,
      "eval_steps_per_second": 3.197,
      "step": 3000
    },
    {
      "epoch": 2.006667777962994,
      "grad_norm": 1.2693958282470703,
      "learning_rate": 0.00016456941074865323,
      "loss": 1.3317,
      "step": 3010
    },
    {
      "epoch": 2.0133355559259876,
      "grad_norm": 1.1773405075073242,
      "learning_rate": 0.00016346565883216328,
      "loss": 1.4184,
      "step": 3020
    },
    {
      "epoch": 2.0200033338889813,
      "grad_norm": 1.2990552186965942,
      "learning_rate": 0.00016236190691567334,
      "loss": 1.4124,
      "step": 3030
    },
    {
      "epoch": 2.026671111851975,
      "grad_norm": 1.4204962253570557,
      "learning_rate": 0.00016125815499918336,
      "loss": 1.4251,
      "step": 3040
    },
    {
      "epoch": 2.0333388898149694,
      "grad_norm": 1.3119755983352661,
      "learning_rate": 0.0001601544030826934,
      "loss": 1.3468,
      "step": 3050
    },
    {
      "epoch": 2.040006667777963,
      "grad_norm": 1.399164080619812,
      "learning_rate": 0.00015905065116620344,
      "loss": 1.414,
      "step": 3060
    },
    {
      "epoch": 2.046674445740957,
      "grad_norm": 1.1934270858764648,
      "learning_rate": 0.0001579468992497135,
      "loss": 1.3247,
      "step": 3070
    },
    {
      "epoch": 2.0533422237039507,
      "grad_norm": 1.4616758823394775,
      "learning_rate": 0.00015684314733322352,
      "loss": 1.3829,
      "step": 3080
    },
    {
      "epoch": 2.0600100016669445,
      "grad_norm": 1.2231369018554688,
      "learning_rate": 0.00015573939541673354,
      "loss": 1.357,
      "step": 3090
    },
    {
      "epoch": 2.0666777796299383,
      "grad_norm": 1.3214682340621948,
      "learning_rate": 0.0001546356435002436,
      "loss": 1.4479,
      "step": 3100
    },
    {
      "epoch": 2.073345557592932,
      "grad_norm": 1.2023756504058838,
      "learning_rate": 0.00015353189158375362,
      "loss": 1.4981,
      "step": 3110
    },
    {
      "epoch": 2.080013335555926,
      "grad_norm": 1.3463263511657715,
      "learning_rate": 0.00015242813966726367,
      "loss": 1.359,
      "step": 3120
    },
    {
      "epoch": 2.0866811135189196,
      "grad_norm": 1.8575807809829712,
      "learning_rate": 0.0001513243877507737,
      "loss": 1.2252,
      "step": 3130
    },
    {
      "epoch": 2.093348891481914,
      "grad_norm": 1.356352686882019,
      "learning_rate": 0.00015022063583428375,
      "loss": 1.3118,
      "step": 3140
    },
    {
      "epoch": 2.1000166694449076,
      "grad_norm": 1.4244155883789062,
      "learning_rate": 0.00014911688391779378,
      "loss": 1.4138,
      "step": 3150
    },
    {
      "epoch": 2.1066844474079014,
      "grad_norm": 1.484602928161621,
      "learning_rate": 0.00014801313200130383,
      "loss": 1.314,
      "step": 3160
    },
    {
      "epoch": 2.113352225370895,
      "grad_norm": 1.0000461339950562,
      "learning_rate": 0.00014690938008481385,
      "loss": 1.4662,
      "step": 3170
    },
    {
      "epoch": 2.120020003333889,
      "grad_norm": 1.201138973236084,
      "learning_rate": 0.0001458056281683239,
      "loss": 1.2888,
      "step": 3180
    },
    {
      "epoch": 2.1266877812968827,
      "grad_norm": 1.3472026586532593,
      "learning_rate": 0.00014470187625183393,
      "loss": 1.3667,
      "step": 3190
    },
    {
      "epoch": 2.1333555592598765,
      "grad_norm": 1.2362172603607178,
      "learning_rate": 0.00014359812433534398,
      "loss": 1.3326,
      "step": 3200
    },
    {
      "epoch": 2.1400233372228703,
      "grad_norm": 1.2151451110839844,
      "learning_rate": 0.00014249437241885404,
      "loss": 1.3756,
      "step": 3210
    },
    {
      "epoch": 2.146691115185864,
      "grad_norm": 1.4059884548187256,
      "learning_rate": 0.00014139062050236406,
      "loss": 1.2707,
      "step": 3220
    },
    {
      "epoch": 2.1533588931488583,
      "grad_norm": 1.6045712232589722,
      "learning_rate": 0.0001402868685858741,
      "loss": 1.3069,
      "step": 3230
    },
    {
      "epoch": 2.160026671111852,
      "grad_norm": 1.1436086893081665,
      "learning_rate": 0.0001391831166693841,
      "loss": 1.3458,
      "step": 3240
    },
    {
      "epoch": 2.166694449074846,
      "grad_norm": 1.5760822296142578,
      "learning_rate": 0.0001380793647528942,
      "loss": 1.3155,
      "step": 3250
    },
    {
      "epoch": 2.1733622270378397,
      "grad_norm": 1.5793882608413696,
      "learning_rate": 0.00013697561283640422,
      "loss": 1.4167,
      "step": 3260
    },
    {
      "epoch": 2.1800300050008334,
      "grad_norm": 1.435902714729309,
      "learning_rate": 0.00013587186091991424,
      "loss": 1.3662,
      "step": 3270
    },
    {
      "epoch": 2.186697782963827,
      "grad_norm": 1.3228919506072998,
      "learning_rate": 0.00013476810900342427,
      "loss": 1.3716,
      "step": 3280
    },
    {
      "epoch": 2.193365560926821,
      "grad_norm": 1.4015460014343262,
      "learning_rate": 0.00013366435708693435,
      "loss": 1.3658,
      "step": 3290
    },
    {
      "epoch": 2.200033338889815,
      "grad_norm": 1.317500114440918,
      "learning_rate": 0.00013256060517044437,
      "loss": 1.4305,
      "step": 3300
    },
    {
      "epoch": 2.206701116852809,
      "grad_norm": 1.3441320657730103,
      "learning_rate": 0.0001314568532539544,
      "loss": 1.4123,
      "step": 3310
    },
    {
      "epoch": 2.213368894815803,
      "grad_norm": 1.6106430292129517,
      "learning_rate": 0.00013035310133746442,
      "loss": 1.4333,
      "step": 3320
    },
    {
      "epoch": 2.2200366727787966,
      "grad_norm": 1.2769826650619507,
      "learning_rate": 0.0001292493494209745,
      "loss": 1.3395,
      "step": 3330
    },
    {
      "epoch": 2.2267044507417904,
      "grad_norm": 1.4832197427749634,
      "learning_rate": 0.00012814559750448453,
      "loss": 1.2544,
      "step": 3340
    },
    {
      "epoch": 2.233372228704784,
      "grad_norm": 1.2302151918411255,
      "learning_rate": 0.00012704184558799455,
      "loss": 1.3569,
      "step": 3350
    },
    {
      "epoch": 2.240040006667778,
      "grad_norm": 1.2689372301101685,
      "learning_rate": 0.00012593809367150458,
      "loss": 1.3835,
      "step": 3360
    },
    {
      "epoch": 2.2467077846307717,
      "grad_norm": 1.3212265968322754,
      "learning_rate": 0.00012483434175501463,
      "loss": 1.3112,
      "step": 3370
    },
    {
      "epoch": 2.2533755625937655,
      "grad_norm": 1.0875294208526611,
      "learning_rate": 0.00012373058983852468,
      "loss": 1.3618,
      "step": 3380
    },
    {
      "epoch": 2.2600433405567593,
      "grad_norm": 1.4472583532333374,
      "learning_rate": 0.0001226268379220347,
      "loss": 1.3118,
      "step": 3390
    },
    {
      "epoch": 2.266711118519753,
      "grad_norm": 1.320688247680664,
      "learning_rate": 0.00012152308600554475,
      "loss": 1.3353,
      "step": 3400
    },
    {
      "epoch": 2.2733788964827473,
      "grad_norm": 1.5169130563735962,
      "learning_rate": 0.00012041933408905479,
      "loss": 1.4521,
      "step": 3410
    },
    {
      "epoch": 2.280046674445741,
      "grad_norm": 1.0826513767242432,
      "learning_rate": 0.00011931558217256483,
      "loss": 1.4416,
      "step": 3420
    },
    {
      "epoch": 2.286714452408735,
      "grad_norm": 1.4831516742706299,
      "learning_rate": 0.00011821183025607487,
      "loss": 1.5411,
      "step": 3430
    },
    {
      "epoch": 2.2933822303717286,
      "grad_norm": 1.1624895334243774,
      "learning_rate": 0.0001171080783395849,
      "loss": 1.4465,
      "step": 3440
    },
    {
      "epoch": 2.3000500083347224,
      "grad_norm": 1.603217363357544,
      "learning_rate": 0.00011600432642309494,
      "loss": 1.3436,
      "step": 3450
    },
    {
      "epoch": 2.306717786297716,
      "grad_norm": 1.174634575843811,
      "learning_rate": 0.00011490057450660498,
      "loss": 1.3921,
      "step": 3460
    },
    {
      "epoch": 2.31338556426071,
      "grad_norm": 1.1419070959091187,
      "learning_rate": 0.00011379682259011502,
      "loss": 1.4894,
      "step": 3470
    },
    {
      "epoch": 2.320053342223704,
      "grad_norm": 1.2974145412445068,
      "learning_rate": 0.00011269307067362506,
      "loss": 1.3411,
      "step": 3480
    },
    {
      "epoch": 2.326721120186698,
      "grad_norm": 1.2478853464126587,
      "learning_rate": 0.00011158931875713509,
      "loss": 1.4304,
      "step": 3490
    },
    {
      "epoch": 2.3333888981496917,
      "grad_norm": 1.1592321395874023,
      "learning_rate": 0.00011048556684064514,
      "loss": 1.4406,
      "step": 3500
    },
    {
      "epoch": 2.3400566761126855,
      "grad_norm": 1.1317843198776245,
      "learning_rate": 0.00010938181492415518,
      "loss": 1.3489,
      "step": 3510
    },
    {
      "epoch": 2.3467244540756793,
      "grad_norm": 1.162841796875,
      "learning_rate": 0.00010827806300766522,
      "loss": 1.3301,
      "step": 3520
    },
    {
      "epoch": 2.353392232038673,
      "grad_norm": 1.3046932220458984,
      "learning_rate": 0.00010717431109117525,
      "loss": 1.4636,
      "step": 3530
    },
    {
      "epoch": 2.360060010001667,
      "grad_norm": 1.1381267309188843,
      "learning_rate": 0.0001060705591746853,
      "loss": 1.3078,
      "step": 3540
    },
    {
      "epoch": 2.3667277879646607,
      "grad_norm": 1.238465428352356,
      "learning_rate": 0.00010496680725819533,
      "loss": 1.4049,
      "step": 3550
    },
    {
      "epoch": 2.3733955659276544,
      "grad_norm": 1.2062510251998901,
      "learning_rate": 0.00010386305534170537,
      "loss": 1.3731,
      "step": 3560
    },
    {
      "epoch": 2.380063343890648,
      "grad_norm": 1.0767267942428589,
      "learning_rate": 0.00010275930342521541,
      "loss": 1.2717,
      "step": 3570
    },
    {
      "epoch": 2.3867311218536424,
      "grad_norm": 1.0299735069274902,
      "learning_rate": 0.00010165555150872545,
      "loss": 1.4106,
      "step": 3580
    },
    {
      "epoch": 2.3933988998166362,
      "grad_norm": 1.608833909034729,
      "learning_rate": 0.00010055179959223549,
      "loss": 1.3098,
      "step": 3590
    },
    {
      "epoch": 2.40006667777963,
      "grad_norm": 1.5696640014648438,
      "learning_rate": 9.944804767574553e-05,
      "loss": 1.1555,
      "step": 3600
    },
    {
      "epoch": 2.406734455742624,
      "grad_norm": 1.5164638757705688,
      "learning_rate": 9.834429575925557e-05,
      "loss": 1.3337,
      "step": 3610
    },
    {
      "epoch": 2.4134022337056176,
      "grad_norm": 1.4016153812408447,
      "learning_rate": 9.724054384276559e-05,
      "loss": 1.4466,
      "step": 3620
    },
    {
      "epoch": 2.4200700116686114,
      "grad_norm": 1.3468552827835083,
      "learning_rate": 9.613679192627564e-05,
      "loss": 1.3762,
      "step": 3630
    },
    {
      "epoch": 2.426737789631605,
      "grad_norm": 1.165545105934143,
      "learning_rate": 9.503304000978567e-05,
      "loss": 1.3779,
      "step": 3640
    },
    {
      "epoch": 2.433405567594599,
      "grad_norm": 1.343563437461853,
      "learning_rate": 9.392928809329572e-05,
      "loss": 1.3046,
      "step": 3650
    },
    {
      "epoch": 2.440073345557593,
      "grad_norm": 1.0750328302383423,
      "learning_rate": 9.282553617680575e-05,
      "loss": 1.3703,
      "step": 3660
    },
    {
      "epoch": 2.446741123520587,
      "grad_norm": 1.3969775438308716,
      "learning_rate": 9.17217842603158e-05,
      "loss": 1.3928,
      "step": 3670
    },
    {
      "epoch": 2.4534089014835807,
      "grad_norm": 1.3114986419677734,
      "learning_rate": 9.061803234382582e-05,
      "loss": 1.2658,
      "step": 3680
    },
    {
      "epoch": 2.4600766794465745,
      "grad_norm": 1.468762993812561,
      "learning_rate": 8.951428042733588e-05,
      "loss": 1.3551,
      "step": 3690
    },
    {
      "epoch": 2.4667444574095683,
      "grad_norm": 1.634859561920166,
      "learning_rate": 8.84105285108459e-05,
      "loss": 1.3036,
      "step": 3700
    },
    {
      "epoch": 2.473412235372562,
      "grad_norm": 1.2644275426864624,
      "learning_rate": 8.730677659435595e-05,
      "loss": 1.4287,
      "step": 3710
    },
    {
      "epoch": 2.480080013335556,
      "grad_norm": 1.259099006652832,
      "learning_rate": 8.620302467786598e-05,
      "loss": 1.3234,
      "step": 3720
    },
    {
      "epoch": 2.4867477912985496,
      "grad_norm": 1.4299572706222534,
      "learning_rate": 8.509927276137603e-05,
      "loss": 1.4322,
      "step": 3730
    },
    {
      "epoch": 2.4934155692615434,
      "grad_norm": 1.4406284093856812,
      "learning_rate": 8.399552084488606e-05,
      "loss": 1.3316,
      "step": 3740
    },
    {
      "epoch": 2.500083347224537,
      "grad_norm": 1.4051684141159058,
      "learning_rate": 8.28917689283961e-05,
      "loss": 1.249,
      "step": 3750
    },
    {
      "epoch": 2.5067511251875314,
      "grad_norm": 1.0406477451324463,
      "learning_rate": 8.178801701190614e-05,
      "loss": 1.354,
      "step": 3760
    },
    {
      "epoch": 2.513418903150525,
      "grad_norm": 1.3918607234954834,
      "learning_rate": 8.068426509541617e-05,
      "loss": 1.3318,
      "step": 3770
    },
    {
      "epoch": 2.520086681113519,
      "grad_norm": 1.0122108459472656,
      "learning_rate": 7.958051317892621e-05,
      "loss": 1.3521,
      "step": 3780
    },
    {
      "epoch": 2.5267544590765127,
      "grad_norm": 1.3131142854690552,
      "learning_rate": 7.847676126243625e-05,
      "loss": 1.3384,
      "step": 3790
    },
    {
      "epoch": 2.5334222370395065,
      "grad_norm": 1.274563193321228,
      "learning_rate": 7.737300934594629e-05,
      "loss": 1.3133,
      "step": 3800
    },
    {
      "epoch": 2.5400900150025003,
      "grad_norm": 1.3744723796844482,
      "learning_rate": 7.626925742945633e-05,
      "loss": 1.4056,
      "step": 3810
    },
    {
      "epoch": 2.546757792965494,
      "grad_norm": 1.3365455865859985,
      "learning_rate": 7.516550551296637e-05,
      "loss": 1.319,
      "step": 3820
    },
    {
      "epoch": 2.5534255709284883,
      "grad_norm": 1.1434341669082642,
      "learning_rate": 7.406175359647641e-05,
      "loss": 1.3283,
      "step": 3830
    },
    {
      "epoch": 2.560093348891482,
      "grad_norm": 1.4029656648635864,
      "learning_rate": 7.295800167998646e-05,
      "loss": 1.4042,
      "step": 3840
    },
    {
      "epoch": 2.566761126854476,
      "grad_norm": 1.215064287185669,
      "learning_rate": 7.185424976349649e-05,
      "loss": 1.3286,
      "step": 3850
    },
    {
      "epoch": 2.5734289048174697,
      "grad_norm": 1.4691652059555054,
      "learning_rate": 7.075049784700654e-05,
      "loss": 1.1506,
      "step": 3860
    },
    {
      "epoch": 2.5800966827804634,
      "grad_norm": 1.4291082620620728,
      "learning_rate": 6.964674593051656e-05,
      "loss": 1.3971,
      "step": 3870
    },
    {
      "epoch": 2.5867644607434572,
      "grad_norm": 1.3384864330291748,
      "learning_rate": 6.854299401402662e-05,
      "loss": 1.2889,
      "step": 3880
    },
    {
      "epoch": 2.593432238706451,
      "grad_norm": 1.3013213872909546,
      "learning_rate": 6.743924209753664e-05,
      "loss": 1.3241,
      "step": 3890
    },
    {
      "epoch": 2.600100016669445,
      "grad_norm": 1.4072288274765015,
      "learning_rate": 6.633549018104668e-05,
      "loss": 1.3055,
      "step": 3900
    },
    {
      "epoch": 2.6067677946324386,
      "grad_norm": 1.073986291885376,
      "learning_rate": 6.523173826455672e-05,
      "loss": 1.3258,
      "step": 3910
    },
    {
      "epoch": 2.6134355725954324,
      "grad_norm": 1.1582658290863037,
      "learning_rate": 6.412798634806676e-05,
      "loss": 1.3689,
      "step": 3920
    },
    {
      "epoch": 2.620103350558426,
      "grad_norm": 1.1161572933197021,
      "learning_rate": 6.30242344315768e-05,
      "loss": 1.4134,
      "step": 3930
    },
    {
      "epoch": 2.6267711285214204,
      "grad_norm": 1.4321438074111938,
      "learning_rate": 6.192048251508684e-05,
      "loss": 1.3838,
      "step": 3940
    },
    {
      "epoch": 2.633438906484414,
      "grad_norm": 1.3234024047851562,
      "learning_rate": 6.081673059859687e-05,
      "loss": 1.2536,
      "step": 3950
    },
    {
      "epoch": 2.640106684447408,
      "grad_norm": 1.1847271919250488,
      "learning_rate": 5.971297868210691e-05,
      "loss": 1.3292,
      "step": 3960
    },
    {
      "epoch": 2.6467744624104017,
      "grad_norm": 1.1941784620285034,
      "learning_rate": 5.8609226765616946e-05,
      "loss": 1.2281,
      "step": 3970
    },
    {
      "epoch": 2.6534422403733955,
      "grad_norm": 1.0865697860717773,
      "learning_rate": 5.7505474849126985e-05,
      "loss": 1.2629,
      "step": 3980
    },
    {
      "epoch": 2.6601100183363893,
      "grad_norm": 1.2017502784729004,
      "learning_rate": 5.640172293263703e-05,
      "loss": 1.2244,
      "step": 3990
    },
    {
      "epoch": 2.6667777962993835,
      "grad_norm": 1.310544490814209,
      "learning_rate": 5.529797101614707e-05,
      "loss": 1.3072,
      "step": 4000
    },
    {
      "epoch": 2.6734455742623773,
      "grad_norm": 1.3612991571426392,
      "learning_rate": 5.419421909965711e-05,
      "loss": 1.2829,
      "step": 4010
    },
    {
      "epoch": 2.680113352225371,
      "grad_norm": 1.4835578203201294,
      "learning_rate": 5.309046718316715e-05,
      "loss": 1.2788,
      "step": 4020
    },
    {
      "epoch": 2.686781130188365,
      "grad_norm": 1.420531153678894,
      "learning_rate": 5.1986715266677186e-05,
      "loss": 1.3772,
      "step": 4030
    },
    {
      "epoch": 2.6934489081513586,
      "grad_norm": 1.3505030870437622,
      "learning_rate": 5.0882963350187225e-05,
      "loss": 1.3463,
      "step": 4040
    },
    {
      "epoch": 2.7001166861143524,
      "grad_norm": 1.1644771099090576,
      "learning_rate": 4.9779211433697264e-05,
      "loss": 1.4462,
      "step": 4050
    },
    {
      "epoch": 2.706784464077346,
      "grad_norm": 1.296617031097412,
      "learning_rate": 4.86754595172073e-05,
      "loss": 1.3583,
      "step": 4060
    },
    {
      "epoch": 2.71345224204034,
      "grad_norm": 1.385800838470459,
      "learning_rate": 4.757170760071734e-05,
      "loss": 1.3826,
      "step": 4070
    },
    {
      "epoch": 2.7201200200033337,
      "grad_norm": 1.0784242153167725,
      "learning_rate": 4.6467955684227374e-05,
      "loss": 1.3388,
      "step": 4080
    },
    {
      "epoch": 2.7267877979663275,
      "grad_norm": 1.1924023628234863,
      "learning_rate": 4.536420376773741e-05,
      "loss": 1.3368,
      "step": 4090
    },
    {
      "epoch": 2.7334555759293213,
      "grad_norm": 1.178196907043457,
      "learning_rate": 4.426045185124745e-05,
      "loss": 1.257,
      "step": 4100
    },
    {
      "epoch": 2.7401233538923155,
      "grad_norm": 1.1449236869812012,
      "learning_rate": 4.315669993475749e-05,
      "loss": 1.3492,
      "step": 4110
    },
    {
      "epoch": 2.7467911318553093,
      "grad_norm": 1.1988673210144043,
      "learning_rate": 4.205294801826753e-05,
      "loss": 1.3136,
      "step": 4120
    },
    {
      "epoch": 2.753458909818303,
      "grad_norm": 1.1349953413009644,
      "learning_rate": 4.094919610177757e-05,
      "loss": 1.3299,
      "step": 4130
    },
    {
      "epoch": 2.760126687781297,
      "grad_norm": 1.070622444152832,
      "learning_rate": 3.984544418528761e-05,
      "loss": 1.3522,
      "step": 4140
    },
    {
      "epoch": 2.7667944657442907,
      "grad_norm": 1.3638585805892944,
      "learning_rate": 3.8741692268797646e-05,
      "loss": 1.2709,
      "step": 4150
    },
    {
      "epoch": 2.7734622437072844,
      "grad_norm": 1.3689301013946533,
      "learning_rate": 3.7637940352307685e-05,
      "loss": 1.3568,
      "step": 4160
    },
    {
      "epoch": 2.7801300216702782,
      "grad_norm": 1.3208739757537842,
      "learning_rate": 3.6534188435817724e-05,
      "loss": 1.3707,
      "step": 4170
    },
    {
      "epoch": 2.7867977996332725,
      "grad_norm": 1.3346346616744995,
      "learning_rate": 3.543043651932776e-05,
      "loss": 1.4268,
      "step": 4180
    },
    {
      "epoch": 2.7934655775962662,
      "grad_norm": 1.1920398473739624,
      "learning_rate": 3.43266846028378e-05,
      "loss": 1.3443,
      "step": 4190
    },
    {
      "epoch": 2.80013335555926,
      "grad_norm": 1.2212119102478027,
      "learning_rate": 3.322293268634784e-05,
      "loss": 1.2107,
      "step": 4200
    },
    {
      "epoch": 2.806801133522254,
      "grad_norm": 1.3583773374557495,
      "learning_rate": 3.211918076985787e-05,
      "loss": 1.4081,
      "step": 4210
    },
    {
      "epoch": 2.8134689114852476,
      "grad_norm": 1.2670776844024658,
      "learning_rate": 3.101542885336791e-05,
      "loss": 1.229,
      "step": 4220
    },
    {
      "epoch": 2.8201366894482414,
      "grad_norm": 1.1841763257980347,
      "learning_rate": 2.9911676936877957e-05,
      "loss": 1.3891,
      "step": 4230
    },
    {
      "epoch": 2.826804467411235,
      "grad_norm": 1.2501925230026245,
      "learning_rate": 2.8807925020387996e-05,
      "loss": 1.3048,
      "step": 4240
    },
    {
      "epoch": 2.833472245374229,
      "grad_norm": 1.1483218669891357,
      "learning_rate": 2.770417310389803e-05,
      "loss": 1.2433,
      "step": 4250
    },
    {
      "epoch": 2.8401400233372227,
      "grad_norm": 1.2344450950622559,
      "learning_rate": 2.660042118740807e-05,
      "loss": 1.2494,
      "step": 4260
    },
    {
      "epoch": 2.8468078013002165,
      "grad_norm": 1.1541417837142944,
      "learning_rate": 2.549666927091811e-05,
      "loss": 1.3057,
      "step": 4270
    },
    {
      "epoch": 2.8534755792632103,
      "grad_norm": 1.4701131582260132,
      "learning_rate": 2.4392917354428148e-05,
      "loss": 1.188,
      "step": 4280
    },
    {
      "epoch": 2.8601433572262045,
      "grad_norm": 1.391108751296997,
      "learning_rate": 2.3289165437938187e-05,
      "loss": 1.198,
      "step": 4290
    },
    {
      "epoch": 2.8668111351891983,
      "grad_norm": 1.2954812049865723,
      "learning_rate": 2.2185413521448226e-05,
      "loss": 1.3684,
      "step": 4300
    },
    {
      "epoch": 2.873478913152192,
      "grad_norm": 1.282576084136963,
      "learning_rate": 2.108166160495826e-05,
      "loss": 1.4157,
      "step": 4310
    },
    {
      "epoch": 2.880146691115186,
      "grad_norm": 1.1937285661697388,
      "learning_rate": 1.99779096884683e-05,
      "loss": 1.3227,
      "step": 4320
    },
    {
      "epoch": 2.8868144690781796,
      "grad_norm": 1.2760138511657715,
      "learning_rate": 1.887415777197834e-05,
      "loss": 1.2605,
      "step": 4330
    },
    {
      "epoch": 2.8934822470411734,
      "grad_norm": 1.415338397026062,
      "learning_rate": 1.7770405855488378e-05,
      "loss": 1.3047,
      "step": 4340
    },
    {
      "epoch": 2.9001500250041676,
      "grad_norm": 1.305734395980835,
      "learning_rate": 1.6666653938998417e-05,
      "loss": 1.1622,
      "step": 4350
    },
    {
      "epoch": 2.9068178029671614,
      "grad_norm": 1.3043853044509888,
      "learning_rate": 1.5562902022508456e-05,
      "loss": 1.2825,
      "step": 4360
    },
    {
      "epoch": 2.913485580930155,
      "grad_norm": 1.1785264015197754,
      "learning_rate": 1.4459150106018495e-05,
      "loss": 1.3579,
      "step": 4370
    },
    {
      "epoch": 2.920153358893149,
      "grad_norm": 1.339805006980896,
      "learning_rate": 1.3355398189528533e-05,
      "loss": 1.3267,
      "step": 4380
    },
    {
      "epoch": 2.9268211368561428,
      "grad_norm": 1.4100626707077026,
      "learning_rate": 1.2251646273038572e-05,
      "loss": 1.3269,
      "step": 4390
    },
    {
      "epoch": 2.9334889148191365,
      "grad_norm": 1.2234785556793213,
      "learning_rate": 1.114789435654861e-05,
      "loss": 1.3795,
      "step": 4400
    },
    {
      "epoch": 2.9401566927821303,
      "grad_norm": 1.3670315742492676,
      "learning_rate": 1.0044142440058648e-05,
      "loss": 1.3374,
      "step": 4410
    },
    {
      "epoch": 2.946824470745124,
      "grad_norm": 1.3064802885055542,
      "learning_rate": 8.940390523568687e-06,
      "loss": 1.3086,
      "step": 4420
    },
    {
      "epoch": 2.953492248708118,
      "grad_norm": 1.226974606513977,
      "learning_rate": 7.836638607078726e-06,
      "loss": 1.2788,
      "step": 4430
    },
    {
      "epoch": 2.9601600266711117,
      "grad_norm": 1.2323359251022339,
      "learning_rate": 6.732886690588764e-06,
      "loss": 1.2591,
      "step": 4440
    },
    {
      "epoch": 2.9668278046341054,
      "grad_norm": 1.2296819686889648,
      "learning_rate": 5.629134774098803e-06,
      "loss": 1.4209,
      "step": 4450
    },
    {
      "epoch": 2.9734955825970997,
      "grad_norm": 1.149755597114563,
      "learning_rate": 4.525382857608842e-06,
      "loss": 1.3774,
      "step": 4460
    },
    {
      "epoch": 2.9801633605600935,
      "grad_norm": 1.10077965259552,
      "learning_rate": 3.4216309411188805e-06,
      "loss": 1.3555,
      "step": 4470
    },
    {
      "epoch": 2.9868311385230872,
      "grad_norm": 1.0496329069137573,
      "learning_rate": 2.317879024628919e-06,
      "loss": 1.3139,
      "step": 4480
    },
    {
      "epoch": 2.993498916486081,
      "grad_norm": 1.1082793474197388,
      "learning_rate": 1.2141271081389575e-06,
      "loss": 1.2669,
      "step": 4490
    },
    {
      "epoch": 3.0,
      "grad_norm": 1.6033544540405273,
      "learning_rate": 1.1037519164899614e-07,
      "loss": 1.3325,
      "step": 4500
    },
    {
      "epoch": 3.0,
      "eval_loss": 1.297846794128418,
      "eval_runtime": 938.2254,
      "eval_samples_per_second": 6.394,
      "eval_steps_per_second": 3.198,
      "step": 4500
    }
  ],
  "logging_steps": 10,
  "max_steps": 4500,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "EarlyStoppingCallback": {
      "args": {
        "early_stopping_patience": 2,
        "early_stopping_threshold": 0.0
      },
      "attributes": {
        "early_stopping_patience_counter": 0
      }
    },
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 3.1282799397883085e+18,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
