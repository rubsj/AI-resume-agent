{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcb849be",
   "metadata": {},
   "source": [
    "# Phase 1: First Steps Notebook — Data Ingestion + Minimal Parsing\n",
    "1. Setup and Install Dependencies\n",
    "2. Load Resume and JD datasets\n",
    "3. Minimal Parsing into JSON Structure\n",
    "4. Save structured JSON for Phase 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dd9696",
   "metadata": {},
   "source": [
    "## Setup and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4abca3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kaggle\n",
      "  Using cached kaggle-1.7.4.2-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting kagglehub\n",
      "  Using cached kagglehub-0.3.11-py3-none-any.whl.metadata (32 kB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Collecting bleach (from kaggle)\n",
      "  Using cached bleach-6.2.0-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting certifi>=14.05.14 (from kaggle)\n",
      "  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting charset-normalizer (from kaggle)\n",
      "  Using cached charset_normalizer-3.4.1-cp312-cp312-win_amd64.whl.metadata (36 kB)\n",
      "Collecting idna (from kaggle)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting protobuf (from kaggle)\n",
      "  Using cached protobuf-6.30.2-cp310-abi3-win_amd64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from kaggle) (2.9.0.post0)\n",
      "Collecting python-slugify (from kaggle)\n",
      "  Using cached python_slugify-8.0.4-py2.py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting requests (from kaggle)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from kaggle) (70.2.0)\n",
      "Requirement already satisfied: six>=1.10 in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from kaggle) (1.17.0)\n",
      "Collecting text-unidecode (from kaggle)\n",
      "  Using cached text_unidecode-1.3-py2.py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting tqdm (from kaggle)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting urllib3>=1.15.1 (from kaggle)\n",
      "  Using cached urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting webencodings (from kaggle)\n",
      "  Using cached webencodings-0.5.1-py2.py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: packaging in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from kagglehub) (25.0)\n",
      "Collecting pyyaml (from kagglehub)\n",
      "  Using cached PyYAML-6.0.2-cp312-cp312-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from pandas) (2.1.2)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: colorama in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from tqdm->kaggle) (0.4.6)\n",
      "Using cached kaggle-1.7.4.2-py3-none-any.whl (173 kB)\n",
      "Using cached kagglehub-0.3.11-py3-none-any.whl (63 kB)\n",
      "Using cached pandas-2.2.3-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "Using cached certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "Using cached bleach-6.2.0-py3-none-any.whl (163 kB)\n",
      "Using cached charset_normalizer-3.4.1-cp312-cp312-win_amd64.whl (102 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached protobuf-6.30.2-cp310-abi3-win_amd64.whl (431 kB)\n",
      "Using cached python_slugify-8.0.4-py2.py3-none-any.whl (10 kB)\n",
      "Using cached text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "Using cached PyYAML-6.0.2-cp312-cp312-win_amd64.whl (156 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: webencodings, text-unidecode, pytz, urllib3, tzdata, tqdm, pyyaml, python-slugify, protobuf, idna, charset-normalizer, certifi, bleach, requests, pandas, kagglehub, kaggle\n",
      "Successfully installed bleach-6.2.0 certifi-2025.1.31 charset-normalizer-3.4.1 idna-3.10 kaggle-1.7.4.2 kagglehub-0.3.11 pandas-2.2.3 protobuf-6.30.2 python-slugify-8.0.4 pytz-2025.2 pyyaml-6.0.2 requests-2.32.3 text-unidecode-1.3 tqdm-4.67.1 tzdata-2025.2 urllib3-2.4.0 webencodings-0.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install kaggle kagglehub pandas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb756b8f",
   "metadata": {},
   "source": [
    "## Util Classes and methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43abc8ac",
   "metadata": {},
   "source": [
    "### Configurations  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1e827a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 🛠 CONFIGURATION\n",
    "# ==============================\n",
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "class Config:\n",
    "    DATASET_DOWNLOAD_DIR = \"datasets\"\n",
    "    JSON_OUTPUT_DIR = \"json_outputs\"\n",
    "    AUTO_CLEANUP = True\n",
    "\n",
    "    @staticmethod\n",
    "    def setup_kaggle_credentials():\n",
    "        kaggle_path = os.path.expanduser('~/.kaggle/kaggle.json')\n",
    "        if not os.path.exists(kaggle_path):\n",
    "            from google.colab import files\n",
    "            print(\"📂 Upload kaggle.json file...\")\n",
    "            uploaded = files.upload()\n",
    "            os.makedirs(os.path.dirname(kaggle_path), exist_ok=True)\n",
    "            for filename in uploaded.keys():\n",
    "                shutil.move(filename, kaggle_path)\n",
    "            os.chmod(kaggle_path, 0o600)\n",
    "            print(f\"✅ Kaggle credentials setup at {kaggle_path}\")\n",
    "        else:\n",
    "            print(f\"✅ Kaggle credentials already exist at {kaggle_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a537a5",
   "metadata": {},
   "source": [
    "### Downloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89480102",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================\n",
    "# DOWNLOADER\n",
    "# ==============================\n",
    "class DatasetDownloader:\n",
    "    @staticmethod\n",
    "    def download_and_extract(dataset_path: str) -> tuple[str, str]:\n",
    "        os.makedirs(Config.DATASET_DOWNLOAD_DIR, exist_ok=True)\n",
    "        dataset_slug = dataset_path.split(\"/\")[-1]\n",
    "        extract_folder_path = os.path.join(Config.DATASET_DOWNLOAD_DIR, dataset_slug)\n",
    "        zip_filename = f\"{dataset_slug}.zip\"\n",
    "        zip_path = os.path.join(Config.DATASET_DOWNLOAD_DIR, zip_filename)\n",
    "\n",
    "        if os.path.exists(extract_folder_path) and any(Path(extract_folder_path).rglob(\"*.csv\")):\n",
    "            print(f\"⚡ Dataset folder already exists at '{extract_folder_path}', skipping download and extraction.\")\n",
    "            return extract_folder_path, zip_filename\n",
    "\n",
    "        print(f\"⬇️ Downloading dataset: {dataset_path} ...\")\n",
    "        !kaggle datasets download -d {dataset_path} -p {Config.DATASET_DOWNLOAD_DIR}\n",
    "\n",
    "        if not os.path.exists(zip_path):\n",
    "            raise FileNotFoundError(f\"❌ Zip file '{zip_filename}' not found after download!\")\n",
    "\n",
    "        os.makedirs(extract_folder_path, exist_ok=True)\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_folder_path)\n",
    "\n",
    "        print(f\"✅ Downloaded and extracted to '{extract_folder_path}'.\")\n",
    "        return extract_folder_path, zip_filename\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1170df00",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2c06119",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================\n",
    "# LOADER\n",
    "# ==============================\n",
    "class DatasetLoader:\n",
    "    @staticmethod\n",
    "    def load_csv(dataset_folder: str, target_csv_name: str) -> pd.DataFrame:\n",
    "        print(f\"🔍 Searching for '{target_csv_name}' inside {dataset_folder}...\")\n",
    "        if not os.path.exists(dataset_folder):\n",
    "            raise FileNotFoundError(f\"❌ Dataset folder '{dataset_folder}' does not exist!\")\n",
    "\n",
    "        for root, _, files in os.walk(dataset_folder):\n",
    "            for file in files:\n",
    "                if file.lower() == target_csv_name.lower():\n",
    "                    csv_path = os.path.join(root, file)\n",
    "                    df = pd.read_csv(csv_path)\n",
    "                    print(f\"✅ Loaded CSV with shape {df.shape}\")\n",
    "                    return df\n",
    "\n",
    "        raise FileNotFoundError(f\"❌ CSV file '{target_csv_name}' not found inside extracted dataset!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a80a5a",
   "metadata": {},
   "source": [
    "### Data Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08247c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================\n",
    "# PROCESSOR\n",
    "# ==============================\n",
    "class DatasetProcessor:\n",
    "    @staticmethod\n",
    "    def filter_fields(df: pd.DataFrame, allowed_fields: List[str]) -> pd.DataFrame:\n",
    "        missing_fields = [field for field in allowed_fields if field not in df.columns]\n",
    "        if missing_fields:\n",
    "            raise ValueError(f\"❌ Fields {missing_fields} not found in dataset!\")\n",
    "\n",
    "        filtered_df = df[allowed_fields]\n",
    "        print(f\"✅ Filtered columns: {list(filtered_df.columns)}\")\n",
    "        return filtered_df\n",
    "\n",
    "    @staticmethod\n",
    "    def save_to_json(df: pd.DataFrame, output_json_name: str):\n",
    "        os.makedirs(Config.JSON_OUTPUT_DIR, exist_ok=True)\n",
    "        output_path = os.path.join(Config.JSON_OUTPUT_DIR, output_json_name)\n",
    "\n",
    "        if os.path.exists(output_path):\n",
    "            os.remove(output_path)\n",
    "            print(f\"🗑️ Existing JSON '{output_path}' deleted.\")\n",
    "\n",
    "        df.to_json(output_path, orient='records', lines=True, force_ascii=False)\n",
    "        print(f\"✅ Data saved to JSON at '{output_path}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fc4fb6",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7245255",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ==============================\n",
    "# CLEANER\n",
    "# ==============================\n",
    "class Cleaner:\n",
    "    @staticmethod\n",
    "    def cleanup_dataset_artifacts(extracted_folder_path: str, zip_filename: str):\n",
    "        if os.path.exists(extracted_folder_path):\n",
    "            shutil.rmtree(extracted_folder_path)\n",
    "            print(f\"🧹 Folder '{extracted_folder_path}' has been deleted successfully.\")\n",
    "\n",
    "        zip_path = os.path.join(Config.DATASET_DOWNLOAD_DIR, zip_filename)\n",
    "        if os.path.exists(zip_path):\n",
    "            os.remove(zip_path)\n",
    "            print(f\"🗑️ Zip file '{zip_path}' has been deleted successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a2525b",
   "metadata": {},
   "source": [
    "### Hybrid Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8d85253",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Ruby\\projects\\AI-resume-agent\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ==============================\n",
    "# HYBRID LOADER\n",
    "# ==============================\n",
    "try:\n",
    "    import kagglehub\n",
    "    from kagglehub import KaggleDatasetAdapter\n",
    "except ImportError:\n",
    "    kagglehub = None\n",
    "\n",
    "class HybridDatasetLoader:\n",
    "    @staticmethod\n",
    "    def load_dataset(dataset_path: str, file_name: str) -> pd.DataFrame:\n",
    "        if kagglehub:\n",
    "            try:\n",
    "                print(f\"📥 Trying KaggleHub for {dataset_path}...\")\n",
    "                df = kagglehub.dataset_load(KaggleDatasetAdapter.PANDAS, dataset_path, file_name)\n",
    "                print(f\"✅ Loaded using KaggleHub: shape = {df.shape}\")\n",
    "                return df\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ KaggleHub failed: {e}\\nFalling back to ZIP-based loader.\")\n",
    "\n",
    "        extracted_folder, _ = DatasetDownloader.download_and_extract(dataset_path)\n",
    "        return DatasetLoader.load_csv(extracted_folder, file_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e917e9d6",
   "metadata": {},
   "source": [
    "### Main flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "766a42bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================\n",
    "# MAIN FLOW\n",
    "# ==============================\n",
    "def process_dataset(dataset_path: str, target_csv_name: str, allowed_fields: List[str], output_json_name: str):\n",
    "    df = HybridDatasetLoader.load_dataset(dataset_path, target_csv_name)\n",
    "    filtered_df = DatasetProcessor.filter_fields(df, allowed_fields)\n",
    "    DatasetProcessor.save_to_json(filtered_df, output_json_name)\n",
    "\n",
    "    dataset_slug = dataset_path.split(\"/\")[-1]\n",
    "    extracted_folder = os.path.join(Config.DATASET_DOWNLOAD_DIR, dataset_slug)\n",
    "    zip_filename = f\"{dataset_slug}.zip\"\n",
    "    if Config.AUTO_CLEANUP:\n",
    "        Cleaner.cleanup_dataset_artifacts(extracted_folder, zip_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60577088",
   "metadata": {},
   "source": [
    "## Login and do the processing of Resume and JD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3bb5022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Kaggle credentials already exist at C:\\Users\\rubyj/.kaggle/kaggle.json\n",
      "📥 Trying KaggleHub for snehaanbhawal/resume-dataset...\n",
      "⚠️ KaggleHub failed: 404 Client Error.\n",
      "\n",
      "Resource not found at URL: https://www.kaggle.com/datasets/snehaanbhawal/resume-dataset/versions/1\n",
      "The server reported the following issues: Data not found\n",
      "Please make sure you specified the correct resource identifiers.\n",
      "Falling back to ZIP-based loader.\n",
      "⬇️ Downloading dataset: snehaanbhawal/resume-dataset ...\n",
      "Dataset URL: https://www.kaggle.com/datasets/snehaanbhawal/resume-dataset\n",
      "License(s): CC0-1.0\n",
      "✅ Downloaded and extracted to 'datasets\\resume-dataset'.\n",
      "🔍 Searching for 'Resume.csv' inside datasets\\resume-dataset...\n",
      "✅ Loaded CSV with shape (2484, 4)\n",
      "✅ Filtered columns: ['Category', 'Resume_str']\n",
      "🗑️ Existing JSON 'json_outputs\\parsed_resumes.json' deleted.\n",
      "✅ Data saved to JSON at 'json_outputs\\parsed_resumes.json'\n",
      "🧹 Folder 'datasets\\resume-dataset' has been deleted successfully.\n",
      "🗑️ Zip file 'datasets\\resume-dataset.zip' has been deleted successfully.\n",
      "📥 Trying KaggleHub for arshkon/linkedin-job-postings...\n",
      "⚠️ KaggleHub failed: Error reading file: 'utf-8' codec can't decode byte 0x80 in position 14: invalid start byte\n",
      "Falling back to ZIP-based loader.\n",
      "⬇️ Downloading dataset: arshkon/linkedin-job-postings ...\n",
      "Dataset URL: https://www.kaggle.com/datasets/arshkon/linkedin-job-postings\n",
      "License(s): CC-BY-SA-4.0\n",
      "✅ Downloaded and extracted to 'datasets\\linkedin-job-postings'.\n",
      "🔍 Searching for 'postings.csv' inside datasets\\linkedin-job-postings...\n",
      "✅ Loaded CSV with shape (123849, 31)\n",
      "✅ Filtered columns: ['title', 'company_name', 'location', 'description', 'skills_desc', 'job_id', 'formatted_experience_level', 'formatted_work_type']\n",
      "🗑️ Existing JSON 'json_outputs\\parsed_jds.json' deleted.\n",
      "✅ Data saved to JSON at 'json_outputs\\parsed_jds.json'\n",
      "🧹 Folder 'datasets\\linkedin-job-postings' has been deleted successfully.\n",
      "🗑️ Zip file 'datasets\\linkedin-job-postings.zip' has been deleted successfully.\n"
     ]
    }
   ],
   "source": [
    "Config.setup_kaggle_credentials()\n",
    "# Process Resume Dataset\n",
    "process_dataset(\n",
    "    dataset_path=\"snehaanbhawal/resume-dataset\",\n",
    "    target_csv_name=\"Resume.csv\",\n",
    "    allowed_fields=[\"Category\", \"Resume_str\"],\n",
    "    output_json_name=\"parsed_resumes.json\"\n",
    ")\n",
    "\n",
    "# Process Job Postings Dataset\n",
    "process_dataset(\n",
    "    dataset_path=\"arshkon/linkedin-job-postings\",\n",
    "    target_csv_name=\"postings.csv\",\n",
    "    allowed_fields=[\"title\", \"company_name\", \"location\", \"description\", \"skills_desc\", \"job_id\" , \"formatted_experience_level\", \"formatted_work_type\"],\n",
    "    output_json_name=\"parsed_jds.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74195950",
   "metadata": {},
   "source": [
    "# Phase 2 -\tParse resume/JD into JSON structured scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27168f09",
   "metadata": {},
   "source": [
    "##  Install Dependencies  & Login to Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "381adace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://download.pytorch.org/whl/nightly/cu128\n",
      "Requirement already satisfied: torch in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (2.8.0.dev20250420+cu128)\n",
      "Requirement already satisfied: torchvision in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (0.22.0.dev20250421+cu128)\n",
      "Requirement already satisfied: torchaudio in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (2.6.0.dev20250421+cu128)\n",
      "Requirement already satisfied: filelock in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: setuptools in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from torch) (70.2.0)\n",
      "Requirement already satisfied: numpy in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from torchvision) (2.1.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: bitsandbytes in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (0.45.5)\n",
      "Requirement already satisfied: torch<3,>=2.0 in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from bitsandbytes) (2.8.0.dev20250420+cu128)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from bitsandbytes) (2.1.2)\n",
      "Requirement already satisfied: filelock in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (2024.10.0)\n",
      "Requirement already satisfied: setuptools in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (70.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (2.1.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q transformers accelerate bitsandbytes sentencepiece pydantic huggingface_hub\n",
    "\n",
    "#%pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu121\n",
    "#%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "%pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128\n",
    "%pip install -U bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d970ff92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# Set your token here securely or prompt for it in Colab\n",
    "# Recommended: store in Colab secrets or environment variable\n",
    "HF_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "if not HF_TOKEN:\n",
    "    from google.colab import userdata\n",
    "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "    if not HF_TOKEN:\n",
    "        # Prompt for token if not set in environment\n",
    "        print(\"🔑 Please enter your Hugging Face token:\")\n",
    "        # For Colab or local prompt input\n",
    "        HF_TOKEN = input(\"🔑 Enter your Hugging Face token: \").strip()\n",
    "\n",
    "login(token=HF_TOKEN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e277a8c7",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd88e640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95271ea9",
   "metadata": {},
   "source": [
    "##  Load Mistral-7B-Instruct with Fallback to Quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a1efba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Checking system resources...\n",
      "🧠 CUDA available: True\n",
      "🔧 Loading tokenizer: mistralai/Mistral-7B-Instruct-v0.1\n",
      "🔁 Trying 8-bit quantized loading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.58s/it]\n",
      "c:\\Ruby\\projects\\AI-resume-agent\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\rubyj\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Quantized 8-bit model loaded.\n",
      "🎯 Model is on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "import torch\n",
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\"\"\"\"\n",
    "def load_mistral_pipeline():\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            token=HF_TOKEN,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            token=HF_TOKEN,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        print(\"✅ Loaded full precision model.\")\n",
    "    except Exception:\n",
    "        print(\"⚠️ Full model failed. Loading quantized 8-bit version.\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            token=HF_TOKEN,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            token=HF_TOKEN,\n",
    "            device_map=\"auto\",\n",
    "            load_in_8bit=True,\n",
    "            quantization_config=BitsAndBytesConfig(load_in_8bit=True),\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        print(\"✅ Loaded 8-bit quantized model.\")\n",
    "\n",
    "    return pipeline(\"text-generation\", model=model, tokenizer=tokenizer, batch_size=2)\n",
    "\n",
    "llm_pipeline = load_mistral_pipeline()\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def load_mistral_pipeline_dynamic(model_name=MODEL_NAME, hf_token=None):\n",
    "    print(\"🔍 Checking system resources...\")\n",
    "    has_cuda = torch.cuda.is_available()\n",
    "\n",
    "    print(f\"🧠 CUDA available: {has_cuda}\")\n",
    "\n",
    "    print(f\"🔧 Loading tokenizer: {model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        token=hf_token,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    print(\"🔁 Trying 8-bit quantized loading...\")\n",
    "    \"\"\"\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        token=hf_token,\n",
    "        device_map=device_map,\n",
    "        quantization_config=BitsAndBytesConfig(load_in_8bit=True),\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,            # load model in 4-bit precision\n",
    "        bnb_4bit_quant_type=\"nf4\",    # pre-trained model should be quantized in 4-bit NF format\n",
    "        bnb_4bit_use_double_quant=True, # Using double quantization as mentioned in QLoRA paper\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16, # During computation, pre-trained model should be loaded in BF16 format\n",
    "        llm_int8_enable_fp32_cpu_offload=True\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        token=hf_token,\n",
    "        quantization_config=bnb_config, # Use bitsandbytes config\n",
    "        device_map=\"auto\",  # Specifying device_map=\"auto\" so that HF Accelerate will determine which GPU to put each layer of the model on\n",
    "        trust_remote_code=True, # Set trust_remote_code=True to use falcon-7b model with custom code\n",
    "    )\n",
    "    print(\"✅ Quantized 8-bit model loaded.\")\n",
    "\n",
    "    print(\"🎯 Model is on device:\", next(model.parameters()).device)\n",
    "    return pipeline(\"text-generation\", model=model, tokenizer=tokenizer, batch_size=2)\n",
    "\n",
    "m_pipeline = load_mistral_pipeline_dynamic(hf_token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b39980b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA version: 12.8\n",
      "GPU: NVIDIA GeForce RTX 5080\n",
      "(12, 0)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")\n",
    "\n",
    "print(torch.cuda.get_device_capability(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af05c12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Detecting system resources...\n",
      "🧠 CUDA available: True\n",
      "📊 Free GPU memory: 7.54 GB\n",
      "🔧 Loading tokenizer...\n",
      "⚙️ Using 8-bit quantized model (low VRAM or CPU fallback)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.31s/it]\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 8-bit model.\n",
      "🎯 Model on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "\n",
    "def get_available_gpu_memory_gb():\n",
    "    \"\"\"Returns available GPU memory (in GB) using nvidia-smi. Returns 0 if GPU not available.\"\"\"\n",
    "    try:\n",
    "        output = subprocess.check_output(\n",
    "            [\"nvidia-smi\", \"--query-gpu=memory.free\", \"--format=csv,nounits,noheader\"],\n",
    "            encoding=\"utf-8\"\n",
    "        )\n",
    "        # First GPU (assume single GPU setup)\n",
    "        free_mem_mb = int(output.strip().split(\"\\n\")[0])\n",
    "        return free_mem_mb / 1024  # Convert MB to GB\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Could not fetch GPU memory: {str(e)}\")\n",
    "        return 0.0\n",
    "\n",
    "def load_mistral_pipeline_dynamic(model_name=\"mistralai/Mistral-7B-Instruct-v0.1\", hf_token=None):\n",
    "    print(\"🔍 Detecting system resources...\")\n",
    "    has_cuda = torch.cuda.is_available()\n",
    "    free_vram_gb = get_available_gpu_memory_gb() if has_cuda else 0\n",
    "\n",
    "    print(f\"🧠 CUDA available: {has_cuda}\")\n",
    "    print(f\"📊 Free GPU memory: {free_vram_gb:.2f} GB\")\n",
    "\n",
    "    device_map = {\"\": 0} if has_cuda else \"cpu\"\n",
    "    load_quantized = (not has_cuda) or (free_vram_gb < 14)  # ~14 GB is a safe threshold for Mistral-7B FP16\n",
    "\n",
    "    print(\"🔧 Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        token=hf_token,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        if load_quantized:\n",
    "            print(\"⚙️ Using 8-bit quantized model (low VRAM or CPU fallback)...\")\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                token=hf_token,\n",
    "                device_map=device_map,\n",
    "                quantization_config=BitsAndBytesConfig(load_in_8bit=True),\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            print(\"✅ Loaded 8-bit model.\")\n",
    "        else:\n",
    "            print(\"⚙️ Using full precision FP16 model...\")\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                token=hf_token,\n",
    "                device_map=device_map,\n",
    "                torch_dtype=torch.float16,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            print(\"✅ Loaded full precision model.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading model: {str(e)}\")\n",
    "        raise RuntimeError(\"Model load failed.\")\n",
    "\n",
    "    print(\"🎯 Model on device:\", next(model.parameters()).device)\n",
    "    return pipeline(\"text-generation\", model=model, tokenizer=tokenizer, batch_size=2)\n",
    "\n",
    "llm_pipeline = load_mistral_pipeline_dynamic(hf_token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d743299d",
   "metadata": {},
   "source": [
    "## JSON Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04247778",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_schema = {\n",
    "    \"basics\": {\n",
    "        \"name\": \"string\",\n",
    "        \"email\": \"string\",\n",
    "        \"phone\": \"string\",\n",
    "        \"location\": \"string\",\n",
    "        \"summary\": \"string\"\n",
    "    },\n",
    "    \"education\": [{\"degree\": \"string\", \"field\": \"string\", \"institution\": \"string\", \"year\": \"string\"}],\n",
    "    \"experience\": [{\"job_title\": \"string\", \"company\": \"string\", \"duration\": \"string\", \"description\": \"string\"}],\n",
    "    \"skills\": [\"string\"],\n",
    "    \"certifications\": [\"string\"],\n",
    "    \"projects\": [\"string\"]\n",
    "}\n",
    "\n",
    "jd_schema = {\n",
    "    \"title\": \"string\",\n",
    "    \"summary\": \"string\",\n",
    "    \"required_experience_years\": \"float\",\n",
    "    \"preferred_degrees\": [\"string\"],\n",
    "    \"required_skills\": [\"string\"],\n",
    "    \"certifications\": [\"string\"],\n",
    "    \"soft_skills\": [\"string\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc3114a",
   "metadata": {},
   "source": [
    "##  Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1fd3ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESUME_PROMPT_TEMPLATE = \"\"\"Extract the structured resume JSON from the raw resume below:\n",
    "--------------------\n",
    "{text}\n",
    "--------------------\n",
    "The output should match this schema (no extra fields):\n",
    "{schema}\n",
    "Return a valid JSON object only.\n",
    "\"\"\"\n",
    "\n",
    "JD_PROMPT_TEMPLATE = \"\"\"Extract structured job description JSON from the raw JD below:\n",
    "--------------------\n",
    "{text}\n",
    "--------------------\n",
    "The output should match this schema:\n",
    "{schema}\n",
    "Return a valid JSON object only.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec7d2c6",
   "metadata": {},
   "source": [
    "## Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34b1aea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_structured_json(text, prompt_template, schema_dict, max_new_tokens=512):\n",
    "    schema_str = json.dumps(schema_dict, indent=2)\n",
    "    prompt = prompt_template.format(text=text.strip()[:1500], schema=schema_str)\n",
    "    response = llm_pipeline(prompt, max_new_tokens=max_new_tokens, do_sample=False)[0][\"generated_text\"]\n",
    "    json_start = response.find(\"{\")\n",
    "    try:\n",
    "        return json.loads(response[json_start:])\n",
    "    except json.JSONDecodeError:\n",
    "        return {\"raw_output\": response}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a69c78",
   "metadata": {},
   "source": [
    "##  Normalize & Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f4c6cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_and_save(input_filename, output_filename, is_resume=True):\n",
    "    input_path = Path(Config.JSON_OUTPUT_DIR) / input_filename\n",
    "    output_path = Path(Config.JSON_OUTPUT_DIR) / output_filename\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = [json.loads(line) for line in f.readlines() if line.strip()]\n",
    "\n",
    "    if output_path.exists():\n",
    "        print(f\"🗑️ Deleting existing file: {output_path}\")\n",
    "        output_path.unlink()\n",
    "\n",
    "    print(f\"⏳ Normalizing: {input_filename} → {output_filename}\")\n",
    "    results = []\n",
    "    for entry in tqdm(data):\n",
    "        text = entry.get(\"Resume_str\" if is_resume else \"description\", \"\")\n",
    "        parsed = extract_structured_json(\n",
    "            text=text,\n",
    "            prompt_template=RESUME_PROMPT_TEMPLATE if is_resume else JD_PROMPT_TEMPLATE,\n",
    "            schema_dict=resume_schema if is_resume else jd_schema\n",
    "        )\n",
    "        results.append(parsed)\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print(f\"✅ Done: Saved {len(results)} entries to {output_path.name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48f65d6",
   "metadata": {},
   "source": [
    "## Phase 2 Main Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43c3cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Normalizing: parsed_resumes.json → normalized_resumes.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2484 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  0%|          | 1/2484 [00:18<12:59:42, 18.84s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  0%|          | 2/2484 [00:35<12:08:26, 17.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  0%|          | 3/2484 [00:51<11:44:14, 17.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  0%|          | 4/2484 [01:30<17:38:04, 25.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  0%|          | 5/2484 [02:12<21:36:05, 31.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  0%|          | 6/2484 [02:29<18:11:59, 26.44s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  0%|          | 7/2484 [02:56<18:29:39, 26.88s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  0%|          | 8/2484 [03:13<16:15:06, 23.63s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  0%|          | 9/2484 [03:53<19:39:33, 28.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  0%|          | 10/2484 [04:35<22:30:26, 32.75s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  0%|          | 11/2484 [04:51<19:08:25, 27.86s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  0%|          | 12/2484 [05:08<16:46:00, 24.42s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|          | 13/2484 [05:24<14:58:53, 21.83s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|          | 14/2484 [05:55<16:58:39, 24.74s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|          | 15/2484 [06:11<15:07:17, 22.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|          | 16/2484 [06:51<18:51:32, 27.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|          | 17/2484 [07:25<20:01:56, 29.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|          | 18/2484 [08:04<22:02:51, 32.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|          | 19/2484 [08:19<18:37:03, 27.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|          | 20/2484 [08:35<16:13:10, 23.70s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|          | 21/2484 [09:07<17:54:35, 26.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|          | 22/2484 [09:23<15:49:28, 23.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|          | 23/2484 [09:39<14:21:12, 21.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|          | 24/2484 [09:54<13:13:18, 19.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|          | 25/2484 [10:35<17:32:31, 25.68s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|          | 26/2484 [11:10<19:27:40, 28.50s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|          | 27/2484 [11:42<20:12:11, 29.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|          | 28/2484 [11:58<17:21:02, 25.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|          | 29/2484 [12:23<17:15:52, 25.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|          | 30/2484 [12:56<18:47:33, 27.57s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|          | 31/2484 [13:24<18:52:33, 27.70s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|▏         | 32/2484 [13:48<18:09:14, 26.65s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|▏         | 33/2484 [14:21<19:27:19, 28.58s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|▏         | 34/2484 [14:37<16:49:21, 24.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|▏         | 35/2484 [14:52<14:56:32, 21.97s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|▏         | 36/2484 [2:30:59<1676:32:42, 2465.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|▏         | 37/2484 [2:31:34<1180:12:44, 1736.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|▏         | 38/2484 [2:31:59<830:59:15, 1223.04s/it] Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|▏         | 39/2484 [2:32:18<585:17:24, 861.78s/it] Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|▏         | 40/2484 [2:32:47<415:19:33, 611.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|▏         | 41/2484 [2:33:33<300:02:06, 442.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|▏         | 42/2484 [2:33:55<214:23:17, 316.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|▏         | 43/2484 [2:34:17<154:31:54, 227.90s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|▏         | 44/2484 [2:34:39<112:37:19, 166.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|▏         | 45/2484 [2:35:01<83:12:04, 122.81s/it] Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|▏         | 46/2484 [2:35:23<62:39:31, 92.52s/it] Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|▏         | 47/2484 [2:36:17<54:49:39, 80.99s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|▏         | 48/2484 [2:37:12<49:32:03, 73.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|▏         | 49/2484 [2:37:56<43:39:39, 64.55s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|▏         | 50/2484 [2:38:18<34:53:16, 51.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|▏         | 51/2484 [2:38:36<28:04:01, 41.53s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|▏         | 52/2484 [2:38:54<23:21:15, 34.57s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|▏         | 53/2484 [2:39:41<25:57:49, 38.45s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|▏         | 54/2484 [2:40:03<22:36:12, 33.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|▏         | 55/2484 [2:41:27<32:47:36, 48.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|▏         | 56/2484 [2:44:10<55:48:57, 82.76s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|▏         | 57/2484 [2:45:19<53:05:44, 78.76s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|▏         | 58/2484 [2:46:26<50:36:43, 75.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|▏         | 59/2484 [2:47:30<48:27:14, 71.93s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|▏         | 60/2484 [2:48:36<47:17:14, 70.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|▏         | 61/2484 [2:49:42<46:16:13, 68.75s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|▏         | 62/2484 [2:50:49<45:57:25, 68.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  3%|▎         | 63/2484 [2:51:54<45:13:47, 67.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  3%|▎         | 64/2484 [2:54:15<60:06:39, 89.42s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  3%|▎         | 65/2484 [2:55:20<55:15:39, 82.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  3%|▎         | 66/2484 [2:56:22<51:04:25, 76.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  3%|▎         | 67/2484 [2:57:29<49:13:28, 73.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  3%|▎         | 68/2484 [2:58:32<47:07:44, 70.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  3%|▎         | 69/2484 [2:59:36<45:56:47, 68.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  3%|▎         | 70/2484 [3:00:41<45:08:07, 67.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  3%|▎         | 71/2484 [3:03:30<65:36:56, 97.89s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  3%|▎         | 72/2484 [3:05:07<65:19:04, 97.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  3%|▎         | 73/2484 [3:06:10<58:18:36, 87.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  3%|▎         | 74/2484 [3:08:58<74:35:34, 111.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  3%|▎         | 75/2484 [3:10:53<75:22:47, 112.65s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  3%|▎         | 76/2484 [3:12:00<66:06:18, 98.83s/it] Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  3%|▎         | 77/2484 [3:13:29<64:03:40, 95.81s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  3%|▎         | 78/2484 [3:15:41<71:15:49, 106.63s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  3%|▎         | 79/2484 [3:18:23<82:22:14, 123.30s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  3%|▎         | 80/2484 [3:19:31<71:23:06, 106.90s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  3%|▎         | 81/2484 [3:20:39<63:29:16, 95.11s/it] Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  3%|▎         | 82/2484 [3:23:37<80:04:02, 120.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  3%|▎         | 83/2484 [3:24:46<69:53:01, 104.78s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  3%|▎         | 84/2484 [3:27:28<81:14:33, 121.86s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  3%|▎         | 85/2484 [3:28:36<70:21:03, 105.57s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  3%|▎         | 86/2484 [3:29:40<62:09:40, 93.32s/it] Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  4%|▎         | 87/2484 [3:30:51<57:33:27, 86.44s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  4%|▎         | 88/2484 [3:33:14<68:50:16, 103.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  4%|▎         | 89/2484 [3:34:20<61:23:52, 92.29s/it] Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "def run_phase2_structured_normalization():\n",
    "    normalize_and_save(\"parsed_resumes.json\", \"normalized_resumes.json\", is_resume=True)\n",
    "    normalize_and_save(\"parsed_jds.json\", \"normalized_jds.json\", is_resume=False)\n",
    "\n",
    "run_phase2_structured_normalization()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
