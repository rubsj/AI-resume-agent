{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcb849be",
   "metadata": {},
   "source": [
    "# Phase 1: First Steps Notebook ‚Äî Data Ingestion + Minimal Parsing\n",
    "1. Setup and Install Dependencies\n",
    "2. Load Resume and JD datasets\n",
    "3. Minimal Parsing into JSON Structure\n",
    "4. Save structured JSON for Phase 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dd9696",
   "metadata": {},
   "source": [
    "## Setup and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4abca3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kaggle\n",
      "  Using cached kaggle-1.7.4.2-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting kagglehub\n",
      "  Using cached kagglehub-0.3.11-py3-none-any.whl.metadata (32 kB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Collecting bleach (from kaggle)\n",
      "  Using cached bleach-6.2.0-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting certifi>=14.05.14 (from kaggle)\n",
      "  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting charset-normalizer (from kaggle)\n",
      "  Using cached charset_normalizer-3.4.1-cp312-cp312-win_amd64.whl.metadata (36 kB)\n",
      "Collecting idna (from kaggle)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting protobuf (from kaggle)\n",
      "  Using cached protobuf-6.30.2-cp310-abi3-win_amd64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from kaggle) (2.9.0.post0)\n",
      "Collecting python-slugify (from kaggle)\n",
      "  Using cached python_slugify-8.0.4-py2.py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting requests (from kaggle)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from kaggle) (70.2.0)\n",
      "Requirement already satisfied: six>=1.10 in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from kaggle) (1.17.0)\n",
      "Collecting text-unidecode (from kaggle)\n",
      "  Using cached text_unidecode-1.3-py2.py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting tqdm (from kaggle)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting urllib3>=1.15.1 (from kaggle)\n",
      "  Using cached urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting webencodings (from kaggle)\n",
      "  Using cached webencodings-0.5.1-py2.py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: packaging in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from kagglehub) (25.0)\n",
      "Collecting pyyaml (from kagglehub)\n",
      "  Using cached PyYAML-6.0.2-cp312-cp312-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from pandas) (2.1.2)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: colorama in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from tqdm->kaggle) (0.4.6)\n",
      "Using cached kaggle-1.7.4.2-py3-none-any.whl (173 kB)\n",
      "Using cached kagglehub-0.3.11-py3-none-any.whl (63 kB)\n",
      "Using cached pandas-2.2.3-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "Using cached certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "Using cached bleach-6.2.0-py3-none-any.whl (163 kB)\n",
      "Using cached charset_normalizer-3.4.1-cp312-cp312-win_amd64.whl (102 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached protobuf-6.30.2-cp310-abi3-win_amd64.whl (431 kB)\n",
      "Using cached python_slugify-8.0.4-py2.py3-none-any.whl (10 kB)\n",
      "Using cached text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "Using cached PyYAML-6.0.2-cp312-cp312-win_amd64.whl (156 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: webencodings, text-unidecode, pytz, urllib3, tzdata, tqdm, pyyaml, python-slugify, protobuf, idna, charset-normalizer, certifi, bleach, requests, pandas, kagglehub, kaggle\n",
      "Successfully installed bleach-6.2.0 certifi-2025.1.31 charset-normalizer-3.4.1 idna-3.10 kaggle-1.7.4.2 kagglehub-0.3.11 pandas-2.2.3 protobuf-6.30.2 python-slugify-8.0.4 pytz-2025.2 pyyaml-6.0.2 requests-2.32.3 text-unidecode-1.3 tqdm-4.67.1 tzdata-2025.2 urllib3-2.4.0 webencodings-0.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install kaggle kagglehub pandas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb756b8f",
   "metadata": {},
   "source": [
    "## Util Classes and methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43abc8ac",
   "metadata": {},
   "source": [
    "### Configurations  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1e827a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# üõ† CONFIGURATION\n",
    "# ==============================\n",
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "class Config:\n",
    "    DATASET_DOWNLOAD_DIR = \"datasets\"\n",
    "    JSON_OUTPUT_DIR = \"json_outputs\"\n",
    "    AUTO_CLEANUP = True\n",
    "\n",
    "    @staticmethod\n",
    "    def setup_kaggle_credentials():\n",
    "        kaggle_path = os.path.expanduser('~/.kaggle/kaggle.json')\n",
    "        if not os.path.exists(kaggle_path):\n",
    "            from google.colab import files\n",
    "            print(\"üìÇ Upload kaggle.json file...\")\n",
    "            uploaded = files.upload()\n",
    "            os.makedirs(os.path.dirname(kaggle_path), exist_ok=True)\n",
    "            for filename in uploaded.keys():\n",
    "                shutil.move(filename, kaggle_path)\n",
    "            os.chmod(kaggle_path, 0o600)\n",
    "            print(f\"‚úÖ Kaggle credentials setup at {kaggle_path}\")\n",
    "        else:\n",
    "            print(f\"‚úÖ Kaggle credentials already exist at {kaggle_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a537a5",
   "metadata": {},
   "source": [
    "### Downloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89480102",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================\n",
    "# DOWNLOADER\n",
    "# ==============================\n",
    "class DatasetDownloader:\n",
    "    @staticmethod\n",
    "    def download_and_extract(dataset_path: str) -> tuple[str, str]:\n",
    "        os.makedirs(Config.DATASET_DOWNLOAD_DIR, exist_ok=True)\n",
    "        dataset_slug = dataset_path.split(\"/\")[-1]\n",
    "        extract_folder_path = os.path.join(Config.DATASET_DOWNLOAD_DIR, dataset_slug)\n",
    "        zip_filename = f\"{dataset_slug}.zip\"\n",
    "        zip_path = os.path.join(Config.DATASET_DOWNLOAD_DIR, zip_filename)\n",
    "\n",
    "        if os.path.exists(extract_folder_path) and any(Path(extract_folder_path).rglob(\"*.csv\")):\n",
    "            print(f\"‚ö° Dataset folder already exists at '{extract_folder_path}', skipping download and extraction.\")\n",
    "            return extract_folder_path, zip_filename\n",
    "\n",
    "        print(f\"‚¨áÔ∏è Downloading dataset: {dataset_path} ...\")\n",
    "        !kaggle datasets download -d {dataset_path} -p {Config.DATASET_DOWNLOAD_DIR}\n",
    "\n",
    "        if not os.path.exists(zip_path):\n",
    "            raise FileNotFoundError(f\"‚ùå Zip file '{zip_filename}' not found after download!\")\n",
    "\n",
    "        os.makedirs(extract_folder_path, exist_ok=True)\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_folder_path)\n",
    "\n",
    "        print(f\"‚úÖ Downloaded and extracted to '{extract_folder_path}'.\")\n",
    "        return extract_folder_path, zip_filename\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1170df00",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2c06119",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================\n",
    "# LOADER\n",
    "# ==============================\n",
    "class DatasetLoader:\n",
    "    @staticmethod\n",
    "    def load_csv(dataset_folder: str, target_csv_name: str) -> pd.DataFrame:\n",
    "        print(f\"üîç Searching for '{target_csv_name}' inside {dataset_folder}...\")\n",
    "        if not os.path.exists(dataset_folder):\n",
    "            raise FileNotFoundError(f\"‚ùå Dataset folder '{dataset_folder}' does not exist!\")\n",
    "\n",
    "        for root, _, files in os.walk(dataset_folder):\n",
    "            for file in files:\n",
    "                if file.lower() == target_csv_name.lower():\n",
    "                    csv_path = os.path.join(root, file)\n",
    "                    df = pd.read_csv(csv_path)\n",
    "                    print(f\"‚úÖ Loaded CSV with shape {df.shape}\")\n",
    "                    return df\n",
    "\n",
    "        raise FileNotFoundError(f\"‚ùå CSV file '{target_csv_name}' not found inside extracted dataset!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a80a5a",
   "metadata": {},
   "source": [
    "### Data Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08247c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================\n",
    "# PROCESSOR\n",
    "# ==============================\n",
    "class DatasetProcessor:\n",
    "    @staticmethod\n",
    "    def filter_fields(df: pd.DataFrame, allowed_fields: List[str]) -> pd.DataFrame:\n",
    "        missing_fields = [field for field in allowed_fields if field not in df.columns]\n",
    "        if missing_fields:\n",
    "            raise ValueError(f\"‚ùå Fields {missing_fields} not found in dataset!\")\n",
    "\n",
    "        filtered_df = df[allowed_fields]\n",
    "        print(f\"‚úÖ Filtered columns: {list(filtered_df.columns)}\")\n",
    "        return filtered_df\n",
    "\n",
    "    @staticmethod\n",
    "    def save_to_json(df: pd.DataFrame, output_json_name: str):\n",
    "        os.makedirs(Config.JSON_OUTPUT_DIR, exist_ok=True)\n",
    "        output_path = os.path.join(Config.JSON_OUTPUT_DIR, output_json_name)\n",
    "\n",
    "        if os.path.exists(output_path):\n",
    "            os.remove(output_path)\n",
    "            print(f\"üóëÔ∏è Existing JSON '{output_path}' deleted.\")\n",
    "\n",
    "        df.to_json(output_path, orient='records', lines=True, force_ascii=False)\n",
    "        print(f\"‚úÖ Data saved to JSON at '{output_path}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fc4fb6",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7245255",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ==============================\n",
    "# CLEANER\n",
    "# ==============================\n",
    "class Cleaner:\n",
    "    @staticmethod\n",
    "    def cleanup_dataset_artifacts(extracted_folder_path: str, zip_filename: str):\n",
    "        if os.path.exists(extracted_folder_path):\n",
    "            shutil.rmtree(extracted_folder_path)\n",
    "            print(f\"üßπ Folder '{extracted_folder_path}' has been deleted successfully.\")\n",
    "\n",
    "        zip_path = os.path.join(Config.DATASET_DOWNLOAD_DIR, zip_filename)\n",
    "        if os.path.exists(zip_path):\n",
    "            os.remove(zip_path)\n",
    "            print(f\"üóëÔ∏è Zip file '{zip_path}' has been deleted successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a2525b",
   "metadata": {},
   "source": [
    "### Hybrid Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8d85253",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Ruby\\projects\\AI-resume-agent\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ==============================\n",
    "# HYBRID LOADER\n",
    "# ==============================\n",
    "try:\n",
    "    import kagglehub\n",
    "    from kagglehub import KaggleDatasetAdapter\n",
    "except ImportError:\n",
    "    kagglehub = None\n",
    "\n",
    "class HybridDatasetLoader:\n",
    "    @staticmethod\n",
    "    def load_dataset(dataset_path: str, file_name: str) -> pd.DataFrame:\n",
    "        if kagglehub:\n",
    "            try:\n",
    "                print(f\"üì• Trying KaggleHub for {dataset_path}...\")\n",
    "                df = kagglehub.dataset_load(KaggleDatasetAdapter.PANDAS, dataset_path, file_name)\n",
    "                print(f\"‚úÖ Loaded using KaggleHub: shape = {df.shape}\")\n",
    "                return df\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è KaggleHub failed: {e}\\nFalling back to ZIP-based loader.\")\n",
    "\n",
    "        extracted_folder, _ = DatasetDownloader.download_and_extract(dataset_path)\n",
    "        return DatasetLoader.load_csv(extracted_folder, file_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e917e9d6",
   "metadata": {},
   "source": [
    "### Main flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "766a42bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================\n",
    "# MAIN FLOW\n",
    "# ==============================\n",
    "def process_dataset(dataset_path: str, target_csv_name: str, allowed_fields: List[str], output_json_name: str):\n",
    "    df = HybridDatasetLoader.load_dataset(dataset_path, target_csv_name)\n",
    "    filtered_df = DatasetProcessor.filter_fields(df, allowed_fields)\n",
    "    DatasetProcessor.save_to_json(filtered_df, output_json_name)\n",
    "\n",
    "    dataset_slug = dataset_path.split(\"/\")[-1]\n",
    "    extracted_folder = os.path.join(Config.DATASET_DOWNLOAD_DIR, dataset_slug)\n",
    "    zip_filename = f\"{dataset_slug}.zip\"\n",
    "    if Config.AUTO_CLEANUP:\n",
    "        Cleaner.cleanup_dataset_artifacts(extracted_folder, zip_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60577088",
   "metadata": {},
   "source": [
    "## Login and do the processing of Resume and JD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3bb5022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Kaggle credentials already exist at C:\\Users\\rubyj/.kaggle/kaggle.json\n",
      "üì• Trying KaggleHub for snehaanbhawal/resume-dataset...\n",
      "‚ö†Ô∏è KaggleHub failed: 404 Client Error.\n",
      "\n",
      "Resource not found at URL: https://www.kaggle.com/datasets/snehaanbhawal/resume-dataset/versions/1\n",
      "The server reported the following issues: Data not found\n",
      "Please make sure you specified the correct resource identifiers.\n",
      "Falling back to ZIP-based loader.\n",
      "‚¨áÔ∏è Downloading dataset: snehaanbhawal/resume-dataset ...\n",
      "Dataset URL: https://www.kaggle.com/datasets/snehaanbhawal/resume-dataset\n",
      "License(s): CC0-1.0\n",
      "‚úÖ Downloaded and extracted to 'datasets\\resume-dataset'.\n",
      "üîç Searching for 'Resume.csv' inside datasets\\resume-dataset...\n",
      "‚úÖ Loaded CSV with shape (2484, 4)\n",
      "‚úÖ Filtered columns: ['Category', 'Resume_str']\n",
      "üóëÔ∏è Existing JSON 'json_outputs\\parsed_resumes.json' deleted.\n",
      "‚úÖ Data saved to JSON at 'json_outputs\\parsed_resumes.json'\n",
      "üßπ Folder 'datasets\\resume-dataset' has been deleted successfully.\n",
      "üóëÔ∏è Zip file 'datasets\\resume-dataset.zip' has been deleted successfully.\n",
      "üì• Trying KaggleHub for arshkon/linkedin-job-postings...\n",
      "‚ö†Ô∏è KaggleHub failed: Error reading file: 'utf-8' codec can't decode byte 0x80 in position 14: invalid start byte\n",
      "Falling back to ZIP-based loader.\n",
      "‚¨áÔ∏è Downloading dataset: arshkon/linkedin-job-postings ...\n",
      "Dataset URL: https://www.kaggle.com/datasets/arshkon/linkedin-job-postings\n",
      "License(s): CC-BY-SA-4.0\n",
      "‚úÖ Downloaded and extracted to 'datasets\\linkedin-job-postings'.\n",
      "üîç Searching for 'postings.csv' inside datasets\\linkedin-job-postings...\n",
      "‚úÖ Loaded CSV with shape (123849, 31)\n",
      "‚úÖ Filtered columns: ['title', 'company_name', 'location', 'description', 'skills_desc', 'job_id', 'formatted_experience_level', 'formatted_work_type']\n",
      "üóëÔ∏è Existing JSON 'json_outputs\\parsed_jds.json' deleted.\n",
      "‚úÖ Data saved to JSON at 'json_outputs\\parsed_jds.json'\n",
      "üßπ Folder 'datasets\\linkedin-job-postings' has been deleted successfully.\n",
      "üóëÔ∏è Zip file 'datasets\\linkedin-job-postings.zip' has been deleted successfully.\n"
     ]
    }
   ],
   "source": [
    "Config.setup_kaggle_credentials()\n",
    "# Process Resume Dataset\n",
    "process_dataset(\n",
    "    dataset_path=\"snehaanbhawal/resume-dataset\",\n",
    "    target_csv_name=\"Resume.csv\",\n",
    "    allowed_fields=[\"Category\", \"Resume_str\"],\n",
    "    output_json_name=\"parsed_resumes.json\"\n",
    ")\n",
    "\n",
    "# Process Job Postings Dataset\n",
    "process_dataset(\n",
    "    dataset_path=\"arshkon/linkedin-job-postings\",\n",
    "    target_csv_name=\"postings.csv\",\n",
    "    allowed_fields=[\"title\", \"company_name\", \"location\", \"description\", \"skills_desc\", \"job_id\" , \"formatted_experience_level\", \"formatted_work_type\"],\n",
    "    output_json_name=\"parsed_jds.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74195950",
   "metadata": {},
   "source": [
    "# Phase 2 -\tParse resume/JD into JSON structured scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27168f09",
   "metadata": {},
   "source": [
    "##  Install Dependencies  & Login to Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "381adace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://download.pytorch.org/whl/nightly/cu128\n",
      "Requirement already satisfied: torch in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (2.8.0.dev20250420+cu128)\n",
      "Requirement already satisfied: torchvision in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (0.22.0.dev20250421+cu128)\n",
      "Requirement already satisfied: torchaudio in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (2.6.0.dev20250421+cu128)\n",
      "Requirement already satisfied: filelock in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: setuptools in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from torch) (70.2.0)\n",
      "Requirement already satisfied: numpy in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from torchvision) (2.1.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: bitsandbytes in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (0.45.5)\n",
      "Requirement already satisfied: torch<3,>=2.0 in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from bitsandbytes) (2.8.0.dev20250420+cu128)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from bitsandbytes) (2.1.2)\n",
      "Requirement already satisfied: filelock in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (2024.10.0)\n",
      "Requirement already satisfied: setuptools in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (70.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\ruby\\projects\\ai-resume-agent\\.venv\\lib\\site-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (2.1.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q transformers accelerate bitsandbytes sentencepiece pydantic huggingface_hub\n",
    "\n",
    "#%pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu121\n",
    "#%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "%pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128\n",
    "%pip install -U bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d970ff92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# Set your token here securely or prompt for it in Colab\n",
    "# Recommended: store in Colab secrets or environment variable\n",
    "HF_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "if not HF_TOKEN:\n",
    "    from google.colab import userdata\n",
    "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "    if not HF_TOKEN:\n",
    "        # Prompt for token if not set in environment\n",
    "        print(\"üîë Please enter your Hugging Face token:\")\n",
    "        # For Colab or local prompt input\n",
    "        HF_TOKEN = input(\"üîë Enter your Hugging Face token: \").strip()\n",
    "\n",
    "login(token=HF_TOKEN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e277a8c7",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd88e640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95271ea9",
   "metadata": {},
   "source": [
    "##  Load Mistral-7B-Instruct with Fallback to Quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a1efba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Checking system resources...\n",
      "üß† CUDA available: True\n",
      "üîß Loading tokenizer: mistralai/Mistral-7B-Instruct-v0.1\n",
      "üîÅ Trying 8-bit quantized loading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:17<00:00,  8.58s/it]\n",
      "c:\\Ruby\\projects\\AI-resume-agent\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\rubyj\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Quantized 8-bit model loaded.\n",
      "üéØ Model is on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "import torch\n",
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\"\"\"\"\n",
    "def load_mistral_pipeline():\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            token=HF_TOKEN,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            token=HF_TOKEN,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        print(\"‚úÖ Loaded full precision model.\")\n",
    "    except Exception:\n",
    "        print(\"‚ö†Ô∏è Full model failed. Loading quantized 8-bit version.\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            token=HF_TOKEN,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            token=HF_TOKEN,\n",
    "            device_map=\"auto\",\n",
    "            load_in_8bit=True,\n",
    "            quantization_config=BitsAndBytesConfig(load_in_8bit=True),\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        print(\"‚úÖ Loaded 8-bit quantized model.\")\n",
    "\n",
    "    return pipeline(\"text-generation\", model=model, tokenizer=tokenizer, batch_size=2)\n",
    "\n",
    "llm_pipeline = load_mistral_pipeline()\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def load_mistral_pipeline_dynamic(model_name=MODEL_NAME, hf_token=None):\n",
    "    print(\"üîç Checking system resources...\")\n",
    "    has_cuda = torch.cuda.is_available()\n",
    "\n",
    "    print(f\"üß† CUDA available: {has_cuda}\")\n",
    "\n",
    "    print(f\"üîß Loading tokenizer: {model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        token=hf_token,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    print(\"üîÅ Trying 8-bit quantized loading...\")\n",
    "    \"\"\"\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        token=hf_token,\n",
    "        device_map=device_map,\n",
    "        quantization_config=BitsAndBytesConfig(load_in_8bit=True),\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,            # load model in 4-bit precision\n",
    "        bnb_4bit_quant_type=\"nf4\",    # pre-trained model should be quantized in 4-bit NF format\n",
    "        bnb_4bit_use_double_quant=True, # Using double quantization as mentioned in QLoRA paper\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16, # During computation, pre-trained model should be loaded in BF16 format\n",
    "        llm_int8_enable_fp32_cpu_offload=True\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        token=hf_token,\n",
    "        quantization_config=bnb_config, # Use bitsandbytes config\n",
    "        device_map=\"auto\",  # Specifying device_map=\"auto\" so that HF Accelerate will determine which GPU to put each layer of the model on\n",
    "        trust_remote_code=True, # Set trust_remote_code=True to use falcon-7b model with custom code\n",
    "    )\n",
    "    print(\"‚úÖ Quantized 8-bit model loaded.\")\n",
    "\n",
    "    print(\"üéØ Model is on device:\", next(model.parameters()).device)\n",
    "    return pipeline(\"text-generation\", model=model, tokenizer=tokenizer, batch_size=2)\n",
    "\n",
    "m_pipeline = load_mistral_pipeline_dynamic(hf_token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b39980b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA version: 12.8\n",
      "GPU: NVIDIA GeForce RTX 5080\n",
      "(12, 0)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")\n",
    "\n",
    "print(torch.cuda.get_device_capability(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af05c12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Detecting system resources...\n",
      "üß† CUDA available: True\n",
      "üìä Free GPU memory: 7.54 GB\n",
      "üîß Loading tokenizer...\n",
      "‚öôÔ∏è Using 8-bit quantized model (low VRAM or CPU fallback)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:06<00:00,  3.31s/it]\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 8-bit model.\n",
      "üéØ Model on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "\n",
    "def get_available_gpu_memory_gb():\n",
    "    \"\"\"Returns available GPU memory (in GB) using nvidia-smi. Returns 0 if GPU not available.\"\"\"\n",
    "    try:\n",
    "        output = subprocess.check_output(\n",
    "            [\"nvidia-smi\", \"--query-gpu=memory.free\", \"--format=csv,nounits,noheader\"],\n",
    "            encoding=\"utf-8\"\n",
    "        )\n",
    "        # First GPU (assume single GPU setup)\n",
    "        free_mem_mb = int(output.strip().split(\"\\n\")[0])\n",
    "        return free_mem_mb / 1024  # Convert MB to GB\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not fetch GPU memory: {str(e)}\")\n",
    "        return 0.0\n",
    "\n",
    "def load_mistral_pipeline_dynamic(model_name=\"mistralai/Mistral-7B-Instruct-v0.1\", hf_token=None):\n",
    "    print(\"üîç Detecting system resources...\")\n",
    "    has_cuda = torch.cuda.is_available()\n",
    "    free_vram_gb = get_available_gpu_memory_gb() if has_cuda else 0\n",
    "\n",
    "    print(f\"üß† CUDA available: {has_cuda}\")\n",
    "    print(f\"üìä Free GPU memory: {free_vram_gb:.2f} GB\")\n",
    "\n",
    "    device_map = {\"\": 0} if has_cuda else \"cpu\"\n",
    "    load_quantized = (not has_cuda) or (free_vram_gb < 14)  # ~14 GB is a safe threshold for Mistral-7B FP16\n",
    "\n",
    "    print(\"üîß Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        token=hf_token,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        if load_quantized:\n",
    "            print(\"‚öôÔ∏è Using 8-bit quantized model (low VRAM or CPU fallback)...\")\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                token=hf_token,\n",
    "                device_map=device_map,\n",
    "                quantization_config=BitsAndBytesConfig(load_in_8bit=True),\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            print(\"‚úÖ Loaded 8-bit model.\")\n",
    "        else:\n",
    "            print(\"‚öôÔ∏è Using full precision FP16 model...\")\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                token=hf_token,\n",
    "                device_map=device_map,\n",
    "                torch_dtype=torch.float16,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            print(\"‚úÖ Loaded full precision model.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading model: {str(e)}\")\n",
    "        raise RuntimeError(\"Model load failed.\")\n",
    "\n",
    "    print(\"üéØ Model on device:\", next(model.parameters()).device)\n",
    "    return pipeline(\"text-generation\", model=model, tokenizer=tokenizer, batch_size=2)\n",
    "\n",
    "llm_pipeline = load_mistral_pipeline_dynamic(hf_token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d743299d",
   "metadata": {},
   "source": [
    "## JSON Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04247778",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_schema = {\n",
    "    \"basics\": {\n",
    "        \"name\": \"string\",\n",
    "        \"email\": \"string\",\n",
    "        \"phone\": \"string\",\n",
    "        \"location\": \"string\",\n",
    "        \"summary\": \"string\"\n",
    "    },\n",
    "    \"education\": [{\"degree\": \"string\", \"field\": \"string\", \"institution\": \"string\", \"year\": \"string\"}],\n",
    "    \"experience\": [{\"job_title\": \"string\", \"company\": \"string\", \"duration\": \"string\", \"description\": \"string\"}],\n",
    "    \"skills\": [\"string\"],\n",
    "    \"certifications\": [\"string\"],\n",
    "    \"projects\": [\"string\"]\n",
    "}\n",
    "\n",
    "jd_schema = {\n",
    "    \"title\": \"string\",\n",
    "    \"summary\": \"string\",\n",
    "    \"required_experience_years\": \"float\",\n",
    "    \"preferred_degrees\": [\"string\"],\n",
    "    \"required_skills\": [\"string\"],\n",
    "    \"certifications\": [\"string\"],\n",
    "    \"soft_skills\": [\"string\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc3114a",
   "metadata": {},
   "source": [
    "##  Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1fd3ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESUME_PROMPT_TEMPLATE = \"\"\"Extract the structured resume JSON from the raw resume below:\n",
    "--------------------\n",
    "{text}\n",
    "--------------------\n",
    "The output should match this schema (no extra fields):\n",
    "{schema}\n",
    "Return a valid JSON object only.\n",
    "\"\"\"\n",
    "\n",
    "JD_PROMPT_TEMPLATE = \"\"\"Extract structured job description JSON from the raw JD below:\n",
    "--------------------\n",
    "{text}\n",
    "--------------------\n",
    "The output should match this schema:\n",
    "{schema}\n",
    "Return a valid JSON object only.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec7d2c6",
   "metadata": {},
   "source": [
    "## Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34b1aea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_structured_json(text, prompt_template, schema_dict, max_new_tokens=512):\n",
    "    schema_str = json.dumps(schema_dict, indent=2)\n",
    "    prompt = prompt_template.format(text=text.strip()[:1500], schema=schema_str)\n",
    "    response = llm_pipeline(prompt, max_new_tokens=max_new_tokens, do_sample=False)[0][\"generated_text\"]\n",
    "    json_start = response.find(\"{\")\n",
    "    try:\n",
    "        return json.loads(response[json_start:])\n",
    "    except json.JSONDecodeError:\n",
    "        return {\"raw_output\": response}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a69c78",
   "metadata": {},
   "source": [
    "##  Normalize & Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f4c6cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_and_save(input_filename, output_filename, is_resume=True):\n",
    "    input_path = Path(Config.JSON_OUTPUT_DIR) / input_filename\n",
    "    output_path = Path(Config.JSON_OUTPUT_DIR) / output_filename\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = [json.loads(line) for line in f.readlines() if line.strip()]\n",
    "\n",
    "    if output_path.exists():\n",
    "        print(f\"üóëÔ∏è Deleting existing file: {output_path}\")\n",
    "        output_path.unlink()\n",
    "\n",
    "    print(f\"‚è≥ Normalizing: {input_filename} ‚Üí {output_filename}\")\n",
    "    results = []\n",
    "    for entry in tqdm(data):\n",
    "        text = entry.get(\"Resume_str\" if is_resume else \"description\", \"\")\n",
    "        parsed = extract_structured_json(\n",
    "            text=text,\n",
    "            prompt_template=RESUME_PROMPT_TEMPLATE if is_resume else JD_PROMPT_TEMPLATE,\n",
    "            schema_dict=resume_schema if is_resume else jd_schema\n",
    "        )\n",
    "        results.append(parsed)\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print(f\"‚úÖ Done: Saved {len(results)} entries to {output_path.name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48f65d6",
   "metadata": {},
   "source": [
    "## Phase 2 Main Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d43c3cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Normalizing: parsed_resumes.json ‚Üí normalized_resumes.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2484 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  0%|          | 1/2484 [00:18<12:59:42, 18.84s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  0%|          | 2/2484 [00:35<12:08:26, 17.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  0%|          | 3/2484 [00:51<11:44:14, 17.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  0%|          | 4/2484 [01:30<17:38:04, 25.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  0%|          | 5/2484 [02:12<21:36:05, 31.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  0%|          | 6/2484 [02:29<18:11:59, 26.44s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  0%|          | 7/2484 [02:56<18:29:39, 26.88s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  0%|          | 8/2484 [03:13<16:15:06, 23.63s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  0%|          | 9/2484 [03:53<19:39:33, 28.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  0%|          | 10/2484 [04:35<22:30:26, 32.75s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  0%|          | 11/2484 [04:51<19:08:25, 27.86s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  0%|          | 12/2484 [05:08<16:46:00, 24.42s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|          | 13/2484 [05:24<14:58:53, 21.83s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|          | 14/2484 [05:55<16:58:39, 24.74s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|          | 15/2484 [06:11<15:07:17, 22.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|          | 16/2484 [06:51<18:51:32, 27.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|          | 17/2484 [07:25<20:01:56, 29.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|          | 18/2484 [08:04<22:02:51, 32.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|          | 19/2484 [08:19<18:37:03, 27.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|          | 20/2484 [08:35<16:13:10, 23.70s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|          | 21/2484 [09:07<17:54:35, 26.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|          | 22/2484 [09:23<15:49:28, 23.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|          | 23/2484 [09:39<14:21:12, 21.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|          | 24/2484 [09:54<13:13:18, 19.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|          | 25/2484 [10:35<17:32:31, 25.68s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|          | 26/2484 [11:10<19:27:40, 28.50s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|          | 27/2484 [11:42<20:12:11, 29.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|          | 28/2484 [11:58<17:21:02, 25.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|          | 29/2484 [12:23<17:15:52, 25.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|          | 30/2484 [12:56<18:47:33, 27.57s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|          | 31/2484 [13:24<18:52:33, 27.70s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|‚ñè         | 32/2484 [13:48<18:09:14, 26.65s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|‚ñè         | 33/2484 [14:21<19:27:19, 28.58s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|‚ñè         | 34/2484 [14:37<16:49:21, 24.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|‚ñè         | 35/2484 [14:52<14:56:32, 21.97s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|‚ñè         | 36/2484 [2:30:59<1676:32:42, 2465.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|‚ñè         | 37/2484 [2:31:34<1180:12:44, 1736.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|‚ñè         | 38/2484 [2:31:59<830:59:15, 1223.04s/it] Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|‚ñè         | 39/2484 [2:32:18<585:17:24, 861.78s/it] Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|‚ñè         | 40/2484 [2:32:47<415:19:33, 611.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|‚ñè         | 41/2484 [2:33:33<300:02:06, 442.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|‚ñè         | 42/2484 [2:33:55<214:23:17, 316.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|‚ñè         | 43/2484 [2:34:17<154:31:54, 227.90s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|‚ñè         | 44/2484 [2:34:39<112:37:19, 166.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|‚ñè         | 45/2484 [2:35:01<83:12:04, 122.81s/it] Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|‚ñè         | 46/2484 [2:35:23<62:39:31, 92.52s/it] Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|‚ñè         | 47/2484 [2:36:17<54:49:39, 80.99s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|‚ñè         | 48/2484 [2:37:12<49:32:03, 73.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|‚ñè         | 49/2484 [2:37:56<43:39:39, 64.55s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|‚ñè         | 50/2484 [2:38:18<34:53:16, 51.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|‚ñè         | 51/2484 [2:38:36<28:04:01, 41.53s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|‚ñè         | 52/2484 [2:38:54<23:21:15, 34.57s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|‚ñè         | 53/2484 [2:39:41<25:57:49, 38.45s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|‚ñè         | 54/2484 [2:40:03<22:36:12, 33.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|‚ñè         | 55/2484 [2:41:27<32:47:36, 48.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|‚ñè         | 56/2484 [2:44:10<55:48:57, 82.76s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|‚ñè         | 57/2484 [2:45:19<53:05:44, 78.76s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|‚ñè         | 58/2484 [2:46:26<50:36:43, 75.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|‚ñè         | 59/2484 [2:47:30<48:27:14, 71.93s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|‚ñè         | 60/2484 [2:48:36<47:17:14, 70.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|‚ñè         | 61/2484 [2:49:42<46:16:13, 68.75s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|‚ñè         | 62/2484 [2:50:49<45:57:25, 68.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  3%|‚ñé         | 63/2484 [2:51:54<45:13:47, 67.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  3%|‚ñé         | 64/2484 [2:54:15<60:06:39, 89.42s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  3%|‚ñé         | 65/2484 [2:55:20<55:15:39, 82.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  3%|‚ñé         | 66/2484 [2:56:22<51:04:25, 76.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  3%|‚ñé         | 67/2484 [2:57:29<49:13:28, 73.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  3%|‚ñé         | 68/2484 [2:58:32<47:07:44, 70.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  3%|‚ñé         | 69/2484 [2:59:36<45:56:47, 68.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  3%|‚ñé         | 70/2484 [3:00:41<45:08:07, 67.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  3%|‚ñé         | 71/2484 [3:03:30<65:36:56, 97.89s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  3%|‚ñé         | 72/2484 [3:05:07<65:19:04, 97.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  3%|‚ñé         | 73/2484 [3:06:10<58:18:36, 87.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  3%|‚ñé         | 74/2484 [3:08:58<74:35:34, 111.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  3%|‚ñé         | 75/2484 [3:10:53<75:22:47, 112.65s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  3%|‚ñé         | 76/2484 [3:12:00<66:06:18, 98.83s/it] Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  3%|‚ñé         | 77/2484 [3:13:29<64:03:40, 95.81s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  3%|‚ñé         | 78/2484 [3:15:41<71:15:49, 106.63s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  3%|‚ñé         | 79/2484 [3:18:23<82:22:14, 123.30s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  3%|‚ñé         | 80/2484 [3:19:31<71:23:06, 106.90s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  3%|‚ñé         | 81/2484 [3:20:39<63:29:16, 95.11s/it] Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  3%|‚ñé         | 82/2484 [3:23:37<80:04:02, 120.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  3%|‚ñé         | 83/2484 [3:24:46<69:53:01, 104.78s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  3%|‚ñé         | 84/2484 [3:27:28<81:14:33, 121.86s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  3%|‚ñé         | 85/2484 [3:28:36<70:21:03, 105.57s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  3%|‚ñé         | 86/2484 [3:29:40<62:09:40, 93.32s/it] Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  4%|‚ñé         | 87/2484 [3:30:51<57:33:27, 86.44s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  4%|‚ñé         | 88/2484 [3:33:14<68:50:16, 103.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  4%|‚ñé         | 89/2484 [3:34:20<61:23:52, 92.29s/it] Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  4%|‚ñé         | 90/2484 [3:37:02<75:15:22, 113.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  4%|‚ñé         | 91/2484 [3:38:13<66:44:14, 100.40s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  4%|‚ñé         | 92/2484 [3:40:45<77:10:22, 116.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  4%|‚ñé         | 93/2484 [3:41:59<68:40:52, 103.41s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  4%|‚ñç         | 94/2484 [3:43:13<62:51:21, 94.68s/it] Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  4%|‚ñç         | 95/2484 [3:45:58<76:45:31, 115.67s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  4%|‚ñç         | 96/2484 [3:47:13<68:37:56, 103.47s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  4%|‚ñç         | 97/2484 [3:49:13<71:55:50, 108.48s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  4%|‚ñç         | 98/2484 [3:51:10<73:36:23, 111.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  4%|‚ñç         | 99/2484 [3:53:39<81:05:37, 122.41s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  4%|‚ñç         | 100/2484 [3:54:55<71:45:28, 108.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  4%|‚ñç         | 101/2484 [3:57:20<79:07:55, 119.54s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  4%|‚ñç         | 102/2484 [3:58:43<71:42:43, 108.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  4%|‚ñç         | 103/2484 [3:59:58<65:06:28, 98.44s/it] Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  4%|‚ñç         | 104/2484 [4:01:16<61:00:35, 92.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  4%|‚ñç         | 105/2484 [4:03:23<67:52:35, 102.71s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  4%|‚ñç         | 106/2484 [4:04:32<61:05:24, 92.48s/it] Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  4%|‚ñç         | 107/2484 [4:05:41<56:34:35, 85.69s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  4%|‚ñç         | 108/2484 [4:07:55<66:03:05, 100.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  4%|‚ñç         | 109/2484 [4:09:03<59:43:39, 90.53s/it] Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  4%|‚ñç         | 110/2484 [4:10:11<55:10:42, 83.67s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  4%|‚ñç         | 111/2484 [4:11:17<51:35:37, 78.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  4%|‚ñç         | 111/2484 [4:12:13<89:52:04, 136.34s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m     normalize_and_save(\u001b[33m\"\u001b[39m\u001b[33mparsed_resumes.json\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mnormalized_resumes.json\u001b[39m\u001b[33m\"\u001b[39m, is_resume=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      3\u001b[39m     normalize_and_save(\u001b[33m\"\u001b[39m\u001b[33mparsed_jds.json\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mnormalized_jds.json\u001b[39m\u001b[33m\"\u001b[39m, is_resume=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mrun_phase2_structured_normalization\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mrun_phase2_structured_normalization\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_phase2_structured_normalization\u001b[39m():\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mnormalize_and_save\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparsed_resumes.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnormalized_resumes.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_resume\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m     normalize_and_save(\u001b[33m\"\u001b[39m\u001b[33mparsed_jds.json\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mnormalized_jds.json\u001b[39m\u001b[33m\"\u001b[39m, is_resume=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mnormalize_and_save\u001b[39m\u001b[34m(input_filename, output_filename, is_resume)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m tqdm(data):\n\u001b[32m     16\u001b[39m     text = entry.get(\u001b[33m\"\u001b[39m\u001b[33mResume_str\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_resume \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdescription\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     parsed = \u001b[43mextract_structured_json\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt_template\u001b[49m\u001b[43m=\u001b[49m\u001b[43mRESUME_PROMPT_TEMPLATE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_resume\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mJD_PROMPT_TEMPLATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[43mschema_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_schema\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_resume\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mjd_schema\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m     results.append(parsed)\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_path, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mextract_structured_json\u001b[39m\u001b[34m(text, prompt_template, schema_dict, max_new_tokens)\u001b[39m\n\u001b[32m      2\u001b[39m schema_str = json.dumps(schema_dict, indent=\u001b[32m2\u001b[39m)\n\u001b[32m      3\u001b[39m prompt = prompt_template.format(text=text.strip()[:\u001b[32m1500\u001b[39m], schema=schema_str)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m response = \u001b[43mllm_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mgenerated_text\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      5\u001b[39m json_start = response.find(\u001b[33m\"\u001b[39m\u001b[33m{\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Ruby\\projects\\AI-resume-agent\\.venv\\Lib\\site-packages\\transformers\\pipelines\\text_generation.py:287\u001b[39m, in \u001b[36mTextGenerationPipeline.__call__\u001b[39m\u001b[34m(self, text_inputs, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    286\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(\u001b[38;5;28mlist\u001b[39m(chats), **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Ruby\\projects\\AI-resume-agent\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:1379\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[39m\n\u001b[32m   1371\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[32m   1372\u001b[39m         \u001b[38;5;28miter\u001b[39m(\n\u001b[32m   1373\u001b[39m             \u001b[38;5;28mself\u001b[39m.get_iterator(\n\u001b[32m   (...)\u001b[39m\u001b[32m   1376\u001b[39m         )\n\u001b[32m   1377\u001b[39m     )\n\u001b[32m   1378\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1379\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Ruby\\projects\\AI-resume-agent\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:1386\u001b[39m, in \u001b[36mPipeline.run_single\u001b[39m\u001b[34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[39m\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[32m   1385\u001b[39m     model_inputs = \u001b[38;5;28mself\u001b[39m.preprocess(inputs, **preprocess_params)\n\u001b[32m-> \u001b[39m\u001b[32m1386\u001b[39m     model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1387\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m.postprocess(model_outputs, **postprocess_params)\n\u001b[32m   1388\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Ruby\\projects\\AI-resume-agent\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:1286\u001b[39m, in \u001b[36mPipeline.forward\u001b[39m\u001b[34m(self, model_inputs, **forward_params)\u001b[39m\n\u001b[32m   1284\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[32m   1285\u001b[39m         model_inputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_inputs, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m-> \u001b[39m\u001b[32m1286\u001b[39m         model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1287\u001b[39m         model_outputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_outputs, device=torch.device(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   1288\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Ruby\\projects\\AI-resume-agent\\.venv\\Lib\\site-packages\\transformers\\pipelines\\text_generation.py:385\u001b[39m, in \u001b[36mTextGenerationPipeline._forward\u001b[39m\u001b[34m(self, model_inputs, **generate_kwargs)\u001b[39m\n\u001b[32m    382\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[32m    383\u001b[39m     generate_kwargs[\u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.generation_config\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ModelOutput):\n\u001b[32m    388\u001b[39m     generated_sequence = output.sequences\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Ruby\\projects\\AI-resume-agent\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Ruby\\projects\\AI-resume-agent\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:2465\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[39m\n\u001b[32m   2457\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2458\u001b[39m         input_ids=input_ids,\n\u001b[32m   2459\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2460\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2461\u001b[39m         **model_kwargs,\n\u001b[32m   2462\u001b[39m     )\n\u001b[32m   2464\u001b[39m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2465\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2466\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2467\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2468\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2469\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2470\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2471\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2472\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2473\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2475\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2476\u001b[39m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[32m   2477\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2478\u001b[39m         input_ids=input_ids,\n\u001b[32m   2479\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2480\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2481\u001b[39m         **model_kwargs,\n\u001b[32m   2482\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Ruby\\projects\\AI-resume-agent\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:3434\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   3432\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   3433\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3434\u001b[39m     outputs = \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3436\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   3437\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   3438\u001b[39m     outputs,\n\u001b[32m   3439\u001b[39m     model_kwargs,\n\u001b[32m   3440\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   3441\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Ruby\\projects\\AI-resume-agent\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Ruby\\projects\\AI-resume-agent\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Ruby\\projects\\AI-resume-agent\\.venv\\Lib\\site-packages\\transformers\\utils\\generic.py:965\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    962\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    966\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    967\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Ruby\\projects\\AI-resume-agent\\.venv\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Ruby\\projects\\AI-resume-agent\\.venv\\Lib\\site-packages\\transformers\\models\\mistral\\modeling_mistral.py:810\u001b[39m, in \u001b[36mMistralForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    805\u001b[39m output_hidden_states = (\n\u001b[32m    806\u001b[39m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_hidden_states\n\u001b[32m    807\u001b[39m )\n\u001b[32m    809\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m810\u001b[39m outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    811\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    812\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    813\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    814\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    815\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    816\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    817\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    818\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    819\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    820\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    821\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    823\u001b[39m hidden_states = outputs.last_hidden_state\n\u001b[32m    824\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Ruby\\projects\\AI-resume-agent\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Ruby\\projects\\AI-resume-agent\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Ruby\\projects\\AI-resume-agent\\.venv\\Lib\\site-packages\\transformers\\utils\\generic.py:965\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    962\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    966\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    967\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Ruby\\projects\\AI-resume-agent\\.venv\\Lib\\site-packages\\transformers\\models\\mistral\\modeling_mistral.py:536\u001b[39m, in \u001b[36mMistralModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[39m\n\u001b[32m    524\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    525\u001b[39m         partial(decoder_layer.\u001b[34m__call__\u001b[39m, **flash_attn_kwargs),\n\u001b[32m    526\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    533\u001b[39m         position_embeddings,\n\u001b[32m    534\u001b[39m     )\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m536\u001b[39m     layer_outputs = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    537\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    538\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    539\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    540\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    541\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    542\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    543\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    546\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    548\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    550\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Ruby\\projects\\AI-resume-agent\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Ruby\\projects\\AI-resume-agent\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Ruby\\projects\\AI-resume-agent\\.venv\\Lib\\site-packages\\transformers\\models\\mistral\\modeling_mistral.py:249\u001b[39m, in \u001b[36mMistralDecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    246\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.input_layernorm(hidden_states)\n\u001b[32m    248\u001b[39m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m hidden_states, self_attn_weights = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    260\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    262\u001b[39m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Ruby\\projects\\AI-resume-agent\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Ruby\\projects\\AI-resume-agent\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Ruby\\projects\\AI-resume-agent\\.venv\\Lib\\site-packages\\transformers\\models\\mistral\\modeling_mistral.py:165\u001b[39m, in \u001b[36mMistralAttention.forward\u001b[39m\u001b[34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[39m\n\u001b[32m    162\u001b[39m hidden_shape = (*input_shape, -\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.head_dim)\n\u001b[32m    164\u001b[39m query_states = \u001b[38;5;28mself\u001b[39m.q_proj(hidden_states).view(hidden_shape).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m key_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mk_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m.view(hidden_shape).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    166\u001b[39m value_states = \u001b[38;5;28mself\u001b[39m.v_proj(hidden_states).view(hidden_shape).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    168\u001b[39m cos, sin = position_embeddings\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Ruby\\projects\\AI-resume-agent\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Ruby\\projects\\AI-resume-agent\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Ruby\\projects\\AI-resume-agent\\.venv\\Lib\\site-packages\\bitsandbytes\\nn\\modules.py:990\u001b[39m, in \u001b[36mLinear8bitLt.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    987\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bias.dtype != x.dtype:\n\u001b[32m    988\u001b[39m     \u001b[38;5;28mself\u001b[39m.bias.data = \u001b[38;5;28mself\u001b[39m.bias.data.to(x.dtype)\n\u001b[32m--> \u001b[39m\u001b[32m990\u001b[39m out = \u001b[43mbnb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    992\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.has_fp16_weights \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.CB \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    993\u001b[39m     \u001b[38;5;28mself\u001b[39m.weight.data = \u001b[38;5;28mself\u001b[39m.state.CB\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Ruby\\projects\\AI-resume-agent\\.venv\\Lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:509\u001b[39m, in \u001b[36mmatmul\u001b[39m\u001b[34m(A, B, out, state, threshold, bias)\u001b[39m\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m threshold > \u001b[32m0.0\u001b[39m:\n\u001b[32m    508\u001b[39m     state.threshold = threshold\n\u001b[32m--> \u001b[39m\u001b[32m509\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMatMul8bitLt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Ruby\\projects\\AI-resume-agent\\.venv\\Lib\\site-packages\\torch\\autograd\\function.py:575\u001b[39m, in \u001b[36mFunction.apply\u001b[39m\u001b[34m(cls, *args, **kwargs)\u001b[39m\n\u001b[32m    572\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch._C._are_functorch_transforms_active():\n\u001b[32m    573\u001b[39m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[32m    574\u001b[39m     args = _functorch.utils.unwrap_dead_wrappers(args)\n\u001b[32m--> \u001b[39m\u001b[32m575\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m    577\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[32m    578\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    579\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    580\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    581\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstaticmethod. For more details, please see \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    582\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    583\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Ruby\\projects\\AI-resume-agent\\.venv\\Lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:326\u001b[39m, in \u001b[36mMatMul8bitLt.forward\u001b[39m\u001b[34m(ctx, A, B, out, bias, state)\u001b[39m\n\u001b[32m    323\u001b[39m     CA, CAt, SCA, SCAt, outlier_cols = F.int8_double_quant(A.to(torch.float16), threshold=state.threshold)\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    325\u001b[39m     \u001b[38;5;66;03m# Fast path\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     CA, SCA, outlier_cols = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mint8_vectorwise_quant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat16\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    327\u001b[39m     CAt = SCAt = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    329\u001b[39m has_grad = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Ruby\\projects\\AI-resume-agent\\.venv\\Lib\\site-packages\\bitsandbytes\\functional.py:2787\u001b[39m, in \u001b[36mint8_vectorwise_quant\u001b[39m\u001b[34m(A, threshold)\u001b[39m\n\u001b[32m   2784\u001b[39m     outliers = A.abs() >= threshold\n\u001b[32m   2786\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m outliers.any():\n\u001b[32m-> \u001b[39m\u001b[32m2787\u001b[39m         outlier_cols = torch.argwhere(\u001b[43moutliers\u001b[49m\u001b[43m.\u001b[49m\u001b[43many\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m).view(-\u001b[32m1\u001b[39m)\n\u001b[32m   2789\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _cuda_device_of(A):\n\u001b[32m   2790\u001b[39m     lib.cint8_vector_quant(\n\u001b[32m   2791\u001b[39m         get_ptr(A),\n\u001b[32m   2792\u001b[39m         get_ptr(out_row),\n\u001b[32m   (...)\u001b[39m\u001b[32m   2797\u001b[39m         _get_tensor_stream(A),\n\u001b[32m   2798\u001b[39m     )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def run_phase2_structured_normalization():\n",
    "    normalize_and_save(\"parsed_resumes.json\", \"normalized_resumes.json\", is_resume=True)\n",
    "    normalize_and_save(\"parsed_jds.json\", \"normalized_jds.json\", is_resume=False)\n",
    "\n",
    "run_phase2_structured_normalization()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
