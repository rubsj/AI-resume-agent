{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f630d895",
   "metadata": {},
   "source": [
    "# Global setup and package installation used in most phases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969fcea6",
   "metadata": {},
   "source": [
    "## Colab + GPU Detection Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7cf79fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def is_running_in_colab():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "def get_available_gpu_memory_gb():\n",
    "    try:\n",
    "        output = subprocess.check_output(\n",
    "            [\"nvidia-smi\", \"--query-gpu=memory.free\", \"--format=csv,nounits,noheader\"],\n",
    "            encoding=\"utf-8\"\n",
    "        )\n",
    "        free_mem_mb = int(output.strip().split(\"\\n\")[0])\n",
    "        return free_mem_mb / 1024\n",
    "    except Exception:\n",
    "        return 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66be54d4",
   "metadata": {},
   "source": [
    "## install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942fba10",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_running_in_colab():\n",
    "    # Install the required packages\n",
    "    !pip install kagglehub pandas\n",
    "    !pip install -q transformers accelerate bitsandbytes sentencepiece pydantic huggingface_hub xformers\n",
    "    !pip install regex json5\n",
    "else:\n",
    "    %pip install kagglehub pandas\n",
    "    %pip install -q transformers accelerate sentencepiece pydantic huggingface_hub xformers\n",
    "    #%pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu121\n",
    "    #%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "    %pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128\n",
    "    %pip install -U bitsandbytes\n",
    "    %pip install regex json5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4a9f61",
   "metadata": {},
   "source": [
    "## Login to huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1235aef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# Set your token here securely or prompt for it in Colab\n",
    "# Recommended: store in Colab secrets or environment variable\n",
    "HF_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "\n",
    "if not HF_TOKEN:\n",
    "    if is_running_in_colab():\n",
    "        # If running in Colab, use the Colab secrets\n",
    "        try:\n",
    "            from google.colab import userdata\n",
    "            HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "            if not HF_TOKEN:\n",
    "                raise ValueError(\"‚ö†Ô∏è Hugging Face token not found in Colab secrets.\")\n",
    "            print(\"üîë Hugging Face token found in Colab secrets.\")\n",
    "        except ImportError:\n",
    "            print(\"‚ö†Ô∏è Unable to authenticate in Colab. Please set your Hugging Face token manually.\")\n",
    "    else:\n",
    "        # Prompt for token if not set in environment\n",
    "        print(\"üîë Please enter your Hugging Face token:\")\n",
    "        # For Colab or local prompt input\n",
    "        HF_TOKEN = input(\"üîë Enter your Hugging Face token: \").strip()\n",
    "\n",
    "login(token=HF_TOKEN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b423735e",
   "metadata": {},
   "source": [
    "## Setup Kaggle Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "409818e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Kaggle credentials already exist at C:\\Users\\rubyj/.kaggle/kaggle.json\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "def setup_kaggle_credentials():\n",
    "    kaggle_path = os.path.expanduser('~/.kaggle/kaggle.json')\n",
    "    if not os.path.exists(kaggle_path):\n",
    "        from google.colab import files\n",
    "        print(\"üìÇ Upload kaggle.json file...\")\n",
    "        uploaded = files.upload()\n",
    "        os.makedirs(os.path.dirname(kaggle_path), exist_ok=True)\n",
    "        for filename in uploaded.keys():\n",
    "            shutil.move(filename, kaggle_path)\n",
    "        os.chmod(kaggle_path, 0o600)\n",
    "        print(f\"‚úÖ Kaggle credentials setup at {kaggle_path}\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Kaggle credentials already exist at {kaggle_path}\")\n",
    "\n",
    "setup_kaggle_credentials()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d384b2f",
   "metadata": {},
   "source": [
    "## Mount Google Drive (Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1517ea98",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_running_in_colab():\n",
    "   from google.colab import drive\n",
    "   drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9964af3",
   "metadata": {},
   "source": [
    "##  Load Mistral-Instruct with Fallback to Quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3882e7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "import torch\n",
    "\n",
    "def load_model_pipeline(model_name: str, hf_token: str):\n",
    "    has_cuda = torch.cuda.is_available()\n",
    "    free_mem = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3) if has_cuda else 0\n",
    "    print(f\"üíª CUDA: {has_cuda} | GPU Memory: {free_mem:.2f} GB\")\n",
    "\n",
    "    device_map = {\"\": 0} if has_cuda else \"cpu\"\n",
    "    use_4bit = has_cuda and free_mem < 24\n",
    "\n",
    "    # Set quantization config\n",
    "    quant_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True if use_4bit else False,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    ) if use_4bit else None\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token, trust_remote_code=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # ‚úÖ Fix warning about pad_token\n",
    "\n",
    "    # Load model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device_map,\n",
    "        quantization_config=quant_config,\n",
    "        torch_dtype=torch.float16 if not quant_config else None,\n",
    "        trust_remote_code=True,\n",
    "        token=hf_token\n",
    "    )\n",
    "\n",
    "    print(f\"‚úÖ Model loaded on {next(model.parameters()).device}\")\n",
    "    return pipeline(\"text-generation\", model=model, tokenizer=tokenizer, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d505348b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíª CUDA: True | GPU Memory: 15.92 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75ba24e4a5ea490ca68587ad8f15c721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "llm_pipeline = load_model_pipeline(\n",
    "    model_name=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    hf_token=HF_TOKEN\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739b4783",
   "metadata": {},
   "source": [
    "# Global utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10779c22",
   "metadata": {},
   "source": [
    "### Utility to merge normalized json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "9a472fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "def merge_json_files(\n",
    "    source_dir: Path,\n",
    "    output_file: Path,\n",
    "    pattern: str,\n",
    "    merged_dir: Path\n",
    "):\n",
    "    source_dir.mkdir(parents=True, exist_ok=True)\n",
    "    merged_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    merged_data = []\n",
    "\n",
    "    # Load existing output if it exists\n",
    "    if output_file.exists():\n",
    "        with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                merged_data = json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"‚ö†Ô∏è Could not decode {output_file}, starting from scratch.\")\n",
    "\n",
    "    # Identify matching files\n",
    "    files_to_merge = sorted(source_dir.glob(pattern))\n",
    "\n",
    "    for file_path in files_to_merge:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "                if isinstance(data, list):\n",
    "                    merged_data.extend(data)\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è Skipping {file_path.name}: not a list.\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Failed to parse {file_path.name}: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Move to merged folder\n",
    "        shutil.move(str(file_path), merged_dir / file_path.name)\n",
    "        print(f\"‚úÖ Merged and moved: {file_path.name}\")\n",
    "\n",
    "    # Write combined output\n",
    "    if merged_data:\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(merged_data, f, indent=2)\n",
    "        print(f\"üíæ Saved to: {output_file}\")\n",
    "    else:\n",
    "        print(\"üì≠ No valid data to merge.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d169a9",
   "metadata": {},
   "source": [
    "### Infer JD Domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "0eac8105",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_keywords_dict = {\n",
    "    'advocate': ['advocate'],\n",
    "    'agriculture': ['agriculture'],\n",
    "    'apparel': ['apparel'],\n",
    "    'arts': ['arts'],\n",
    "    'automobile': ['automobile'],\n",
    "    'aviation': ['aviation'],\n",
    "    'banking': ['banking'],\n",
    "    'bpo': ['bpo'],\n",
    "    'business development': ['business', 'development', 'business development', 'business-development'],\n",
    "    'chef': ['chef'],\n",
    "    'construction': ['construction'],\n",
    "    'consultant': ['consultant'],\n",
    "    'data scientist': ['data', 'data analyst', 'data scientist', 'scientist'],\n",
    "    'designing': ['designing', 'designer'],\n",
    "    'digital media': ['digital', 'digital marketing executive', 'media', 'digital media', 'digital-media'],\n",
    "    'engineering': ['engineering'],\n",
    "    'finance': ['finance', 'financial analyst'],\n",
    "    'healthcare': ['healthcare'],\n",
    "    'hr': ['hr'],\n",
    "    'information technology': ['information', 'technology', 'information technology', 'information-technology'],\n",
    "    'public relations': ['public', 'relations', 'public relations', 'public-relations'],\n",
    "    'marketing': ['marketing'],\n",
    "    'sales': ['sales', 'sales executive'],\n",
    "    'teacher': ['teacher'],\n",
    "    'technician': ['technician'],\n",
    "    'training': ['training'],\n",
    "    'web designing': ['web', 'designing'],\n",
    "    'fitness': ['fitness'],\n",
    "    'accountant': ['accountant', 'accounting']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "adb59d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_domain_from_title(title):\n",
    "    title_lower = title.lower()\n",
    "    for domain, keywords in domain_keywords_dict.items():\n",
    "        if any(kw in title_lower for kw in keywords):\n",
    "            return domain\n",
    "    return \"unknown\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "0d8712f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# no longer being used\n",
    "def infer_job_domain_llm(description: str) -> str:\n",
    "    print(\"üîç Inferring job domain using LLM...\")\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Given a job description, return only the most likely domain of the job as a one-word or short noun phrase. Do not include explanation, punctuation, or label. Just return the domain.\n",
    "\n",
    "Job Description:\n",
    "{description.strip()}\n",
    "\n",
    "Domain:\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = llm_pipeline(prompt)[0]['generated_text']\n",
    "\n",
    "        # Extract only the part after the final \"Domain:\" (handles echo case)\n",
    "        if \"Domain:\" in response:\n",
    "            response = response.split(\"Domain:\")[-1]\n",
    "\n",
    "        # Get the first non-empty word/line\n",
    "        domain = next((line.strip().lower() for line in response.strip().splitlines() if line.strip()), \"unknown\")\n",
    "\n",
    "        # Clean up unwanted characters\n",
    "        domain = re.sub(r\"[^a-zA-Z &\\-]\", \"\", domain)\n",
    "\n",
    "        return domain if domain else \"unknown\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è LLM inference failed: {e}\")\n",
    "        return \"unknown\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a17bfc2",
   "metadata": {},
   "source": [
    "### Configurations  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "14b50e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# üõ† CONFIGURATION\n",
    "# ==============================\n",
    "\n",
    "class Config:\n",
    "    DATASET_DOWNLOAD_DIR = \"datasets\"\n",
    "    JSON_OUTPUT_DIR = \"json_outputs_run3\"\n",
    "    JSON_OUTPUT_NORMALIZED_DIR = \"json_outputs_run3/normalized\"\n",
    "    AUTO_CLEANUP = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f7f202",
   "metadata": {},
   "source": [
    "## Utility to save json to a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "85be866c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "# üì¶ Save JSON Output with Safety\n",
    "def save_json_output(data, output_path: str, indent: int = 4, overwrite: bool = True):\n",
    "    output_dir = os.path.dirname(output_path)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    if os.path.exists(output_path):\n",
    "        if overwrite:\n",
    "            os.remove(output_path)\n",
    "        else:\n",
    "            raise FileExistsError(f\"File {output_path} already exists and overwrite=False.\")\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=indent, ensure_ascii=False)\n",
    "\n",
    "    print(f\"‚úÖ Saved output to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461b99a9",
   "metadata": {},
   "source": [
    "## Utility to load file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f349cd",
   "metadata": {},
   "source": [
    "### load_ndjson_file() (for resume/jd input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "9445d7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "def load_ndjson_file(file_path: Path) -> List[dict]:\n",
    "    if not os.path.isfile(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return [json.loads(line) for line in file if line.strip()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1a0736",
   "metadata": {},
   "source": [
    "### load_json_file() (for checkpoint & metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "8b796868",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_file(file_path: Path) -> dict:\n",
    "    if not os.path.isfile(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a1b769",
   "metadata": {},
   "source": [
    "# Phase 2 -\tParse resume/JD into JSON structured scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7917da",
   "metadata": {},
   "source": [
    "## Define Pydantic Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "42d92205",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Education(BaseModel):\n",
    "    degree: Optional[str] = None\n",
    "    field: Optional[str] = None\n",
    "    institution: Optional[str] = None\n",
    "    year: Optional[str] = None\n",
    "    gpa: Optional[float] = None\n",
    "\n",
    "class Experience(BaseModel):\n",
    "    job_title: Optional[str] = None\n",
    "    company: Optional[str] = None\n",
    "    start_date: Optional[str] = None\n",
    "    end_date: Optional[str] = None\n",
    "    duration_in_months: Optional[int] = None\n",
    "    description: Optional[List[str]] = None\n",
    "    \n",
    "class Certification(BaseModel):\n",
    "    certification: Optional[str] = None\n",
    "    date_issued: Optional[str] = None\n",
    "\n",
    "class Project(BaseModel):\n",
    "    project_title: Optional[str] = None\n",
    "    description: Optional[str] = None\n",
    "    start_date: Optional[str] = None\n",
    "    end_date: Optional[str] = None\n",
    "    url: Optional[str] = None\n",
    "    \n",
    "class Language(BaseModel):\n",
    "    language: Optional[str] = None\n",
    "    proficiency: Optional[str] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "10276bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from pydantic import BaseModel\n",
    "import re\n",
    "\n",
    "\n",
    "class ResumeSchema(BaseModel):\n",
    "    resume_id: Optional[str] = None\n",
    "    summary: Optional[str] = None\n",
    "    education: Optional[List[Education]] = None\n",
    "    experience: Optional[List[Experience]] = None\n",
    "    skills: Optional[List[str]] = None\n",
    "    certifications: Optional[List[Certification]] = None\n",
    "    projects: Optional[List[Project]] = None\n",
    "    languages: Optional[List[Language]] = None\n",
    "    total_experience_years: Optional[float] = None\n",
    "\n",
    "    @classmethod\n",
    "    def normalize(cls, resume_dict: dict) -> dict:\n",
    "        resume_dict = dict(resume_dict)\n",
    "\n",
    "        # Normalize sections\n",
    "        for key in [\"skills\", \"certifications\", \"projects\", \"languages\"]:\n",
    "            if not isinstance(resume_dict.get(key), list):\n",
    "                resume_dict[key] = []\n",
    "\n",
    "        # Normalize Experience\n",
    "        normalized_exp = []\n",
    "        for item in resume_dict.get(\"experience\", []):\n",
    "            if not isinstance(item, dict):\n",
    "                continue\n",
    "            normalized_exp.append({\n",
    "                \"job_title\": item.get(\"job_title\", item.get(\"title\", \"\")),\n",
    "                \"company\": item.get(\"company\", \"\"),\n",
    "                \"start_date\": item.get(\"start_date\", \"\"),\n",
    "                \"end_date\": item.get(\"end_date\", \"\"),\n",
    "                \"duration_in_months\": item.get(\"duration_in_months\", None),\n",
    "                \"description\": item.get(\"description\", \"\")\n",
    "            })\n",
    "        resume_dict[\"experience\"] = normalized_exp\n",
    "\n",
    "        # Normalize Education\n",
    "        normalized_edu = []\n",
    "        for item in resume_dict.get(\"education\", []):\n",
    "            if not isinstance(item, dict):\n",
    "                continue\n",
    "            degree = item.get(\"degree\", \"\")\n",
    "            field = item.get(\"field\", \"\")\n",
    "            if not field:\n",
    "                match = re.search(r\"in\\\\s+(.+)\", degree, flags=re.IGNORECASE)\n",
    "                field = match.group(1).strip() if match else \"\"\n",
    "            year = str(item.get(\"year\", \"\")) if item.get(\"year\") else \"\"\n",
    "            gpa = item.get(\"gpa\", None)\n",
    "            normalized_edu.append({\n",
    "                \"degree\": degree,\n",
    "                \"field\": field,\n",
    "                \"institution\": item.get(\"institution\", \"\"),\n",
    "                \"year\": year,\n",
    "                \"gpa\": gpa\n",
    "            })\n",
    "        resume_dict[\"education\"] = normalized_edu\n",
    "\n",
    "        # Total Experience fallback\n",
    "        if \"total_experience_years\" not in resume_dict:\n",
    "            resume_dict[\"total_experience_years\"] = 0.0\n",
    "\n",
    "        return resume_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "7d198213",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JobDescriptionSchema(BaseModel):\n",
    "    jd_id: str\n",
    "    title: str\n",
    "    summary: str\n",
    "    required_experience_years: float\n",
    "    preferred_degrees: List[str]\n",
    "    required_skills: List[str]\n",
    "    optional_skills: List[str]\n",
    "    certifications: List[str]\n",
    "    soft_skills: List[str]\n",
    "    job_location: str\n",
    "    remote_option: Optional[bool] = False\n",
    "    employment_type: Optional[str] = None\n",
    "    inferred_domain: str = \"unknown\"\n",
    "\n",
    "    @classmethod\n",
    "    def normalize(cls, jd_dict: dict) -> dict:\n",
    "        jd_dict = dict(jd_dict)\n",
    "\n",
    "        aliases = {\n",
    "            \"years_required\": \"required_experience_years\",\n",
    "            \"requirements\": \"required_skills\",\n",
    "            \"degree_preferences\": \"preferred_degrees\",\n",
    "            \"certs\": \"certifications\",\n",
    "            \"skills_soft\": \"soft_skills\",\n",
    "            \"job_summary\": \"summary\"\n",
    "        }\n",
    "        for old, new in aliases.items():\n",
    "            if old in jd_dict and new not in jd_dict:\n",
    "                jd_dict[new] = jd_dict.pop(old)\n",
    "\n",
    "        # Required Experience Extraction\n",
    "        def extract_experience_years(text: str) -> float:\n",
    "            if not isinstance(text, str):\n",
    "                return 0.0\n",
    "            match = re.search(r'(\\\\d+(\\\\.\\\\d+)?)\\\\s*\\\\+?\\\\s*(years?|yrs?)', text.lower())\n",
    "            return float(match.group(1)) if match else 0.0\n",
    "\n",
    "        try:\n",
    "            val = jd_dict.get(\"required_experience_years\")\n",
    "            if val is None:\n",
    "                jd_dict[\"required_experience_years\"] = extract_experience_years(jd_dict.get(\"summary\", \"\"))\n",
    "            elif isinstance(val, str):\n",
    "                jd_dict[\"required_experience_years\"] = float(val.split()[0])\n",
    "            else:\n",
    "                jd_dict[\"required_experience_years\"] = float(val)\n",
    "        except Exception:\n",
    "            jd_dict[\"required_experience_years\"] = 0.0\n",
    "\n",
    "        # Normalize fields\n",
    "        for field in [\"preferred_degrees\", \"required_skills\", \"optional_skills\", \"certifications\", \"soft_skills\"]:\n",
    "            if not isinstance(jd_dict.get(field), list):\n",
    "                jd_dict[field] = []\n",
    "\n",
    "        for field in [\"title\", \"summary\", \"job_location\", \"employment_type\"]:\n",
    "            jd_dict[field] = jd_dict.get(field, \"\") or \"\"\n",
    "\n",
    "        # Remote Option\n",
    "        remote_flag = jd_dict.get(\"remote_option\", None)\n",
    "        if remote_flag is None:\n",
    "            remote_flag = \"remote\" in jd_dict.get(\"summary\", \"\").lower()\n",
    "        jd_dict[\"remote_option\"] = bool(remote_flag)\n",
    "\n",
    "        return jd_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "3699f84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import get_origin, get_args, Union\n",
    "from pydantic import BaseModel\n",
    "\n",
    "def generate_example_structure(model_class) -> dict:\n",
    "    def default_for_type(field_type):\n",
    "        origin = get_origin(field_type)\n",
    "        args = get_args(field_type)\n",
    "\n",
    "        if origin is list and args:\n",
    "            inner_type = args[0]\n",
    "            return [default_for_type(inner_type)]\n",
    "        elif origin is Union and type(None) in args:\n",
    "            non_none_types = [arg for arg in args if arg is not type(None)]\n",
    "            return default_for_type(non_none_types[0]) if non_none_types else \"\"\n",
    "        elif field_type is str:\n",
    "            return \"\"\n",
    "        elif field_type in [float, int]:\n",
    "            return 0.0\n",
    "        elif isinstance(field_type, type) and issubclass(field_type, BaseModel):\n",
    "            return generate_example_structure(field_type)\n",
    "        else:\n",
    "            return \"\"\n",
    "\n",
    "    return {\n",
    "        field_name: default_for_type(field.annotation)\n",
    "        for field_name, field in model_class.model_fields.items()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "318f60b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "import json\n",
    "from typing import Optional, Type\n",
    "\n",
    "@lru_cache(maxsize=4)\n",
    "def get_schema_str(schema_model: Optional[Type]) -> str:\n",
    "    \"\"\"\n",
    "    Returns a cached JSON schema string for a given Pydantic model.\n",
    "    Uses LRU cache to avoid recomputing the schema for every call.\n",
    "    \"\"\"\n",
    "    if schema_model is None:\n",
    "        return \"{}\"\n",
    "    example = generate_example_structure(schema_model)\n",
    "    return json.dumps(example, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274f774b",
   "metadata": {},
   "source": [
    "##  Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "c8cb3894",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESUME_PROMPT_TEMPLATE = \"\"\"\n",
    "[INST] \n",
    "You are a JSON resume parser and experience calculator.\n",
    "\n",
    "Given the following resume text, extract a structured JSON object that conforms to this schema:\n",
    "\n",
    "{schema}\n",
    "\n",
    "Instructions:\n",
    "\n",
    "- Parse all fields exactly as described in the schema.\n",
    "- summary:\n",
    "  - Look near the top (first 30 lines). Extract 1‚Äì3 sentences summarizing the candidate‚Äôs profile or qualifications.\n",
    "  - Accept summary even if it's not labeled as such. Ignore if it‚Äôs clearly just a skill list or experience bullet.\n",
    "- experience:\n",
    "  - For each job, extract job_title, company, location, start_date, end_date.\n",
    "  - Also extract a description as bullet-like lines.\n",
    "    - Limit to a maximum of 8‚Äì12 bullet points per job.\n",
    "    - Do not repeat content.\n",
    "    - Do not invent duties or tasks.\n",
    "    - Stick to what's clearly in the resume text.\n",
    "  - Include all job roles chronologically.\n",
    "  - Parse date formats like \"March 2007\", \"Mar 07\", \"03/2007\", \"Current\", \"Present\", \"Today\".\n",
    "\n",
    "- education:\n",
    "  - Extract degree, field (if separable), institution, graduation year, and optionally GPA.\n",
    "- skills:\n",
    "  - Extract both hard skills and soft skills as a list.\n",
    "- certifications:\n",
    "  - Include any professional affiliations, certificates, licenses.\n",
    "- projects:\n",
    "  - Extract any projects or major initiatives mentioned.\n",
    "- languages:\n",
    "  - Extract spoken/written languages if mentioned.\n",
    "- total_experience_years:\n",
    "  - Sum experience durations, excluding overlapping periods.\n",
    "  - If dates are missing, skip them from this calculation.\n",
    "- If a field is missing, use \"\" for strings and [] for lists.\n",
    "- Return ONLY a valid JSON object.\n",
    "- Your output MUST start with a {{ and end with a }}.\n",
    "- Do NOT include explanations, notes, or markdown formatting.\n",
    "- Ensure each field is separated by a comma, and no trailing commas remain.\n",
    "- Do not include backticks or markdown formatting.\n",
    "\n",
    "\n",
    "Resume Text:\n",
    "--------------------\n",
    "{text}\n",
    "--------------------\n",
    "[/INST]\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "10ce3a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "JD_PROMPT_TEMPLATE = \"\"\"\n",
    "You are a JSON job description parser and experience extractor.\n",
    "\n",
    "Given the following job description text, extract a structured JSON following this schema:\n",
    "\n",
    "{schema}\n",
    "\n",
    "Instructions:\n",
    "- Parse title, summary, skills, certifications, and other fields exactly as shown.\n",
    "- Pay special attention to \"required_experience_years\":\n",
    "    - If experience years are explicitly listed, extract that number.\n",
    "    - Accept formats like \"5+ years\", \"3-5 years\", \"8 years required\", etc.\n",
    "    - If multiple ranges are mentioned (e.g., \"3-5 years\"), use the lower value (3 years).\n",
    "    - If no years are mentioned explicitly, infer from job title level:\n",
    "        - \"Senior\", \"Lead\" ‚Üí Assume 5+ years\n",
    "        - \"Mid-level\", \"Experienced\" ‚Üí Assume 3 years\n",
    "        - \"Entry level\", \"Junior\" ‚Üí Assume 0-1 years\n",
    "    - If still ambiguous, default to 0 years.\n",
    "- Handle remote/hybrid jobs:\n",
    "    - Set \"remote_option\" = true if remote keywords are present (remote, work from home, hybrid, WFH).\n",
    "- Infer the **inferred_domain** from the job description:\n",
    "    - Return a short domain noun (e.g., \"software\", \"marketing\", \"data science\", \"finance\", \"healthcare\").\n",
    "    - Use the title and summary to guide inference.\n",
    "    - If uncertain, use \"unknown\".\n",
    "- If a field is missing, leave it empty (\"\") or as an empty list [] depending on the field type.\n",
    "- Return ONLY a valid JSON object. No extra text, no explanations, no markdown formatting.\n",
    "- Your output MUST start with a {{.\n",
    "\n",
    "Job Description Text:\n",
    "--------------------\n",
    "{text}\n",
    "--------------------\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeb4f67",
   "metadata": {},
   "source": [
    "##  Inference + Validation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d521772",
   "metadata": {},
   "source": [
    "### Generate Raw LLM Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "195bd924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_llm_output(prompt: str, max_new_tokens: int = 4096) -> str:\n",
    "    \"\"\"Run LLM and return the generated text with token count logging.\"\"\"\n",
    "    try:\n",
    "        # üî¢ Print input token count\n",
    "        input_tokens = llm_pipeline.tokenizer.encode(prompt, add_special_tokens=False)\n",
    "        print(f\"üßÆ Prompt token count: {len(input_tokens)} | Max new tokens: {max_new_tokens} | Estimated total: {len(input_tokens) + max_new_tokens}\")\n",
    "\n",
    "        # üîÅ Generate\n",
    "        outputs = llm_pipeline(prompt, max_new_tokens=max_new_tokens, do_sample=False)\n",
    "        \n",
    "        output_tokens = llm_pipeline.tokenizer.encode(outputs[0][\"generated_text\"], add_special_tokens=False)\n",
    "        print(f\"üìù Output token count: {len(output_tokens)}\")\n",
    "\n",
    "        return outputs[0][\"generated_text\"]\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"LLM generation failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94efd107",
   "metadata": {},
   "source": [
    "### Sanitize Output: Strip Prompt, Fix Cutoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "efa85362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_llm_output(response: str, prompt: str) -> str:\n",
    "    raw = response.replace(prompt, \"\").strip()\n",
    "\n",
    "    # Truncate garbage after the last closing brace\n",
    "    raw = re.sub(r'}[^}]*$', '}', raw)\n",
    "\n",
    "    # Remove markdown bullets or --- headers at end\n",
    "    raw = re.sub(r'(---|‚Ä¢|‚Äì|-)\\s*$', '', raw, flags=re.MULTILINE)\n",
    "\n",
    "    return raw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8d3f67",
   "metadata": {},
   "source": [
    "### Regex-based JSON Block Extractor and raw data processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "cb189464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "import json5\n",
    "\n",
    "def extract_json_block(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts the first valid JSON object from a text using the `regex` module and parses with `json5`.\n",
    "    This is more robust than standard `json` and can handle trailing commas, comments, etc.\n",
    "    \"\"\"\n",
    "    # Recursive regex pattern to find balanced curly braces (non-greedy)\n",
    "    pattern = r'(\\{(?:[^{}]|(?R))*\\})'\n",
    "\n",
    "    for match in regex.finditer(pattern, text, flags=regex.DOTALL):\n",
    "        json_candidate = match.group(1)\n",
    "        try:\n",
    "            return json5.loads(json_candidate)\n",
    "        except json5.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "    raise ValueError(\"‚ùå No valid JSON object found using regex and json5.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "7ebf8d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import regex\n",
    "\n",
    "def clean_raw_data(raw: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans LLM-generated JSON-like output using regex to fix common issues.\n",
    "    Returns a cleaned string ready for json.loads().\n",
    "    \"\"\"\n",
    "    raw = raw.strip().strip(\"`\").strip()\n",
    "\n",
    "    # Remove triple backtick code fences (e.g., ```json ... ```)\n",
    "    raw = regex.sub(r\"^```(?:json)?\\s*|\\s*```$\", \"\", raw, flags=regex.IGNORECASE)\n",
    "\n",
    "    # Extract content starting at first '{' and ending at the last '}'\n",
    "    start_idx = raw.find('{')\n",
    "    end_idx = raw.rfind('}')\n",
    "    if start_idx == -1 or end_idx == -1 or end_idx <= start_idx:\n",
    "        raise ValueError(\"Cannot locate complete JSON object in the output\")\n",
    "\n",
    "    json_body = raw[start_idx:end_idx + 1]\n",
    "\n",
    "    # Clean up common JSON mistakes\n",
    "    json_body = regex.sub(r\",\\s*([\\]}])\", r\"\\1\", json_body)        # trailing commas\n",
    "    json_body = regex.sub(r\",\\s*,\", \",\", json_body)                # double commas\n",
    "    json_body = regex.sub(r'\"\\s*:\\s*:', '\":', json_body)           # \"::\" to \":\"\n",
    "    json_body = regex.sub(r'(\"\\s*:[^,}\\]]+\")\\s*(\")', r'\\1,\\2', json_body)  # missing commas\n",
    "\n",
    "    return json_body\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0de79d7",
   "metadata": {},
   "source": [
    "### Final Orchestrator: Fault-Tolerant Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "c69d7930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from typing import Dict, Optional, Type\n",
    "\n",
    "def inject_ids(parsed: Dict, schema_model: Optional[Type]) -> Dict:\n",
    "    \"\"\"\n",
    "    Injects a UUID as `resume_id` or `jd_id` based on the schema model name.\n",
    "    \"\"\"\n",
    "    if not schema_model:\n",
    "        print(\"‚ö†Ô∏è No schema model provided for ID injection.\")\n",
    "        return parsed\n",
    "    schema_name = schema_model.__name__.lower()\n",
    "    if schema_name.startswith(\"resume\"):\n",
    "        parsed[\"resume_id\"] = str(uuid.uuid4())\n",
    "    elif schema_name.startswith(\"jobdescription\") and not parsed.get(\"jd_id\", \"\").strip():\n",
    "        parsed[\"jd_id\"] = str(uuid.uuid4())\n",
    "    return parsed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "dc83f2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "def extract_structured_json(\n",
    "    text: str,\n",
    "    prompt_template: str,\n",
    "    schema_model: Union[None, type] = None,\n",
    "    max_new_tokens: int = 4096,\n",
    "    retries: int = 0,\n",
    "    validate: bool = True,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Runs LLM to extract structured JSON and validates against schema.\n",
    "    Includes: prompt sanitization, retry, echo detection, brace parser fallback, schema validation.\n",
    "    \"\"\"\n",
    "    get_schema_str.cache_clear()\n",
    "    schema_str = get_schema_str(schema_model)\n",
    "    prompt = prompt_template.format(text=text, schema=schema_str)\n",
    "    raw_output = \"\"\n",
    "    attempt = 0\n",
    "\n",
    "    while attempt <= retries:\n",
    "        try:\n",
    "            # Step 1: Get LLM output\n",
    "            response = generate_llm_output(prompt, max_new_tokens)\n",
    "            print(\"üß™ LLM output was:\\n\", response)  # Preview first 300 chars\n",
    "            raw_output = sanitize_llm_output(response, prompt)\n",
    "            #print(\"üß™ Raw output was:\\n\", raw_output)\n",
    "\n",
    "            # Step 2: Detect schema echo or instruction echo\n",
    "            if \"$schema\" in raw_output or \"Ensure these rules\" in raw_output:\n",
    "                raise ValueError(\"LLM echoed schema or instruction block instead of generating JSON.\")\n",
    "\n",
    "            # Step 3: Try JSON load directly\n",
    "            json_start = raw_output.find(\"{\")\n",
    "            if json_start == -1:\n",
    "                raise ValueError(\"No opening '{' found in LLM output.\")\n",
    "            \n",
    "            cleaned_output = raw_output[json_start:]\n",
    "            cleaned_output = clean_raw_data(cleaned_output)\n",
    "\n",
    "            parsed = json.loads(cleaned_output)\n",
    "            parsed = inject_ids(parsed, schema_model)\n",
    "            \n",
    "            print(\"üß™ Parsed output was:\\n\", parsed)  # Preview first 300 chars\n",
    "            \n",
    "\n",
    "            # Step 4: Optional schema validation\n",
    "            if validate and schema_model:\n",
    "                if hasattr(schema_model, \"normalize\"):\n",
    "                    parsed = schema_model.normalize(parsed)\n",
    "                schema_model.model_validate(parsed)\n",
    "\n",
    "            return parsed\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Attempt {attempt + 1} failed: {e}\")\n",
    "            #print(\"üß™ Raw output was:\\n\", raw_output)  # Preview first 300 chars\n",
    "            attempt += 1\n",
    "\n",
    "    # Step 5: Fallback using brace matching\n",
    "    try:\n",
    "        parsed = extract_json_block(raw_output)\n",
    "        parsed = inject_ids(parsed, schema_model)\n",
    "            \n",
    "        if validate and schema_model:\n",
    "            if hasattr(schema_model, \"normalize\"):\n",
    "                parsed = schema_model.normalize(parsed)\n",
    "            schema_model.model_validate(parsed)\n",
    "        return parsed\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"raw_output\": raw_output.strip(),\n",
    "            \"error\": f\"Regex fallback failed: {e}\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "33f8e0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import ValidationError\n",
    "\n",
    "def pydantic_validate(model_class, data):\n",
    "    \"\"\"\n",
    "    Version-safe validator that supports both Pydantic v1 and v2.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Pydantic v2\n",
    "        return model_class.model_validate(data)\n",
    "    except AttributeError:\n",
    "        # Fallback to Pydantic v1\n",
    "        return model_class.parse_obj(data)\n",
    "\n",
    "\n",
    "def validate_entry(entry, is_resume):\n",
    "    try:\n",
    "        model = ResumeSchema if is_resume else JobDescriptionSchema\n",
    "        if hasattr(model, \"normalize\"):\n",
    "            normalized = model.normalize(entry)\n",
    "        else:\n",
    "            normalized = entry\n",
    "        pydantic_validate(model, normalized)\n",
    "        return True, None\n",
    "    except ValidationError as ve:\n",
    "        return False, str(ve)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143847db",
   "metadata": {},
   "source": [
    "##  Normalize in Batches with Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "a7325133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_metadata_summary(\n",
    "    output_dir: Path,\n",
    "    is_resume: bool,\n",
    "    input_file: str,\n",
    "    total_records: int,\n",
    "    total_valid: int,\n",
    "    total_invalid: int,\n",
    "    start_index: int,\n",
    "    end_index: int,\n",
    "    timestamp: str,\n",
    "    batch_id: str\n",
    "):\n",
    "    summary = {\n",
    "        \"batch_id\": batch_id,\n",
    "        \"timestamp\": timestamp,\n",
    "        \"input_file\": input_file,\n",
    "        \"input_type\": \"resume\" if is_resume else \"job_description\",\n",
    "        \"records_start_index\": start_index,\n",
    "        \"records_end_index\": end_index,\n",
    "        \"records_total\": total_records,\n",
    "        \"records_valid\": total_valid,\n",
    "        \"records_invalid\": total_invalid,\n",
    "        \"output_dir\": str(output_dir)\n",
    "    }\n",
    "    summary_file = output_dir / f\"meta_{'resumes' if is_resume else 'jds'}_{start_index}_{end_index}_{timestamp}_{batch_id}.json\"\n",
    "    save_json_output(summary, str(summary_file), overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "72001714",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_jd_text(record: dict) -> str:\n",
    "    \"\"\"\n",
    "    Constructs a rich text string from all non-empty fields of a JD record.\n",
    "    \"\"\"\n",
    "    jd_parts = []\n",
    "    for k, v in record.items():\n",
    "        label = k.replace('_', ' ').title()\n",
    "\n",
    "        if v is None:\n",
    "            continue\n",
    "\n",
    "        if isinstance(v, list):\n",
    "            if v:\n",
    "                value_str = \", \".join(str(i) for i in v if i)\n",
    "                jd_parts.append(f\"{label}: {value_str}\")\n",
    "        elif isinstance(v, (str, int, float)):\n",
    "            value_str = str(v).strip()\n",
    "            if value_str:\n",
    "                jd_parts.append(f\"{label}: {value_str}\")\n",
    "\n",
    "    return \"\\n\".join(jd_parts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "f3beca6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer\n",
    "from typing import Type\n",
    "import json\n",
    "\n",
    "MAX_CONTEXT_TOKENS = 32768  # Mistral context window\n",
    "SAFETY_MARGIN = 512         # Tokens reserved to prevent overflow\n",
    "\n",
    "def compute_max_new_tokens(\n",
    "    resume_text: str,\n",
    "    prompt_template: str,\n",
    "    schema_model: Type\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Computes max_new_tokens such that prompt + generated output stay within the model's context window.\n",
    "    Raises an error if prompt alone exceeds usable token budget.\n",
    "    \"\"\"\n",
    "    # üß± Get schema string for prompt injection\n",
    "    schema_str = get_schema_str(schema_model)\n",
    "    \n",
    "    # üß† Render full prompt\n",
    "    full_prompt = prompt_template.format(text=resume_text, schema=schema_str)\n",
    "\n",
    "    # üî¢ Tokenize full prompt\n",
    "    prompt_token_count = len(llm_pipeline.tokenizer.encode(full_prompt, add_special_tokens=False))\n",
    "    max_allowed_tokens = MAX_CONTEXT_TOKENS - SAFETY_MARGIN\n",
    "\n",
    "    if prompt_token_count >= max_allowed_tokens:\n",
    "        raise ValueError(\n",
    "            f\"Prompt too long: {prompt_token_count} tokens. Limit (with margin) is {max_allowed_tokens}.\"\n",
    "        )\n",
    "\n",
    "    return max_allowed_tokens - prompt_token_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "1c30f70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "def normalize_resume_lines(text: str) -> str:\n",
    "    # Clean common formatting noise\n",
    "    text = text.replace('\\\\/', '/').replace('\\\\t', ' ').replace('\\\\n', '\\n')\n",
    "\n",
    "    # Collapse tabs and unify repeated spacing\n",
    "    text = re.sub(r'\\t+', ' ', text)\n",
    "    text = re.sub(r' {3,}', '\\n', text)  # Break on 3+ spaces (common section divider)\n",
    "\n",
    "    # Normalize spacing around newlines\n",
    "    text = re.sub(r' *\\n *', '\\n', text)\n",
    "    text = re.sub(r'\\n{2,}', '\\n', text)\n",
    "\n",
    "    # Remove trailing space\n",
    "    return text.strip()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "589fcbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_resume_text(text: str) -> str:\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)\n",
    "    text = re.sub(r'\\n{2,}', '\\n\\n', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "ed1d8c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from datetime import datetime\n",
    "from typing import List\n",
    "\n",
    "def normalize_batch(\n",
    "    records: List[dict],\n",
    "    start_idx: int,\n",
    "    end_idx: int,\n",
    "    is_resume: bool,\n",
    "    output_dir: Path,\n",
    "    prompt_template,\n",
    "    schema_model\n",
    "):\n",
    "    results, invalids = [], []\n",
    "\n",
    "    for record in records:\n",
    "        if is_resume:\n",
    "            raw_resume_text  = record.get(\"Resume_str\", \"\")\n",
    "            # Step 1: Restore structure\n",
    "            normalized = normalize_resume_lines(raw_resume_text)\n",
    "            # Step 2: Final cleanup for formatting\n",
    "            text = clean_resume_text(normalized)\n",
    "            max_new_tokens =4096\n",
    "        else:\n",
    "            text = render_jd_text(record)\n",
    "            max_new_tokens = 2048\n",
    "\n",
    "        parsed = extract_structured_json(\n",
    "            text=text,\n",
    "            prompt_template=prompt_template,\n",
    "            schema_model=schema_model,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            validate=False\n",
    "        )\n",
    "\n",
    "        if \"raw_output\" in parsed or \"error\" in parsed:\n",
    "            invalids.append({\n",
    "                \"input\": text,\n",
    "                \"output\": parsed,\n",
    "                \"error\": parsed.get(\"error\", \"Malformed or unstructured output\")\n",
    "            })\n",
    "        else:\n",
    "            results.append(parsed)\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "    batch_id = uuid.uuid4().hex[:6]\n",
    "    prefix = \"resumes\" if is_resume else \"jds\"\n",
    "\n",
    "    if results:\n",
    "        save_json_output(\n",
    "            results,\n",
    "            output_path=output_dir / f\"{prefix}_valid_{start_idx}_{end_idx}_{timestamp}_{batch_id}.json\"\n",
    "        )\n",
    "    if invalids:\n",
    "        save_json_output(\n",
    "            invalids,\n",
    "            output_path=output_dir / f\"{prefix}_invalid_{start_idx}_{end_idx}_{timestamp}_{batch_id}.json\"\n",
    "        )\n",
    "\n",
    "    return results, invalids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "ca36dd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def normalize_file_in_batches(\n",
    "    input_filename: str,\n",
    "    output_dir: Path,\n",
    "    is_resume: bool = True,\n",
    "    input_dir: Path = Path(\"json_outputs\"),\n",
    "    save_every: int = 5,\n",
    "    limit: int = None\n",
    "):\n",
    "    input_path = input_dir / input_filename\n",
    "    data = load_ndjson_file(input_path)\n",
    "    \n",
    "\n",
    "    checkpoint_file = output_dir / f\"checkpoint_{'resumes' if is_resume else 'jds'}.json\"\n",
    "    start_index = 0\n",
    "    if checkpoint_file.exists():\n",
    "        checkpoint = load_json_file(checkpoint_file)\n",
    "        start_index = checkpoint.get(\"last_index\", 0)\n",
    "        print(f\"üîÅ Resuming from index {start_index}\")\n",
    "\n",
    "    # Apply limit\n",
    "    data_to_process = data[start_index:]\n",
    "    if limit is not None:\n",
    "        data_to_process = data_to_process[:limit]\n",
    "\n",
    "    prompt_template = RESUME_PROMPT_TEMPLATE if is_resume else JD_PROMPT_TEMPLATE\n",
    "    schema_model = ResumeSchema if is_resume else JobDescriptionSchema\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    total_valid, total_invalid = 0, 0\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "    batch_id = uuid.uuid4().hex[:6]\n",
    "    actual_start = start_index\n",
    "    actual_end = start_index + len(data_to_process)\n",
    "\n",
    "    for i in tqdm(range(0, len(data_to_process), save_every)):\n",
    "        batch = data_to_process[i:i + save_every]\n",
    "        batch_start = start_index + i\n",
    "        batch_end = batch_start + len(batch)  # ‚úÖ Accurate\n",
    "\n",
    "        results, invalids = normalize_batch(\n",
    "            records=batch,\n",
    "            start_idx=batch_start,\n",
    "            end_idx=batch_end,\n",
    "            is_resume=is_resume,\n",
    "            output_dir=output_dir,\n",
    "            prompt_template=prompt_template,\n",
    "            schema_model=schema_model\n",
    "        )\n",
    "\n",
    "        total_valid += len(results)\n",
    "        total_invalid += len(invalids)\n",
    "\n",
    "        save_json_output({\"last_index\": batch_end}, str(checkpoint_file), overwrite=True)\n",
    "\n",
    "    # ‚úÖ Save metadata summary\n",
    "    \"\"\"\n",
    "    save_metadata_summary(\n",
    "        output_dir=output_dir,\n",
    "        is_resume=is_resume,\n",
    "        input_file=input_filename,\n",
    "        total_records=len(data_to_process),\n",
    "        total_valid=total_valid,\n",
    "        total_invalid=total_invalid,\n",
    "        start_index=actual_start,\n",
    "        end_index=actual_end,\n",
    "        timestamp=timestamp,\n",
    "        batch_id=batch_id\n",
    "    )\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34057ff1",
   "metadata": {},
   "source": [
    "## Run Phase 2 End-to-End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf23348",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_file_in_batches(\n",
    "    input_filename=\"parsed_jds.json\",\n",
    "    input_dir=Path(Config.JSON_OUTPUT_DIR),\n",
    "    output_dir=Path(Config.JSON_OUTPUT_NORMALIZED_DIR),\n",
    "    is_resume=False,\n",
    "    save_every=5,\n",
    "    limit=60  # ‚úÖ Process only 20 records max\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "3b33fc60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÅ Resuming from index 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßÆ Prompt token count: 2001 | Max new tokens: 4096 | Estimated total: 6097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:30<00:00, 30.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Output token count: 3137\n",
      "üß™ LLM output was:\n",
      " \n",
      "[INST] \n",
      "You are a JSON resume parser and experience calculator.\n",
      "\n",
      "Given the following resume text, extract a structured JSON object that conforms to this schema:\n",
      "\n",
      "{\n",
      "  \"resume_id\": \"\",\n",
      "  \"summary\": \"\",\n",
      "  \"education\": [\n",
      "    {\n",
      "      \"degree\": \"\",\n",
      "      \"field\": \"\",\n",
      "      \"institution\": \"\",\n",
      "      \"year\": \"\",\n",
      "      \"gpa\": 0.0\n",
      "    }\n",
      "  ],\n",
      "  \"experience\": [\n",
      "    {\n",
      "      \"job_title\": \"\",\n",
      "      \"company\": \"\",\n",
      "      \"start_date\": \"\",\n",
      "      \"end_date\": \"\",\n",
      "      \"duration_in_months\": 0.0,\n",
      "      \"description\": [\n",
      "        \"\"\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"skills\": [\n",
      "    \"\"\n",
      "  ],\n",
      "  \"certifications\": [\n",
      "    {\n",
      "      \"certification\": \"\",\n",
      "      \"date_issued\": \"\"\n",
      "    }\n",
      "  ],\n",
      "  \"projects\": [\n",
      "    {\n",
      "      \"project_title\": \"\",\n",
      "      \"description\": \"\",\n",
      "      \"start_date\": \"\",\n",
      "      \"end_date\": \"\",\n",
      "      \"url\": \"\"\n",
      "    }\n",
      "  ],\n",
      "  \"languages\": [\n",
      "    {\n",
      "      \"language\": \"\",\n",
      "      \"proficiency\": \"\"\n",
      "    }\n",
      "  ],\n",
      "  \"total_experience_years\": 0.0\n",
      "}\n",
      "\n",
      "Instructions:\n",
      "\n",
      "- Parse all fields exactly as described in the schema.\n",
      "- summary:\n",
      "  - Look near the top (first 30 lines). Extract 1‚Äì3 sentences summarizing the candidate‚Äôs profile or qualifications.\n",
      "  - Accept summary even if it's not labeled as such. Ignore if it‚Äôs clearly just a skill list or experience bullet.\n",
      "- experience:\n",
      "  - For each job, extract job_title, company, location, start_date, end_date.\n",
      "  - Also extract a description as bullet-like lines.\n",
      "    - Limit to a maximum of 8‚Äì12 bullet points per job.\n",
      "    - Do not repeat content.\n",
      "    - Do not invent duties or tasks.\n",
      "    - Stick to what's clearly in the resume text.\n",
      "  - Include all job roles chronologically.\n",
      "  - Parse date formats like \"March 2007\", \"Mar 07\", \"03/2007\", \"Current\", \"Present\", \"Today\".\n",
      "\n",
      "- education:\n",
      "  - Extract degree, field (if separable), institution, graduation year, and optionally GPA.\n",
      "- skills:\n",
      "  - Extract both hard skills and soft skills as a list.\n",
      "- certifications:\n",
      "  - Include any professional affiliations, certificates, licenses.\n",
      "- projects:\n",
      "  - Extract any projects or major initiatives mentioned.\n",
      "- languages:\n",
      "  - Extract spoken/written languages if mentioned.\n",
      "- total_experience_years:\n",
      "  - Sum experience durations, excluding overlapping periods.\n",
      "  - If dates are missing, skip them from this calculation.\n",
      "- If a field is missing, use \"\" for strings and [] for lists.\n",
      "- Return ONLY a valid JSON object.\n",
      "- Your output MUST start with a { and end with a }.\n",
      "- Do NOT include explanations, notes, or markdown formatting.\n",
      "- Ensure each field is separated by a comma, and no trailing commas remain.\n",
      "- Do not include backticks or markdown formatting.\n",
      "\n",
      "\n",
      "Resume Text:\n",
      "--------------------\n",
      "ACCOUNTANT\n",
      "Highlights\n",
      "Microsoft Office : Intermediate in all the Microsoft Office components( Excel, Word, PowerPoint, Outlook and Access); Very familiar with ( Macros, V-look ups, SQL, calculating formulas and manipulating reports as well as smart view) Running query reports and creating reports. SAP PeopleSoft:\n",
      "Navigate within Resource one to retrieve financial reports and run queries\n",
      "Experience\n",
      "08/2014\n",
      "to\n",
      "Current\n",
      "Accountant\n",
      "Company Name\n",
      "Ôºç\n",
      "City ,\n",
      "State\n",
      "An accountant within the Experimental Therapeutics Research department, which prepares grant, restricted and non-restricted financial reports to PI's within the department. Create financial models and analysis for all restricted and non-restricted accounts within our department. Prepares monthly financial reports for PI's. Reconcile the ncRNA Core on a monthly basis. Assists with the completion of effort reporting as well as completes Job Data Update Form ( JDUF) via the EPAF system and expense transfers as needed to clear deficit. Assist with the preparation of operating budget. Assist with financial model for specific grants within our department. Monitors and reviews departmental transactions to ensure compliance with established financial controls in accordance with divisional and institutional policies and to ensure applicable revenue and expenses are captured. Communicates with Grants and Contracts to ensure that requests completed in a timely manner. Reviews Open PO on a monthly basis and close all PO's as necessary. Analyzes and evaluates operations of financial systems, prepares recommendations and documents to update fiscal services and other related policies and procedures with respect to operating systems. Assist with the invoicing via ILABS. Construct a macro-enabled reconciliation template which significantly reduces keying errors while maximize reconciliation efficiency. Implemented a macro-enabled reconciliation template, which increased effectiveness by twenty percent and significantly reduced keying errors. Prepares training materials within the department to increase productivity and standardized processes. Completes special projects as assigned by the department Administrator.\n",
      "12/2012\n",
      "to\n",
      "08/2014\n",
      "Staff Accountant\n",
      "Company Name\n",
      "Ôºç\n",
      "City ,\n",
      "State\n",
      "Full cycle accountant for several operating companies within Sysco Foods Company; completed and issued weekly, monthly, and quarterly financial statements to the operating companies for review and submit to the corporate office. Uplaod and run reports in business objects for CFO ofoperating company to review. Verified journal ledger entries of cash and check payments, purchases, expenses and trial balances by examining and authenticating inventory items. Prepared monthly P&L for several operating companies within the Sysco Corporation and research variances, reviewed transactions in the GL for accuracy, and researched transactions that deviate from the purpose of the account. Reconciled aging AR transactions and performed financial analysis, identified and explained deviations from planned or historical data. Assisted in evaluating control systems in the accounting process to ensure operating companies comply with GAAP as well as provided the necessary information to both internal and external auditors on an as needed basis. Reviewed account reconciliation from other accountants in a timely manner.\n",
      "01/2010\n",
      "to\n",
      "02/2012\n",
      "Accountant /Auditor\n",
      "Company Name\n",
      "Ôºç\n",
      "City ,\n",
      "State\n",
      "Served as both an accountant and auditor within the department of financial services. The role consisted of utilizing software applications to compile, retrieve, and summarize accounting information for analysis and reporting purposes. Examined accounting source documents for accuracy, completeness, and compliance with departmental and state rules, regulations, and agreements. Provided financial trainings for over 240 faculty and staff employees to enforce the rules and regulations of the procurement card set by set the State of Texas procurement regulation. Maintained controlling records required to ensure accuracy of all data entered into the accounting system. Identified and reported situations not in compliance with PVAMU internal controls, policies, and procedures, recommending improvements to the accounting process to optimize internal control. Performed duties in compliance with GAAP, company and department policies and procedures, internal controls and Sarbanes-Oxley requirements. Reviewed expense report for irregularities and recommended corrective measures to improve internal controls. Prepared adjusting and closing entries, statements, and analyze financial operations.\n",
      "Education\n",
      "May 2011\n",
      "Masters of Science :\n",
      "Accounting\n",
      "Prairie View A&M University\n",
      "Ôºç\n",
      "City ,\n",
      "State\n",
      "Accounting\n",
      "Dec 2009\n",
      "BBA :\n",
      "Accounting\n",
      "Prairie View A&M University\n",
      "Ôºç\n",
      "City ,\n",
      "State\n",
      "Accounting\n",
      "Professional Affiliations\n",
      "CPA Candidate Participated in various community activities, including the Phi Beta Lambda and National\n",
      "Association of Black Accountants Texas Society of CPA's Beta Gamma Sigma Greater Women's Chamber of Society Completed all the Research Operations Curriculum in 2015\n",
      "Skills\n",
      "account reconciliation, accounting, accountant, accounting system, AR, budget, business objects, closing, Contracts, financial, financial analysis, financial controls, financial operations, financial reports, financial statements, GL, Grants, inventory, invoicing, ledger, Macros, Access, Excel, Microsoft Office, office, Outlook, PowerPoint, Word, Monitors, operating systems, PeopleSoft, policies, processes, procurement, reporting, Research, SAP, Sarbanes-Oxley, training materials, view\n",
      "--------------------\n",
      "[/INST]\n",
      "{\n",
      "\"resume_id\": \"\",\n",
      "\"summary\": \"ACCOUNTANT with experience in financial reporting, analysis, and reconciliation. Proficient in Microsoft Office and SAP PeopleSoft. Seeking to utilize accounting skills and experience to contribute to a dynamic and collaborative work environment.\",\n",
      "\"education\": [\n",
      "{\n",
      "\"degree\": \"Masters of Science\",\n",
      "\"field\": \"Accounting\",\n",
      "\"institution\": \"Prairie View A&M University\",\n",
      "\"year\": \"2011\",\n",
      "\"gpa\": 0.0\n",
      "}\n",
      "],\n",
      "\"experience\": [\n",
      "{\n",
      "\"job_title\": \"Accountant\",\n",
      "\"company\": \"Company Name\",\n",
      "\"start_date\": \"08/2014\",\n",
      "\"end_date\": \"Current\",\n",
      "\"duration_in_months\": 0.0,\n",
      "\"description\": [\n",
      "\"Prepared financial models and analysis for all restricted and non-restricted accounts within the department. Created monthly financial reports for PI's. Reconciled the ncRNA Core on a monthly basis. Assisted with the completion of effort reporting as well as completes Job Data Update Form ( JDUF) via the EPAF system and expense transfers as needed to clear deficit. Assisted with the preparation of operating budget. Assisted with financial model for specific grants within our department. Monitored and reviewed departmental transactions to ensure compliance with established financial controls in accordance with divisional and institutional policies and to ensure applicable revenue and expenses are captured. Communicated with Grants and Contracts to ensure that requests completed in a timely manner. Reviews Open PO on a monthly basis and closes all PO's as necessary. Analyzes and evaluates operations of financial systems, prepares recommendations and documents to update fiscal services and other related policies and procedures with respect to operating systems. Assists with the invoicing via ILABS. Constructed a macro-enabled reconciliation template which significantly reduces keying errors while maximize reconciliation efficiency. Implemented a macro-enabled reconciliation template, which increased effectiveness by twenty percent and significantly reduced keying errors. Prepares training materials within the department to increase productivity and standardized processes. Completes special projects as assigned by the department Administrator.\"\n",
      "]\n",
      "},\n",
      "{\n",
      "\"job_title\": \"Staff Accountant\",\n",
      "\"company\": \"Company Name\",\n",
      "\"start_date\": \"12/2012\",\n",
      "\"end_date\": \"08/2014\",\n",
      "\"duration_in_months\": 0.0,\n",
      "\"description\": [\n",
      "\"Completed and issued weekly, monthly, and quarterly financial statements to the operating companies for review and submit to the corporate office. Uplaod and run reports in business objects for CFO ofoperating company to review. Verified journal ledger entries of cash and check payments, purchases, expenses and trial balances by examining and authenticating inventory items. Prepared monthly P&L for several operating companies within the Sysco Corporation and research variances, reviewed transactions in the GL for accuracy, and researched transactions that deviate from the purpose of the account. Reconciled aging AR transactions and performed financial analysis, identified and explained deviations from planned or historical data. Assisted in evaluating control systems in the accounting process to ensure operating companies comply with GAAP as well as provided the necessary information to both internal and external auditors on an as needed basis. Reviewed account reconciliation from other accountants in a timely manner.\"\n",
      "]\n",
      "}\n",
      "],\n",
      "\"skills\": [\n",
      "\"account reconciliation\",\n",
      "\"accounting\",\n",
      "\"accountant\",\n",
      "\"accounting system\",\n",
      "\"AR\",\n",
      "\"budget\",\n",
      "\"business objects\",\n",
      "\"closing\",\n",
      "\"Contracts\",\n",
      "\"financial\",\n",
      "\"financial analysis\",\n",
      "\"financial controls\",\n",
      "\"financial operations\",\n",
      "\"financial reports\",\n",
      "\"financial statements\",\n",
      "\"GL\",\n",
      "\"Grants\",\n",
      "\"inventory\",\n",
      "\"invoicing\",\n",
      "\"ledger\",\n",
      "\"Macros\",\n",
      "\"Access\",\n",
      "\"Excel\",\n",
      "\"Microsoft Office\",\n",
      "\"Office\",\n",
      "\"Outlook\",\n",
      "\"PowerPoint\",\n",
      "\"Word\",\n",
      "\"Monitors\",\n",
      "\"operating systems\",\n",
      "\"PeopleSoft\",\n",
      "\"policies\",\n",
      "\"processes\",\n",
      "\"procurement\",\n",
      "\"reporting\",\n",
      "\"Research\",\n",
      "\"SAP\",\n",
      "\"Sarbanes-Oxley\",\n",
      "\"training materials\"\n",
      "],\n",
      "\"certifications\": [\n",
      "{\n",
      "\"certification\": \"CPA Candidate\"\n",
      "}\n",
      "],\n",
      "\"projects\": [\n",
      "{\n",
      "\"project_title\": \"Accounting System Implementation\",\n",
      "\"description\": \"Implemented a new accounting system to improve financial reporting and analysis. Developed training materials and provided training to department staff. Conducted testing and validation to ensure system accuracy and efficiency.\",\n",
      "\"start_date\": \"01/2010\",\n",
      "\"end_date\": \"02/2012\",\n",
      "\"url\": \"\"\n",
      "}\n",
      "],\n",
      "\"languages\": [\n",
      "{\n",
      "\"language\": \"English\"\n",
      "}\n",
      "],\n",
      "\"total_experience_years\": 0.0\n",
      "}\n",
      "üß™ Parsed output was:\n",
      " {'resume_id': '8a5c5abc-282b-48dc-9570-ef26d35c825b', 'summary': 'ACCOUNTANT with experience in financial reporting, analysis, and reconciliation. Proficient in Microsoft Office and SAP PeopleSoft. Seeking to utilize accounting skills and experience to contribute to a dynamic and collaborative work environment.', 'education': [{'degree': 'Masters of Science', 'field': 'Accounting', 'institution': 'Prairie View A&M University', 'year': '2011', 'gpa': 0.0}], 'experience': [{'job_title': 'Accountant', 'company': 'Company Name', 'start_date': '08/2014', 'end_date': 'Current', 'duration_in_months': 0.0, 'description': [\"Prepared financial models and analysis for all restricted and non-restricted accounts within the department. Created monthly financial reports for PI's. Reconciled the ncRNA Core on a monthly basis. Assisted with the completion of effort reporting as well as completes Job Data Update Form ( JDUF) via the EPAF system and expense transfers as needed to clear deficit. Assisted with the preparation of operating budget. Assisted with financial model for specific grants within our department. Monitored and reviewed departmental transactions to ensure compliance with established financial controls in accordance with divisional and institutional policies and to ensure applicable revenue and expenses are captured. Communicated with Grants and Contracts to ensure that requests completed in a timely manner. Reviews Open PO on a monthly basis and closes all PO's as necessary. Analyzes and evaluates operations of financial systems, prepares recommendations and documents to update fiscal services and other related policies and procedures with respect to operating systems. Assists with the invoicing via ILABS. Constructed a macro-enabled reconciliation template which significantly reduces keying errors while maximize reconciliation efficiency. Implemented a macro-enabled reconciliation template, which increased effectiveness by twenty percent and significantly reduced keying errors. Prepares training materials within the department to increase productivity and standardized processes. Completes special projects as assigned by the department Administrator.\"]}, {'job_title': 'Staff Accountant', 'company': 'Company Name', 'start_date': '12/2012', 'end_date': '08/2014', 'duration_in_months': 0.0, 'description': ['Completed and issued weekly, monthly, and quarterly financial statements to the operating companies for review and submit to the corporate office. Uplaod and run reports in business objects for CFO ofoperating company to review. Verified journal ledger entries of cash and check payments, purchases, expenses and trial balances by examining and authenticating inventory items. Prepared monthly P&L for several operating companies within the Sysco Corporation and research variances, reviewed transactions in the GL for accuracy, and researched transactions that deviate from the purpose of the account. Reconciled aging AR transactions and performed financial analysis, identified and explained deviations from planned or historical data. Assisted in evaluating control systems in the accounting process to ensure operating companies comply with GAAP as well as provided the necessary information to both internal and external auditors on an as needed basis. Reviewed account reconciliation from other accountants in a timely manner.']}], 'skills': ['account reconciliation', 'accounting', 'accountant', 'accounting system', 'AR', 'budget', 'business objects', 'closing', 'Contracts', 'financial', 'financial analysis', 'financial controls', 'financial operations', 'financial reports', 'financial statements', 'GL', 'Grants', 'inventory', 'invoicing', 'ledger', 'Macros', 'Access', 'Excel', 'Microsoft Office', 'Office', 'Outlook', 'PowerPoint', 'Word', 'Monitors', 'operating systems', 'PeopleSoft', 'policies', 'processes', 'procurement', 'reporting', 'Research', 'SAP', 'Sarbanes-Oxley', 'training materials'], 'certifications': [{'certification': 'CPA Candidate'}], 'projects': [{'project_title': 'Accounting System Implementation', 'description': 'Implemented a new accounting system to improve financial reporting and analysis. Developed training materials and provided training to department staff. Conducted testing and validation to ensure system accuracy and efficiency.', 'start_date': '01/2010', 'end_date': '02/2012', 'url': ','}], 'languages': [{'language': 'English'}], 'total_experience_years': 0.0}\n",
      "‚úÖ Saved output to json_outputs_run3\\normalized\\resumes_valid_1_2_20250524_1215_0c45c4.json\n",
      "‚úÖ Saved output to json_outputs_run3\\normalized\\checkpoint_resumes.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "normalize_file_in_batches(\n",
    "    input_filename=\"parsed_resumes.json\",\n",
    "    input_dir=Path(Config.JSON_OUTPUT_DIR),\n",
    "    output_dir=Path(Config.JSON_OUTPUT_NORMALIZED_DIR),\n",
    "    is_resume=True,\n",
    "    save_every=1,\n",
    "    limit=1  # ‚úÖ Process only 20 records max\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7409286b",
   "metadata": {},
   "source": [
    "## Merge normalized files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3940c601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "normalized_dir = Path(Config.JSON_OUTPUT_NORMALIZED_DIR)\n",
    "merged_dir = normalized_dir / \"merged\"\n",
    "\n",
    "merge_json_files(\n",
    "    source_dir=normalized_dir,\n",
    "    output_file=normalized_dir / \"normalized_jds.json\",\n",
    "    pattern=\"jds_valid*.json\",\n",
    "    merged_dir=merged_dir\n",
    ")\n",
    "\n",
    "merge_json_files(\n",
    "    source_dir=normalized_dir,\n",
    "    output_file=normalized_dir / \"normalized_resumes.json\",\n",
    "    pattern=\"resumes_valid*.json\",\n",
    "    merged_dir=merged_dir\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
