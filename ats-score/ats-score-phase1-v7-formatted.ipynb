{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcb849be",
   "metadata": {},
   "source": [
    "# Phase 1: First Steps Notebook ‚Äî Data Ingestion + Minimal Parsing\n",
    "1. Setup and Install Dependencies\n",
    "2. Load Resume and JD datasets\n",
    "3. Minimal Parsing into JSON Structure\n",
    "4. Save structured JSON for Phase 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dd9696",
   "metadata": {},
   "source": [
    "## Setup and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4abca3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install kagglehub pandas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb756b8f",
   "metadata": {},
   "source": [
    "## Util Classes and methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43abc8ac",
   "metadata": {},
   "source": [
    "### Configurations  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e827a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# üõ† CONFIGURATION\n",
    "# ==============================\n",
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "class Config:\n",
    "    DATASET_DOWNLOAD_DIR = \"datasets\"\n",
    "    JSON_OUTPUT_DIR = \"json_outputs\"\n",
    "    AUTO_CLEANUP = True\n",
    "\n",
    "    @staticmethod\n",
    "    def setup_kaggle_credentials():\n",
    "        kaggle_path = os.path.expanduser('~/.kaggle/kaggle.json')\n",
    "        if not os.path.exists(kaggle_path):\n",
    "            from google.colab import files\n",
    "            print(\"üìÇ Upload kaggle.json file...\")\n",
    "            uploaded = files.upload()\n",
    "            os.makedirs(os.path.dirname(kaggle_path), exist_ok=True)\n",
    "            for filename in uploaded.keys():\n",
    "                shutil.move(filename, kaggle_path)\n",
    "            os.chmod(kaggle_path, 0o600)\n",
    "            print(f\"‚úÖ Kaggle credentials setup at {kaggle_path}\")\n",
    "        else:\n",
    "            print(f\"‚úÖ Kaggle credentials already exist at {kaggle_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a537a5",
   "metadata": {},
   "source": [
    "### Downloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89480102",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================\n",
    "# DOWNLOADER\n",
    "# ==============================\n",
    "class DatasetDownloader:\n",
    "    @staticmethod\n",
    "    def download_and_extract(dataset_path: str) -> tuple[str, str]:\n",
    "        os.makedirs(Config.DATASET_DOWNLOAD_DIR, exist_ok=True)\n",
    "        dataset_slug = dataset_path.split(\"/\")[-1]\n",
    "        extract_folder_path = os.path.join(Config.DATASET_DOWNLOAD_DIR, dataset_slug)\n",
    "        zip_filename = f\"{dataset_slug}.zip\"\n",
    "        zip_path = os.path.join(Config.DATASET_DOWNLOAD_DIR, zip_filename)\n",
    "\n",
    "        if os.path.exists(extract_folder_path) and any(Path(extract_folder_path).rglob(\"*.csv\")):\n",
    "            print(f\"‚ö° Dataset folder already exists at '{extract_folder_path}', skipping download and extraction.\")\n",
    "            return extract_folder_path, zip_filename\n",
    "\n",
    "        print(f\"‚¨áÔ∏è Downloading dataset: {dataset_path} ...\")\n",
    "        !kaggle datasets download -d {dataset_path} -p {Config.DATASET_DOWNLOAD_DIR}\n",
    "\n",
    "        if not os.path.exists(zip_path):\n",
    "            raise FileNotFoundError(f\"‚ùå Zip file '{zip_filename}' not found after download!\")\n",
    "\n",
    "        os.makedirs(extract_folder_path, exist_ok=True)\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_folder_path)\n",
    "\n",
    "        print(f\"‚úÖ Downloaded and extracted to '{extract_folder_path}'.\")\n",
    "        return extract_folder_path, zip_filename\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1170df00",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c06119",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================\n",
    "# LOADER\n",
    "# ==============================\n",
    "class DatasetLoader:\n",
    "    @staticmethod\n",
    "    def load_csv(dataset_folder: str, target_csv_name: str) -> pd.DataFrame:\n",
    "        print(f\"üîç Searching for '{target_csv_name}' inside {dataset_folder}...\")\n",
    "        if not os.path.exists(dataset_folder):\n",
    "            raise FileNotFoundError(f\"‚ùå Dataset folder '{dataset_folder}' does not exist!\")\n",
    "\n",
    "        for root, _, files in os.walk(dataset_folder):\n",
    "            for file in files:\n",
    "                if file.lower() == target_csv_name.lower():\n",
    "                    csv_path = os.path.join(root, file)\n",
    "                    df = pd.read_csv(csv_path)\n",
    "                    print(f\"‚úÖ Loaded CSV with shape {df.shape}\")\n",
    "                    return df\n",
    "\n",
    "        raise FileNotFoundError(f\"‚ùå CSV file '{target_csv_name}' not found inside extracted dataset!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a80a5a",
   "metadata": {},
   "source": [
    "### Data Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08247c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================\n",
    "# PROCESSOR\n",
    "# ==============================\n",
    "class DatasetProcessor:\n",
    "    @staticmethod\n",
    "    def filter_fields(df: pd.DataFrame, allowed_fields: List[str]) -> pd.DataFrame:\n",
    "        missing_fields = [field for field in allowed_fields if field not in df.columns]\n",
    "        if missing_fields:\n",
    "            raise ValueError(f\"‚ùå Fields {missing_fields} not found in dataset!\")\n",
    "\n",
    "        filtered_df = df[allowed_fields]\n",
    "        print(f\"‚úÖ Filtered columns: {list(filtered_df.columns)}\")\n",
    "        return filtered_df\n",
    "\n",
    "    @staticmethod\n",
    "    def save_to_json(df: pd.DataFrame, output_json_name: str):\n",
    "        os.makedirs(Config.JSON_OUTPUT_DIR, exist_ok=True)\n",
    "        output_path = os.path.join(Config.JSON_OUTPUT_DIR, output_json_name)\n",
    "\n",
    "        if os.path.exists(output_path):\n",
    "            os.remove(output_path)\n",
    "            print(f\"üóëÔ∏è Existing JSON '{output_path}' deleted.\")\n",
    "\n",
    "        df.to_json(output_path, orient='records', lines=True, force_ascii=False)\n",
    "        print(f\"‚úÖ Data saved to JSON at '{output_path}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fc4fb6",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7245255",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ==============================\n",
    "# CLEANER\n",
    "# ==============================\n",
    "class Cleaner:\n",
    "    @staticmethod\n",
    "    def cleanup_dataset_artifacts(extracted_folder_path: str, zip_filename: str):\n",
    "        if os.path.exists(extracted_folder_path):\n",
    "            shutil.rmtree(extracted_folder_path)\n",
    "            print(f\"üßπ Folder '{extracted_folder_path}' has been deleted successfully.\")\n",
    "\n",
    "        zip_path = os.path.join(Config.DATASET_DOWNLOAD_DIR, zip_filename)\n",
    "        if os.path.exists(zip_path):\n",
    "            os.remove(zip_path)\n",
    "            print(f\"üóëÔ∏è Zip file '{zip_path}' has been deleted successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a2525b",
   "metadata": {},
   "source": [
    "### Hybrid Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d85253",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ==============================\n",
    "# HYBRID LOADER\n",
    "# ==============================\n",
    "try:\n",
    "    import kagglehub\n",
    "    from kagglehub import KaggleDatasetAdapter\n",
    "except ImportError:\n",
    "    kagglehub = None\n",
    "\n",
    "class HybridDatasetLoader:\n",
    "    @staticmethod\n",
    "    def load_dataset(dataset_path: str, file_name: str) -> pd.DataFrame:\n",
    "        if kagglehub:\n",
    "            try:\n",
    "                print(f\"üì• Trying KaggleHub for {dataset_path}...\")\n",
    "                df = kagglehub.dataset_load(KaggleDatasetAdapter.PANDAS, dataset_path, file_name)\n",
    "                print(f\"‚úÖ Loaded using KaggleHub: shape = {df.shape}\")\n",
    "                return df\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è KaggleHub failed: {e}\\nFalling back to ZIP-based loader.\")\n",
    "\n",
    "        extracted_folder, _ = DatasetDownloader.download_and_extract(dataset_path)\n",
    "        return DatasetLoader.load_csv(extracted_folder, file_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e917e9d6",
   "metadata": {},
   "source": [
    "### Main flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766a42bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================\n",
    "# MAIN FLOW\n",
    "# ==============================\n",
    "def process_dataset(dataset_path: str, target_csv_name: str, allowed_fields: List[str], output_json_name: str):\n",
    "    df = HybridDatasetLoader.load_dataset(dataset_path, target_csv_name)\n",
    "    filtered_df = DatasetProcessor.filter_fields(df, allowed_fields)\n",
    "    DatasetProcessor.save_to_json(filtered_df, output_json_name)\n",
    "\n",
    "    dataset_slug = dataset_path.split(\"/\")[-1]\n",
    "    extracted_folder = os.path.join(Config.DATASET_DOWNLOAD_DIR, dataset_slug)\n",
    "    zip_filename = f\"{dataset_slug}.zip\"\n",
    "    if Config.AUTO_CLEANUP:\n",
    "        Cleaner.cleanup_dataset_artifacts(extracted_folder, zip_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60577088",
   "metadata": {},
   "source": [
    "## Login and do the processing of Resume and JD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bb5022",
   "metadata": {},
   "outputs": [],
   "source": [
    "Config.setup_kaggle_credentials()\n",
    "# Process Resume Dataset\n",
    "process_dataset(\n",
    "    dataset_path=\"snehaanbhawal/resume-dataset\",\n",
    "    target_csv_name=\"Resume.csv\",\n",
    "    allowed_fields=[\"Category\", \"Resume_str\"],\n",
    "    output_json_name=\"parsed_resumes.json\"\n",
    ")\n",
    "\n",
    "# Process Job Postings Dataset\n",
    "process_dataset(\n",
    "    dataset_path=\"arshkon/linkedin-job-postings\",\n",
    "    target_csv_name=\"postings.csv\",\n",
    "    allowed_fields=[\"title\", \"company_name\", \"location\", \"description\", \"skills_desc\", \"job_id\" , \"formatted_experience_level\", \"formatted_work_type\"],\n",
    "    output_json_name=\"parsed_jds.json\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
