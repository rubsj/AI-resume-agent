{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23e1fe36",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d241be",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gradio\n",
    "%pip install bitsandbytes accelerate\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6799d40",
   "metadata": {},
   "source": [
    "## Login to huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40ac167d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# Set your token here securely or prompt for it in Colab\n",
    "# Recommended: store in Colab secrets or environment variable\n",
    "HF_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "\n",
    "if not HF_TOKEN:\n",
    "    # Prompt for token if not set in environment\n",
    "    print(\"üîë Please enter your Hugging Face token:\")\n",
    "    # For Colab or local prompt input\n",
    "    HF_TOKEN = input(\"üîë Enter your Hugging Face token: \").strip()\n",
    "\n",
    "login(token=HF_TOKEN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f61900",
   "metadata": {},
   "source": [
    "## create test data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4808004e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "JSON_OUTPUT_SCORING_DIR = Path(\"json_outputs_all_data/scoring\")\n",
    "JSON_OUTPUT_SCORING_FT_DATA = Path(\"json_outputs_all_data/scoring/FT_data\")\n",
    "JSON_OUTPUT_FINE_TUNE_TEST_DATA = Path(\"json_outputs_all_data/scoring/non-ft-test-data\")\n",
    "\n",
    "# üîç Get all scored filenames (ending in .json)\n",
    "all_scored_files = {\n",
    "    f.name for f in JSON_OUTPUT_SCORING_DIR.glob(\"*.json\")\n",
    "}\n",
    "\n",
    "# üìÇ Get all filenames used in fine-tuning (assuming full file names are used)\n",
    "used_in_ft_files = {\n",
    "    f.name for f in JSON_OUTPUT_SCORING_FT_DATA.glob(\"*.json\")\n",
    "}\n",
    "\n",
    "# üö´ Get files not used in FT\n",
    "unused_files = sorted(all_scored_files - used_in_ft_files)\n",
    "\n",
    "# üíæ Save list to file\n",
    "output_file = JSON_OUTPUT_FINE_TUNE_TEST_DATA / \"unused_for_ft_test_files.txt\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    f.write(\"\\n\".join(unused_files))\n",
    "\n",
    "print(f\"‚úÖ Found {len(unused_files)} unused files. List saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c463f6",
   "metadata": {},
   "source": [
    "#  Step-by-Step Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91482f0e",
   "metadata": {},
   "source": [
    "### Setup Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d7f5093",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Markdown\n",
    "import gradio as gr  # Optional interactive UI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd9a9eb",
   "metadata": {},
   "source": [
    "### Load Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34ed5d99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "668a479c7ba7403d8dc612ed53b7cffd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(152064, 3584)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): lora.Linear4bit(\n",
       "            (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=True)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=3584, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=3584, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (k_proj): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
       "          (v_proj): lora.Linear4bit(\n",
       "            (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=3584, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (o_proj): Linear4bit(in_features=3584, out_features=3584, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=18944, out_features=3584, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "MODEL_PATH = \"rubsj/Qwen2-Resume-ATS\"  # or local path\n",
    "\n",
    "# 1. Load 4-bit quant config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",  # or \"fp4\"\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# 2. Load tokenizer (no quantization here)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
    "\n",
    "# 3. Load model with quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    device_map=\"auto\",  # automatic GPU/CPU placement\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f052cb",
   "metadata": {},
   "source": [
    "### Define Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6370fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCORING_PROMPT_TEMPLATE = \"\"\"\n",
    "<|im_start|>system\n",
    "You are an ATS scoring assistant. Your job is to evaluate how well a resume matches a job description using the JSON schema provided below.\n",
    "\n",
    "- Respond with valid JSON only.\n",
    "- All float values must be between 0.0 and 1.0.\n",
    "- Include all fields, even if the score is 0.0.\n",
    "- Do not explain, translate, or add anything outside the JSON.\n",
    "\n",
    "<|im_end|>\n",
    "<|im_start|>user\n",
    "Evaluate the resume against the job description using this schema:\n",
    "\n",
    "Schema:\n",
    "{{\n",
    "  \"final_ats_score\": float,\n",
    "  \"certifications\": float,\n",
    "  \"education\": float,\n",
    "  \"experience\": float,\n",
    "  \"grammar_cleanliness\": float,\n",
    "  \"leadership\": float,\n",
    "  \"responsibilities\": float,\n",
    "  \"skills\": float,\n",
    "  \"soft_skills\": float,\n",
    "  \"tools\": float,\n",
    "  \"transferable_skills\": float\n",
    "}}\n",
    "\n",
    "<RESUME>\n",
    "{resume_text}\n",
    "</RESUME>\n",
    "\n",
    "<JD>\n",
    "{jd_text}\n",
    "</JD>\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25a8b9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPECTED_KEYS = [\n",
    "    \"final_ats_score\", \"certifications\", \"education\", \"experience\",\n",
    "    \"grammar_cleanliness\", \"leadership\", \"responsibilities\", \"skills\",\n",
    "    \"soft_skills\", \"tools\", \"transferable_skills\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b40b7da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import regex  # ‚úÖ Must use `regex` for recursive pattern support\n",
    "from typing import Union, Dict\n",
    "\n",
    "def extract_and_parse_json(text: str) -> Union[Dict, None]:\n",
    "    \"\"\"\n",
    "    Extract the first valid JSON object from text using recursive pattern matching.\n",
    "    Returns a dictionary if successful, else None.\n",
    "    \"\"\"\n",
    "    pattern = regex.compile(r\"\\{(?:[^{}]|(?R))*\\}\")\n",
    "    matches = pattern.findall(text)\n",
    "\n",
    "    for match in matches:\n",
    "        try:\n",
    "            parsed = json.loads(match)\n",
    "            if isinstance(parsed, dict):\n",
    "                return parsed\n",
    "        except json.JSONDecodeError:\n",
    "            continue  # try next match\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0dc11031",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "def fill_missing_scores(score_dict: Dict) -> Dict:\n",
    "    \"\"\"Ensure all expected fields exist, filling in 0.0 if missing.\"\"\"\n",
    "    return {key: float(score_dict.get(key, 0.0)) for key in EXPECTED_KEYS}\n",
    "\n",
    "\n",
    "\n",
    "def score_resume_vs_jd(resume_text: str, jd_text: str) -> Tuple[str, str]:\n",
    "    prompt = SCORING_PROMPT_TEMPLATE.format(resume_text=resume_text, jd_text=jd_text)\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=False,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    json_result = extract_and_parse_json(decoded)\n",
    "    if isinstance(json_result, dict):\n",
    "        # patch partial output into full ATS schema\n",
    "        patched_result = fill_missing_scores(json_result)\n",
    "        return json.dumps(patched_result, indent=2), \"‚úÖ Valid or partial JSON parsed\"\n",
    "\n",
    "    # fallback for complete failure\n",
    "    return decoded, \"‚ö†Ô∏è JSON decode failed (invalid structure)\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d062342",
   "metadata": {},
   "source": [
    "### Upload Interface (Notebook Widgets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dde2e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "resume_box = widgets.Textarea(placeholder='Paste resume text here', layout=widgets.Layout(width='100%', height='200px'))\n",
    "jd_box = widgets.Textarea(placeholder='Paste job description text here', layout=widgets.Layout(width='100%', height='200px'))\n",
    "run_button = widgets.Button(description=\"Score Resume\")\n",
    "output_area = widgets.Output()\n",
    "\n",
    "def on_run_clicked(b):\n",
    "    output_area.clear_output()\n",
    "    with output_area:\n",
    "        result = score_resume_vs_jd(resume_box.value, jd_box.value)\n",
    "        print(f\"üîç ATS Score Output: {result}\"   )\n",
    "        display(Markdown(f\"### üîç ATS Score Output\\n```\\n{result}\\n```\"))\n",
    "\n",
    "run_button.on_click(on_run_clicked)\n",
    "display(resume_box, jd_box, run_button, output_area)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bf8492",
   "metadata": {},
   "source": [
    "### Optional: Gradio Interface (if you prefer a web-style UI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65879ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: Qwen/Qwen2-7B-Instruct\n",
      "Tokenizer loaded: rubsj/Qwen2-Resume-ATS\n"
     ]
    }
   ],
   "source": [
    "print(\"Model loaded:\", model.config.name_or_path)\n",
    "print(\"Tokenizer loaded:\", tokenizer.name_or_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4508556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7860\n",
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "def gradio_interface(resume, jd):\n",
    "    return score_resume_vs_jd(resume, jd)\n",
    "\n",
    "gr.close_all()\n",
    "gr.Interface(\n",
    "    fn=gradio_interface,\n",
    "    inputs=[\"textbox\", \"textbox\"],\n",
    "    outputs=[\"textbox\", \"textbox\"],\n",
    "    title=\"Resume vs JD Scorer (with Debug Info)\",\n",
    "    description=\"Paste resume and JD, see model output + debug tokens info\"\n",
    ").launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
