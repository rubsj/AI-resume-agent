{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f630d895",
   "metadata": {},
   "source": [
    "# Global setup and package installation used in most phases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969fcea6",
   "metadata": {},
   "source": [
    "## Colab + GPU Detection Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cf79fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def is_running_in_colab():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "def get_available_gpu_memory_gb():\n",
    "    try:\n",
    "        output = subprocess.check_output(\n",
    "            [\"nvidia-smi\", \"--query-gpu=memory.free\", \"--format=csv,nounits,noheader\"],\n",
    "            encoding=\"utf-8\"\n",
    "        )\n",
    "        free_mem_mb = int(output.strip().split(\"\\n\")[0])\n",
    "        return free_mem_mb / 1024\n",
    "    except Exception:\n",
    "        return 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66be54d4",
   "metadata": {},
   "source": [
    "## install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942fba10",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_running_in_colab():\n",
    "    # Install the required packages\n",
    "    !pip install kagglehub pandas\n",
    "    !pip install -q transformers accelerate bitsandbytes sentencepiece pydantic huggingface_hub xformers\n",
    "    !pip install regex json5 \n",
    "else:\n",
    "    %pip install kagglehub pandas\n",
    "    %pip install -q transformers accelerate sentencepiece pydantic huggingface_hub xformers\n",
    "    #%pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu121\n",
    "    #%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "    %pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128\n",
    "    %pip install -U bitsandbytes\n",
    "    %pip install regex json5 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4a9f61",
   "metadata": {},
   "source": [
    "## Login to huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1235aef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# Set your token here securely or prompt for it in Colab\n",
    "# Recommended: store in Colab secrets or environment variable\n",
    "HF_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "\n",
    "if not HF_TOKEN:\n",
    "    if is_running_in_colab():\n",
    "        # If running in Colab, use the Colab secrets\n",
    "        try:\n",
    "            from google.colab import userdata\n",
    "            HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "            if not HF_TOKEN:\n",
    "                raise ValueError(\"‚ö†Ô∏è Hugging Face token not found in Colab secrets.\")\n",
    "            print(\"üîë Hugging Face token found in Colab secrets.\")\n",
    "        except ImportError:\n",
    "            print(\"‚ö†Ô∏è Unable to authenticate in Colab. Please set your Hugging Face token manually.\")\n",
    "    else:\n",
    "        # Prompt for token if not set in environment\n",
    "        print(\"üîë Please enter your Hugging Face token:\")\n",
    "        # For Colab or local prompt input\n",
    "        HF_TOKEN = input(\"üîë Enter your Hugging Face token: \").strip()\n",
    "\n",
    "login(token=HF_TOKEN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b423735e",
   "metadata": {},
   "source": [
    "## Setup Kaggle Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409818e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def setup_kaggle_credentials():\n",
    "    kaggle_path = os.path.expanduser('~/.kaggle/kaggle.json')\n",
    "    if not os.path.exists(kaggle_path):\n",
    "        from google.colab import files\n",
    "        print(\"üìÇ Upload kaggle.json file...\")\n",
    "        uploaded = files.upload()\n",
    "        os.makedirs(os.path.dirname(kaggle_path), exist_ok=True)\n",
    "        for filename in uploaded.keys():\n",
    "            shutil.move(filename, kaggle_path)\n",
    "        os.chmod(kaggle_path, 0o600)\n",
    "        print(f\"‚úÖ Kaggle credentials setup at {kaggle_path}\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Kaggle credentials already exist at {kaggle_path}\")\n",
    "\n",
    "setup_kaggle_credentials()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d384b2f",
   "metadata": {},
   "source": [
    "## Mount Google Drive (Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1517ea98",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_running_in_colab():\n",
    "   from google.colab import drive\n",
    "   drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9964af3",
   "metadata": {},
   "source": [
    "##  Load Qwen-Instruct with Fallback to Quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3882e7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "import torch\n",
    "\n",
    "def load_model_pipeline(model_name: str, hf_token: str):\n",
    "    has_cuda = torch.cuda.is_available()\n",
    "    free_mem = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3) if has_cuda else 0\n",
    "    print(f\"üíª CUDA: {has_cuda} | GPU Memory: {free_mem:.2f} GB\")\n",
    "\n",
    "    device_map = {\"\": 0} if has_cuda else \"cpu\"\n",
    "    use_4bit = has_cuda and free_mem < 24\n",
    "\n",
    "    # Set quantization config\n",
    "    quant_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True if use_4bit else False,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    ) if use_4bit else None\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token, trust_remote_code=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # ‚úÖ Fix warning about pad_token\n",
    "\n",
    "    # Load model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device_map,\n",
    "        quantization_config=quant_config,\n",
    "        torch_dtype=torch.float16 if not quant_config else None,\n",
    "        trust_remote_code=True,\n",
    "        token=hf_token\n",
    "    )\n",
    "\n",
    "    print(f\"‚úÖ Model loaded on {next(model.parameters()).device}\")\n",
    "    return pipeline(\"text-generation\", model=model, tokenizer=tokenizer, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d505348b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_pipeline = load_model_pipeline(\n",
    "    model_name=\"Qwen/Qwen2-7B-Instruct\",\n",
    "    hf_token=HF_TOKEN\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739b4783",
   "metadata": {},
   "source": [
    "# Global utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10779c22",
   "metadata": {},
   "source": [
    "### Utility to merge normalized json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a472fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "def merge_json_files(\n",
    "    source_dir: Path,\n",
    "    output_file: Path,\n",
    "    pattern: str,\n",
    "    merged_dir: Path\n",
    "):\n",
    "    source_dir.mkdir(parents=True, exist_ok=True)\n",
    "    merged_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    merged_data = []\n",
    "\n",
    "    # Load existing output if it exists\n",
    "    if output_file.exists():\n",
    "        with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                merged_data = json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"‚ö†Ô∏è Could not decode {output_file}, starting from scratch.\")\n",
    "\n",
    "    # Identify matching files\n",
    "    files_to_merge = sorted(source_dir.glob(pattern))\n",
    "\n",
    "    for file_path in files_to_merge:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "                if isinstance(data, list):\n",
    "                    merged_data.extend(data)\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è Skipping {file_path.name}: not a list.\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Failed to parse {file_path.name}: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Move to merged folder\n",
    "        shutil.move(str(file_path), merged_dir / file_path.name)\n",
    "        print(f\"‚úÖ Merged and moved: {file_path.name}\")\n",
    "\n",
    "    # Write combined output\n",
    "    if merged_data:\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(merged_data, f, indent=2)\n",
    "        print(f\"üíæ Saved to: {output_file}\")\n",
    "    else:\n",
    "        print(\"üì≠ No valid data to merge.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a17bfc2",
   "metadata": {},
   "source": [
    "### Configurations  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b50e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# üõ† CONFIGURATION\n",
    "# ==============================\n",
    "\n",
    "class Config:\n",
    "    DATASET_DOWNLOAD_DIR = \"datasets\"\n",
    "    JSON_OUTPUT_DIR = \"json_outputs_run3\"\n",
    "    JSON_OUTPUT_NORMALIZED_DIR = \"json_outputs_run3/normalized\"\n",
    "    AUTO_CLEANUP = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f7f202",
   "metadata": {},
   "source": [
    "## Utility to save json to a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85be866c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "# üì¶ Save JSON Output with Safety\n",
    "def save_json_output(data, output_path: str, indent: int = 4, overwrite: bool = True):\n",
    "    output_dir = os.path.dirname(output_path)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    if os.path.exists(output_path):\n",
    "        if overwrite:\n",
    "            os.remove(output_path)\n",
    "        else:\n",
    "            raise FileExistsError(f\"File {output_path} already exists and overwrite=False.\")\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=indent, ensure_ascii=False)\n",
    "\n",
    "    print(f\"‚úÖ Saved output to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461b99a9",
   "metadata": {},
   "source": [
    "## Utility to load file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f349cd",
   "metadata": {},
   "source": [
    "### load_ndjson_file() (for resume/jd input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9445d7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "def load_ndjson_file(file_path: Path) -> List[dict]:\n",
    "    if not os.path.isfile(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return [json.loads(line) for line in file if line.strip()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1a0736",
   "metadata": {},
   "source": [
    "### load_json_file() (for checkpoint & metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b796868",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_file(file_path: Path) -> dict:\n",
    "    if not os.path.isfile(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a1b769",
   "metadata": {},
   "source": [
    "# Phase 2 -\tParse resume/JD into JSON structured scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7917da",
   "metadata": {},
   "source": [
    "## Define Pydantic Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d92205",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Education(BaseModel):\n",
    "    degree: Optional[str] = None\n",
    "    field: Optional[str] = None\n",
    "    institution: Optional[str] = None\n",
    "    year: Optional[str] = None\n",
    "    gpa: Optional[float] = None\n",
    "\n",
    "class Experience(BaseModel):\n",
    "    job_title: Optional[str] = None\n",
    "    company: Optional[str] = None\n",
    "    start_date: Optional[str] = None\n",
    "    end_date: Optional[str] = None\n",
    "    description: Optional[List[str]] = None\n",
    "    \n",
    "class Certification(BaseModel):\n",
    "    certification: Optional[str] = None\n",
    "    date_issued: Optional[str] = None\n",
    "\n",
    "class Project(BaseModel):\n",
    "    project_title: Optional[str] = None\n",
    "    description: Optional[str] = None\n",
    "    start_date: Optional[str] = None\n",
    "    end_date: Optional[str] = None\n",
    "    url: Optional[str] = None\n",
    "    \n",
    "class Language(BaseModel):\n",
    "    language: Optional[str] = None\n",
    "    proficiency: Optional[str] = None\n",
    "    \n",
    "class Other(BaseModel):\n",
    "    section_name: Optional[str] = None\n",
    "    content: Optional[str] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10276bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from pydantic import BaseModel\n",
    "import re\n",
    "\n",
    "\n",
    "class ResumeSchema(BaseModel):\n",
    "    resume_id: Optional[str] = None\n",
    "    summary: Optional[str] = None\n",
    "    education: Optional[List[Education]] = None\n",
    "    experience: Optional[List[Experience]] = None\n",
    "    skills: Optional[List[str]] = None\n",
    "    certifications: Optional[List[Certification]] = None\n",
    "    projects: Optional[List[Project]] = None\n",
    "    languages: Optional[List[Language]] = None\n",
    "    other: Optional[List[Other]] = None\n",
    "    total_experience_years: Optional[float] = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d198213",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JobDescriptionSchema(BaseModel):\n",
    "    jd_id: Optional[str] = None\n",
    "    inferred_domain: str = \"unknown\"\n",
    "    title: Optional[str] = None\n",
    "    summary: Optional[str] = None\n",
    "    required_experience_years: Optional[str] = None\n",
    "    preferred_degrees: Optional[List[str]] = None\n",
    "    required_skills: Optional[List[str]] = None\n",
    "    optional_skills: Optional[List[str]] = None\n",
    "    tools_and_technologies: Optional[List[str]] = None\n",
    "    certifications: Optional[List[str]] = None\n",
    "    soft_skills: Optional[List[str]] = None\n",
    "    job_responsibilities: Optional[List[str]] = None\n",
    "    job_location: Optional[str] = None\n",
    "    remote_option: Optional[bool] = False\n",
    "    employment_type: Optional[str] = None\n",
    "    travel_requirements: Optional[str] = None\n",
    "    physical_requirements: Optional[str] = None\n",
    "    benefits: Optional[List[str]] = None\n",
    "    company_information: Optional[str] = None\n",
    "    equal_opportunity_policy: Optional[str] = None\n",
    "    other: Optional[List[Other]] = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7a6c15",
   "metadata": {},
   "source": [
    "### Generate schema string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3699f84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import get_origin, get_args, Union\n",
    "from pydantic import BaseModel\n",
    "\n",
    "def generate_example_structure(model_class) -> dict:\n",
    "    def default_for_type(field_type):\n",
    "        origin = get_origin(field_type)\n",
    "        args = get_args(field_type)\n",
    "\n",
    "        if origin is list and args:\n",
    "            inner_type = args[0]\n",
    "            return [default_for_type(inner_type)]\n",
    "        elif origin is Union and type(None) in args:\n",
    "            non_none_types = [arg for arg in args if arg is not type(None)]\n",
    "            return default_for_type(non_none_types[0]) if non_none_types else \"\"\n",
    "        elif field_type is str:\n",
    "            return \"\"\n",
    "        elif field_type in [float, int]:\n",
    "            return 0.0\n",
    "        elif isinstance(field_type, type) and issubclass(field_type, BaseModel):\n",
    "            return generate_example_structure(field_type)\n",
    "        else:\n",
    "            return \"\"\n",
    "\n",
    "    return {\n",
    "        field_name: default_for_type(field.annotation)\n",
    "        for field_name, field in model_class.model_fields.items()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80341360",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "import json\n",
    "from typing import Optional, Type\n",
    "\n",
    "@lru_cache(maxsize=4)\n",
    "def get_schema_str(schema_model: Optional[Type]) -> str:\n",
    "    \"\"\"\n",
    "    Returns a cleaned JSON schema string (as example structure) for a given Pydantic model.\n",
    "    Filters out fields not needed for the prompt like 'resume_id', 'duration_in_months', etc.\n",
    "    \"\"\"\n",
    "    if schema_model is None:\n",
    "        return \"{}\"\n",
    "    \n",
    "    example = generate_example_structure(schema_model)\n",
    "\n",
    "    # ‚õîÔ∏è Fields to remove before rendering into the prompt\n",
    "    exclude_fields = {} #{\"resume_id\", \"total_experience_years\", \"jd_id\"}\n",
    "\n",
    "    def recursive_filter(obj):\n",
    "        if isinstance(obj, dict):\n",
    "            return {\n",
    "                k: recursive_filter(v)\n",
    "                for k, v in obj.items()\n",
    "                if k not in exclude_fields\n",
    "            }\n",
    "        elif isinstance(obj, list):\n",
    "            return [recursive_filter(v) for v in obj]\n",
    "        return obj\n",
    "\n",
    "    filtered = recursive_filter(example)\n",
    "\n",
    "    return json.dumps(filtered, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4debc085",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ‚úÖ Step 2: Generate schema string from your updated ResumeSchema\n",
    "schema_str = get_schema_str(ResumeSchema)\n",
    "#schema_str = get_schema_str(JobDescriptionSchema)\n",
    "\n",
    "\n",
    "# ‚úÖ Step 3: Print and inspect\n",
    "print(schema_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274f774b",
   "metadata": {},
   "source": [
    "##  Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3d77db",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESUME_PROMPT_TEMPLATE = \"\"\"\n",
    "<|im_start|>system\n",
    "You are a strict resume-to-JSON parser. Your job is to extract structured data from resumes using the exact schema and instructions provided.\n",
    "- Follow the schema format and keys exactly.\n",
    "- Nest values correctly under each section.\n",
    "- Do not hallucinate or paraphrase.\n",
    "- Only include information present in the resume.\n",
    "- For any data not found, fill with \"\" or empty list [] or 0.0 as applicable.\n",
    "- Any resume content in sections like \"Affiliations\", \"Accomplishments\", \"Achievements\", \"Awards\", \"Honors\", \"Volunteer Work\", \"Memberships\", \"Leadership\", or \"Contributions\" that does not map to the fixed schema must be included under \"other\" with appropriate \"section_name\" and \"content\".\n",
    "- Your output must be valid JSON only ‚Äî no explanation, no markdown, no extra formatting.\n",
    "<|im_end|>\n",
    "\n",
    "<|im_start|>user\n",
    "Extract structured JSON from the resume below using this schema:\n",
    "\n",
    "Schema:\n",
    "{schema}\n",
    "\n",
    "Resume:\n",
    "<<<\n",
    "{text}\n",
    ">>>\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f75f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "JD_PROMPT_TEMPLATE = \"\"\"\n",
    "<|im_start|>system\n",
    "You are a strict job description to JSON converter. Your job is to extract structured JSON from job descriptions using the exact schema and rules below.\n",
    "\n",
    "- Use only the content present in the input text.\n",
    "- Do NOT hallucinate, guess, or paraphrase.\n",
    "- Extract all values VERBATIM where possible, especially for \"required_experience_years\", \"preferred_degrees\", \"soft_skills\", and any bullet-style lists.\n",
    "- For missing values, use \"\" for strings, [] for lists.\n",
    "- Populate \"inferred_domain\" based on the content (e.g., marketing, software, healthcare).\n",
    "- Extract soft skills into \"soft_skills\", NOT \"required_skills\" (e.g., communication, detail-oriented, go-getter mindset).\n",
    "- Extract tools and platforms used (e.g., Microsoft Office, Adobe Creative Suite, Facebook, Salesforce) into \"tools_and_technologies\".\n",
    "- Extract job responsibilities (duties, tasks, outcomes) as a list of bullet points or descriptive phrases into \"job_responsibilities\".\n",
    "- Do NOT duplicate data across fields. For example, if a responsibility contains a tool, extract the tool separately into \"tools_and_technologies\".\n",
    "- For remote roles, set \"remote_option\" to: \"yes\" if explicitly remote, \"no\" if clearly in-office, \"\" if not mentioned.\n",
    "- Extract \"travel_requirements\", \"physical_requirements\", and \"benefits\" from job postings if mentioned.\n",
    "- Extract \"company_information\" from About section and \"equal_opportunity_policy\" from EEO or DEI disclosures.\n",
    "- Do not use role titles in \"required_experience_years\". Extract years explicitly.\n",
    "- Ensure no placeholder symbols like \",\" are included in any fields.\n",
    "- Move technical proficiencies (e.g., \"Proficient in Excel\") to \"tools_and_technologies\", not \"optional_skills\".\n",
    "- Parse short company descriptions from the \"About\" section into \"company_information\".\n",
    "- If any remaining information doesn‚Äôt fit the schema, include it under \"other\" with \"section_name\" and \"content\".\n",
    "- Output MUST be a single valid JSON object ‚Äî no explanations, no markdown, no extra formatting.\n",
    "<|im_end|>\n",
    "\n",
    "<|im_start|>user\n",
    "Extract the following structured JSON from this job description.\n",
    "\n",
    "Schema:\n",
    "{schema}\n",
    "\n",
    "Job Description:\n",
    "<<<\n",
    "{text}\n",
    ">>>\n",
    "<|im_end|>\n",
    "\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeb4f67",
   "metadata": {},
   "source": [
    "##  Inference + Validation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d521772",
   "metadata": {},
   "source": [
    "### Generate Raw LLM Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195bd924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_llm_output(prompt: str, max_new_tokens: int = 4096) -> str:\n",
    "    \"\"\"Run LLM and return the generated text with token count logging.\"\"\"\n",
    "    try:\n",
    "        # üî¢ Print input token count\n",
    "        input_tokens = llm_pipeline.tokenizer.encode(prompt, add_special_tokens=False)\n",
    "        print(f\"üßÆ Prompt token count: {len(input_tokens)} | Max new tokens: {max_new_tokens} | Estimated total: {len(input_tokens) + max_new_tokens}\")\n",
    "\n",
    "        # üîÅ Generate\n",
    "        outputs = llm_pipeline(prompt, max_new_tokens=max_new_tokens, do_sample=False, temperature=None, top_p=None, top_k=None)\n",
    "        \n",
    "        output_tokens = llm_pipeline.tokenizer.encode(outputs[0][\"generated_text\"], add_special_tokens=False)\n",
    "        print(f\"üìù Output token count: {len(output_tokens)}\")\n",
    "\n",
    "        return outputs[0][\"generated_text\"]\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"LLM generation failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94efd107",
   "metadata": {},
   "source": [
    "### Sanitize Output: Strip Prompt, Fix Cutoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa85362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_llm_output(response: str, prompt: str) -> str:\n",
    "    raw = response.replace(prompt, \"\").strip()\n",
    "\n",
    "    # Truncate garbage after the last closing brace\n",
    "    raw = re.sub(r'}[^}]*$', '}', raw)\n",
    "\n",
    "    # Remove markdown bullets or --- headers at end\n",
    "    raw = re.sub(r'(---|‚Ä¢|‚Äì|-)\\s*$', '', raw, flags=re.MULTILINE)\n",
    "\n",
    "    return raw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8d3f67",
   "metadata": {},
   "source": [
    "### Regex-based JSON Block Extractor and raw data processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9b0003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "import json5\n",
    "\n",
    "def extract_json_block(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts the first valid JSON object from a text using the `regex` module and parses with `json5`.\n",
    "    This is more robust than standard `json` and can handle trailing commas, comments, etc.\n",
    "    \"\"\"\n",
    "    # Recursive regex pattern to find balanced curly braces (non-greedy)\n",
    "    pattern = r'(\\{(?:[^{}]|(?R))*\\})'\n",
    "\n",
    "    for match in regex.finditer(pattern, text, flags=regex.DOTALL):\n",
    "        json_candidate = match.group(1)\n",
    "        try:\n",
    "            return json5.loads(json_candidate)\n",
    "        except json5.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "    raise ValueError(\"‚ùå No valid JSON object found using regex and json5.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f7f103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_raw_data(raw: str) -> str:\n",
    "    import regex\n",
    "    import json\n",
    "    import json5\n",
    "    \n",
    "    raw = raw.strip().strip(\"`\")\n",
    "\n",
    "    # Remove code fences\n",
    "    raw = regex.sub(r\"^```(?:json)?\\s*|\\s*```$\", \"\", raw, flags=regex.IGNORECASE)\n",
    "\n",
    "    # Extract content between first '{' and last '}'\n",
    "    start_idx = raw.find('{')\n",
    "    end_idx = raw.rfind('}')\n",
    "    if start_idx == -1 or end_idx == -1 or end_idx <= start_idx:\n",
    "        raise ValueError(\"Cannot locate complete JSON object in the output\")\n",
    "\n",
    "    json_body = raw[start_idx:end_idx + 1]\n",
    "\n",
    "    # Fix common issues\n",
    "    json_body = regex.sub(r\",\\s*([\\]}])\", r\"\\1\", json_body)        # remove trailing commas\n",
    "    json_body = regex.sub(r\",\\s*,\", \",\", json_body)                # remove double commas\n",
    "    json_body = regex.sub(r'\"\\s*:\\s*:', '\":', json_body)           # fix \"::\"\n",
    "    json_body = regex.sub(r'(\"\\s*:[^,}\\]]+\")\\s*(\")', r'\\1,\\2', json_body)  # fix missing commas\n",
    "\n",
    "    # Final fallback: force valid quote usage\n",
    "    json_body = regex.sub(r\"‚Äò|‚Äô\", '\"', json_body)\n",
    "    json_body = regex.sub(r\"‚Äú|‚Äù\", '\"', json_body)\n",
    "    \n",
    "    # ‚úÖ Try json.loads, fallback to json5\n",
    "    try:\n",
    "        json.loads(json_body)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"‚ö†Ô∏è Standard JSON decode failed: {e} ‚Äî using json5 as fallback.\")\n",
    "        parsed = json5.loads(json_body)\n",
    "        return json.dumps(parsed, ensure_ascii=False, indent=2)  # return as clean valid JSON\n",
    "\n",
    "    return json_body\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6789c18b",
   "metadata": {},
   "source": [
    "### Format Resume string for LLM processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf7d4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "CANONICAL_HEADER_PATTERNS = [\n",
    "    r'\\b(?:work|professional)?\\s*experience\\b',\n",
    "    r'\\b(?:education(?:\\s+(?:and|&)\\s+training)?|training(?:\\s+(?:and|&)\\s+(?:development|certifications?|programs?)|s)?|specialized\\s+training)\\b',\n",
    "    r'\\b(?:technical\\s+)?skills?\\b',\n",
    "    r'\\b(?:certifications?|licenses?|certifications?\\s*(?:and|&)\\s*licenses?)\\b',\n",
    "    r'\\b(?:projects?|key\\s+projects|project\\s+highlights)\\b',\n",
    "    r'\\blanguages?\\b',\n",
    "    r'\\b(?:executive\\s+)?summary\\b',\n",
    "    r'\\b(?:professional\\s+)?affiliations?\\b',\n",
    "    r'\\b(?:awards?|accomplishments?|honors|achievements)\\b',\n",
    "    r'\\bpublications?\\b',\n",
    "    r'\\b(?:interests|hobbies|extracurricular\\s+activities)\\b',\n",
    "    r'\\b(?:volunteer\\s+experience|community\\s+involvement|volunteering)\\b',\n",
    "    r'\\bobjective\\b',\n",
    "    r'\\breferences\\b',\n",
    "    r'\\bprofile\\b'\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "def is_known_section_header(line):\n",
    "    stripped = line.strip()\n",
    "    return any(\n",
    "        re.search(rf'^\\s*{pattern}\\s*[:\\-]?\\s*$', stripped, re.IGNORECASE)\n",
    "        for pattern in CANONICAL_HEADER_PATTERNS\n",
    "    )\n",
    "\n",
    "\n",
    "def is_date_like(text):\n",
    "    return bool(re.match(r'(?i)^\\d{1,2}/\\d{4}$|^[A-Za-z]{3,9} \\d{4}$|^\\d{4}$', text.strip()))\n",
    "\n",
    "def is_location_like(text):\n",
    "    return bool(re.fullmatch(r'(City|State|[A-Z][a-z]{1,15})', text.strip()))\n",
    "\n",
    "def is_degree_fragment(text):\n",
    "    return bool(re.fullmatch(r'[A-Za-z]{2,5}', text.strip()))  # BBA, MSc, etc.\n",
    "\n",
    "\n",
    "def detect_real_headers(lines):\n",
    "    section_lines = []\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        stripped = line.strip()\n",
    "        if not stripped:\n",
    "            continue\n",
    "\n",
    "        # ‚úÖ Canonical match: skip heuristics & filtering\n",
    "        if is_known_section_header(stripped):\n",
    "            section_lines.append((i, stripped))\n",
    "            continue\n",
    "\n",
    "        # ‚ùå Filtering for heuristic-only (to avoid false positives)\n",
    "        if is_date_like(stripped) or is_location_like(stripped) or is_degree_fragment(stripped):\n",
    "            continue\n",
    "\n",
    "    return section_lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277fbc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "def preprocess_resume_text(text: str) -> str:\n",
    "    # Normalize escaped newlines and tabs into actual characters\n",
    "    text = text.replace('\\\\n', '\\n').replace('\\\\t', '\\t')\n",
    "\n",
    "    # Clean actual tabs and escaped slashes\n",
    "    text = text.replace('\\\\/', '/').replace('\\t', ' ')\n",
    "    # Clean tabs and escaped slashes\n",
    "    text = text.replace('\\\\/', '/').replace('\\t', ' ')\n",
    "\n",
    "    # ‚úÖ Normalize: fix spaced slashes in dates (e.g. '08 / 2014' ‚Üí '08/2014')\n",
    "    text = re.sub(r'(?<=\\d)\\s*/\\s*(?=\\d{4})', '/', text)\n",
    "\n",
    "    # ‚úÖ Protect date ranges by marking them before splitting on 3+ spaces\n",
    "    text = re.sub(\n",
    "        r'(?i)(\\d{1,2}/\\d{4})\\s*(to|-)\\s*(\\d{1,2}/\\d{4}|current|present)',\n",
    "        r'__DATERANGE__\\1 to \\3__ENDDATE__',\n",
    "        text\n",
    "    )\n",
    "\n",
    "    # ‚úÖ Collapse \"Company ‚Äî City, State\" formatting\n",
    "    text = re.sub(\n",
    "        r'(?i)([A-Za-z0-9&.,()\\-\\' ]{2,})\\s*[-Ôºç‚Äì‚Äî]{1,2}\\s*([A-Z][a-z]+)\\s*,\\s*([A-Z][a-z]+)',\n",
    "        r'\\1 ‚Äî \\2 , \\3',\n",
    "        text\n",
    "    )\n",
    "\n",
    "    # ‚úÖ Convert 3+ spaces into newlines\n",
    "    text = re.sub(r' {3,}', '\\n', text)\n",
    "\n",
    "    # Normalize excessive newlines and spaces\n",
    "    text = re.sub(r'\\n{2,}', '\\n', text)\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)\n",
    "\n",
    "    # ‚úÖ Restore protected date ranges\n",
    "    text = text.replace('__DATERANGE__', '\\n').replace('__ENDDATE__', '\\n')\n",
    "\n",
    "    # Add breaks around known section headers\n",
    "    def insert_header_breaks(line):\n",
    "        stripped = line.strip()\n",
    "        if is_known_section_header(stripped):\n",
    "            return f\"\\n{stripped}\\n\"\n",
    "        return stripped\n",
    "\n",
    "    lines = text.splitlines()\n",
    "    lines = [insert_header_breaks(line) for line in lines]\n",
    "    text = \"\\n\".join(lines)\n",
    "\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a70056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "def split_run_on_sentences(text):\n",
    "    \"\"\"Split long lines into sentence-like segments while preserving periods.\"\"\"\n",
    "    lines = text.splitlines()\n",
    "    split_lines = []\n",
    "\n",
    "    for line in lines:\n",
    "        stripped = line.strip()\n",
    "\n",
    "        # Skip short or bullet lines\n",
    "        if not stripped or stripped.startswith('-') or len(stripped) < 50:\n",
    "            split_lines.append(stripped)\n",
    "            continue\n",
    "\n",
    "        # Split after period only if followed by space and capital letter\n",
    "        if stripped.count('.') >= 2:\n",
    "            # This keeps the period with the sentence\n",
    "            segments = re.split(r'(?<=\\.) (?=[A-Z])', stripped)\n",
    "            split_lines.extend([seg.strip() for seg in segments])\n",
    "        else:\n",
    "            split_lines.append(stripped)\n",
    "\n",
    "    return split_lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9e9ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_resume_for_llm(text: str) -> str:\n",
    "    text = preprocess_resume_text(text)\n",
    " \n",
    "    lines = text.splitlines()\n",
    "     # Sentence splitting\n",
    "    lines = split_run_on_sentences(\"\\n\".join(lines))\n",
    "  \n",
    "    headers = detect_real_headers(lines)\n",
    "    print(\"Detected Headers:\", headers)\n",
    "\n",
    "    header_indices = {idx for idx, _ in headers}\n",
    "\n",
    "    output_lines = []\n",
    "    for i, line in enumerate(lines):\n",
    "        stripped = line.strip()\n",
    "\n",
    "        if not stripped:\n",
    "            continue\n",
    "\n",
    "        # Skip lines that are isolated numbers (e.g. caused by bad splitting)\n",
    "        if stripped.isdigit():\n",
    "            continue\n",
    "\n",
    "        # Insert a line break before date ranges (if not already done)\n",
    "        if re.match(r'(?i)^\\d{1,2}/\\d{4} to (\\d{1,2}/\\d{4}|current|present)$', stripped):\n",
    "            output_lines.append(\"\")  # add a break before\n",
    "            output_lines.append(stripped)\n",
    "            continue\n",
    "\n",
    "        # Header detection\n",
    "        if i in header_indices:\n",
    "            output_lines.append(\"\")  # add a blank line before header\n",
    "            output_lines.append(stripped.upper())\n",
    "            output_lines.append(\"\")  # and after\n",
    "        else:\n",
    "            output_lines.append(stripped)\n",
    "\n",
    "    formatted = \"\\n\".join(output_lines)\n",
    "\n",
    "    # Normalize inline date ranges (if still embedded)\n",
    "    formatted = re.sub(\n",
    "        r'(?i)(\\d{1,2}/\\d{4})\\s*(to|-)\\s*(\\d{1,2}/\\d{4}|current|present)',\n",
    "        r'\\1 to \\3',\n",
    "        formatted\n",
    "    )\n",
    "\n",
    "    # Format SKILLS section as bullets\n",
    "    formatted = re.sub(\n",
    "        r'(?i)(\\nSKILLS\\n)([^\\n]+)',\n",
    "        lambda m: m.group(1) + \"\\n\" + \"\\n\".join(f\"- {s.strip()}\" for s in m.group(2).split(',')),\n",
    "        formatted\n",
    "    )\n",
    "\n",
    "    return formatted.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53668fd5",
   "metadata": {},
   "source": [
    "### Format Job Description for LLM processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39503099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_jd_for_llm(jd: dict) -> str:\n",
    "    # Extract basic fields with defaults\n",
    "    title = jd.get(\"title\", \"\")\n",
    "    description = jd.get(\"description\", \"\").strip()\n",
    "    location = jd.get(\"location\", \"\")\n",
    "    work_type = jd.get(\"formatted_work_type\", \"\")\n",
    "    skills_desc = jd.get(\"skills_desc\", \"\")\n",
    "    experience_level = jd.get(\"formatted_experience_level\", \"\")\n",
    "    \n",
    "    # Build human-readable prompt string for LLM\n",
    "    prompt_parts = [\n",
    "        f\"Job Title: {title}\",\n",
    "        f\"Location: {location}\",\n",
    "        f\"Employment Type: {work_type}\",\n",
    "        f\"Listed Skills: {skills_desc}\",\n",
    "        f\"Description:\\n{description}\",\n",
    "        f\"Experience Level (if available): {experience_level}\",\n",
    "    ]\n",
    "    \n",
    "    return \"\\n\\n\".join(prompt_parts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0de79d7",
   "metadata": {},
   "source": [
    "### Final Orchestrator: Fault-Tolerant Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e18858f",
   "metadata": {},
   "source": [
    "#### inject ID in parsed json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69d7930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from typing import Dict, Optional, Type\n",
    "\n",
    "def inject_ids(parsed: Dict, schema_model: Optional[Type], record_id: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Injects a UUID as `resume_id` or `jd_id` based on the schema model name.\n",
    "    \"\"\"\n",
    "    if not schema_model:\n",
    "        print(\"‚ö†Ô∏è No schema model provided for ID injection.\")\n",
    "        return parsed\n",
    "    schema_name = schema_model.__name__.lower()\n",
    "    if schema_name.startswith(\"resume\"):\n",
    "        parsed[\"resume_id\"] = record_id\n",
    "    elif schema_name.startswith(\"jobdescription\"):\n",
    "        parsed[\"jd_id\"] = record_id\n",
    "    return parsed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf34a91",
   "metadata": {},
   "source": [
    "#### Inject total experience in parsed resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13100697",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil import parser\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Set, Optional\n",
    "\n",
    "def parse_date(date_str: str) -> Optional[datetime]:\n",
    "    \"\"\"Parses date string into datetime, handling edge cases like 'Present'. Returns None if invalid.\"\"\"\n",
    "    if not date_str or date_str.strip().lower() in {\"present\", \"current\", \"till date\"}:\n",
    "        return datetime.today()\n",
    "    try:\n",
    "        return parser.parse(date_str, default=datetime(2000, 1, 1))\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "\n",
    "def get_months_between(start: datetime, end: datetime) -> List[Tuple[int, int]]:\n",
    "    \"\"\"Returns a list of (year, month) tuples between start and end inclusive.\"\"\"\n",
    "    months = []\n",
    "    current = datetime(start.year, start.month, 1)\n",
    "    end = datetime(end.year, end.month, 1)\n",
    "    while current <= end:\n",
    "        months.append((current.year, current.month))\n",
    "        if current.month == 12:\n",
    "            current = datetime(current.year + 1, 1, 1)\n",
    "        else:\n",
    "            current = datetime(current.year, current.month + 1, 1)\n",
    "    return months\n",
    "\n",
    "def calculate_total_experience_years(resume: Dict) -> float:\n",
    "    \"\"\"Calculates total experience in years with strict date validation and deduplication.\"\"\"\n",
    "    unique_months: Set[Tuple[int, int]] = set()\n",
    "    experiences = resume.get(\"experience\", [])\n",
    "\n",
    "    for exp in experiences:\n",
    "        start_str = exp.get(\"start_date\", \"\")\n",
    "        end_str = exp.get(\"end_date\", \"\")\n",
    "\n",
    "        start_date = parse_date(start_str)\n",
    "        end_date = parse_date(end_str)\n",
    "\n",
    "        if start_date is None or end_date is None:\n",
    "            continue  # ‚ùå skip invalid/malformed dates\n",
    "        if start_date > end_date:\n",
    "            continue  # ‚ùå skip logically incorrect ranges\n",
    "\n",
    "        months = get_months_between(start_date, end_date)\n",
    "        unique_months.update(months)\n",
    "\n",
    "    total_months = len(unique_months)\n",
    "    total_years = round(total_months / 12.0, 1)\n",
    "    return total_years\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6373c0",
   "metadata": {},
   "source": [
    "#### Inject and reorder fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af02ab85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import Dict, Optional\n",
    "\n",
    "def inject_and_reorder_top_fields(parsed: Dict, is_resume: Optional[bool] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Injects derived fields (like total_experience_years) and reorders ID-related fields to the top.\n",
    "    - For resumes: resume_id and total_experience_years come first.\n",
    "    - For JDs: jd_id comes first.\n",
    "    \"\"\"\n",
    "    if is_resume:\n",
    "        resume_id = parsed.get(\"resume_id\", \"\")\n",
    "        total_exp = calculate_total_experience_years(parsed)\n",
    "        return OrderedDict([\n",
    "            (\"resume_id\", resume_id),\n",
    "            (\"total_experience_years\", total_exp),\n",
    "            *[(k, v) for k, v in parsed.items() if k not in {\"resume_id\", \"total_experience_years\"}]\n",
    "        ])\n",
    "    else:\n",
    "        jd_id = parsed.get(\"jd_id\", \"\")\n",
    "        return OrderedDict([\n",
    "            (\"jd_id\", jd_id),\n",
    "            *[(k, v) for k, v in parsed.items() if k != \"jd_id\"]\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56af3483",
   "metadata": {},
   "source": [
    "#### Extract JSON from LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc83f2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Union\n",
    "\n",
    "def extract_structured_json(\n",
    "    text: str,\n",
    "    prompt_template: str,\n",
    "    schema_model: Union[None, type] = None,\n",
    "    max_new_tokens: int = 4096,\n",
    "    retries: int = 0,\n",
    "    record_id: str = \"\",\n",
    "    is_resume: bool = None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Runs LLM to extract structured JSON and validates against schema.\n",
    "    Includes: prompt sanitization, retry, echo detection, brace parser fallback, schema validation.\n",
    "    \"\"\"\n",
    "    schema_str = get_schema_str(schema_model)\n",
    "    prompt = prompt_template.format(text=text, schema=schema_str)\n",
    "    raw_output = \"\"\n",
    "    attempt = 0\n",
    "\n",
    "    while attempt <= retries:\n",
    "        try:\n",
    "            # Step 1: Get LLM output\n",
    "            #print(f\"\\n prompt: \\n {prompt} \\n\")\n",
    "            response = generate_llm_output(prompt, max_new_tokens)\n",
    "            #print(\"üß™ LLM output was:\\n\", response)  # Preview first 300 chars\n",
    "            raw_output = sanitize_llm_output(response, prompt)\n",
    "         \n",
    "\n",
    "            # Step 2: Detect schema echo or instruction echo\n",
    "            if \"$schema\" in raw_output or \"Ensure these rules\" in raw_output:\n",
    "                raise ValueError(\"LLM echoed schema or instruction block instead of generating JSON.\")\n",
    "\n",
    "            # Step 3: Try JSON load directly\n",
    "            json_start = raw_output.find(\"{\")\n",
    "            if json_start == -1:\n",
    "                raise ValueError(\"No opening '{' found in LLM output.\")\n",
    "            \n",
    "            cleaned_output = raw_output[json_start:]\n",
    "            cleaned_output = clean_raw_data(cleaned_output)\n",
    "            \n",
    "            #print(\"üß™ Cleaned output to parse:\\n\", cleaned_output)\n",
    "\n",
    "            parsed = json.loads(cleaned_output)\n",
    "            parsed = inject_ids(parsed, schema_model, record_id)\n",
    "            parsed = inject_and_reorder_top_fields(parsed, is_resume)\n",
    "            \n",
    "            #print(\"\\nüß™ Parsed output was:\\n\", parsed) \n",
    "\n",
    "            return parsed\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Attempt {attempt + 1} failed: {e}\")\n",
    "            print(\"üß™ Raw output was:\\n\", raw_output) \n",
    "            attempt += 1\n",
    "\n",
    "    # Step 5: Fallback using brace matching\n",
    "    try:\n",
    "        parsed = extract_json_block(raw_output)\n",
    "        parsed = inject_ids(parsed, schema_model, record_id)\n",
    "        parsed = inject_and_reorder_top_fields(parsed, is_resume)\n",
    "        print(\"üß™ Fallback parsed output was:\\n\", parsed)  \n",
    "        return parsed\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"raw_output\": raw_output.strip(),\n",
    "            \"error\": f\"Regex fallback failed: {e}\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143847db",
   "metadata": {},
   "source": [
    "##  Normalize in Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7325133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_metadata_summary(\n",
    "    output_dir: Path,\n",
    "    is_resume: bool,\n",
    "    input_file: str,\n",
    "    total_records: int,\n",
    "    total_valid: int,\n",
    "    total_invalid: int,\n",
    "    start_index: int,\n",
    "    end_index: int,\n",
    "    timestamp: str,\n",
    "    batch_id: str\n",
    "):\n",
    "    summary = {\n",
    "        \"batch_id\": batch_id,\n",
    "        \"timestamp\": timestamp,\n",
    "        \"input_file\": input_file,\n",
    "        \"input_type\": \"resume\" if is_resume else \"job_description\",\n",
    "        \"records_start_index\": start_index,\n",
    "        \"records_end_index\": end_index,\n",
    "        \"records_total\": total_records,\n",
    "        \"records_valid\": total_valid,\n",
    "        \"records_invalid\": total_invalid,\n",
    "        \"output_dir\": str(output_dir)\n",
    "    }\n",
    "    summary_file = output_dir / f\"meta_{'resumes' if is_resume else 'jds'}_{start_index}_{end_index}_{timestamp}_{batch_id}.json\"\n",
    "    save_json_output(summary, str(summary_file), overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a73a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "def normalize_record(\n",
    "    record: dict,\n",
    "    is_resume: bool,\n",
    "    output_dir: Path,\n",
    "    prompt_template,\n",
    "    schema_model\n",
    "):\n",
    "    if is_resume:\n",
    "        raw_resume_text = record.get(\"Resume_str\", \"\")\n",
    "        text = format_resume_for_llm(raw_resume_text)\n",
    "        max_new_tokens = 4096\n",
    "        record_id = record.get(\"ID\", str(uuid.uuid4()))\n",
    "    else:\n",
    "        text = format_jd_for_llm(record)\n",
    "        max_new_tokens = 2048\n",
    "        record_id = record.get(\"job_id\", str(uuid.uuid4()))\n",
    "\n",
    "    parsed = extract_structured_json(\n",
    "        text=text,\n",
    "        prompt_template=prompt_template,\n",
    "        schema_model=schema_model,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        record_id=record_id,\n",
    "        is_resume=is_resume\n",
    "    )\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "    prefix = \"resumes\" if is_resume else \"jds\"\n",
    "    \n",
    "    # ‚úÖ Handle complete failure\n",
    "    if parsed is None:\n",
    "        output_type = \"invalid\"\n",
    "        output_filename = f\"{prefix}_{output_type}_{record_id}_{timestamp}.json\"\n",
    "        save_json_output(\n",
    "            [{\n",
    "                \"record_id\": record_id,\n",
    "                \"domain\": \"unknown\",\n",
    "                \"input_text\": text,\n",
    "                \"output_json\": None,\n",
    "                \"error\": \"LLM and fallback extraction both failed\"\n",
    "            }],\n",
    "            output_path=output_dir / output_filename\n",
    "        )\n",
    "        return output_type\n",
    "\n",
    "    # ‚úÖ Determine domain\n",
    "    if is_resume:\n",
    "        domain = record.get(\"Category\", \"unknown\")\n",
    "    else:\n",
    "        domain = parsed.get(\"inferred_domain\", \"unknown\")\n",
    "        \n",
    "    # ‚úÖ Normal path\n",
    "    output_type = \"invalid\" if \"error\" in parsed or \"raw_output\" in parsed else \"valid\"\n",
    "    output_filename = f\"{prefix}_{record_id}_{output_type}_{timestamp}.json\"\n",
    "\n",
    "    # ‚úÖ Save structured record with input and output\n",
    "    output_data = {\n",
    "        \"record_id\": record_id,\n",
    "        \"domain\": domain,\n",
    "        \"input_text\": text,\n",
    "        \"output_json\": parsed\n",
    "    }\n",
    "    save_json_output([output_data], output_path=output_dir / output_filename)\n",
    "    return output_type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfdd15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "\n",
    "def normalize_file_in_batches(\n",
    "    input_filename: str,\n",
    "    output_dir: Path,\n",
    "    is_resume: bool = True,\n",
    "    input_dir: Path = Path(\"json_outputs\"),\n",
    "    limit: int = None\n",
    "):\n",
    "    input_path = input_dir / input_filename\n",
    "    data = load_ndjson_file(input_path)\n",
    "\n",
    "    checkpoint_file = output_dir / f\"checkpoint_{'resumes' if is_resume else 'jds'}.json\"\n",
    "    start_index = 0\n",
    "    if checkpoint_file.exists():\n",
    "        checkpoint = load_json_file(checkpoint_file)\n",
    "        start_index = checkpoint.get(\"last_index\", 0)\n",
    "        print(f\"üîÅ Resuming from index {start_index}\")\n",
    "\n",
    "    data_to_process = data[start_index:]\n",
    "    if limit is not None:\n",
    "        data_to_process = data_to_process[:limit]\n",
    "\n",
    "    prompt_template = RESUME_PROMPT_TEMPLATE if is_resume else JD_PROMPT_TEMPLATE\n",
    "    schema_model = ResumeSchema if is_resume else JobDescriptionSchema\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    total_valid, total_invalid = 0, 0\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "    batch_id = uuid.uuid4().hex[:6]\n",
    "    actual_start = start_index\n",
    "    actual_end = start_index + len(data_to_process)\n",
    "\n",
    "    for idx, record in enumerate(tqdm(data_to_process, desc=\"üîÑ Normalizing records\")):\n",
    "        absolute_idx = start_index + idx\n",
    "        output_type = normalize_record(\n",
    "            record=record,\n",
    "            is_resume=is_resume,\n",
    "            output_dir=output_dir,\n",
    "            prompt_template=prompt_template,\n",
    "            schema_model=schema_model\n",
    "        )\n",
    "        if output_type == \"valid\":\n",
    "            total_valid += 1\n",
    "        else:\n",
    "            total_invalid += 1\n",
    "\n",
    "        # ‚úÖ Save checkpoint after each record\n",
    "        save_json_output({\"last_index\": absolute_idx + 1}, str(checkpoint_file), overwrite=True)\n",
    "\n",
    "    # ‚úÖ Save summary metadata\n",
    "    save_metadata_summary(\n",
    "        output_dir=output_dir,\n",
    "        is_resume=is_resume,\n",
    "        input_file=input_filename,\n",
    "        total_records=len(data_to_process),\n",
    "        total_valid=total_valid,\n",
    "        total_invalid=total_invalid,\n",
    "        start_index=actual_start,\n",
    "        end_index=actual_end,\n",
    "        timestamp=timestamp,\n",
    "        batch_id=batch_id\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34057ff1",
   "metadata": {},
   "source": [
    "## Run Phase 2 End-to-End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf23348",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_file_in_batches(\n",
    "    input_filename=\"parsed_jds.json\",\n",
    "    input_dir=Path(Config.JSON_OUTPUT_DIR),\n",
    "    output_dir=Path(Config.JSON_OUTPUT_NORMALIZED_DIR),\n",
    "    is_resume=False,\n",
    "    limit=1  # ‚úÖ Process only 20 records max\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b33fc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_file_in_batches(\n",
    "    input_filename=\"parsed_resumes.json\",\n",
    "    input_dir=Path(Config.JSON_OUTPUT_DIR),\n",
    "    output_dir=Path(Config.JSON_OUTPUT_NORMALIZED_DIR),\n",
    "    is_resume=True,\n",
    "    limit=1  # ‚úÖ Process only 20 records max\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a896e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_file_in_batches(\n",
    "    input_filename=\"parsed_jds.json\",\n",
    "    input_dir=Path(Config.JSON_OUTPUT_DIR),\n",
    "    output_dir=Path(Config.JSON_OUTPUT_NORMALIZED_DIR),\n",
    "    is_resume=False,\n",
    ")\n",
    "\n",
    "normalize_file_in_batches(\n",
    "    input_filename=\"parsed_resumes.json\",\n",
    "    input_dir=Path(Config.JSON_OUTPUT_DIR),\n",
    "    output_dir=Path(Config.JSON_OUTPUT_NORMALIZED_DIR),\n",
    "    is_resume=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7409286b",
   "metadata": {},
   "source": [
    "## Merge normalized files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3940c601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "normalized_dir = Path(Config.JSON_OUTPUT_NORMALIZED_DIR)\n",
    "merged_dir = normalized_dir / \"merged\"\n",
    "\n",
    "merge_json_files(\n",
    "    source_dir=normalized_dir,\n",
    "    output_file=normalized_dir / \"normalized_jds.json\",\n",
    "    pattern=\"jds_valid*.json\",\n",
    "    merged_dir=merged_dir\n",
    ")\n",
    "\n",
    "merge_json_files(\n",
    "    source_dir=normalized_dir,\n",
    "    output_file=normalized_dir / \"normalized_resumes.json\",\n",
    "    pattern=\"resumes_valid*.json\",\n",
    "    merged_dir=merged_dir\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
