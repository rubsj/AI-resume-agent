{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f630d895",
   "metadata": {},
   "source": [
    "# Global setup and package installation used in most phases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969fcea6",
   "metadata": {},
   "source": [
    "## Colab + GPU Detection Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cf79fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def is_running_in_colab():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "def get_available_gpu_memory_gb():\n",
    "    try:\n",
    "        output = subprocess.check_output(\n",
    "            [\"nvidia-smi\", \"--query-gpu=memory.free\", \"--format=csv,nounits,noheader\"],\n",
    "            encoding=\"utf-8\"\n",
    "        )\n",
    "        free_mem_mb = int(output.strip().split(\"\\n\")[0])\n",
    "        return free_mem_mb / 1024\n",
    "    except Exception:\n",
    "        return 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66be54d4",
   "metadata": {},
   "source": [
    "## install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942fba10",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_running_in_colab():\n",
    "    # Install the required packages\n",
    "    !pip install kagglehub pandas\n",
    "    !pip install -q transformers accelerate bitsandbytes sentencepiece pydantic huggingface_hub xformers\n",
    "    !pip install regex json5\n",
    "    !pip install sentence-transformers scikit-learn\n",
    "    !pip install rapidfuzz unidecode\n",
    "\n",
    "else:\n",
    "    %pip install kagglehub pandas\n",
    "    %pip install -q transformers accelerate sentencepiece pydantic huggingface_hub xformers\n",
    "    #%pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu121\n",
    "    #%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "    %pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128\n",
    "    %pip install -U bitsandbytes\n",
    "    %pip install regex json5\n",
    "    %pip install sentence-transformers scikit-learn\n",
    "    %pip install rapidfuzz unidecode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4a9f61",
   "metadata": {},
   "source": [
    "## Login to huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1235aef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# Set your token here securely or prompt for it in Colab\n",
    "# Recommended: store in Colab secrets or environment variable\n",
    "HF_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "\n",
    "if not HF_TOKEN:\n",
    "    if is_running_in_colab():\n",
    "        # If running in Colab, use the Colab secrets\n",
    "        try:\n",
    "            from google.colab import userdata\n",
    "            HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "            if not HF_TOKEN:\n",
    "                raise ValueError(\"‚ö†Ô∏è Hugging Face token not found in Colab secrets.\")\n",
    "            print(\"üîë Hugging Face token found in Colab secrets.\")\n",
    "        except ImportError:\n",
    "            print(\"‚ö†Ô∏è Unable to authenticate in Colab. Please set your Hugging Face token manually.\")\n",
    "    else:\n",
    "        # Prompt for token if not set in environment\n",
    "        print(\"üîë Please enter your Hugging Face token:\")\n",
    "        # For Colab or local prompt input\n",
    "        HF_TOKEN = input(\"üîë Enter your Hugging Face token: \").strip()\n",
    "\n",
    "login(token=HF_TOKEN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b423735e",
   "metadata": {},
   "source": [
    "## Setup Kaggle Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409818e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def setup_kaggle_credentials():\n",
    "    kaggle_path = os.path.expanduser('~/.kaggle/kaggle.json')\n",
    "    if not os.path.exists(kaggle_path):\n",
    "        from google.colab import files\n",
    "        print(\"üìÇ Upload kaggle.json file...\")\n",
    "        uploaded = files.upload()\n",
    "        os.makedirs(os.path.dirname(kaggle_path), exist_ok=True)\n",
    "        for filename in uploaded.keys():\n",
    "            shutil.move(filename, kaggle_path)\n",
    "        os.chmod(kaggle_path, 0o600)\n",
    "        print(f\"‚úÖ Kaggle credentials setup at {kaggle_path}\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Kaggle credentials already exist at {kaggle_path}\")\n",
    "\n",
    "setup_kaggle_credentials()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d384b2f",
   "metadata": {},
   "source": [
    "## Mount Google Drive (Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1517ea98",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_running_in_colab():\n",
    "   from google.colab import drive\n",
    "   drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9964af3",
   "metadata": {},
   "source": [
    "##  Load Qwen-Instruct with Fallback to Quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6d840d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "import torch\n",
    "\n",
    "def load_model_pipeline(model_name: str, hf_token: str):\n",
    "    has_cuda = torch.cuda.is_available()\n",
    "    free_mem = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3) if has_cuda else 0\n",
    "    print(f\"üíª CUDA: {has_cuda} | GPU Memory: {free_mem:.2f} GB\")\n",
    "\n",
    "    device_map = {\"\": 0} if has_cuda else \"cpu\"\n",
    "    use_4bit = has_cuda and free_mem < 24\n",
    "\n",
    "    # Set quantization config\n",
    "    quant_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True if use_4bit else False,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    ) if use_4bit else None\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token, trust_remote_code=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # ‚úÖ Fix warning about pad_token\n",
    "\n",
    "    # Load model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device_map,\n",
    "        quantization_config=quant_config,\n",
    "        torch_dtype=torch.float16 if not quant_config else None,\n",
    "        trust_remote_code=True,\n",
    "        token=hf_token\n",
    "    )\n",
    "\n",
    "    print(f\"‚úÖ Model loaded on {next(model.parameters()).device}\")\n",
    "    return pipeline(\"text-generation\", model=model, tokenizer=tokenizer, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f811301",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_pipeline = load_model_pipeline(\n",
    "    model_name=\"Qwen/Qwen2-7B-Instruct\",\n",
    "    hf_token=HF_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739b4783",
   "metadata": {},
   "source": [
    "# Global utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10779c22",
   "metadata": {},
   "source": [
    "### Utility to merge json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a472fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "def merge_json_files(\n",
    "    source_dir: Path,\n",
    "    output_file: Path,\n",
    "    pattern: str,\n",
    "    merged_dir: Path\n",
    "):\n",
    "    source_dir.mkdir(parents=True, exist_ok=True)\n",
    "    merged_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    merged_data = []\n",
    "\n",
    "    # Load existing output if it exists\n",
    "    if output_file.exists():\n",
    "        with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                merged_data = json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"‚ö†Ô∏è Could not decode {output_file}, starting from scratch.\")\n",
    "\n",
    "    # Identify matching files\n",
    "    files_to_merge = sorted(source_dir.glob(pattern))\n",
    "\n",
    "    for file_path in files_to_merge:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "                if isinstance(data, list):\n",
    "                    merged_data.extend(data)\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è Skipping {file_path.name}: not a list.\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Failed to parse {file_path.name}: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Move to merged folder\n",
    "        shutil.move(str(file_path), merged_dir / file_path.name)\n",
    "        print(f\"‚úÖ Merged and moved: {file_path.name}\")\n",
    "\n",
    "    # Write combined output\n",
    "    if merged_data:\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(merged_data, f, indent=2)\n",
    "        print(f\"üíæ Saved to: {output_file}\")\n",
    "    else:\n",
    "        print(\"üì≠ No valid data to merge.\")\n",
    "\n",
    "# === Usage ===\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63df9fc3",
   "metadata": {},
   "source": [
    "### Utility to save json to a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727b44eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "# üì¶ Save JSON Output with Safety\n",
    "def save_json_output(data, output_path: str, indent: int = 4, overwrite: bool = True):\n",
    "    output_dir = os.path.dirname(output_path)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    if os.path.exists(output_path):\n",
    "        if overwrite:\n",
    "            os.remove(output_path)\n",
    "        else:\n",
    "            raise FileExistsError(f\"File {output_path} already exists and overwrite=False.\")\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=indent, ensure_ascii=False)\n",
    "\n",
    "    print(f\"‚úÖ Saved output to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc498258",
   "metadata": {},
   "source": [
    "### Utility to load file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf31583",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import json\n",
    "\n",
    "# üìÇ Load normalized JSON data\n",
    "def load_json_file(file_path: str) -> Any:\n",
    "    if not os.path.isfile(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3308e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Utility: Load structured JSON records from one or more files\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def load_json_records(paths: List[Path]) -> List[Dict]:\n",
    "    records = []\n",
    "    for path in paths:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            if isinstance(data, list):\n",
    "                records.extend(data)\n",
    "            else:\n",
    "                records.append(data)\n",
    "    return records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b329b93",
   "metadata": {},
   "source": [
    "### Configurations  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4308e95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# üõ† CONFIGURATION\n",
    "# ==============================\n",
    "\n",
    "class Config:\n",
    "    DATASET_DOWNLOAD_DIR = \"datasets\"\n",
    "    JSON_OUTPUT_DIR = \"json_outputs_run3\"\n",
    "    JSON_OUTPUT_NORMALIZED_DIR = \"json_outputs_run3/normalized\"\n",
    "    JSON_OUTPUT_NORMALIZED_JD = \"json_outputs_run3/normalized/jd\"\n",
    "    JSON_OUTPUT_NORMALIZED_RESUME = \"json_outputs_run3/normalized/resume\"\n",
    "    JSON_OUTPUT_SCORING_DIR = \"json_outputs_run3/scoring\"\n",
    "    AUTO_CLEANUP = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7b2072",
   "metadata": {},
   "source": [
    "# Pre Phas3 Embedding-Based Resume-JD Relevance Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2862ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from typing import Tuple\n",
    "\n",
    "# Load embedding model once (lightweight)\n",
    "domain_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def compute_domain_similarity(domain1: str, domain2: str, threshold: float = 0.5) -> Tuple[float, bool]:\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between two domain strings using sentence embeddings.\n",
    "\n",
    "    Returns:\n",
    "        (similarity_score, is_similar) where:\n",
    "            - similarity_score is a float between 0 and 1\n",
    "            - is_similar is True if similarity >= threshold\n",
    "    \"\"\"\n",
    "    if not domain1 or not domain2:\n",
    "        return 1.0, True  # Treat empty/missing domain as matching\n",
    "\n",
    "    embeddings = domain_model.encode([domain1, domain2], convert_to_tensor=True)\n",
    "    similarity = util.cos_sim(embeddings[0], embeddings[1]).item()\n",
    "    return round(similarity, 4), similarity >= threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86afb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_score, is_similar = compute_domain_similarity(\"healthcare analytics\", \"medical data science\")\n",
    "print(f\"Similarity: {sim_score}, Match: {is_similar}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509dc162",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from pathlib import Path\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "def generate_relevance_map(\n",
    "    resume_files: List[Path],\n",
    "    jd_files: List[Path],\n",
    "    model_name: str = \"all-MiniLM-L6-v2\",\n",
    "    min_score: float = 0.2\n",
    ") -> Dict[str, List[Dict]]:\n",
    "    model = SentenceTransformer(model_name)\n",
    "\n",
    "    resumes = load_json_records(resume_files)\n",
    "    jds = load_json_records(jd_files)\n",
    "\n",
    "    resumes_df = pd.DataFrame([r for r in resumes if r.get(\"input_text\") and r.get(\"output_json\")])\n",
    "    jds_df = pd.DataFrame([j for j in jds if j.get(\"input_text\") and j.get(\"output_json\")])\n",
    "\n",
    "    resume_embeds = model.encode(resumes_df[\"input_text\"].tolist(), convert_to_tensor=True)\n",
    "    jd_embeds = model.encode(jds_df[\"input_text\"].tolist(), convert_to_tensor=True)\n",
    "    scores = util.cos_sim(resume_embeds, jd_embeds).cpu().numpy()\n",
    "\n",
    "    results = []\n",
    "    for i, r_row in resumes_df.iterrows():\n",
    "        for j, jd_row in jds_df.iterrows():\n",
    "            r_domain = r_row[\"domain\"].strip().lower()\n",
    "            jd_domain = jd_row[\"domain\"].strip().lower()\n",
    "            domain_score, domain_match = compute_domain_similarity(r_domain, jd_domain, threshold=0.5)\n",
    "            if not domain_match:\n",
    "                continue  # Skip this resume‚ÄìJD pair\n",
    "\n",
    "\n",
    "            score = float(scores[i][j])\n",
    "            if score < min_score:\n",
    "                continue\n",
    "\n",
    "            label = (\n",
    "                \"strong\" if score >= 0.65 else\n",
    "                \"medium\" if score >= 0.4 else\n",
    "                \"weak\"\n",
    "            )\n",
    "\n",
    "            # Append result with extended metadata\n",
    "            results.append({\n",
    "                \"resume_id\": r_row[\"record_id\"],\n",
    "                \"jd_id\": jd_row[\"record_id\"],\n",
    "                \"resume_domain\": r_row[\"domain\"],\n",
    "                \"jd_domain\": jd_row[\"domain\"],\n",
    "                \"domain_similarity\": round(domain_score, 3),\n",
    "                \"resume_jd_similarity\": round(score, 3),\n",
    "                \"semantic_match_label\": label\n",
    "            })\n",
    "\n",
    "    print(f\"Generated relevance map with {len(results)} pairs.\")\n",
    "    # print count of weak, medium, strong matches\n",
    "    weak_count = sum(1 for r in results if r[\"semantic_match_label\"] == \"weak\")\n",
    "    medium_count = sum(1 for r in results if r[\"semantic_match_label\"] == \"medium\")\n",
    "    strong_count = sum(1 for r in results if r[\"semantic_match_label\"] == \"strong\")\n",
    "    print(f\"Weak matches: {weak_count}, Medium matches: {medium_count}, Strong matches: {strong_count}\")\n",
    "\n",
    "    top_k = 3  # Change as needed\n",
    "    reverse_map = defaultdict(list)\n",
    "\n",
    "    # Group by resume_id and sort by similarity score\n",
    "    for r in results:\n",
    "        reverse_map[r[\"resume_id\"]].append(r)\n",
    "\n",
    "    # Sort and keep top K\n",
    "    resume_top_matches = {\n",
    "        resume_id: sorted(matches, key=lambda x: x[\"resume_jd_similarity\"], reverse=True)[:top_k]\n",
    "        for resume_id, matches in reverse_map.items()\n",
    "    }\n",
    "\n",
    "\n",
    "    return {\n",
    "    \"semantic_relevance_scores\": results,\n",
    "    \"resume_top_matches\": resume_top_matches\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a049b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "resume_paths = list(Path(Config.JSON_OUTPUT_NORMALIZED_RESUME).glob(\"resumes_*.json\"))\n",
    "jd_paths = list(Path(Config.JSON_OUTPUT_NORMALIZED_JD).glob(\"jds_*.json\"))\n",
    "relevance_data = generate_relevance_map(resume_paths, jd_paths)\n",
    "relevance_map_file = os.path.join(Config.JSON_OUTPUT_SCORING_DIR, 'semantic_relevance_scores.json')\n",
    "save_json_output(relevance_data, relevance_map_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e89760a",
   "metadata": {},
   "source": [
    "# Phase 3 Rubric-Based Scoring Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e31b1a",
   "metadata": {},
   "source": [
    "## Rule-Based Scoring Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a6b5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_config = {\n",
    "    \"matching\": {\n",
    "        # Global fuzzy threshold for fuzzy matching (0‚Äì100)\n",
    "        \"fuzzy_threshold_default\": 85,\n",
    "        \"section_thresholds\": {\n",
    "            \"skills\": 85,\n",
    "            \"tools\": 80,\n",
    "            \"certifications\": 88,\n",
    "            \"responsibilities\": 83,\n",
    "            \"education\": 87\n",
    "        }\n",
    "    },\n",
    "    \"weights\": {\n",
    "        \"skills\": {\n",
    "            \"exact\": 1.0,\n",
    "            \"substring\": 0.8,\n",
    "            \"fuzzy\": 0.5\n",
    "        },\n",
    "        \"tools\": {\n",
    "            \"exact\": 1.0,\n",
    "            \"substring\": 0.7,\n",
    "            \"fuzzy\": 0.4\n",
    "        },\n",
    "        \"certifications\": {\n",
    "            \"exact\": 1.0,\n",
    "            \"substring\": 0.9,\n",
    "            \"fuzzy\": 0.5\n",
    "        },\n",
    "        \"responsibilities\": {\n",
    "            \"exact\": 1.0,\n",
    "            \"substring\": 0.85,\n",
    "            \"fuzzy\": 0.5\n",
    "        },\n",
    "        \"education\": {\n",
    "            \"exact\": 1.0,\n",
    "            \"substring\": 0.75,\n",
    "            \"fuzzy\": 0.5\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53494bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import fuzz\n",
    "import re\n",
    "from typing import List, Tuple, Dict, Any\n",
    "from unidecode import unidecode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62163dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text: str) -> str:\n",
    "    text = unidecode(text.lower())\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", text)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "def normalize_score(score: float) -> float:\n",
    "    return max(0.0, min(round(score, 3), 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76af97bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_match(jd_terms: List[str], resume_text: str, section: str) -> Tuple[Dict[str, int], List[Dict[str, str]]]:\n",
    "    \"\"\"\n",
    "    Classifies JD terms matched in resume_text as 'exact', 'substring', or 'fuzzy'.\n",
    "    Returns match counts and match breakdown list.\n",
    "    \"\"\"\n",
    "    if not jd_terms:\n",
    "        return {\"exact\": 0, \"substring\": 0, \"fuzzy\": 0}, []\n",
    "\n",
    "\n",
    "    fuzzy_threshold = scoring_config[\"matching\"].get(\"section_thresholds\", {}).get(\n",
    "        section,\n",
    "        scoring_config[\"matching\"].get(\"fuzzy_threshold_default\", 85)\n",
    "    )\n",
    "    resume_tokens = set(re.findall(r\"\\b[\\w\\+\\-\\.#]{3,}\\b\", resume_text.lower()))\n",
    "    normalized_text = normalize(resume_text)\n",
    "\n",
    "    counts = {\"exact\": 0, \"substring\": 0, \"fuzzy\": 0}\n",
    "    matched_terms = []\n",
    "\n",
    "    for term in jd_terms:\n",
    "        normalized_term = normalize(term)\n",
    "\n",
    "        if normalized_term in resume_tokens:\n",
    "            counts[\"exact\"] += 1\n",
    "            matched_terms.append({\"term\": term, \"type\": \"exact\"})\n",
    "        elif normalized_term in normalized_text:\n",
    "            counts[\"substring\"] += 1\n",
    "            matched_terms.append({\"term\": term, \"type\": \"substring\"})\n",
    "        elif fuzz.partial_ratio(normalized_term, normalized_text) >= fuzzy_threshold:\n",
    "            counts[\"fuzzy\"] += 1\n",
    "            matched_terms.append({\"term\": term, \"type\": \"fuzzy\"})\n",
    "\n",
    "    return counts, matched_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162e5a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_from_match_counts(counts: Dict[str, int], total: int, weights: Dict[str, float]) -> float:\n",
    "    weighted_sum = (\n",
    "        weights.get(\"exact\", 1.0) * counts[\"exact\"] +\n",
    "        weights.get(\"substring\", 0.8) * counts[\"substring\"] +\n",
    "        weights.get(\"fuzzy\", 0.5) * counts[\"fuzzy\"]\n",
    "    )\n",
    "    return normalize_score(weighted_sum / total) if total else 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27e5128",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_skills_rule(resume_skills, resume_other, jd_required, jd_optional):\n",
    "    resume_text = (\n",
    "        \" \".join(resume_skills or []) + \" \" +\n",
    "        \" \".join(section.get(\"content\", \"\") for section in resume_other or [])\n",
    "    )\n",
    "\n",
    "    weights = scoring_config[\"weights\"].get(\"skills\", {\"exact\": 1.0, \"substring\": 0.8, \"fuzzy\": 0.5})\n",
    "\n",
    "    r_counts, r_matches = hybrid_match(jd_required, resume_text, section=\"skills\")\n",
    "    o_counts, o_matches = hybrid_match(jd_optional, resume_text, section=\"skills\")\n",
    "\n",
    "    r_score = score_from_match_counts(r_counts, len(jd_required), weights)\n",
    "    o_score = score_from_match_counts(o_counts, len(jd_optional), weights)\n",
    "\n",
    "    final_score = normalize_score(0.8 * r_score + 0.2 * o_score)\n",
    "    reason = (\n",
    "        f\"Required: {r_counts}, Optional: {o_counts}. \"\n",
    "        f\"Matched skills: {[m['term'] + ' (' + m['type'] + ')' for m in r_matches + o_matches]}\"\n",
    "    )\n",
    "    return final_score, reason\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7febd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_certifications_rule(resume_certs, resume_other, jd_certs):\n",
    "    if not jd_certs:\n",
    "        return 1.0, \"No certifications required by JD.\"\n",
    "\n",
    "    cert_text = (\n",
    "        \" \".join(cert.get(\"certification\", \"\") for cert in resume_certs or []) + \" \" +\n",
    "        \" \".join(section.get(\"content\", \"\") for section in resume_other or [])\n",
    "    )\n",
    "\n",
    "    weights = scoring_config[\"weights\"].get(\"certifications\", {\"exact\": 1.0, \"substring\": 0.9, \"fuzzy\": 0.5})\n",
    "\n",
    "    counts, matches = hybrid_match(jd_certs, cert_text, section=\"certifications\")\n",
    "    score = score_from_match_counts(counts, len(jd_certs), weights)\n",
    "\n",
    "    reason = f\"Certifications matched: {counts}. Matched: {[m['term'] + ' (' + m['type'] + ')' for m in matches]}\"\n",
    "    return score, reason\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b22de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_education_rule(resume_education, jd_degrees):\n",
    "    if not jd_degrees:\n",
    "        return 1.0, \"No preferred degrees listed in JD.\"\n",
    "    if not resume_education:\n",
    "        return 0.0, \"No education information found in resume.\"\n",
    "\n",
    "    resume_text = \" \".join((edu.get(\"degree\", \"\") or \"\") for edu in resume_education)\n",
    "\n",
    "    weights = scoring_config[\"weights\"].get(\"education\", {\"exact\": 1.0, \"substring\": 0.75, \"fuzzy\": 0.5})\n",
    "\n",
    "    counts, matches = hybrid_match(jd_degrees, resume_text, section=\"education\")\n",
    "    score = score_from_match_counts(counts, len(jd_degrees), weights)\n",
    "\n",
    "    reason = f\"Degrees matched: {counts}. Matched: {[m['term'] + ' (' + m['type'] + ')' for m in matches]}\"\n",
    "    return score, reason\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b161703a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_experience_rule(resume_years, jd_required_years, cap: float = 40.0):\n",
    "    if not jd_required_years or resume_years is None:\n",
    "        return 0.5, \"Missing required or actual experience data.\"\n",
    "\n",
    "    numbers = re.findall(r'\\d+(?:\\.\\d+)?', jd_required_years)\n",
    "    if not numbers:\n",
    "        return 1.0, \"JD experience string did not specify clear years.\"\n",
    "\n",
    "    required_years = float(min(numbers))\n",
    "    if required_years == 0:\n",
    "        return 1.0, \"JD required years = 0.\"\n",
    "\n",
    "    resume_years_capped = min(resume_years, cap)\n",
    "    score = resume_years_capped / required_years\n",
    "    reason = f\"Resume: {resume_years} yrs (capped to {resume_years_capped}), JD requires: {required_years} yrs.\"\n",
    "    return normalize_score(score), reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3266e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_tools_rule(resume_skills, resume_experience, resume_other, resume_projects, jd_tools):\n",
    "    if not jd_tools:\n",
    "        return 1.0, \"No tools required by JD.\"\n",
    "\n",
    "    resume_text = (\n",
    "        \" \".join(resume_skills or []) + \" \" +\n",
    "        \" \".join(\" \".join(exp.get(\"description\", [])) for exp in resume_experience or []) + \" \" +\n",
    "        \" \".join(section.get(\"content\", \"\") for section in resume_other or []) + \" \" +\n",
    "        \" \".join(project.get(\"description\", \"\") for project in resume_projects or [])\n",
    "    )\n",
    "\n",
    "    weights = scoring_config[\"weights\"].get(\"tools\", {\"exact\": 1.0, \"substring\": 0.7, \"fuzzy\": 0.4})\n",
    "\n",
    "    counts, matches = hybrid_match(jd_tools, resume_text, section =\"tools\")\n",
    "    score = score_from_match_counts(counts, len(jd_tools), weights)\n",
    "\n",
    "    reason = f\"Tools matched: {counts}. Matched: {[m['term'] + ' (' + m['type'] + ')' for m in matches]}\"\n",
    "    return score, reason\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dfeb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_responsibilities_rule(resume_experience, resume_other, resume_projects, jd_responsibilities):\n",
    "    if not jd_responsibilities:\n",
    "        return 1.0, \"No responsibilities listed in JD.\"\n",
    "\n",
    "    resume_text = (\n",
    "        \" \".join(\" \".join(exp.get(\"description\", [])) for exp in resume_experience or []) + \" \" +\n",
    "        \" \".join(section.get(\"content\", \"\") for section in resume_other or []) + \" \" +\n",
    "        \" \".join(project.get(\"description\", \"\") for project in resume_projects or [])\n",
    "    )\n",
    "\n",
    "    weights = scoring_config[\"weights\"].get(\"responsibilities\", {\"exact\": 1.0, \"substring\": 0.85, \"fuzzy\": 0.4})\n",
    "\n",
    "    counts, matches = hybrid_match(jd_responsibilities, resume_text, section=\"responsibilities\")\n",
    "    score = score_from_match_counts(counts, len(jd_responsibilities), weights)\n",
    "\n",
    "    reason = f\"Responsibilities matched: {counts}. Matched: {[m['term'] + ' (' + m['type'] + ')' for m in matches]}\"\n",
    "    return score, reason\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a24b934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_all_rule_scores(resume_json: Dict[str, Any], jd_json: Dict[str, Any]) -> Dict[str, Dict[str, Any]]:\n",
    "    scores = {}\n",
    "\n",
    "    skills_score, skills_reason = score_skills_rule(\n",
    "        resume_json.get(\"skills\", []),\n",
    "        resume_json.get(\"other\", []),\n",
    "        jd_json.get(\"required_skills\", []),\n",
    "        jd_json.get(\"optional_skills\", [])\n",
    "    )\n",
    "    scores[\"skills\"] = {\"score\": skills_score, \"reason\": skills_reason}\n",
    "\n",
    "    cert_score, cert_reason = score_certifications_rule(\n",
    "        resume_json.get(\"certifications\", []),\n",
    "        resume_json.get(\"other\", []),\n",
    "        jd_json.get(\"certifications\", [])\n",
    "    )\n",
    "    scores[\"certifications\"] = {\"score\": cert_score, \"reason\": cert_reason}\n",
    "\n",
    "    edu_score, edu_reason = score_education_rule(\n",
    "        resume_json.get(\"education\", []),\n",
    "        jd_json.get(\"preferred_degrees\", [])\n",
    "    )\n",
    "    scores[\"education\"] = {\"score\": edu_score, \"reason\": edu_reason}\n",
    "\n",
    "    exp_score, exp_reason = score_experience_rule(\n",
    "        resume_json.get(\"total_experience_years\", 0.0),\n",
    "        jd_json.get(\"required_experience_years\", \"\")\n",
    "    )\n",
    "    scores[\"experience\"] = {\"score\": exp_score, \"reason\": exp_reason}\n",
    "\n",
    "    tools_score, tools_reason = score_tools_rule(\n",
    "        resume_json.get(\"skills\", []),\n",
    "        resume_json.get(\"experience\", []),\n",
    "        resume_json.get(\"other\", []),\n",
    "        resume_json.get(\"projects\", []),\n",
    "        jd_json.get(\"tools_and_technologies\", [])\n",
    "    )\n",
    "    scores[\"tools\"] = {\"score\": tools_score, \"reason\": tools_reason}\n",
    "\n",
    "    resp_score, resp_reason = score_responsibilities_rule(\n",
    "        resume_json.get(\"experience\", []),\n",
    "        resume_json.get(\"other\", []),\n",
    "        resume_json.get(\"projects\", []),\n",
    "        jd_json.get(\"job_responsibilities\", [])\n",
    "    )\n",
    "    scores[\"responsibilities\"] = {\"score\": resp_score, \"reason\": resp_reason}\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42629c1",
   "metadata": {},
   "source": [
    "### unit test for each scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d10d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Testing: score_skills_rule ===\")\n",
    "resume_skills = [\"Python\", \"SQL\", \"Excel\"]\n",
    "resume_other = [{\"section_name\": \"Training\", \"content\": \"Completed MongoDB, Tableau, Excel\"}]\n",
    "jd_required_skills = [\"Python\", \"MongoDB\"]\n",
    "jd_optional_skills = [\"Tableau\", \"Java\"]\n",
    "\n",
    "score, reason = score_skills_rule(resume_skills, resume_other, jd_required_skills, jd_optional_skills)\n",
    "print(f\"Score: {score}\\nReason: {reason}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68848f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Testing: score_certifications_rule ===\")\n",
    "resume_certs = [{\"certification\": \"AWS Certified\"}, {\"certification\": \"Azure\"}]\n",
    "resume_other = [{\"section_name\": \"Achievements\", \"content\": \"Google Cloud certified\"}]\n",
    "jd_certs = [\"AWS Certified\", \"Google Cloud\"]\n",
    "\n",
    "score, reason = score_certifications_rule(resume_certs, resume_other, jd_certs)\n",
    "print(f\"Score: {score}\\nReason: {reason}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae023ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Testing: score_education_rule ===\")\n",
    "resume_education = [{\"degree\": \"Bachelor of Computer Science\"}, {\"degree\": \"MBA\"}]\n",
    "jd_degrees = [\"Computer Science\", \"Information Technology\"]\n",
    "\n",
    "score, reason = score_education_rule(resume_education, jd_degrees)\n",
    "print(f\"Score: {score}\\nReason: {reason}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42095c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Testing: score_experience_rule ===\")\n",
    "resume_years = 6.0\n",
    "jd_experience = \"3‚Äì5 years\"\n",
    "\n",
    "score, reason = score_experience_rule(resume_years, jd_experience)\n",
    "print(f\"Score: {score}\\nReason: {reason}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc2aa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Testing: score_tools_rule ===\")\n",
    "resume_skills = [\"Python\", \"Docker\"]\n",
    "resume_experience = [\n",
    "    {\"job_title\": \"DevOps Engineer\", \"description\": [\"Used AWS, Docker, and Jenkins\"]},\n",
    "    {\"job_title\": \"Software Engineer\", \"description\": [\"Built APIs with Flask\"]}\n",
    "]\n",
    "resume_other = [{\"section_name\": \"Misc\", \"content\": \"Worked on Kubernetes and Terraform\"}]\n",
    "resume_projects = [{\"description\": \"Built ML model with Scikit-learn and deployed on AWS\"}]\n",
    "jd_tools = [\"AWS\", \"Docker\", \"Kubernetes\", \"GCP\"]\n",
    "\n",
    "score, reason = score_tools_rule(resume_skills, resume_experience, resume_other, resume_projects, jd_tools)\n",
    "print(f\"Score: {score}\\nReason: {reason}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042ea423",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Testing: score_responsibilities_rule ===\")\n",
    "resume_experience = [\n",
    "    {\"job_title\": \"Data Analyst\", \"description\": [\"Created dashboards using Power BI\", \"Cleaned large datasets\"]},\n",
    "]\n",
    "resume_other = [{\"section_name\": \"Leadership\", \"content\": \"Led team of 5 analysts\"}]\n",
    "resume_projects = [{\"description\": \"Automated data pipeline using Python\"}]\n",
    "jd_responsibilities = [\n",
    "    \"Created dashboards using Power BI\",\n",
    "    \"Automated data pipeline using Python\",\n",
    "    \"Built ETL workflows\"\n",
    "]\n",
    "\n",
    "score, reason = score_responsibilities_rule(resume_experience, resume_other, resume_projects, jd_responsibilities)\n",
    "print(f\"Score: {score}\\nReason: {reason}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e67572f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "print(\"=== Testing: compute_all_rule_scores ===\")\n",
    "\n",
    "resume_json = {\n",
    "    \"skills\": [\"Python\", \"Docker\"],\n",
    "    \"certifications\": [{\"certification\": \"AWS Certified\"}, {\"certification\": \"Azure\"}],\n",
    "    \"education\": [{\"degree\": \"Bachelor of Computer Science\"}, {\"degree\": \"MBA\"}],\n",
    "    \"total_experience_years\": 4.5,\n",
    "    \"experience\": [\n",
    "        {\"job_title\": \"DevOps Engineer\", \"description\": [\"Used AWS, Docker, and Jenkins\"]},\n",
    "        {\"job_title\": \"Data Analyst\", \"description\": [\"Created dashboards using Power BI\"]}\n",
    "    ],\n",
    "    \"other\": [\n",
    "        {\"section_name\": \"Leadership\", \"content\": \"Led team of 5 analysts\"},\n",
    "        {\"section_name\": \"Achievements\", \"content\": \"Google Cloud certified\"}\n",
    "    ],\n",
    "    \"projects\": [\n",
    "        {\"description\": \"Built ML model with Scikit-learn and deployed on AWS\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "jd_json = {\n",
    "    \"required_skills\": [\"Python\", \"MongoDB\"],\n",
    "    \"optional_skills\": [\"Tableau\", \"Java\"],\n",
    "    \"certifications\": [\"AWS Certified\", \"Google Cloud\"],\n",
    "    \"preferred_degrees\": [\"Computer Science\", \"Information Technology\"],\n",
    "    \"required_experience_years\": \"3+ years\",\n",
    "    \"tools_and_technologies\": [\"AWS\", \"Docker\", \"Kubernetes\", \"GCP\"],\n",
    "    \"job_responsibilities\": [\n",
    "        \"Created dashboards using Power BI\",\n",
    "        \"Automated data pipeline using Python\",\n",
    "        \"Built ETL workflows\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "results = compute_all_rule_scores(resume_json, jd_json)\n",
    "\n",
    "\n",
    "pprint(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b948ff22",
   "metadata": {},
   "source": [
    "## LLM-Based Scoring Functions (Structured Prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f8cffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_SCORING_SCHEMA = \"\"\"{\n",
    "  \"skills\": {\n",
    "    \"score\": float,\n",
    "    \"reason\": str\n",
    "  },\n",
    "  \"certifications\": {\n",
    "    \"score\": float,\n",
    "    \"reason\": str\n",
    "  },\n",
    "  \"education\": {\n",
    "    \"score\": float,\n",
    "    \"reason\": str\n",
    "  },\n",
    "  \"experience\": {\n",
    "    \"score\": float,\n",
    "    \"reason\": str\n",
    "  },\n",
    "  \"tools\": {\n",
    "    \"score\": float,\n",
    "    \"reason\": str\n",
    "  },\n",
    "  \"responsibilities\": {\n",
    "    \"score\": float,\n",
    "    \"reason\": str\n",
    "  },\n",
    "  \"soft_skills\": {\n",
    "    \"score\": float,\n",
    "    \"reason\": str\n",
    "  },\n",
    "  \"transferable_skills\": {\n",
    "    \"score\": float,\n",
    "    \"reason\": str\n",
    "  },\n",
    "  \"leadership\": {\n",
    "    \"score\": float,\n",
    "    \"reason\": str\n",
    "  },\n",
    "  \"grammar_cleanliness\": {\n",
    "    \"score\": float,\n",
    "    \"reason\": str\n",
    "  }\n",
    "}\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54453faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_SCORING_PROMPT_TEMPLATE = \"\"\"\n",
    "You are an expert resume evaluator.\n",
    "\n",
    "Your task is to **compare** a candidate's resume and a job description and assign **section-wise ATS scores**. Each section receives:\n",
    "- a score between 0.0 and 1.0\n",
    "- a short reason explaining why\n",
    "\n",
    "You must return a valid JSON object. Do not return the resume. Do not repeat input. Do not include markdown or explanations.\n",
    "\n",
    "RESUME:\n",
    "{resume_json}\n",
    "\n",
    "JOB DESCRIPTION:\n",
    "{jd_json}\n",
    "\n",
    "Output format (STRICTLY FOLLOW THIS STRUCTURE):\n",
    "{schema}\n",
    "\n",
    "Now respond ONLY with a JSON object in this format:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381ad05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "import json5\n",
    "from typing import Dict\n",
    "\n",
    "def extract_json_block(text: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Extract the last valid JSON object block from the text using recursive regex and json5.\n",
    "    Handles smart quotes, trailing commas, and prefers LLM's final output JSON.\n",
    "    \"\"\"\n",
    "    # Normalize smart quotes\n",
    "    text = text.replace(\"‚Äú\", \"\\\"\").replace(\"‚Äù\", \"\\\"\").replace(\"‚Äò\", \"'\").replace(\"‚Äô\", \"'\")\n",
    "\n",
    "    # Match all nested JSON-like blocks\n",
    "    matches = regex.findall(r\"\\{(?:[^{}]|(?R))*\\}\", text, flags=regex.DOTALL)\n",
    "\n",
    "    expected_keys = {\"skills\", \"experience\", \"education\", \"certifications\"}\n",
    "\n",
    "    for block in reversed(matches):\n",
    "        try:\n",
    "            parsed = json5.loads(block)\n",
    "            if isinstance(parsed, dict) and expected_keys.intersection(parsed.keys()):\n",
    "                return parsed\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    print(\"‚ùå No valid JSON block found in LLM output.\")\n",
    "    print(\"üîé Last few lines:\\n\", text[-500:])\n",
    "    raise ValueError(\"No valid JSON block found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2c4afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_with_llm(resume_json: dict, jd_json: dict, resume_id=\"resume\", jd_id=\"jd\") -> dict:\n",
    "    \"\"\"\n",
    "    Use an LLM pipeline to compute ATS scores with reasoning per section.\n",
    "    \"\"\"\n",
    "    prompt = LLM_SCORING_PROMPT_TEMPLATE.format(\n",
    "        schema=LLM_SCORING_SCHEMA,\n",
    "        resume_json=json.dumps(resume_json, indent=2),\n",
    "        jd_json=json.dumps(jd_json, indent=2)\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        outputs = llm_pipeline(\n",
    "            prompt,\n",
    "            max_new_tokens=1024,\n",
    "            do_sample=False,\n",
    "            temperature=None,\n",
    "            top_p=None,\n",
    "            top_k=None,\n",
    "            pad_token_id=llm_pipeline.tokenizer.pad_token_id\n",
    "        )\n",
    "        response_text = outputs[0][\"generated_text\"]\n",
    "        #print(\"üí¨ LLM response preview:\\n\", response_text)  \n",
    "\n",
    "        return extract_json_block(response_text)\n",
    "        \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå LLM inference failed for {resume_id} x {jd_id}: {str(e)}\")\n",
    "        print(\"üß™ Raw output preview:\\n\", response_text)\n",
    "        return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f5d7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "print(\"=== Testing: score_with_llm ===\")\n",
    "\n",
    "# Sample resume JSON\n",
    "resume_json = {\n",
    "    \"skills\": [\"Python\", \"Docker\"],\n",
    "    \"certifications\": [{\"certification\": \"AWS Certified\"}, {\"certification\": \"Azure\"}],\n",
    "    \"education\": [{\"degree\": \"Bachelor of Computer Science\"}, {\"degree\": \"MBA\"}],\n",
    "    \"total_experience_years\": 4.5,\n",
    "    \"experience\": [\n",
    "        {\"job_title\": \"DevOps Engineer\", \"description\": [\"Used AWS, Docker, and Jenkins\"]},\n",
    "        {\"job_title\": \"Data Analyst\", \"description\": [\"Created dashboards using Power BI\"]}\n",
    "    ],\n",
    "    \"other\": [\n",
    "        {\"section_name\": \"Leadership\", \"content\": \"Led team of 5 analysts\"},\n",
    "        {\"section_name\": \"Achievements\", \"content\": \"Google Cloud certified\"}\n",
    "    ],\n",
    "    \"projects\": [\n",
    "        {\"description\": \"Built ML model with Scikit-learn and deployed on AWS\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Sample job description JSON\n",
    "jd_json = {\n",
    "    \"required_skills\": [\"Python\", \"MongoDB\"],\n",
    "    \"optional_skills\": [\"Tableau\", \"Java\"],\n",
    "    \"certifications\": [\"AWS Certified\", \"Google Cloud\"],\n",
    "    \"preferred_degrees\": [\"Computer Science\", \"Information Technology\"],\n",
    "    \"required_experience_years\": \"3+ years\",\n",
    "    \"tools_and_technologies\": [\"AWS\", \"Docker\", \"Kubernetes\", \"GCP\"],\n",
    "    \"job_responsibilities\": [\n",
    "        \"Created dashboards using Power BI\",\n",
    "        \"Automated data pipeline using Python\",\n",
    "        \"Built ETL workflows\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Run LLM-based scoring\n",
    "llm_scores = score_with_llm(resume_json, jd_json)\n",
    "\n",
    "print(\"=== LLM Scoring Output ===\")\n",
    "pprint(llm_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fef8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "print(\"=== Testing: score_with_llm ===\")\n",
    "\n",
    "# Sample resume JSON\n",
    "resume_json = {\n",
    "            \"resume_id\": 88907739,\n",
    "            \"total_experience_years\": 11.3,\n",
    "            \"summary\": \"High-achieving management professional and effective consultant possessing excellent communication, organizational and analytical capabilities with about 4 years of experience in devising innovative strategies and solutions to resolve complex business challenges.\",\n",
    "            \"education\": [\n",
    "                {\n",
    "                    \"degree\": \"Master of Science\",\n",
    "                    \"field\": \"Software Management\",\n",
    "                    \"institution\": \"Carnegie Mellon University\",\n",
    "                    \"year\": \",\",\n",
    "                    \"gpa\": 3.8\n",
    "                },\n",
    "                {\n",
    "                    \"degree\": \"MBA\",\n",
    "                    \"field\": \"International Business\",\n",
    "                    \"institution\": \"Institute of Technology & Management\",\n",
    "                    \"year\": \",\",\n",
    "                    \"gpa\": 4.0\n",
    "                },\n",
    "                {\n",
    "                    \"degree\": \"MBA\",\n",
    "                    \"field\": \"International Business\",\n",
    "                    \"institution\": \"International Business Institute of Technology and Management India\",\n",
    "                    \"year\": \",\",\n",
    "                    \"gpa\": 4.0\n",
    "                }\n",
    "            ],\n",
    "            \"experience\": [\n",
    "                {\n",
    "                    \"job_title\": \"Consultant\",\n",
    "                    \"company\": \"Company Name\",\n",
    "                    \"start_date\": \"06/2015\",\n",
    "                    \"end_date\": \"Current\",\n",
    "                    \"description\": [\n",
    "                        \"Managed and delivered a project to implement and integrate a new content management platform to create a unified brand experience, support scalability, growth and enhance digital presence for client's business - post acquisition\",\n",
    "                        \"Led cross-functional global teams consisting of technical, business and functional representatives and achieved key milestones on time with quality deliverables\",\n",
    "                        \"Prioritized, escalated and resolved issues with internal and external stakeholders\",\n",
    "                        \"Directly managed 3rd party vendor and offshore teams.\"\n",
    "                    ]\n",
    "                },\n",
    "                {\n",
    "                    \"job_title\": \"Product Strategy Intern\",\n",
    "                    \"company\": \"Company Name\",\n",
    "                    \"start_date\": \"09/2015\",\n",
    "                    \"end_date\": \"12/2015\",\n",
    "                    \"description\": [\n",
    "                        \"Led a practicum team at Carnegie Mellon University to understand IBM Bluemix (PaaS), cloud based solution and use business frameworks to perform market, competitor and customer journey analysis\",\n",
    "                        \"Liaised with cross functional teams to assess opportunities in marketplace, determine synergies and align business unit goals with corporate strategy\",\n",
    "                        \"Worked with senior management and stakeholders to develop strategy for to enhance awareness, increase conversion and explore new market opportunities to scale the client's user base.\"\n",
    "                    ]\n",
    "                },\n",
    "                {\n",
    "                    \"job_title\": \"Assistant Operations Manager\",\n",
    "                    \"company\": \"Company Name\",\n",
    "                    \"start_date\": \"07/2012\",\n",
    "                    \"end_date\": \"10/2013\",\n",
    "                    \"description\": [\n",
    "                        \"Business Strategy & Vendor Management: Automation of Hub, typical model and replication\",\n",
    "                        \"Reported to Chief Operating Officer to recommend company wide automation strategies and vendor selection\",\n",
    "                        \"Conducted gap analysis, market research, competitor and financial analysis to propose short, mid and long term strategies to the Executive team\",\n",
    "                        \"Project Management: RFID Project Member of the core project management team responsible for coordinated of cross-functional teams to achieve project milestones\",\n",
    "                        \"Focused on process improvement and optimization to enhance team productivity\",\n",
    "                        \"Defined the Key Performance Indicator's to evaluate vendors.\"\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            \"skills\": [\n",
    "                \"Strategy & Operations Process Optimization\",\n",
    "                \"Digital Transformation\",\n",
    "                \"Cross Functional Team Management\",\n",
    "                \"Project/Product Management\",\n",
    "                \"Agile/Lean Methodologies\",\n",
    "                \"Work History\",\n",
    "                \"Client\",\n",
    "                \"Data Analysis\",\n",
    "                \"E-Commerce\",\n",
    "                \"senior management\",\n",
    "                \"Financial\",\n",
    "                \"financial analysis\",\n",
    "                \"functional\",\n",
    "                \"Google Analytics\",\n",
    "                \"Government\",\n",
    "                \"Hub\",\n",
    "                \"IBM\",\n",
    "                \"International Business\",\n",
    "                \"investments\",\n",
    "                \"IP\",\n",
    "                \"Marketing plan\",\n",
    "                \"market research\",\n",
    "                \"Market Strategy\",\n",
    "                \"marketing\",\n",
    "                \"market\",\n",
    "                \"MBA\",\n",
    "                \".NET\",\n",
    "                \"academic\",\n",
    "                \"ADA\",\n",
    "                \"Adobe\",\n",
    "                \"Apple\",\n",
    "                \"approach\",\n",
    "                \"Automation\",\n",
    "                \"business development\",\n",
    "                \"Business Process\",\n",
    "                \"Business Strategy\",\n",
    "                \"Consulting\",\n",
    "                \"content management\",\n",
    "                \"Conversion\",\n",
    "                \"Client\",\n",
    "                \"Data Analysis\",\n",
    "                \"E-Commerce\",\n",
    "                \"senior management\",\n",
    "                \"Financial\",\n",
    "                \"financial analysis\",\n",
    "                \"functional\",\n",
    "                \"Google Analytics\",\n",
    "                \"Government\",\n",
    "                \"Hub\",\n",
    "                \"IBM\",\n",
    "                \"International Business\",\n",
    "                \"investments\",\n",
    "                \"IP\",\n",
    "                \"Marketing plan\",\n",
    "                \"market research\",\n",
    "                \"Market Strategy\",\n",
    "                \"marketing\",\n",
    "                \"market\",\n",
    "                \"MBA\",\n",
    "                \"C#\",\n",
    "                \"Excel\",\n",
    "                \"Microsoft Office Suite\",\n",
    "                \"Power Point\",\n",
    "                \"Word\",\n",
    "                \"Network\",\n",
    "                \"Object Oriented Analysis and Design\",\n",
    "                \"optimization\",\n",
    "                \"policies\",\n",
    "                \"process improvement\",\n",
    "                \"Project Management\",\n",
    "                \"proposals\",\n",
    "                \"quality\",\n",
    "                \"Requirement\",\n",
    "                \"Research\",\n",
    "                \"RFP\",\n",
    "                \"Scrum\",\n",
    "                \"SDLC\",\n",
    "                \"Speech\",\n",
    "                \"MS SQL\",\n",
    "                \"Strategy\",\n",
    "                \"Strategy Development\",\n",
    "                \"Vendor Management\",\n",
    "                \"Management\",\n",
    "                \"Visio\",\n",
    "                \"websites\"\n",
    "            ],\n",
    "            \"certifications\": [],\n",
    "            \"projects\": [\n",
    "                {\n",
    "                    \"project_title\": \"Online E-commerce store\",\n",
    "                    \"description\": \"Conceptualized and launched Online E-commerce store, developed Product Strategy and Roadmap, and produced Engineering, Financial and Marketing plan\",\n",
    "                    \"start_date\": \"08/2014\",\n",
    "                    \"end_date\": \"12/2015\"\n",
    "                },\n",
    "                {\n",
    "                    \"project_title\": \"Commercialization of IP\",\n",
    "                    \"description\": \"Developed Go-to-Market Strategy, Product Roadmap and proposed Business Model to launch CMU's Automatic Speech Recognition Technology and presented to Sand Hill Angel Investors\",\n",
    "                    \"start_date\": \"08/2014\",\n",
    "                    \"end_date\": \"12/2015\"\n",
    "                },\n",
    "                {\n",
    "                    \"project_title\": \"Survivable Social Network on Chip\",\n",
    "                    \"description\": \"Performed Object Oriented Analysis and Design along with the estimation, planning, development, measurement and tracking of the software project using the hybrid development approach\",\n",
    "                    \"start_date\": \"08/2014\",\n",
    "                    \"end_date\": \"12/2015\"\n",
    "                }\n",
    "            ],\n",
    "            \"languages\": [],\n",
    "            \"other\": []\n",
    "        }\n",
    "\n",
    "# Sample job description JSON\n",
    "jd_json = {\n",
    "            \"jd_id\": 3906094741,\n",
    "            \"inferred_domain\": \"consulting\",\n",
    "            \"title\": \"Director, Property Tax\",\n",
    "            \"summary\": \"Director, Property Tax role at Kroll, focusing on tax consulting and valuation projects.\",\n",
    "            \"required_experience_years\": \"7\",\n",
    "            \"preferred_degrees\": [\n",
    "                \"Accounting\",\n",
    "                \"Economics\",\n",
    "                \"Finance\",\n",
    "                \"Management\",\n",
    "                \"Real Estate\"\n",
    "            ],\n",
    "            \"required_skills\": [\n",
    "                \"Management\",\n",
    "                \"Sales\"\n",
    "            ],\n",
    "            \"optional_skills\": [],\n",
    "            \"tools_and_technologies\": [\n",
    "                \"Excel\",\n",
    "                \"Word\",\n",
    "                \"PowerPoint\"\n",
    "            ],\n",
    "            \"certifications\": [\n",
    "                \"ASA\",\n",
    "                \"CPA\",\n",
    "                \"CFA\",\n",
    "                \"MAI\"\n",
    "            ],\n",
    "            \"soft_skills\": [\n",
    "                \"Leadership\",\n",
    "                \"Client Relationship Management\",\n",
    "                \"Analytical Skills\",\n",
    "                \"Independence\",\n",
    "                \"Teamwork\",\n",
    "                \"Communication\",\n",
    "                \"Diversity Awareness\"\n",
    "            ],\n",
    "            \"job_responsibilities\": [\n",
    "                \"Client Research\",\n",
    "                \"Data Analysis\",\n",
    "                \"Presentation Development\",\n",
    "                \"Valuation Techniques\",\n",
    "                \"Tax Hearing Preparation\",\n",
    "                \"Project Reporting\",\n",
    "                \"Tax Projection Scenarios\",\n",
    "                \"Business Solution Implementation\",\n",
    "                \"Junior Staff Development\",\n",
    "                \"Practice Growth\"\n",
    "            ],\n",
    "            \"job_location\": \"Atlanta, GA\",\n",
    "            \"remote_option\": \",\",\n",
    "            \"employment_type\": \"full-time\",\n",
    "            \"travel_requirements\": \"N/A\",\n",
    "            \"physical_requirements\": \"N/A\",\n",
    "            \"benefits\": [],\n",
    "            \"company_information\": \"Kroll is a global firm providing services in governance, risk, and transparency.\",\n",
    "            \"equal_opportunity_policy\": \"Kroll is committed to creating an inclusive work environment and is an equal opportunity employer.\",\n",
    "            \"other\": [\n",
    "                {\n",
    "                    \"section_name\": \"Experience Level\",\n",
    "                    \"content\": \"Director\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "# Run LLM-based scoring\n",
    "llm_scores = score_with_llm(resume_json, jd_json)\n",
    "\n",
    "print(\"=== LLM Scoring Output ===\")\n",
    "pprint(llm_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e71a89",
   "metadata": {},
   "source": [
    "## Combine Section Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e152afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_weights_config = {\n",
    "    \"skills\": {\"rule\": 0.6, \"llm\": 0.4},\n",
    "    \"certifications\": {\"rule\": 0.5, \"llm\": 0.5},\n",
    "    \"education\": {\"rule\": 0.5, \"llm\": 0.5},\n",
    "    \"experience\": {\"rule\": 0.5, \"llm\": 0.5},\n",
    "    \"tools\": {\"rule\": 0.6, \"llm\": 0.4},\n",
    "    \"responsibilities\": {\"rule\": 0.5, \"llm\": 0.5},\n",
    "    \"soft_skills\": {\"rule\": 0.0, \"llm\": 1.0},\n",
    "    \"transferable_skills\": {\"rule\": 0.0, \"llm\": 1.0},\n",
    "    \"leadership\": {\"rule\": 0.0, \"llm\": 1.0},\n",
    "    \"grammar_cleanliness\": {\"rule\": 0.0, \"llm\": 1.0}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1830d9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "\n",
    "def merge_scores(rule_scores: Dict[str, Dict[str, Any]],\n",
    "                 llm_scores: Dict[str, Dict[str, Any]],\n",
    "                 weights_config: Dict[str, Dict[str, float]]) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Merges rule-based and LLM-based scores using weighted averages and combines reasons.\n",
    "    Returns final section-wise score dictionary.\n",
    "    \"\"\"\n",
    "    merged = {}\n",
    "    all_sections = set(rule_scores) | set(llm_scores)\n",
    "\n",
    "    for section in sorted(all_sections):\n",
    "        rule = rule_scores.get(section, {})\n",
    "        llm = llm_scores.get(section, {})\n",
    "\n",
    "        rule_score = rule.get(\"score\", 0.0)\n",
    "        llm_score = llm.get(\"score\", 0.0)\n",
    "        rule_reason = rule.get(\"reason\", \"\")\n",
    "        llm_reason = llm.get(\"reason\", \"\")\n",
    "\n",
    "        weights = weights_config.get(section, {\"rule\": 0.5, \"llm\": 0.5})\n",
    "        final_score = (rule_score * weights[\"rule\"]) + (llm_score * weights[\"llm\"])\n",
    "\n",
    "        merged_reason = f\"(Rule {weights['rule']:.1f}): {rule_reason} | (LLM {weights['llm']:.1f}): {llm_reason}\"\n",
    "\n",
    "        merged[section] = {\n",
    "            \"score\": round(final_score, 3),\n",
    "            \"reason\": merged_reason\n",
    "        }\n",
    "\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf44863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_total_ats_score(merged_scores: Dict[str, Dict[str, Any]],\n",
    "                            section_weights: Dict[str, float]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Computes the final weighted ATS score from merged section scores.\n",
    "    Returns dict with final score and breakdown.\n",
    "    \"\"\"\n",
    "    total_weight = sum(section_weights.values())\n",
    "    if total_weight == 0:\n",
    "        raise ValueError(\"Total section weights must be greater than zero.\")\n",
    "\n",
    "    weighted_sum = 0.0\n",
    "    breakdown = []\n",
    "\n",
    "    for section, weight in section_weights.items():\n",
    "        score = merged_scores.get(section, {}).get(\"score\", 0.0)\n",
    "        weighted_sum += score * weight\n",
    "        breakdown.append(f\"{section}: {score:.2f} √ó {weight:.2f}\")\n",
    "\n",
    "    final_score = round(weighted_sum / total_weight, 3)\n",
    "    return {\n",
    "        \"final_ats_score\": final_score,\n",
    "        \"explanation\": f\"Weighted average across sections ‚Üí {' | '.join(breakdown)}\"\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ffb6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "section_weights_config = {\n",
    "    \"skills\": 0.15,\n",
    "    \"certifications\": 0.10,\n",
    "    \"education\": 0.10,\n",
    "    \"experience\": 0.20,\n",
    "    \"tools\": 0.10,\n",
    "    \"responsibilities\": 0.15,\n",
    "    \"soft_skills\": 0.05,\n",
    "    \"transferable_skills\": 0.05,\n",
    "    \"leadership\": 0.05,\n",
    "    \"grammar_cleanliness\": 0.05\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349a56e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "print(\"=== Testing: merge_scores and compute_total_ats_score ===\")\n",
    "\n",
    "# Sample rule-based scores\n",
    "rule_scores = {\n",
    "    \"skills\": {\"score\": 0.6, \"reason\": \"Rule: Matched some required skills.\"},\n",
    "    \"certifications\": {\"score\": 0.4, \"reason\": \"Rule: No direct certification match.\"},\n",
    "    \"education\": {\"score\": 0.9, \"reason\": \"Rule: Degree aligned well.\"},\n",
    "    \"experience\": {\"score\": 1.0, \"reason\": \"Rule: Resume years > JD years.\"},\n",
    "    \"tools\": {\"score\": 0.5, \"reason\": \"Rule: Partial tool match.\"},\n",
    "    \"responsibilities\": {\"score\": 0.3, \"reason\": \"Rule: Low overlap on tasks.\"}\n",
    "}\n",
    "\n",
    "# Sample LLM-based scores\n",
    "llm_scores = {\n",
    "    \"skills\": {\"score\": 0.8, \"reason\": \"LLM: Python and Docker match JD.\"},\n",
    "    \"certifications\": {\"score\": 0.6, \"reason\": \"LLM: AWS match, missing GCP.\"},\n",
    "    \"education\": {\"score\": 0.7, \"reason\": \"LLM: One degree matches preferred list.\"},\n",
    "    \"experience\": {\"score\": 0.7, \"reason\": \"LLM: Related roles, less domain alignment.\"},\n",
    "    \"tools\": {\"score\": 0.4, \"reason\": \"LLM: Excel mentioned, others missing.\"},\n",
    "    \"responsibilities\": {\"score\": 0.2, \"reason\": \"LLM: Few relevant duties matched.\"},\n",
    "    \"soft_skills\": {\"score\": 0.5, \"reason\": \"LLM: Communication and teamwork evident.\"},\n",
    "    \"transferable_skills\": {\"score\": 0.4, \"reason\": \"LLM: PM and cross-functional skills.\"},\n",
    "    \"leadership\": {\"score\": 0.6, \"reason\": \"LLM: Led global teams in resume.\"},\n",
    "    \"grammar_cleanliness\": {\"score\": 0.9, \"reason\": \"LLM: Clean formatting and language.\"}\n",
    "}\n",
    "\n",
    "\n",
    "# Merge section scores\n",
    "merged_scores = merge_scores(rule_scores, llm_scores, merge_weights_config)\n",
    "\n",
    "# Print merged scores\n",
    "print(\"\\n=== Merged Scores ===\")\n",
    "pprint(merged_scores)\n",
    "\n",
    "# Compute total ATS score\n",
    "final_result = compute_total_ats_score(merged_scores, section_weights_config)\n",
    "\n",
    "# Print final ATS score\n",
    "print(\"\\n=== Final ATS Score ===\")\n",
    "pprint(final_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97597c9",
   "metadata": {},
   "source": [
    "## Utilities to load semantic matching Resume-JD and find matching pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4462daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import os\n",
    "\n",
    "# ‚úÖ Updated: Load and filter relevant resume‚ÄìJD pairs\n",
    "def find_matching_pairs(\n",
    "    relevance_json_path: str,\n",
    "    match_labels: List[str] = [\"strong\", \"medium\", \"weak\"]\n",
    ") -> List[Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Load relevance map and return (resume_id, jd_id) tuples for selected match labels.\n",
    "    \"\"\"\n",
    "    data = load_json_file(relevance_json_path)\n",
    "    results = data.get(\"semantic_relevance_scores\", [])\n",
    "\n",
    "    filtered_pairs = [\n",
    "        (item[\"resume_id\"], item[\"jd_id\"])\n",
    "        for item in results\n",
    "        if item.get(\"semantic_match_label\") in match_labels\n",
    "    ]\n",
    "    return filtered_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62c08a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_map_file = os.path.join(Config.JSON_OUTPUT_SCORING_DIR, 'semantic_relevance_scores.json')\n",
    "matching_pairs = find_matching_pairs(relevance_map_file, match_labels=[\"strong\", \"medium\", \"weak\"])\n",
    "\n",
    "print(f\"‚úÖ Found {len(matching_pairs)} matching resume‚ÄìJD pairs\")\n",
    "print(matching_pairs[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e7c7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_match_metadata(resume_id: str, jd_id: str, relevance_map: Dict[str, List[Dict]]) -> Dict:\n",
    "    for entry in relevance_map.get(\"semantic_relevance_scores\", []):\n",
    "        if str(entry[\"resume_id\"]) == str(resume_id) and str(entry[\"jd_id\"]) == str(jd_id):\n",
    "            return {\n",
    "                \"domain\": entry.get(\"resume_domain\", \"\"),\n",
    "                \"resume_jd_similarity\": entry.get(\"resume_jd_similarity\", 0.0),\n",
    "                \"semantic_match_label\": entry.get(\"semantic_match_label\", \"weak\")\n",
    "            }\n",
    "    raise ValueError(f\"No semantic match metadata found for resume_id={resume_id}, jd_id={jd_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7b0994",
   "metadata": {},
   "source": [
    "## Phase 3: Scoring Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1d129a",
   "metadata": {},
   "source": [
    "### Checkpoint Handling (JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef277819",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "\n",
    "def load_resume_checkpoint(path: str) -> int:\n",
    "    if not os.path.exists(path):\n",
    "        return 0\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return json.load(file).get(\"last_index\", 0)\n",
    "\n",
    "def save_resume_checkpoint(path: str, index: int):\n",
    "    data = {\n",
    "        \"last_index\": index,\n",
    "        \"timestamp\": datetime.now(timezone.utc).isoformat()\n",
    "    }\n",
    "    save_json_output(data, path)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5c063c",
   "metadata": {},
   "source": [
    "### Scoring a Single Resume-JD Pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb267f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Optional, Any\n",
    "from pathlib import Path\n",
    "\n",
    "def find_record_by_id(\n",
    "    records: List[Dict[str, Any]],\n",
    "    record_id: str,\n",
    "    id_field: str = \"record_id\"\n",
    ") -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"Finds a record in a list of dicts by a specified record ID field.\"\"\"\n",
    "    return next((r for r in records if str(r.get(id_field)) == str(record_id)), None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1815e470",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def score_resume_vs_jd(\n",
    "    resume_id: str,\n",
    "    jd_id: str,\n",
    "    resume_json_dir: str,\n",
    "    jd_json_dir: str,\n",
    "    relevance_map: Dict[str, List[Dict]],\n",
    "    rule_weight: float = 0.5,\n",
    "    llm_weight: float = 0.5\n",
    ") -> Dict:\n",
    "    # Load resume + JD parsed JSONs\n",
    "    resume_files = list(Path(resume_json_dir).glob(f\"resumes_{resume_id}_*.json\"))\n",
    "    jd_files = list(Path(jd_json_dir).glob(f\"jds_{jd_id}_*.json\"))\n",
    "\n",
    "    if not resume_files:\n",
    "        raise FileNotFoundError(f\"No resume file found for resume_id: {resume_id}\")\n",
    "    if not jd_files:\n",
    "        raise FileNotFoundError(f\"No JD file found for jd_id: {jd_id}\")\n",
    "\n",
    "    resume_records = load_json_file(str(resume_files[0]))\n",
    "    jd_records = load_json_file(str(jd_files[0]))\n",
    "\n",
    "\n",
    "    resume_record = find_record_by_id(resume_records, resume_id)\n",
    "    jd_record = find_record_by_id(jd_records, jd_id)\n",
    "\n",
    "\n",
    "    if not resume_record:\n",
    "        raise ValueError(f\"Resume record_id {resume_id} not found in file {resume_files[0].name}\")\n",
    "    if not jd_record:\n",
    "        raise ValueError(f\"JD record_id {jd_id} not found in file {jd_files[0].name}\")\n",
    "\n",
    "    resume_data = resume_record.get(\"output_json\", {})\n",
    "    jd_data = jd_record.get(\"output_json\", {})\n",
    "\n",
    "\n",
    "    # Rule-based scores\n",
    "    rule_scores = compute_all_rule_scores(resume_data, jd_data)\n",
    "\n",
    "    # LLM-based scores\n",
    "    llm_scores = score_with_llm(resume_data, jd_data, resume_id=resume_id, jd_id=jd_id)\n",
    "\n",
    "    # Merge section-wise scores\n",
    "    section_scores = merge_scores(rule_scores, llm_scores, merge_weights_config)\n",
    "\n",
    "    # Compute final ATS score\n",
    "    final_score_result = compute_total_ats_score(section_scores, section_weights_config)\n",
    "    final_score = final_score_result[\"final_ats_score\"]\n",
    "\n",
    "    # Derive match quality from final ATS score\n",
    "    if final_score >= 0.75:\n",
    "        match_quality = \"strong\"\n",
    "    elif final_score >= 0.5:\n",
    "        match_quality = \"medium\"\n",
    "    else:\n",
    "        match_quality = \"weak\"\n",
    "\n",
    "    # Enrich with metadata from semantic map\n",
    "    match_meta = get_match_metadata(resume_id, jd_id, relevance_map)\n",
    "\n",
    "    return {\n",
    "        \"resume_id\": resume_id,\n",
    "        \"jd_id\": jd_id,\n",
    "        \"domain\": match_meta[\"domain\"],\n",
    "        \"resume_jd_similarity\": match_meta[\"resume_jd_similarity\"],\n",
    "        \"semantic_match_label\": match_meta[\"semantic_match_label\"],  # Relevance from Phase 1\n",
    "        \"section_scores\": section_scores,\n",
    "        \"match_quality\": match_quality,  # ATS score-based match\n",
    "        \"final_ats_score\": final_score\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0458abd",
   "metadata": {},
   "source": [
    "### Test Single Resume-JD Pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11194bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import os\n",
    "\n",
    "# === Setup file paths ===\n",
    "relevance_map_file = os.path.join(Config.JSON_OUTPUT_SCORING_DIR, 'semantic_relevance_scores.json')\n",
    "resume_json_dir = Config.JSON_OUTPUT_NORMALIZED_RESUME\n",
    "jd_json_dir = Config.JSON_OUTPUT_NORMALIZED_JD\n",
    "\n",
    "# === Load relevance map ===\n",
    "relevance_map = load_json_file(relevance_map_file)\n",
    "\n",
    "# === Pick a strong match from the top for testing ===\n",
    "sample_pair = None\n",
    "for item in relevance_map[\"semantic_relevance_scores\"]:\n",
    "    if item[\"semantic_match_label\"] == \"strong\":\n",
    "        sample_pair = (item[\"resume_id\"], item[\"jd_id\"])\n",
    "        break\n",
    "\n",
    "if not sample_pair:\n",
    "    raise ValueError(\"No strong match found in relevance map to test.\")\n",
    "\n",
    "resume_id, jd_id = sample_pair\n",
    "print(f\"=== Testing score_resume_vs_jd() for resume_id={resume_id}, jd_id={jd_id} ===\")\n",
    "\n",
    "# === Run scoring ===\n",
    "result = score_resume_vs_jd(\n",
    "    resume_id=resume_id,\n",
    "    jd_id=jd_id,\n",
    "    resume_json_dir=resume_json_dir,\n",
    "    jd_json_dir=jd_json_dir,\n",
    "    relevance_map=relevance_map\n",
    ")\n",
    "\n",
    "# === Show result ===\n",
    "pprint(result)\n",
    "\n",
    "\n",
    "test_file = os.path.join(Config.JSON_OUTPUT_SCORING_DIR, 'test_phase3_scoring.json')\n",
    "\n",
    "save_json_output(result, test_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d1fabc",
   "metadata": {},
   "source": [
    "### Main Modular Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b50913",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from typing import List, Dict\n",
    "import os\n",
    "import time\n",
    "\n",
    "def score_and_save_in_batches(\n",
    "    resumes: List[Dict],\n",
    "    jd_lookup: Dict[str, Dict],\n",
    "    relevance_map: Dict[str, List[Dict]],\n",
    "    output_dir: str = Config.JSON_OUTPUT_SCORING_DIR,\n",
    "    save_every: int = 5,\n",
    "    limit: int = 20,\n",
    "    relevance_threshold: float = 0.4,\n",
    "    rule_weight: float = 0.5,\n",
    "    llm_weight: float = 0.5,\n",
    "    resume_from_checkpoint: bool = True\n",
    "):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    checkpoint_file = os.path.join(output_dir, \"checkpoint.json\")\n",
    "\n",
    "    # Determine starting index\n",
    "    start = load_resume_checkpoint(checkpoint_file) if resume_from_checkpoint else 0\n",
    "    end = min(start + limit, len(resumes))\n",
    "\n",
    "    successes, failures = [], []\n",
    "    timestamp = int(time.time())\n",
    "\n",
    "    for idx in tqdm(range(start, end), desc=\"Scoring resumes\"):\n",
    "        resume = resumes[idx]\n",
    "        resume_id = resume.get(\"resume_id\", f\"resume_{idx}\")\n",
    " \n",
    "\n",
    "        relevant_jds = get_relevant_jds(resume_id, relevance_map, jd_lookup, threshold=relevance_threshold)\n",
    "        if not relevant_jds:\n",
    "            print(f\"‚ö†Ô∏è No relevant JDs found for {resume_id}\")\n",
    "            continue\n",
    "\n",
    "        for jd in relevant_jds:\n",
    "            jd_id = jd.get(\"jd_id\", \"\")\n",
    "            try:\n",
    "                relevance_score = next((m[\"score\"] for m in relevance_map[resume_id] if m[\"jd_id\"] == jd_id), 0.0)\n",
    "                result = score_resume_jd_pair(\n",
    "                    resume, jd, resume_id, jd_id,\n",
    "                    rule_weight, llm_weight,\n",
    "                    relevance_score\n",
    "                )\n",
    "                successes.append(result)\n",
    "            except Exception as e:\n",
    "                failures.append({\n",
    "                    \"resume_id\": resume_id,\n",
    "                    \"jd_id\": jd_id,\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "\n",
    "        if (idx - start + 1) % save_every == 0:\n",
    "            partial_success_file = f\"{output_dir}/scored_part_{start}_{idx}_{timestamp}.json\"\n",
    "            partial_fail_file = f\"{output_dir}/failed_part_{start}_{idx}_{timestamp}.json\"\n",
    "            if successes:\n",
    "                save_json_output(successes, partial_success_file)\n",
    "            if failures:\n",
    "                save_json_output(failures, partial_fail_file)\n",
    "            save_resume_checkpoint(checkpoint_file, idx + 1)\n",
    "\n",
    "    # Final save\n",
    "    success_file = f\"{output_dir}/scored_final_{start}_{end}_{timestamp}.json\"\n",
    "    fail_file = f\"{output_dir}/failed_final_{start}_{end}_{timestamp}.json\"\n",
    "    if successes:\n",
    "        save_json_output(successes, success_file)\n",
    "    if failures:\n",
    "        save_json_output(failures, fail_file)\n",
    "    save_resume_checkpoint(checkpoint_file, end)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4fd7f6",
   "metadata": {},
   "source": [
    "## Execute Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998fc6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "jd_lookup = {jd[\"jd_id\"]: jd for jd in jds}\n",
    "\n",
    "score_and_save_in_batches(\n",
    "    resumes=resumes,\n",
    "    jd_lookup=jd_lookup,\n",
    "    relevance_map=relevance_map,\n",
    "    output_dir=Config.JSON_OUTPUT_SCORING_DIR,\n",
    "    save_every=5,\n",
    "    limit=1,\n",
    "    relevance_threshold=0.45,\n",
    "    rule_weight=0.5,\n",
    "    llm_weight=0.5,\n",
    "    resume_from_checkpoint=True  # set True to resume, False to start fresh\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
