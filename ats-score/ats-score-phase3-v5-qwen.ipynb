{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f630d895",
   "metadata": {},
   "source": [
    "# Global setup and package installation used in most phases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969fcea6",
   "metadata": {},
   "source": [
    "## Colab + GPU Detection Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cf79fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def is_running_in_colab():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "def get_available_gpu_memory_gb():\n",
    "    try:\n",
    "        output = subprocess.check_output(\n",
    "            [\"nvidia-smi\", \"--query-gpu=memory.free\", \"--format=csv,nounits,noheader\"],\n",
    "            encoding=\"utf-8\"\n",
    "        )\n",
    "        free_mem_mb = int(output.strip().split(\"\\n\")[0])\n",
    "        return free_mem_mb / 1024\n",
    "    except Exception:\n",
    "        return 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66be54d4",
   "metadata": {},
   "source": [
    "## install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942fba10",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_running_in_colab():\n",
    "    # Install the required packages\n",
    "    !pip install kagglehub pandas\n",
    "    !pip install -q transformers accelerate bitsandbytes sentencepiece pydantic huggingface_hub xformers\n",
    "    !pip install regex json5\n",
    "    !pip install sentence-transformers scikit-learn\n",
    "    !pip install rapidfuzz unidecode\n",
    "\n",
    "else:\n",
    "    %pip install kagglehub pandas\n",
    "    %pip install -q transformers accelerate sentencepiece pydantic huggingface_hub xformers\n",
    "    #%pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu121\n",
    "    #%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "    %pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128\n",
    "    %pip install -U bitsandbytes\n",
    "    %pip install regex json5\n",
    "    %pip install sentence-transformers scikit-learn\n",
    "    %pip install rapidfuzz unidecode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4a9f61",
   "metadata": {},
   "source": [
    "## Login to huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1235aef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# Set your token here securely or prompt for it in Colab\n",
    "# Recommended: store in Colab secrets or environment variable\n",
    "HF_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "\n",
    "if not HF_TOKEN:\n",
    "    if is_running_in_colab():\n",
    "        # If running in Colab, use the Colab secrets\n",
    "        try:\n",
    "            from google.colab import userdata\n",
    "            HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "            if not HF_TOKEN:\n",
    "                raise ValueError(\"‚ö†Ô∏è Hugging Face token not found in Colab secrets.\")\n",
    "            print(\"üîë Hugging Face token found in Colab secrets.\")\n",
    "        except ImportError:\n",
    "            print(\"‚ö†Ô∏è Unable to authenticate in Colab. Please set your Hugging Face token manually.\")\n",
    "    else:\n",
    "        # Prompt for token if not set in environment\n",
    "        print(\"üîë Please enter your Hugging Face token:\")\n",
    "        # For Colab or local prompt input\n",
    "        HF_TOKEN = input(\"üîë Enter your Hugging Face token: \").strip()\n",
    "\n",
    "login(token=HF_TOKEN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b423735e",
   "metadata": {},
   "source": [
    "## Setup Kaggle Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409818e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def setup_kaggle_credentials():\n",
    "    kaggle_path = os.path.expanduser('~/.kaggle/kaggle.json')\n",
    "    if not os.path.exists(kaggle_path):\n",
    "        from google.colab import files\n",
    "        print(\"üìÇ Upload kaggle.json file...\")\n",
    "        uploaded = files.upload()\n",
    "        os.makedirs(os.path.dirname(kaggle_path), exist_ok=True)\n",
    "        for filename in uploaded.keys():\n",
    "            shutil.move(filename, kaggle_path)\n",
    "        os.chmod(kaggle_path, 0o600)\n",
    "        print(f\"‚úÖ Kaggle credentials setup at {kaggle_path}\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Kaggle credentials already exist at {kaggle_path}\")\n",
    "\n",
    "setup_kaggle_credentials()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d384b2f",
   "metadata": {},
   "source": [
    "## Mount Google Drive (Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1517ea98",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_running_in_colab():\n",
    "   from google.colab import drive\n",
    "   drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9964af3",
   "metadata": {},
   "source": [
    "##  Load Qwen-Instruct with Fallback to Quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "be6d840d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "import torch\n",
    "\n",
    "def load_model_pipeline(model_name: str, hf_token: str):\n",
    "    has_cuda = torch.cuda.is_available()\n",
    "    free_mem = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3) if has_cuda else 0\n",
    "    print(f\"üíª CUDA: {has_cuda} | GPU Memory: {free_mem:.2f} GB\")\n",
    "\n",
    "    device_map = {\"\": 0} if has_cuda else \"cpu\"\n",
    "    use_4bit = has_cuda and free_mem < 24\n",
    "\n",
    "    # Set quantization config\n",
    "    quant_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True if use_4bit else False,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    ) if use_4bit else None\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token, trust_remote_code=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # ‚úÖ Fix warning about pad_token\n",
    "\n",
    "    # Load model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device_map,\n",
    "        quantization_config=quant_config,\n",
    "        torch_dtype=torch.float16 if not quant_config else None,\n",
    "        trust_remote_code=True,\n",
    "        token=hf_token\n",
    "    )\n",
    "\n",
    "    print(f\"‚úÖ Model loaded on {next(model.parameters()).device}\")\n",
    "    return pipeline(\"text-generation\", model=model, tokenizer=tokenizer, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5f811301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíª CUDA: True | GPU Memory: 15.92 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "531bf4f63b454d3aad4533cc76addcc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "llm_pipeline = load_model_pipeline(\n",
    "    model_name=\"Qwen/Qwen2-7B-Instruct\",\n",
    "    hf_token=HF_TOKEN\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739b4783",
   "metadata": {},
   "source": [
    "# Global utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10779c22",
   "metadata": {},
   "source": [
    "### Utility to merge json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9a472fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "def merge_json_files(\n",
    "    source_dir: Path,\n",
    "    output_file: Path,\n",
    "    pattern: str,\n",
    "    merged_dir: Path\n",
    "):\n",
    "    source_dir.mkdir(parents=True, exist_ok=True)\n",
    "    merged_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    merged_data = []\n",
    "\n",
    "    # Load existing output if it exists\n",
    "    if output_file.exists():\n",
    "        with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                merged_data = json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"‚ö†Ô∏è Could not decode {output_file}, starting from scratch.\")\n",
    "\n",
    "    # Identify matching files\n",
    "    files_to_merge = sorted(source_dir.glob(pattern))\n",
    "\n",
    "    for file_path in files_to_merge:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "                if isinstance(data, list):\n",
    "                    merged_data.extend(data)\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è Skipping {file_path.name}: not a list.\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Failed to parse {file_path.name}: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Move to merged folder\n",
    "        shutil.move(str(file_path), merged_dir / file_path.name)\n",
    "        print(f\"‚úÖ Merged and moved: {file_path.name}\")\n",
    "\n",
    "    # Write combined output\n",
    "    if merged_data:\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(merged_data, f, indent=2)\n",
    "        print(f\"üíæ Saved to: {output_file}\")\n",
    "    else:\n",
    "        print(\"üì≠ No valid data to merge.\")\n",
    "\n",
    "# === Usage ===\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63df9fc3",
   "metadata": {},
   "source": [
    "### Utility to save json to a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "727b44eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "# üì¶ Save JSON Output with Safety\n",
    "def save_json_output(data, output_path: str, indent: int = 4, overwrite: bool = True):\n",
    "    output_dir = os.path.dirname(output_path)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    if os.path.exists(output_path):\n",
    "        if overwrite:\n",
    "            os.remove(output_path)\n",
    "        else:\n",
    "            raise FileExistsError(f\"File {output_path} already exists and overwrite=False.\")\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=indent, ensure_ascii=False)\n",
    "\n",
    "    print(f\"‚úÖ Saved output to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc498258",
   "metadata": {},
   "source": [
    "### Utility to load file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dcf31583",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import json\n",
    "\n",
    "# üìÇ Load normalized JSON data\n",
    "def load_json_file(file_path: str) -> Any:\n",
    "    if not os.path.isfile(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b329b93",
   "metadata": {},
   "source": [
    "### Configurations  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4308e95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# üõ† CONFIGURATION\n",
    "# ==============================\n",
    "\n",
    "class Config:\n",
    "    DATASET_DOWNLOAD_DIR = \"datasets\"\n",
    "    JSON_OUTPUT_DIR = \"json_outputs_run3\"\n",
    "    JSON_OUTPUT_NORMALIZED_DIR = \"json_outputs_run3/normalized\"\n",
    "    JSON_OUTPUT_SCORING_DIR = \"json_outputs_run3/scoring\"\n",
    "    AUTO_CLEANUP = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e89760a",
   "metadata": {},
   "source": [
    "# Phase 3 Rubric-Based Scoring Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e31b1a",
   "metadata": {},
   "source": [
    "## Rule-Based Scoring Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "61a6b5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_config = {\n",
    "    \"matching\": {\n",
    "        # Global fuzzy threshold for fuzzy matching (0‚Äì100)\n",
    "        \"fuzzy_threshold_default\": 85,\n",
    "        \"section_thresholds\": {\n",
    "            \"skills\": 85,\n",
    "            \"tools\": 80,\n",
    "            \"certifications\": 88,\n",
    "            \"responsibilities\": 83,\n",
    "            \"education\": 87\n",
    "        }\n",
    "    },\n",
    "    \"weights\": {\n",
    "        \"skills\": {\n",
    "            \"exact\": 1.0,\n",
    "            \"substring\": 0.8,\n",
    "            \"fuzzy\": 0.5\n",
    "        },\n",
    "        \"tools\": {\n",
    "            \"exact\": 1.0,\n",
    "            \"substring\": 0.7,\n",
    "            \"fuzzy\": 0.4\n",
    "        },\n",
    "        \"certifications\": {\n",
    "            \"exact\": 1.0,\n",
    "            \"substring\": 0.9,\n",
    "            \"fuzzy\": 0.5\n",
    "        },\n",
    "        \"responsibilities\": {\n",
    "            \"exact\": 1.0,\n",
    "            \"substring\": 0.85,\n",
    "            \"fuzzy\": 0.5\n",
    "        },\n",
    "        \"education\": {\n",
    "            \"exact\": 1.0,\n",
    "            \"substring\": 0.75,\n",
    "            \"fuzzy\": 0.5\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "53494bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import fuzz\n",
    "import re\n",
    "from typing import List, Tuple, Dict, Any\n",
    "from unidecode import unidecode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62163dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text: str) -> str:\n",
    "    text = unidecode(text.lower())\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", text)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "def normalize_score(score: float) -> float:\n",
    "    return max(0.0, min(round(score, 3), 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76af97bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_match(jd_terms: List[str], resume_text: str, section: str) -> Tuple[Dict[str, int], List[Dict[str, str]]]:\n",
    "    \"\"\"\n",
    "    Classifies JD terms matched in resume_text as 'exact', 'substring', or 'fuzzy'.\n",
    "    Returns match counts and match breakdown list.\n",
    "    \"\"\"\n",
    "    if not jd_terms:\n",
    "        return {\"exact\": 0, \"substring\": 0, \"fuzzy\": 0}, []\n",
    "\n",
    "\n",
    "    fuzzy_threshold = scoring_config[\"matching\"].get(\"section_thresholds\", {}).get(\n",
    "        section,\n",
    "        scoring_config[\"matching\"].get(\"fuzzy_threshold_default\", 85)\n",
    "    )\n",
    "    resume_tokens = set(re.findall(r\"\\b[\\w\\+\\-\\.#]{3,}\\b\", resume_text.lower()))\n",
    "    normalized_text = normalize(resume_text)\n",
    "\n",
    "    counts = {\"exact\": 0, \"substring\": 0, \"fuzzy\": 0}\n",
    "    matched_terms = []\n",
    "\n",
    "    for term in jd_terms:\n",
    "        normalized_term = normalize(term)\n",
    "\n",
    "        if normalized_term in resume_tokens:\n",
    "            counts[\"exact\"] += 1\n",
    "            matched_terms.append({\"term\": term, \"type\": \"exact\"})\n",
    "        elif normalized_term in normalized_text:\n",
    "            counts[\"substring\"] += 1\n",
    "            matched_terms.append({\"term\": term, \"type\": \"substring\"})\n",
    "        elif fuzz.partial_ratio(normalized_term, normalized_text) >= fuzzy_threshold:\n",
    "            counts[\"fuzzy\"] += 1\n",
    "            matched_terms.append({\"term\": term, \"type\": \"fuzzy\"})\n",
    "\n",
    "    return counts, matched_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162e5a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_from_match_counts(counts: Dict[str, int], total: int, weights: Dict[str, float]) -> float:\n",
    "    weighted_sum = (\n",
    "        weights.get(\"exact\", 1.0) * counts[\"exact\"] +\n",
    "        weights.get(\"substring\", 0.8) * counts[\"substring\"] +\n",
    "        weights.get(\"fuzzy\", 0.5) * counts[\"fuzzy\"]\n",
    "    )\n",
    "    return normalize_score(weighted_sum / total) if total else 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27e5128",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_skills_rule(resume_skills, resume_other, jd_required, jd_optional):\n",
    "    resume_text = (\n",
    "        \" \".join(resume_skills or []) + \" \" +\n",
    "        \" \".join(section.get(\"content\", \"\") for section in resume_other or [])\n",
    "    )\n",
    "\n",
    "    weights = scoring_config[\"weights\"].get(\"skills\", {\"exact\": 1.0, \"substring\": 0.8, \"fuzzy\": 0.5})\n",
    "\n",
    "    r_counts, r_matches = hybrid_match(jd_required, resume_text, section=\"skills\")\n",
    "    o_counts, o_matches = hybrid_match(jd_optional, resume_text, section=\"skills\")\n",
    "\n",
    "    r_score = score_from_match_counts(r_counts, len(jd_required), weights)\n",
    "    o_score = score_from_match_counts(o_counts, len(jd_optional), weights)\n",
    "\n",
    "    final_score = normalize_score(0.8 * r_score + 0.2 * o_score)\n",
    "    reason = (\n",
    "        f\"Required: {r_counts}, Optional: {o_counts}. \"\n",
    "        f\"Matched skills: {[m['term'] + ' (' + m['type'] + ')' for m in r_matches + o_matches]}\"\n",
    "    )\n",
    "    return final_score, reason\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7febd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_certifications_rule(resume_certs, resume_other, jd_certs):\n",
    "    if not jd_certs:\n",
    "        return 1.0, \"No certifications required by JD.\"\n",
    "\n",
    "    cert_text = (\n",
    "        \" \".join(cert.get(\"certification\", \"\") for cert in resume_certs or []) + \" \" +\n",
    "        \" \".join(section.get(\"content\", \"\") for section in resume_other or [])\n",
    "    )\n",
    "\n",
    "    weights = scoring_config[\"weights\"].get(\"certifications\", {\"exact\": 1.0, \"substring\": 0.9, \"fuzzy\": 0.5})\n",
    "\n",
    "    counts, matches = hybrid_match(jd_certs, cert_text, section=\"certifications\")\n",
    "    score = score_from_match_counts(counts, len(jd_certs), weights)\n",
    "\n",
    "    reason = f\"Certifications matched: {counts}. Matched: {[m['term'] + ' (' + m['type'] + ')' for m in matches]}\"\n",
    "    return score, reason\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b22de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_education_rule(resume_education, jd_degrees):\n",
    "    if not jd_degrees:\n",
    "        return 1.0, \"No preferred degrees listed in JD.\"\n",
    "    if not resume_education:\n",
    "        return 0.0, \"No education information found in resume.\"\n",
    "\n",
    "    resume_text = \" \".join((edu.get(\"degree\", \"\") or \"\") for edu in resume_education)\n",
    "\n",
    "    weights = scoring_config[\"weights\"].get(\"education\", {\"exact\": 1.0, \"substring\": 0.75, \"fuzzy\": 0.5})\n",
    "\n",
    "    counts, matches = hybrid_match(jd_degrees, resume_text, section=\"education\")\n",
    "    score = score_from_match_counts(counts, len(jd_degrees), weights)\n",
    "\n",
    "    reason = f\"Degrees matched: {counts}. Matched: {[m['term'] + ' (' + m['type'] + ')' for m in matches]}\"\n",
    "    return score, reason\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b161703a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_experience_rule(resume_years, jd_required_years, cap: float = 40.0):\n",
    "    if not jd_required_years or resume_years is None:\n",
    "        return 0.5, \"Missing required or actual experience data.\"\n",
    "\n",
    "    numbers = re.findall(r'\\d+(?:\\.\\d+)?', jd_required_years)\n",
    "    if not numbers:\n",
    "        return 1.0, \"JD experience string did not specify clear years.\"\n",
    "\n",
    "    required_years = float(min(numbers))\n",
    "    if required_years == 0:\n",
    "        return 1.0, \"JD required years = 0.\"\n",
    "\n",
    "    resume_years_capped = min(resume_years, cap)\n",
    "    score = resume_years_capped / required_years\n",
    "    reason = f\"Resume: {resume_years} yrs (capped to {resume_years_capped}), JD requires: {required_years} yrs.\"\n",
    "    return normalize_score(score), reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3266e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_tools_rule(resume_skills, resume_experience, resume_other, resume_projects, jd_tools):\n",
    "    if not jd_tools:\n",
    "        return 1.0, \"No tools required by JD.\"\n",
    "\n",
    "    resume_text = (\n",
    "        \" \".join(resume_skills or []) + \" \" +\n",
    "        \" \".join(\" \".join(exp.get(\"description\", [])) for exp in resume_experience or []) + \" \" +\n",
    "        \" \".join(section.get(\"content\", \"\") for section in resume_other or []) + \" \" +\n",
    "        \" \".join(project.get(\"description\", \"\") for project in resume_projects or [])\n",
    "    )\n",
    "\n",
    "    weights = scoring_config[\"weights\"].get(\"tools\", {\"exact\": 1.0, \"substring\": 0.7, \"fuzzy\": 0.4})\n",
    "\n",
    "    counts, matches = hybrid_match(jd_tools, resume_text, section =\"tools\")\n",
    "    score = score_from_match_counts(counts, len(jd_tools), weights)\n",
    "\n",
    "    reason = f\"Tools matched: {counts}. Matched: {[m['term'] + ' (' + m['type'] + ')' for m in matches]}\"\n",
    "    return score, reason\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dfeb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_responsibilities_rule(resume_experience, resume_other, resume_projects, jd_responsibilities):\n",
    "    if not jd_responsibilities:\n",
    "        return 1.0, \"No responsibilities listed in JD.\"\n",
    "\n",
    "    resume_text = (\n",
    "        \" \".join(\" \".join(exp.get(\"description\", [])) for exp in resume_experience or []) + \" \" +\n",
    "        \" \".join(section.get(\"content\", \"\") for section in resume_other or []) + \" \" +\n",
    "        \" \".join(project.get(\"description\", \"\") for project in resume_projects or [])\n",
    "    )\n",
    "\n",
    "    weights = scoring_config[\"weights\"].get(\"responsibilities\", {\"exact\": 1.0, \"substring\": 0.85, \"fuzzy\": 0.4})\n",
    "\n",
    "    counts, matches = hybrid_match(jd_responsibilities, resume_text, section=\"responsibilities\")\n",
    "    score = score_from_match_counts(counts, len(jd_responsibilities), weights)\n",
    "\n",
    "    reason = f\"Responsibilities matched: {counts}. Matched: {[m['term'] + ' (' + m['type'] + ')' for m in matches]}\"\n",
    "    return score, reason\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a24b934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_all_rule_scores(resume_json: Dict[str, Any], jd_json: Dict[str, Any]) -> Dict[str, Dict[str, Any]]:\n",
    "    scores = {}\n",
    "\n",
    "    skills_score, skills_reason = score_skills_rule(\n",
    "        resume_json.get(\"skills\", []),\n",
    "        resume_json.get(\"other\", []),\n",
    "        jd_json.get(\"required_skills\", []),\n",
    "        jd_json.get(\"optional_skills\", [])\n",
    "    )\n",
    "    scores[\"skills\"] = {\"score\": skills_score, \"reason\": skills_reason}\n",
    "\n",
    "    cert_score, cert_reason = score_certifications_rule(\n",
    "        resume_json.get(\"certifications\", []),\n",
    "        resume_json.get(\"other\", []),\n",
    "        jd_json.get(\"certifications\", [])\n",
    "    )\n",
    "    scores[\"certifications\"] = {\"score\": cert_score, \"reason\": cert_reason}\n",
    "\n",
    "    edu_score, edu_reason = score_education_rule(\n",
    "        resume_json.get(\"education\", []),\n",
    "        jd_json.get(\"preferred_degrees\", [])\n",
    "    )\n",
    "    scores[\"education\"] = {\"score\": edu_score, \"reason\": edu_reason}\n",
    "\n",
    "    exp_score, exp_reason = score_experience_rule(\n",
    "        resume_json.get(\"total_experience_years\", 0.0),\n",
    "        jd_json.get(\"required_experience_years\", \"\")\n",
    "    )\n",
    "    scores[\"experience\"] = {\"score\": exp_score, \"reason\": exp_reason}\n",
    "\n",
    "    tools_score, tools_reason = score_tools_rule(\n",
    "        resume_json.get(\"skills\", []),\n",
    "        resume_json.get(\"experience\", []),\n",
    "        resume_json.get(\"other\", []),\n",
    "        resume_json.get(\"projects\", []),\n",
    "        jd_json.get(\"tools_and_technologies\", [])\n",
    "    )\n",
    "    scores[\"tools\"] = {\"score\": tools_score, \"reason\": tools_reason}\n",
    "\n",
    "    resp_score, resp_reason = score_responsibilities_rule(\n",
    "        resume_json.get(\"experience\", []),\n",
    "        resume_json.get(\"other\", []),\n",
    "        resume_json.get(\"projects\", []),\n",
    "        jd_json.get(\"job_responsibilities\", [])\n",
    "    )\n",
    "    scores[\"responsibilities\"] = {\"score\": resp_score, \"reason\": resp_reason}\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42629c1",
   "metadata": {},
   "source": [
    "### unit test for each scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d10d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Testing: score_skills_rule ===\")\n",
    "resume_skills = [\"Python\", \"SQL\", \"Excel\"]\n",
    "resume_other = [{\"section_name\": \"Training\", \"content\": \"Completed MongoDB, Tableau, Excel\"}]\n",
    "jd_required_skills = [\"Python\", \"MongoDB\"]\n",
    "jd_optional_skills = [\"Tableau\", \"Java\"]\n",
    "\n",
    "score, reason = score_skills_rule(resume_skills, resume_other, jd_required_skills, jd_optional_skills)\n",
    "print(f\"Score: {score}\\nReason: {reason}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68848f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Testing: score_certifications_rule ===\")\n",
    "resume_certs = [{\"certification\": \"AWS Certified\"}, {\"certification\": \"Azure\"}]\n",
    "resume_other = [{\"section_name\": \"Achievements\", \"content\": \"Google Cloud certified\"}]\n",
    "jd_certs = [\"AWS Certified\", \"Google Cloud\"]\n",
    "\n",
    "score, reason = score_certifications_rule(resume_certs, resume_other, jd_certs)\n",
    "print(f\"Score: {score}\\nReason: {reason}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae023ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Testing: score_education_rule ===\")\n",
    "resume_education = [{\"degree\": \"Bachelor of Computer Science\"}, {\"degree\": \"MBA\"}]\n",
    "jd_degrees = [\"Computer Science\", \"Information Technology\"]\n",
    "\n",
    "score, reason = score_education_rule(resume_education, jd_degrees)\n",
    "print(f\"Score: {score}\\nReason: {reason}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42095c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Testing: score_experience_rule ===\")\n",
    "resume_years = 6.0\n",
    "jd_experience = \"3‚Äì5 years\"\n",
    "\n",
    "score, reason = score_experience_rule(resume_years, jd_experience)\n",
    "print(f\"Score: {score}\\nReason: {reason}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc2aa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Testing: score_tools_rule ===\")\n",
    "resume_skills = [\"Python\", \"Docker\"]\n",
    "resume_experience = [\n",
    "    {\"job_title\": \"DevOps Engineer\", \"description\": [\"Used AWS, Docker, and Jenkins\"]},\n",
    "    {\"job_title\": \"Software Engineer\", \"description\": [\"Built APIs with Flask\"]}\n",
    "]\n",
    "resume_other = [{\"section_name\": \"Misc\", \"content\": \"Worked on Kubernetes and Terraform\"}]\n",
    "resume_projects = [{\"description\": \"Built ML model with Scikit-learn and deployed on AWS\"}]\n",
    "jd_tools = [\"AWS\", \"Docker\", \"Kubernetes\", \"GCP\"]\n",
    "\n",
    "score, reason = score_tools_rule(resume_skills, resume_experience, resume_other, resume_projects, jd_tools)\n",
    "print(f\"Score: {score}\\nReason: {reason}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042ea423",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Testing: score_responsibilities_rule ===\")\n",
    "resume_experience = [\n",
    "    {\"job_title\": \"Data Analyst\", \"description\": [\"Created dashboards using Power BI\", \"Cleaned large datasets\"]},\n",
    "]\n",
    "resume_other = [{\"section_name\": \"Leadership\", \"content\": \"Led team of 5 analysts\"}]\n",
    "resume_projects = [{\"description\": \"Automated data pipeline using Python\"}]\n",
    "jd_responsibilities = [\n",
    "    \"Created dashboards using Power BI\",\n",
    "    \"Automated data pipeline using Python\",\n",
    "    \"Built ETL workflows\"\n",
    "]\n",
    "\n",
    "score, reason = score_responsibilities_rule(resume_experience, resume_other, resume_projects, jd_responsibilities)\n",
    "print(f\"Score: {score}\\nReason: {reason}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e67572f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "print(\"=== Testing: compute_all_rule_scores ===\")\n",
    "\n",
    "resume_json = {\n",
    "    \"skills\": [\"Python\", \"Docker\"],\n",
    "    \"certifications\": [{\"certification\": \"AWS Certified\"}, {\"certification\": \"Azure\"}],\n",
    "    \"education\": [{\"degree\": \"Bachelor of Computer Science\"}, {\"degree\": \"MBA\"}],\n",
    "    \"total_experience_years\": 4.5,\n",
    "    \"experience\": [\n",
    "        {\"job_title\": \"DevOps Engineer\", \"description\": [\"Used AWS, Docker, and Jenkins\"]},\n",
    "        {\"job_title\": \"Data Analyst\", \"description\": [\"Created dashboards using Power BI\"]}\n",
    "    ],\n",
    "    \"other\": [\n",
    "        {\"section_name\": \"Leadership\", \"content\": \"Led team of 5 analysts\"},\n",
    "        {\"section_name\": \"Achievements\", \"content\": \"Google Cloud certified\"}\n",
    "    ],\n",
    "    \"projects\": [\n",
    "        {\"description\": \"Built ML model with Scikit-learn and deployed on AWS\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "jd_json = {\n",
    "    \"required_skills\": [\"Python\", \"MongoDB\"],\n",
    "    \"optional_skills\": [\"Tableau\", \"Java\"],\n",
    "    \"certifications\": [\"AWS Certified\", \"Google Cloud\"],\n",
    "    \"preferred_degrees\": [\"Computer Science\", \"Information Technology\"],\n",
    "    \"required_experience_years\": \"3+ years\",\n",
    "    \"tools_and_technologies\": [\"AWS\", \"Docker\", \"Kubernetes\", \"GCP\"],\n",
    "    \"job_responsibilities\": [\n",
    "        \"Created dashboards using Power BI\",\n",
    "        \"Automated data pipeline using Python\",\n",
    "        \"Built ETL workflows\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "results = compute_all_rule_scores(resume_json, jd_json)\n",
    "\n",
    "\n",
    "pprint(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b948ff22",
   "metadata": {},
   "source": [
    "## LLM-Based Scoring Functions (Structured Prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c4f8cffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_SCORING_SCHEMA = \"\"\"{\n",
    "  \"skills\": {\n",
    "    \"score\": float,\n",
    "    \"reason\": str\n",
    "  },\n",
    "  \"certifications\": {\n",
    "    \"score\": float,\n",
    "    \"reason\": str\n",
    "  },\n",
    "  \"education\": {\n",
    "    \"score\": float,\n",
    "    \"reason\": str\n",
    "  },\n",
    "  \"experience\": {\n",
    "    \"score\": float,\n",
    "    \"reason\": str\n",
    "  },\n",
    "  \"tools\": {\n",
    "    \"score\": float,\n",
    "    \"reason\": str\n",
    "  },\n",
    "  \"responsibilities\": {\n",
    "    \"score\": float,\n",
    "    \"reason\": str\n",
    "  },\n",
    "  \"soft_skills\": {\n",
    "    \"score\": float,\n",
    "    \"reason\": str\n",
    "  },\n",
    "  \"transferable_skills\": {\n",
    "    \"score\": float,\n",
    "    \"reason\": str\n",
    "  },\n",
    "  \"leadership\": {\n",
    "    \"score\": float,\n",
    "    \"reason\": str\n",
    "  },\n",
    "  \"grammar_cleanliness\": {\n",
    "    \"score\": float,\n",
    "    \"reason\": str\n",
    "  }\n",
    "}\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "54453faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_SCORING_PROMPT_TEMPLATE = \"\"\"\n",
    "You are an expert resume evaluator.\n",
    "\n",
    "Your task is to **compare** a candidate's resume and a job description and assign **section-wise ATS scores**. Each section receives:\n",
    "- a score between 0.0 and 1.0\n",
    "- a short reason explaining why\n",
    "\n",
    "You must return a valid JSON object. Do not return the resume. Do not repeat input. Do not include markdown or explanations.\n",
    "\n",
    "RESUME:\n",
    "{resume_json}\n",
    "\n",
    "JOB DESCRIPTION:\n",
    "{jd_json}\n",
    "\n",
    "Output format (STRICTLY FOLLOW THIS STRUCTURE):\n",
    "{schema}\n",
    "\n",
    "Now respond ONLY with a JSON object in this format:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "381ad05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "import json5\n",
    "from typing import Dict\n",
    "\n",
    "def extract_json_block(text: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Extract the last valid JSON object block from the text using recursive regex and json5.\n",
    "    Handles smart quotes, trailing commas, and prefers LLM's final output JSON.\n",
    "    \"\"\"\n",
    "    # Normalize smart quotes\n",
    "    text = text.replace(\"‚Äú\", \"\\\"\").replace(\"‚Äù\", \"\\\"\").replace(\"‚Äò\", \"'\").replace(\"‚Äô\", \"'\")\n",
    "\n",
    "    # Match all nested JSON-like blocks\n",
    "    matches = regex.findall(r\"\\{(?:[^{}]|(?R))*\\}\", text, flags=regex.DOTALL)\n",
    "\n",
    "    expected_keys = {\"skills\", \"experience\", \"education\", \"certifications\"}\n",
    "\n",
    "    for block in reversed(matches):\n",
    "        try:\n",
    "            parsed = json5.loads(block)\n",
    "            if isinstance(parsed, dict) and expected_keys.intersection(parsed.keys()):\n",
    "                return parsed\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    print(\"‚ùå No valid JSON block found in LLM output.\")\n",
    "    print(\"üîé Last few lines:\\n\", text[-500:])\n",
    "    raise ValueError(\"No valid JSON block found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5c2c4afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_with_llm(resume_json: dict, jd_json: dict, resume_id=\"resume\", jd_id=\"jd\") -> dict:\n",
    "    \"\"\"\n",
    "    Use an LLM pipeline to compute ATS scores with reasoning per section.\n",
    "    \"\"\"\n",
    "    prompt = LLM_SCORING_PROMPT_TEMPLATE.format(\n",
    "        schema=LLM_SCORING_SCHEMA,\n",
    "        resume_json=json.dumps(resume_json, indent=2),\n",
    "        jd_json=json.dumps(jd_json, indent=2)\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        outputs = llm_pipeline(\n",
    "            prompt,\n",
    "            max_new_tokens=1024,\n",
    "            do_sample=False,\n",
    "            temperature=None,\n",
    "            top_p=None,\n",
    "            top_k=None,\n",
    "            pad_token_id=llm_pipeline.tokenizer.pad_token_id\n",
    "        )\n",
    "        response_text = outputs[0][\"generated_text\"]\n",
    "        #print(\"üí¨ LLM response preview:\\n\", response_text)  \n",
    "\n",
    "        return extract_json_block(response_text)\n",
    "        \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå LLM inference failed for {resume_id} x {jd_id}: {str(e)}\")\n",
    "        print(\"üß™ Raw output preview:\\n\", response_text)\n",
    "        return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f5d7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "print(\"=== Testing: score_with_llm ===\")\n",
    "\n",
    "# Sample resume JSON\n",
    "resume_json = {\n",
    "    \"skills\": [\"Python\", \"Docker\"],\n",
    "    \"certifications\": [{\"certification\": \"AWS Certified\"}, {\"certification\": \"Azure\"}],\n",
    "    \"education\": [{\"degree\": \"Bachelor of Computer Science\"}, {\"degree\": \"MBA\"}],\n",
    "    \"total_experience_years\": 4.5,\n",
    "    \"experience\": [\n",
    "        {\"job_title\": \"DevOps Engineer\", \"description\": [\"Used AWS, Docker, and Jenkins\"]},\n",
    "        {\"job_title\": \"Data Analyst\", \"description\": [\"Created dashboards using Power BI\"]}\n",
    "    ],\n",
    "    \"other\": [\n",
    "        {\"section_name\": \"Leadership\", \"content\": \"Led team of 5 analysts\"},\n",
    "        {\"section_name\": \"Achievements\", \"content\": \"Google Cloud certified\"}\n",
    "    ],\n",
    "    \"projects\": [\n",
    "        {\"description\": \"Built ML model with Scikit-learn and deployed on AWS\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Sample job description JSON\n",
    "jd_json = {\n",
    "    \"required_skills\": [\"Python\", \"MongoDB\"],\n",
    "    \"optional_skills\": [\"Tableau\", \"Java\"],\n",
    "    \"certifications\": [\"AWS Certified\", \"Google Cloud\"],\n",
    "    \"preferred_degrees\": [\"Computer Science\", \"Information Technology\"],\n",
    "    \"required_experience_years\": \"3+ years\",\n",
    "    \"tools_and_technologies\": [\"AWS\", \"Docker\", \"Kubernetes\", \"GCP\"],\n",
    "    \"job_responsibilities\": [\n",
    "        \"Created dashboards using Power BI\",\n",
    "        \"Automated data pipeline using Python\",\n",
    "        \"Built ETL workflows\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Run LLM-based scoring\n",
    "llm_scores = score_with_llm(resume_json, jd_json)\n",
    "\n",
    "print(\"=== LLM Scoring Output ===\")\n",
    "pprint(llm_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fef8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "print(\"=== Testing: score_with_llm ===\")\n",
    "\n",
    "# Sample resume JSON\n",
    "resume_json = {\n",
    "            \"resume_id\": 88907739,\n",
    "            \"total_experience_years\": 11.3,\n",
    "            \"summary\": \"High-achieving management professional and effective consultant possessing excellent communication, organizational and analytical capabilities with about 4 years of experience in devising innovative strategies and solutions to resolve complex business challenges.\",\n",
    "            \"education\": [\n",
    "                {\n",
    "                    \"degree\": \"Master of Science\",\n",
    "                    \"field\": \"Software Management\",\n",
    "                    \"institution\": \"Carnegie Mellon University\",\n",
    "                    \"year\": \",\",\n",
    "                    \"gpa\": 3.8\n",
    "                },\n",
    "                {\n",
    "                    \"degree\": \"MBA\",\n",
    "                    \"field\": \"International Business\",\n",
    "                    \"institution\": \"Institute of Technology & Management\",\n",
    "                    \"year\": \",\",\n",
    "                    \"gpa\": 4.0\n",
    "                },\n",
    "                {\n",
    "                    \"degree\": \"MBA\",\n",
    "                    \"field\": \"International Business\",\n",
    "                    \"institution\": \"International Business Institute of Technology and Management India\",\n",
    "                    \"year\": \",\",\n",
    "                    \"gpa\": 4.0\n",
    "                }\n",
    "            ],\n",
    "            \"experience\": [\n",
    "                {\n",
    "                    \"job_title\": \"Consultant\",\n",
    "                    \"company\": \"Company Name\",\n",
    "                    \"start_date\": \"06/2015\",\n",
    "                    \"end_date\": \"Current\",\n",
    "                    \"description\": [\n",
    "                        \"Managed and delivered a project to implement and integrate a new content management platform to create a unified brand experience, support scalability, growth and enhance digital presence for client's business - post acquisition\",\n",
    "                        \"Led cross-functional global teams consisting of technical, business and functional representatives and achieved key milestones on time with quality deliverables\",\n",
    "                        \"Prioritized, escalated and resolved issues with internal and external stakeholders\",\n",
    "                        \"Directly managed 3rd party vendor and offshore teams.\"\n",
    "                    ]\n",
    "                },\n",
    "                {\n",
    "                    \"job_title\": \"Product Strategy Intern\",\n",
    "                    \"company\": \"Company Name\",\n",
    "                    \"start_date\": \"09/2015\",\n",
    "                    \"end_date\": \"12/2015\",\n",
    "                    \"description\": [\n",
    "                        \"Led a practicum team at Carnegie Mellon University to understand IBM Bluemix (PaaS), cloud based solution and use business frameworks to perform market, competitor and customer journey analysis\",\n",
    "                        \"Liaised with cross functional teams to assess opportunities in marketplace, determine synergies and align business unit goals with corporate strategy\",\n",
    "                        \"Worked with senior management and stakeholders to develop strategy for to enhance awareness, increase conversion and explore new market opportunities to scale the client's user base.\"\n",
    "                    ]\n",
    "                },\n",
    "                {\n",
    "                    \"job_title\": \"Assistant Operations Manager\",\n",
    "                    \"company\": \"Company Name\",\n",
    "                    \"start_date\": \"07/2012\",\n",
    "                    \"end_date\": \"10/2013\",\n",
    "                    \"description\": [\n",
    "                        \"Business Strategy & Vendor Management: Automation of Hub, typical model and replication\",\n",
    "                        \"Reported to Chief Operating Officer to recommend company wide automation strategies and vendor selection\",\n",
    "                        \"Conducted gap analysis, market research, competitor and financial analysis to propose short, mid and long term strategies to the Executive team\",\n",
    "                        \"Project Management: RFID Project Member of the core project management team responsible for coordinated of cross-functional teams to achieve project milestones\",\n",
    "                        \"Focused on process improvement and optimization to enhance team productivity\",\n",
    "                        \"Defined the Key Performance Indicator's to evaluate vendors.\"\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            \"skills\": [\n",
    "                \"Strategy & Operations Process Optimization\",\n",
    "                \"Digital Transformation\",\n",
    "                \"Cross Functional Team Management\",\n",
    "                \"Project/Product Management\",\n",
    "                \"Agile/Lean Methodologies\",\n",
    "                \"Work History\",\n",
    "                \"Client\",\n",
    "                \"Data Analysis\",\n",
    "                \"E-Commerce\",\n",
    "                \"senior management\",\n",
    "                \"Financial\",\n",
    "                \"financial analysis\",\n",
    "                \"functional\",\n",
    "                \"Google Analytics\",\n",
    "                \"Government\",\n",
    "                \"Hub\",\n",
    "                \"IBM\",\n",
    "                \"International Business\",\n",
    "                \"investments\",\n",
    "                \"IP\",\n",
    "                \"Marketing plan\",\n",
    "                \"market research\",\n",
    "                \"Market Strategy\",\n",
    "                \"marketing\",\n",
    "                \"market\",\n",
    "                \"MBA\",\n",
    "                \".NET\",\n",
    "                \"academic\",\n",
    "                \"ADA\",\n",
    "                \"Adobe\",\n",
    "                \"Apple\",\n",
    "                \"approach\",\n",
    "                \"Automation\",\n",
    "                \"business development\",\n",
    "                \"Business Process\",\n",
    "                \"Business Strategy\",\n",
    "                \"Consulting\",\n",
    "                \"content management\",\n",
    "                \"Conversion\",\n",
    "                \"Client\",\n",
    "                \"Data Analysis\",\n",
    "                \"E-Commerce\",\n",
    "                \"senior management\",\n",
    "                \"Financial\",\n",
    "                \"financial analysis\",\n",
    "                \"functional\",\n",
    "                \"Google Analytics\",\n",
    "                \"Government\",\n",
    "                \"Hub\",\n",
    "                \"IBM\",\n",
    "                \"International Business\",\n",
    "                \"investments\",\n",
    "                \"IP\",\n",
    "                \"Marketing plan\",\n",
    "                \"market research\",\n",
    "                \"Market Strategy\",\n",
    "                \"marketing\",\n",
    "                \"market\",\n",
    "                \"MBA\",\n",
    "                \"C#\",\n",
    "                \"Excel\",\n",
    "                \"Microsoft Office Suite\",\n",
    "                \"Power Point\",\n",
    "                \"Word\",\n",
    "                \"Network\",\n",
    "                \"Object Oriented Analysis and Design\",\n",
    "                \"optimization\",\n",
    "                \"policies\",\n",
    "                \"process improvement\",\n",
    "                \"Project Management\",\n",
    "                \"proposals\",\n",
    "                \"quality\",\n",
    "                \"Requirement\",\n",
    "                \"Research\",\n",
    "                \"RFP\",\n",
    "                \"Scrum\",\n",
    "                \"SDLC\",\n",
    "                \"Speech\",\n",
    "                \"MS SQL\",\n",
    "                \"Strategy\",\n",
    "                \"Strategy Development\",\n",
    "                \"Vendor Management\",\n",
    "                \"Management\",\n",
    "                \"Visio\",\n",
    "                \"websites\"\n",
    "            ],\n",
    "            \"certifications\": [],\n",
    "            \"projects\": [\n",
    "                {\n",
    "                    \"project_title\": \"Online E-commerce store\",\n",
    "                    \"description\": \"Conceptualized and launched Online E-commerce store, developed Product Strategy and Roadmap, and produced Engineering, Financial and Marketing plan\",\n",
    "                    \"start_date\": \"08/2014\",\n",
    "                    \"end_date\": \"12/2015\"\n",
    "                },\n",
    "                {\n",
    "                    \"project_title\": \"Commercialization of IP\",\n",
    "                    \"description\": \"Developed Go-to-Market Strategy, Product Roadmap and proposed Business Model to launch CMU's Automatic Speech Recognition Technology and presented to Sand Hill Angel Investors\",\n",
    "                    \"start_date\": \"08/2014\",\n",
    "                    \"end_date\": \"12/2015\"\n",
    "                },\n",
    "                {\n",
    "                    \"project_title\": \"Survivable Social Network on Chip\",\n",
    "                    \"description\": \"Performed Object Oriented Analysis and Design along with the estimation, planning, development, measurement and tracking of the software project using the hybrid development approach\",\n",
    "                    \"start_date\": \"08/2014\",\n",
    "                    \"end_date\": \"12/2015\"\n",
    "                }\n",
    "            ],\n",
    "            \"languages\": [],\n",
    "            \"other\": []\n",
    "        }\n",
    "\n",
    "# Sample job description JSON\n",
    "jd_json = {\n",
    "            \"jd_id\": 3906094741,\n",
    "            \"inferred_domain\": \"consulting\",\n",
    "            \"title\": \"Director, Property Tax\",\n",
    "            \"summary\": \"Director, Property Tax role at Kroll, focusing on tax consulting and valuation projects.\",\n",
    "            \"required_experience_years\": \"7\",\n",
    "            \"preferred_degrees\": [\n",
    "                \"Accounting\",\n",
    "                \"Economics\",\n",
    "                \"Finance\",\n",
    "                \"Management\",\n",
    "                \"Real Estate\"\n",
    "            ],\n",
    "            \"required_skills\": [\n",
    "                \"Management\",\n",
    "                \"Sales\"\n",
    "            ],\n",
    "            \"optional_skills\": [],\n",
    "            \"tools_and_technologies\": [\n",
    "                \"Excel\",\n",
    "                \"Word\",\n",
    "                \"PowerPoint\"\n",
    "            ],\n",
    "            \"certifications\": [\n",
    "                \"ASA\",\n",
    "                \"CPA\",\n",
    "                \"CFA\",\n",
    "                \"MAI\"\n",
    "            ],\n",
    "            \"soft_skills\": [\n",
    "                \"Leadership\",\n",
    "                \"Client Relationship Management\",\n",
    "                \"Analytical Skills\",\n",
    "                \"Independence\",\n",
    "                \"Teamwork\",\n",
    "                \"Communication\",\n",
    "                \"Diversity Awareness\"\n",
    "            ],\n",
    "            \"job_responsibilities\": [\n",
    "                \"Client Research\",\n",
    "                \"Data Analysis\",\n",
    "                \"Presentation Development\",\n",
    "                \"Valuation Techniques\",\n",
    "                \"Tax Hearing Preparation\",\n",
    "                \"Project Reporting\",\n",
    "                \"Tax Projection Scenarios\",\n",
    "                \"Business Solution Implementation\",\n",
    "                \"Junior Staff Development\",\n",
    "                \"Practice Growth\"\n",
    "            ],\n",
    "            \"job_location\": \"Atlanta, GA\",\n",
    "            \"remote_option\": \",\",\n",
    "            \"employment_type\": \"full-time\",\n",
    "            \"travel_requirements\": \"N/A\",\n",
    "            \"physical_requirements\": \"N/A\",\n",
    "            \"benefits\": [],\n",
    "            \"company_information\": \"Kroll is a global firm providing services in governance, risk, and transparency.\",\n",
    "            \"equal_opportunity_policy\": \"Kroll is committed to creating an inclusive work environment and is an equal opportunity employer.\",\n",
    "            \"other\": [\n",
    "                {\n",
    "                    \"section_name\": \"Experience Level\",\n",
    "                    \"content\": \"Director\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "# Run LLM-based scoring\n",
    "llm_scores = score_with_llm(resume_json, jd_json)\n",
    "\n",
    "print(\"=== LLM Scoring Output ===\")\n",
    "pprint(llm_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e71a89",
   "metadata": {},
   "source": [
    "## Combine Section Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d277dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_section_scores(\n",
    "    rule_scores: Dict[str, Dict],\n",
    "    llm_scores: Dict[str, Dict],\n",
    "    rule_weight: float = 0.5,\n",
    "    llm_weight: float = 0.5\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Merge rule-based and LLM-based section scores using weighted average.\n",
    "    Each section includes:\n",
    "    - score (merged)\n",
    "    - details: { \"rule\": ..., \"llm\": ... }\n",
    "    Returns:\n",
    "    {\n",
    "      \"final_section_scores\": {\n",
    "         section_name: {\n",
    "            \"score\": float,\n",
    "            \"details\": { \"rule\": str, \"llm\": str }\n",
    "         }\n",
    "      },\n",
    "      \"final_ats_score\": float,\n",
    "      \"weights_used\": { section: { \"rule\": float, \"llm\": float } }\n",
    "    }\n",
    "    \"\"\"\n",
    "    merged_scores = {}\n",
    "    section_weights_used = {}\n",
    "    all_sections = set(rule_scores.keys()) | set(llm_scores.keys())\n",
    "\n",
    "    for section in all_sections:\n",
    "        rule_score = rule_scores.get(section, {}).get(\"score\")\n",
    "        rule_details = rule_scores.get(section, {}).get(\"details\")\n",
    "        llm_score = llm_scores.get(section, {}).get(\"score\")\n",
    "        llm_details = llm_scores.get(section, {}).get(\"details\")\n",
    "\n",
    "        if rule_score is not None and llm_score is not None:\n",
    "            score = (rule_score * rule_weight) + (llm_score * llm_weight)\n",
    "            weight_info = {\"rule\": rule_weight, \"llm\": llm_weight}\n",
    "        elif rule_score is not None:\n",
    "            score = rule_score\n",
    "            weight_info = {\"rule\": 1.0, \"llm\": 0.0}\n",
    "        elif llm_score is not None:\n",
    "            score = llm_score\n",
    "            weight_info = {\"rule\": 0.0, \"llm\": 1.0}\n",
    "        else:\n",
    "            score = 0.0\n",
    "            weight_info = {\"rule\": 0.0, \"llm\": 0.0}\n",
    "\n",
    "        merged_scores[section] = {\n",
    "            \"score\": round(score, 4),\n",
    "            \"details\": {\n",
    "                \"rule\": rule_details or \"N/A\",\n",
    "                \"llm\": llm_details or \"N/A\"\n",
    "            }\n",
    "        }\n",
    "        section_weights_used[section] = weight_info\n",
    "\n",
    "    final_ats_score = round(\n",
    "        sum(sec[\"score\"] for sec in merged_scores.values()) / len(merged_scores),\n",
    "        4\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"final_section_scores\": merged_scores,\n",
    "        \"final_ats_score\": final_ats_score,\n",
    "        \"weights_used\": section_weights_used\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b465428d",
   "metadata": {},
   "source": [
    "## Test phase 3 - scoring logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dd2aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìÇ Mini Test Resumes\n",
    "test_resumes = [\n",
    "    {\n",
    "        \"resume_id\": \"61e6bdda-548c-4d24-87f3-5d97fdef032b\",\n",
    "        \"basics\": {\n",
    "            \"name\": \"Alice Smith\",\n",
    "            \"email\": \"alice@example.com\",\n",
    "            \"phone\": \"123-456-7890\",\n",
    "            \"location\": \"New York, NY\",\n",
    "            \"current_title\": \"Software Engineer\",\n",
    "            \"linkedin_url\": \"\"\n",
    "        },\n",
    "        \"education\": [\n",
    "            {\"degree\": \"B.Sc. Computer Science\", \"field\": \"Computer Science\", \"institution\": \"NYU\", \"year\": \"2018\", \"gpa\": \"3.7\"}\n",
    "        ],\n",
    "        \"experience\": [\n",
    "            {\"job_title\": \"Software Developer\", \"company\": \"ABC Corp\", \"start_date\": \"06/2018\", \"end_date\": \"08/2021\", \"duration_in_months\": 38, \"description\": \"Developed web applications.\"}\n",
    "        ],\n",
    "        \"skills\": [\"Python\", \"Django\", \"SQL\"],\n",
    "        \"certifications\": [\"AWS Certified Developer\"],\n",
    "        \"projects\": [\"E-commerce platform\"],\n",
    "        \"languages\": [\"English\"],\n",
    "        \"total_experience_years\": 3.2\n",
    "    },\n",
    "    {\n",
    "        \"resume_id\": \"f14f29c5-8ed9-493a-975d-c210655ff0aa\",\n",
    "        \"basics\": {\n",
    "            \"name\": \"Bob Johnson\",\n",
    "            \"email\": \"bob@example.com\",\n",
    "            \"phone\": \"987-654-3210\",\n",
    "            \"location\": \"San Francisco, CA\",\n",
    "            \"current_title\": \"Data Analyst\",\n",
    "            \"linkedin_url\": \"\"\n",
    "        },\n",
    "        \"education\": [\n",
    "            {\"degree\": \"B.A. Statistics\", \"field\": \"Statistics\", \"institution\": \"UCLA\", \"year\": \"2017\", \"gpa\": \"3.5\"}\n",
    "        ],\n",
    "        \"experience\": [\n",
    "            {\"job_title\": \"Data Analyst\", \"company\": \"XYZ Inc\", \"start_date\": \"01/2018\", \"end_date\": \"12/2020\", \"duration_in_months\": 36, \"description\": \"Analyzed data trends.\"}\n",
    "        ],\n",
    "        \"skills\": [\"SQL\", \"Tableau\", \"Python\"],\n",
    "        \"certifications\": [],\n",
    "        \"projects\": [\"Sales analytics dashboard\"],\n",
    "        \"languages\": [\"English\"],\n",
    "        \"total_experience_years\": 3.0\n",
    "    }\n",
    "]\n",
    "\n",
    "# üìÇ Mini Test JDs\n",
    "test_jds = [\n",
    "    {\n",
    "        \"jd_id\": \"9a62f845-94f2-40fe-a63b-e6f5cbd765c5\",\n",
    "        \"title\": \"Backend Engineer\",\n",
    "        \"summary\": \"Looking for a backend engineer with 3+ years experience in Python and SQL. AWS certification preferred.\",\n",
    "        \"required_experience_years\": 3.0,\n",
    "        \"preferred_degrees\": [\"B.Sc. Computer Science\"],\n",
    "        \"required_skills\": [\"Python\", \"SQL\"],\n",
    "        \"optional_skills\": [\"Django\"],\n",
    "        \"certifications\": [\"AWS Certified Developer\"],\n",
    "        \"soft_skills\": [\"Teamwork\", \"Communication\"],\n",
    "        \"job_location\": \"New York, NY\",\n",
    "        \"remote_option\": True,\n",
    "        \"employment_type\": \"Full-time\",\n",
    "        \"inferred_domain\": \"engineering\"\n",
    "    },\n",
    "    {\n",
    "        \"jd_id\": \"c890e8d6-9f04-429b-9a27-c4f5fcb59ce5\",\n",
    "        \"title\": \"Business Data Analyst\",\n",
    "        \"summary\": \"Seeking a Data Analyst with 2+ years experience in SQL, Excel, and data visualization tools.\",\n",
    "        \"required_experience_years\": 2.0,\n",
    "        \"preferred_degrees\": [\"B.A. Statistics\"],\n",
    "        \"required_skills\": [\"SQL\", \"Excel\"],\n",
    "        \"optional_skills\": [\"Tableau\"],\n",
    "        \"certifications\": [],\n",
    "        \"soft_skills\": [\"Analytical thinking\", \"Attention to detail\"],\n",
    "        \"job_location\": \"San Francisco, CA\",\n",
    "        \"remote_option\": False,\n",
    "        \"employment_type\": \"Full-time\",\n",
    "        \"inferred_domain\": \"technology\"\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb31be41",
   "metadata": {},
   "source": [
    "#### Scoring Loop (Test Batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6ee026",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "# Replace with your test resumes and JDs\n",
    "#test_resumes = resumes[:2]  # or load from a separate test dataset\n",
    "#test_jds = jds[:2]\n",
    "\n",
    "test_results = []\n",
    "\n",
    "for resume_idx, resume in tqdm(enumerate(test_resumes), desc=\"Scoring Test Batch\", total=len(test_resumes)):\n",
    "    resume_id = resume.get(\"resume_id\", f\"resume_{resume_idx}\")\n",
    "    \n",
    "    for jd_idx, jd in enumerate(test_jds):\n",
    "        jd_id = jd.get(\"jd_id\", f\"jd_{jd_idx}\")\n",
    "\n",
    "        # Run both scoring engines\n",
    "        rule_scores = rule_based_scoring(resume, jd)\n",
    "        llm_scores = llm_based_scoring(resume, jd, resume_id, jd_id)\n",
    "\n",
    "        # Merge section scores and compute final ATS score\n",
    "        merged_result = merge_section_scores(rule_scores, llm_scores, rule_weight=0.5, llm_weight=0.5)\n",
    "\n",
    "        test_results.append({\n",
    "            \"resume_id\": resume_id,\n",
    "            \"job_id\": jd_id,\n",
    "            \"rule_based\": rule_scores,\n",
    "            \"llm_based\": llm_scores,\n",
    "            \"final_section_scores\": merged_result[\"final_section_scores\"],\n",
    "            \"final_ats_score\": merged_result[\"final_ats_score\"],\n",
    "            \"weights_used\": merged_result[\"weights_used\"],\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"model_used\": \"Nous-Hermes-2-Mistral-7B-DPO\"\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1420d4eb",
   "metadata": {},
   "source": [
    "#### Save Mini Test Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4ba4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üíæ Save Mini Test Output\n",
    "import os\n",
    "test_file = os.path.join(Config.JSON_OUTPUT_SCORING_DIR, 'test_phase3_hybrid_parallel_scoring.json')\n",
    "\n",
    "save_json_output(test_results, test_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1515d8d8",
   "metadata": {},
   "source": [
    "## Embedding-Based Relevance Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aec5ca0",
   "metadata": {},
   "source": [
    "### Text Construction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9c2e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resume_to_text(resume: Dict) -> str:\n",
    "    def safe_join(items):\n",
    "        return \", \".join(str(i) for i in items if i)\n",
    "\n",
    "    def extract_certifications(cert_list):\n",
    "        return [\n",
    "            c.get(\"name\", \"\") if isinstance(c, dict) else str(c)\n",
    "            for c in cert_list\n",
    "        ]\n",
    "\n",
    "    def extract_education(edus):\n",
    "        return [\n",
    "            f\"{e.get('degree', '')} from {e.get('institution', '')} ({e.get('year', '')})\"\n",
    "            for e in edus if isinstance(e, dict)\n",
    "        ]\n",
    "\n",
    "    def extract_experience(exps):\n",
    "        results = []\n",
    "        for exp in exps:\n",
    "            parts = [\n",
    "                exp.get(\"title\", \"\"),\n",
    "                exp.get(\"company\", \"\"),\n",
    "                exp.get(\"location\", \"\"),\n",
    "                f\"{exp.get('start_date', '')} to {exp.get('end_date', '')}\"\n",
    "            ]\n",
    "            results.append(\" | \".join(filter(None, parts)))\n",
    "        return results\n",
    "\n",
    "    basics = resume.get(\"basics\", {})\n",
    "    title = basics.get(\"current_title\", \"\")\n",
    "    location = basics.get(\"location\", \"\")\n",
    "\n",
    "    skills = safe_join(resume.get(\"skills\", []))\n",
    "    certs = safe_join(extract_certifications(resume.get(\"certifications\", [])))\n",
    "    education = safe_join(extract_education(resume.get(\"education\", [])))\n",
    "    experience = safe_join(extract_experience(resume.get(\"experience\", [])))\n",
    "    projects = safe_join(resume.get(\"projects\", []))\n",
    "    languages = safe_join(resume.get(\"languages\", []))\n",
    "\n",
    "    summary = f\"\"\"\n",
    "Title: {title}\n",
    "Location: {location}\n",
    "Skills: {skills}\n",
    "Certifications: {certs}\n",
    "Education: {education}\n",
    "Experience: {experience}\n",
    "Projects: {projects}\n",
    "Languages: {languages}\n",
    "\"\"\"\n",
    "    return summary.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5095a12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jd_to_text(jd: Dict) -> str:\n",
    "    def safe_join(items):\n",
    "        return \", \".join(str(i) for i in items if i)\n",
    "\n",
    "    title = jd.get(\"title\", \"\")\n",
    "    summary = jd.get(\"summary\", jd.get(\"description\", \"\"))\n",
    "    required_skills = safe_join(jd.get(\"required_skills\", []))\n",
    "    optional_skills = safe_join(jd.get(\"optional_skills\", []))\n",
    "    soft_skills = safe_join(jd.get(\"soft_skills\", []))\n",
    "    certifications = safe_join(jd.get(\"certifications\", []))\n",
    "    degrees = safe_join(jd.get(\"preferred_degrees\", []))\n",
    "    domain = jd.get(\"inferred_domain\", \"unknown\")\n",
    "    location = jd.get(\"job_location\", \"\")\n",
    "    emp_type = jd.get(\"employment_type\", \"\")\n",
    "    remote = \"Remote\" if jd.get(\"remote_option\") else \"Onsite\"\n",
    "\n",
    "    jd_text = f\"\"\"\n",
    "Title: {title}\n",
    "Domain: {domain}\n",
    "Location: {location} ({remote})\n",
    "Employment Type: {emp_type}\n",
    "Summary: {summary}\n",
    "Required Skills: {required_skills}\n",
    "Optional Skills: {optional_skills}\n",
    "Soft Skills: {soft_skills}\n",
    "Preferred Degrees: {degrees}\n",
    "Certifications: {certifications}\n",
    "\"\"\"\n",
    "    return jd_text.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b0b182",
   "metadata": {},
   "source": [
    "### Embedding + Relevance Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10b7fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def generate_semantic_relevance_map(\n",
    "    resumes: List[Dict],\n",
    "    jds: List[Dict],\n",
    "    top_n: int = 10,\n",
    "    model_name: str = \"all-MiniLM-L6-v2\"\n",
    ") -> Dict[str, List[Dict]]:\n",
    "    \"\"\"\n",
    "    Generates a relevance map using cosine similarity of sentence embeddings\n",
    "    between each resume and all job descriptions.\n",
    "\n",
    "    Returns:\n",
    "        Dict[resume_id, List[{jd_id, score}]]\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer(model_name)\n",
    "\n",
    "    print(\"üß† Encoding job descriptions...\")\n",
    "    jd_ids, jd_texts = [], []\n",
    "    for jd in jds:\n",
    "        try:\n",
    "            if not jd.get(\"jd_id\"):\n",
    "                continue\n",
    "            text = jd_to_text(jd)\n",
    "            if text.strip():\n",
    "                jd_ids.append(jd[\"jd_id\"])\n",
    "                jd_texts.append(text)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Skipping JD due to error: {e}\")\n",
    "            continue\n",
    "\n",
    "    jd_embeddings = model.encode(jd_texts, show_progress_bar=True, batch_size=32)\n",
    "\n",
    "    relevance_map = {}\n",
    "\n",
    "    print(\"üìÑ Encoding resumes and computing similarities...\")\n",
    "    for resume in tqdm(resumes, desc=\"Generating semantic relevance map\"):\n",
    "        try:\n",
    "            resume_id = resume.get(\"resume_id\")\n",
    "            if not resume_id:\n",
    "                continue\n",
    "\n",
    "            resume_text = resume_to_text(resume)\n",
    "            if not resume_text.strip():\n",
    "                continue\n",
    "\n",
    "            resume_emb = model.encode([resume_text])[0]\n",
    "            sim_scores = cosine_similarity([resume_emb], jd_embeddings)[0]\n",
    "            top_indices = np.argsort(sim_scores)[::-1][:top_n]\n",
    "\n",
    "            top_matches = [\n",
    "                {\"jd_id\": jd_ids[i], \"score\": round(float(sim_scores[i]), 4)}\n",
    "                for i in top_indices\n",
    "            ]\n",
    "\n",
    "            relevance_map[resume_id] = top_matches\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error processing resume {resume.get('resume_id', 'unknown')}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return relevance_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7b0994",
   "metadata": {},
   "source": [
    "## Phase 3: Scoring Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1d129a",
   "metadata": {},
   "source": [
    "### Checkpoint Handling (JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef277819",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "\n",
    "def load_resume_checkpoint(path: str) -> int:\n",
    "    if not os.path.exists(path):\n",
    "        return 0\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return json.load(file).get(\"last_index\", 0)\n",
    "\n",
    "def save_resume_checkpoint(path: str, index: int):\n",
    "    data = {\n",
    "        \"last_index\": index,\n",
    "        \"timestamp\": datetime.now(timezone.utc).isoformat()\n",
    "    }\n",
    "    save_json_output(data, path)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae98c44",
   "metadata": {},
   "source": [
    "### Get Relevant JDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec779031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_jds(\n",
    "    resume_id: str,\n",
    "    relevance_map: Dict[str, List[Dict]],\n",
    "    jd_lookup: Dict[str, Dict],\n",
    "    threshold: float = 0.5\n",
    ") -> List[Dict]:\n",
    "    matches = relevance_map.get(resume_id, [])\n",
    "    return [\n",
    "        jd_lookup[m[\"jd_id\"]]\n",
    "        for m in matches\n",
    "        if m[\"score\"] >= threshold and m[\"jd_id\"] in jd_lookup\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5c063c",
   "metadata": {},
   "source": [
    "### Scoring a Single Resume-JD Pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1bfb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_resume_jd_pair(\n",
    "    resume: Dict,\n",
    "    jd: Dict,\n",
    "    resume_id: str,\n",
    "    jd_id: str,\n",
    "    rule_weight: float,\n",
    "    llm_weight: float,\n",
    "    relevance_score: float,\n",
    ") -> Dict:\n",
    "    rule_scores = rule_based_scoring(resume, jd)\n",
    "    llm_scores = llm_based_scoring(resume, jd, resume_id, jd_id)\n",
    "    merged = merge_section_scores(rule_scores, llm_scores, rule_weight, llm_weight)\n",
    "\n",
    "    merged.update({\n",
    "        \"resume_id\": resume_id,\n",
    "        \"job_id\": jd_id,\n",
    "        \"timestamp\": datetime.now(timezone.utc).isoformat(),\n",
    "        \"inferred_domain\": jd.get(\"inferred_domain\", \"unknown\"),\n",
    "        \"relevance_score\": relevance_score,\n",
    "        \"match_quality\": (\n",
    "            \"strong\" if merged[\"final_ats_score\"] >= 0.75 else\n",
    "            \"medium\" if merged[\"final_ats_score\"] >= 0.5 else\n",
    "            \"weak\"\n",
    "        )\n",
    "    })\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d1fabc",
   "metadata": {},
   "source": [
    "### Main Modular Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b50913",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from typing import List, Dict\n",
    "import os\n",
    "import time\n",
    "\n",
    "def score_and_save_in_batches(\n",
    "    resumes: List[Dict],\n",
    "    jd_lookup: Dict[str, Dict],\n",
    "    relevance_map: Dict[str, List[Dict]],\n",
    "    output_dir: str = Config.JSON_OUTPUT_SCORING_DIR,\n",
    "    save_every: int = 5,\n",
    "    limit: int = 20,\n",
    "    relevance_threshold: float = 0.4,\n",
    "    rule_weight: float = 0.5,\n",
    "    llm_weight: float = 0.5,\n",
    "    resume_from_checkpoint: bool = True\n",
    "):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    checkpoint_file = os.path.join(output_dir, \"checkpoint.json\")\n",
    "\n",
    "    # Determine starting index\n",
    "    start = load_resume_checkpoint(checkpoint_file) if resume_from_checkpoint else 0\n",
    "    end = min(start + limit, len(resumes))\n",
    "\n",
    "    successes, failures = [], []\n",
    "    timestamp = int(time.time())\n",
    "\n",
    "    for idx in tqdm(range(start, end), desc=\"Scoring resumes\"):\n",
    "        resume = resumes[idx]\n",
    "        resume_id = resume.get(\"resume_id\", f\"resume_{idx}\")\n",
    " \n",
    "\n",
    "        relevant_jds = get_relevant_jds(resume_id, relevance_map, jd_lookup, threshold=relevance_threshold)\n",
    "        if not relevant_jds:\n",
    "            print(f\"‚ö†Ô∏è No relevant JDs found for {resume_id}\")\n",
    "            continue\n",
    "\n",
    "        for jd in relevant_jds:\n",
    "            jd_id = jd.get(\"jd_id\", \"\")\n",
    "            try:\n",
    "                relevance_score = next((m[\"score\"] for m in relevance_map[resume_id] if m[\"jd_id\"] == jd_id), 0.0)\n",
    "                result = score_resume_jd_pair(\n",
    "                    resume, jd, resume_id, jd_id,\n",
    "                    rule_weight, llm_weight,\n",
    "                    relevance_score\n",
    "                )\n",
    "                successes.append(result)\n",
    "            except Exception as e:\n",
    "                failures.append({\n",
    "                    \"resume_id\": resume_id,\n",
    "                    \"jd_id\": jd_id,\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "\n",
    "        if (idx - start + 1) % save_every == 0:\n",
    "            partial_success_file = f\"{output_dir}/scored_part_{start}_{idx}_{timestamp}.json\"\n",
    "            partial_fail_file = f\"{output_dir}/failed_part_{start}_{idx}_{timestamp}.json\"\n",
    "            if successes:\n",
    "                save_json_output(successes, partial_success_file)\n",
    "            if failures:\n",
    "                save_json_output(failures, partial_fail_file)\n",
    "            save_resume_checkpoint(checkpoint_file, idx + 1)\n",
    "\n",
    "    # Final save\n",
    "    success_file = f\"{output_dir}/scored_final_{start}_{end}_{timestamp}.json\"\n",
    "    fail_file = f\"{output_dir}/failed_final_{start}_{end}_{timestamp}.json\"\n",
    "    if successes:\n",
    "        save_json_output(successes, success_file)\n",
    "    if failures:\n",
    "        save_json_output(failures, fail_file)\n",
    "    save_resume_checkpoint(checkpoint_file, end)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa81952",
   "metadata": {},
   "source": [
    "## Load Normalized Resumes and JDs and relevance score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e79d4e",
   "metadata": {},
   "source": [
    "### Load Resumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e992097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "resumes_path = os.path.join(Config.JSON_OUTPUT_NORMALIZED_DIR, 'normalized_resumes.json')\n",
    "resumes = load_json_file(resumes_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2379d41f",
   "metadata": {},
   "source": [
    "### Load JDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3d0e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "jds_path = os.path.join(Config.JSON_OUTPUT_NORMALIZED_DIR, 'normalized_jds.json')\n",
    "jds = load_json_file(jds_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8555c133",
   "metadata": {},
   "source": [
    "### create and save relevance map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c247df62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "relevance_map = generate_semantic_relevance_map(resumes, jds, top_n=10)\n",
    "\n",
    "relevance_map_file = os.path.join(Config.JSON_OUTPUT_SCORING_DIR, 'semantic_relevance_scores.json')\n",
    "save_json_output(relevance_map, relevance_map_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f67dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loaded {len(resumes)} resumes and {len(jds)} job descriptions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4fd7f6",
   "metadata": {},
   "source": [
    "## Execute Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998fc6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "jd_lookup = {jd[\"jd_id\"]: jd for jd in jds}\n",
    "\n",
    "score_and_save_in_batches(\n",
    "    resumes=resumes,\n",
    "    jd_lookup=jd_lookup,\n",
    "    relevance_map=relevance_map,\n",
    "    output_dir=Config.JSON_OUTPUT_SCORING_DIR,\n",
    "    save_every=5,\n",
    "    limit=1,\n",
    "    relevance_threshold=0.45,\n",
    "    rule_weight=0.5,\n",
    "    llm_weight=0.5,\n",
    "    resume_from_checkpoint=True  # set True to resume, False to start fresh\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
