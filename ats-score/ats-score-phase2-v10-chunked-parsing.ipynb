{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f630d895",
   "metadata": {},
   "source": [
    "# Global setup and package installation used in most phases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969fcea6",
   "metadata": {},
   "source": [
    "## Colab + GPU Detection Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cf79fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def is_running_in_colab():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "def get_available_gpu_memory_gb():\n",
    "    try:\n",
    "        output = subprocess.check_output(\n",
    "            [\"nvidia-smi\", \"--query-gpu=memory.free\", \"--format=csv,nounits,noheader\"],\n",
    "            encoding=\"utf-8\"\n",
    "        )\n",
    "        free_mem_mb = int(output.strip().split(\"\\n\")[0])\n",
    "        return free_mem_mb / 1024\n",
    "    except Exception:\n",
    "        return 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66be54d4",
   "metadata": {},
   "source": [
    "## install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942fba10",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_running_in_colab():\n",
    "    # Install the required packages\n",
    "    !pip install kagglehub pandas\n",
    "    !pip install -q transformers accelerate bitsandbytes sentencepiece pydantic huggingface_hub xformers\n",
    "    !pip install regex json5\n",
    "else:\n",
    "    %pip install kagglehub pandas\n",
    "    %pip install -q transformers accelerate sentencepiece pydantic huggingface_hub xformers\n",
    "    #%pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu121\n",
    "    #%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "    %pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128\n",
    "    %pip install -U bitsandbytes\n",
    "    %pip install regex json5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4a9f61",
   "metadata": {},
   "source": [
    "## Login to huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1235aef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# Set your token here securely or prompt for it in Colab\n",
    "# Recommended: store in Colab secrets or environment variable\n",
    "HF_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "\n",
    "if not HF_TOKEN:\n",
    "    if is_running_in_colab():\n",
    "        # If running in Colab, use the Colab secrets\n",
    "        try:\n",
    "            from google.colab import userdata\n",
    "            HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "            if not HF_TOKEN:\n",
    "                raise ValueError(\"‚ö†Ô∏è Hugging Face token not found in Colab secrets.\")\n",
    "            print(\"üîë Hugging Face token found in Colab secrets.\")\n",
    "        except ImportError:\n",
    "            print(\"‚ö†Ô∏è Unable to authenticate in Colab. Please set your Hugging Face token manually.\")\n",
    "    else:\n",
    "        # Prompt for token if not set in environment\n",
    "        print(\"üîë Please enter your Hugging Face token:\")\n",
    "        # For Colab or local prompt input\n",
    "        HF_TOKEN = input(\"üîë Enter your Hugging Face token: \").strip()\n",
    "\n",
    "login(token=HF_TOKEN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b423735e",
   "metadata": {},
   "source": [
    "## Setup Kaggle Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409818e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def setup_kaggle_credentials():\n",
    "    kaggle_path = os.path.expanduser('~/.kaggle/kaggle.json')\n",
    "    if not os.path.exists(kaggle_path):\n",
    "        from google.colab import files\n",
    "        print(\"üìÇ Upload kaggle.json file...\")\n",
    "        uploaded = files.upload()\n",
    "        os.makedirs(os.path.dirname(kaggle_path), exist_ok=True)\n",
    "        for filename in uploaded.keys():\n",
    "            shutil.move(filename, kaggle_path)\n",
    "        os.chmod(kaggle_path, 0o600)\n",
    "        print(f\"‚úÖ Kaggle credentials setup at {kaggle_path}\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Kaggle credentials already exist at {kaggle_path}\")\n",
    "\n",
    "setup_kaggle_credentials()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d384b2f",
   "metadata": {},
   "source": [
    "## Mount Google Drive (Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1517ea98",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_running_in_colab():\n",
    "   from google.colab import drive\n",
    "   drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9964af3",
   "metadata": {},
   "source": [
    "##  Load Nous-Hermes-mistral-Instruct with Fallback to Quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3882e7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "import torch\n",
    "\n",
    "def load_model_pipeline(model_name: str, hf_token: str):\n",
    "    has_cuda = torch.cuda.is_available()\n",
    "    free_mem = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3) if has_cuda else 0\n",
    "    print(f\"üíª CUDA: {has_cuda} | GPU Memory: {free_mem:.2f} GB\")\n",
    "\n",
    "    device_map = {\"\": 0} if has_cuda else \"cpu\"\n",
    "    use_4bit = has_cuda and free_mem < 24\n",
    "\n",
    "    # Set quantization config\n",
    "    quant_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True if use_4bit else False,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    ) if use_4bit else None\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token, trust_remote_code=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # ‚úÖ Fix warning about pad_token\n",
    "\n",
    "    # Load model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device_map,\n",
    "        quantization_config=quant_config,\n",
    "        torch_dtype=torch.float16 if not quant_config else None,\n",
    "        trust_remote_code=True,\n",
    "        token=hf_token\n",
    "    )\n",
    "\n",
    "    print(f\"‚úÖ Model loaded on {next(model.parameters()).device}\")\n",
    "    return pipeline(\"text-generation\", model=model, tokenizer=tokenizer, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d505348b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_pipeline = load_model_pipeline(\n",
    "    model_name=\"NousResearch/Nous-Hermes-2-Mistral-7B-DPO\",\n",
    "    hf_token=HF_TOKEN\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739b4783",
   "metadata": {},
   "source": [
    "# Global utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10779c22",
   "metadata": {},
   "source": [
    "### Utility to merge normalized json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a472fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "def merge_json_files(\n",
    "    source_dir: Path,\n",
    "    output_file: Path,\n",
    "    pattern: str,\n",
    "    merged_dir: Path\n",
    "):\n",
    "    source_dir.mkdir(parents=True, exist_ok=True)\n",
    "    merged_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    merged_data = []\n",
    "\n",
    "    # Load existing output if it exists\n",
    "    if output_file.exists():\n",
    "        with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                merged_data = json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"‚ö†Ô∏è Could not decode {output_file}, starting from scratch.\")\n",
    "\n",
    "    # Identify matching files\n",
    "    files_to_merge = sorted(source_dir.glob(pattern))\n",
    "\n",
    "    for file_path in files_to_merge:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "                if isinstance(data, list):\n",
    "                    merged_data.extend(data)\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è Skipping {file_path.name}: not a list.\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Failed to parse {file_path.name}: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Move to merged folder\n",
    "        shutil.move(str(file_path), merged_dir / file_path.name)\n",
    "        print(f\"‚úÖ Merged and moved: {file_path.name}\")\n",
    "\n",
    "    # Write combined output\n",
    "    if merged_data:\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(merged_data, f, indent=2)\n",
    "        print(f\"üíæ Saved to: {output_file}\")\n",
    "    else:\n",
    "        print(\"üì≠ No valid data to merge.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a17bfc2",
   "metadata": {},
   "source": [
    "### Configurations  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b50e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# üõ† CONFIGURATION\n",
    "# ==============================\n",
    "\n",
    "class Config:\n",
    "    DATASET_DOWNLOAD_DIR = \"datasets\"\n",
    "    JSON_OUTPUT_DIR = \"json_outputs_run3\"\n",
    "    JSON_OUTPUT_NORMALIZED_DIR = \"json_outputs_run3/normalized\"\n",
    "    AUTO_CLEANUP = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f7f202",
   "metadata": {},
   "source": [
    "## Utility to save json to a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85be866c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "# üì¶ Save JSON Output with Safety\n",
    "def save_json_output(data, output_path: str, indent: int = 4, overwrite: bool = True):\n",
    "    output_dir = os.path.dirname(output_path)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    if os.path.exists(output_path):\n",
    "        if overwrite:\n",
    "            os.remove(output_path)\n",
    "        else:\n",
    "            raise FileExistsError(f\"File {output_path} already exists and overwrite=False.\")\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=indent, ensure_ascii=False)\n",
    "\n",
    "    print(f\"‚úÖ Saved output to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461b99a9",
   "metadata": {},
   "source": [
    "## Utility to load file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f349cd",
   "metadata": {},
   "source": [
    "### load_ndjson_file() (for resume/jd input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9445d7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "def load_ndjson_file(file_path: Path) -> List[dict]:\n",
    "    if not os.path.isfile(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return [json.loads(line) for line in file if line.strip()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1a0736",
   "metadata": {},
   "source": [
    "### load_json_file() (for checkpoint & metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b796868",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_file(file_path: Path) -> dict:\n",
    "    if not os.path.isfile(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a1b769",
   "metadata": {},
   "source": [
    "# Phase 2 -\tParse resume/JD into JSON structured scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9590a4d",
   "metadata": {},
   "source": [
    "### define utilities for schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fe5908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Optional\n",
    "from datetime import datetime, timezone\n",
    "from dateutil import parser\n",
    "\n",
    "\n",
    "def estimate_total_experience_years(\n",
    "    experiences: List[dict],\n",
    "    fallback_fields: Optional[List[str]] = None\n",
    ") -> float:\n",
    "\n",
    "    now = datetime.now(timezone.utc)\n",
    "    total_months = 0\n",
    "    date_ranges = []\n",
    "\n",
    "    for exp in experiences:\n",
    "        start_raw = (exp.get(\"start_date\") or \"\").strip()\n",
    "        end_raw = (exp.get(\"end_date\") or \"\").strip().lower()\n",
    "\n",
    "        try:\n",
    "            if not start_raw:\n",
    "                continue\n",
    "            start = parser.parse(start_raw, default=datetime(1900, 1, 1))\n",
    "            end = now if end_raw in {\"\", \"current\", \"present\"} else parser.parse(end_raw, default=now)\n",
    "            if end >= start:\n",
    "                date_ranges.append((start, end))\n",
    "        except Exception:\n",
    "            try:\n",
    "                months = int(exp.get(\"duration_in_months\", 0))\n",
    "                if months > 0:\n",
    "                    total_months += months\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "    date_ranges.sort()\n",
    "    merged = []\n",
    "    for start, end in date_ranges:\n",
    "        if not merged or start > merged[-1][1]:\n",
    "            merged.append([start, end])\n",
    "        else:\n",
    "            merged[-1][1] = max(merged[-1][1], end)\n",
    "\n",
    "    for start, end in merged:\n",
    "        months = (end.year - start.year) * 12 + (end.month - start.month)\n",
    "        total_months += max(0, months)\n",
    "\n",
    "    if total_months > 0:\n",
    "        return round(total_months / 12.0, 1)\n",
    "\n",
    "    if fallback_fields:\n",
    "        combined_text = \" \".join(fallback_fields).lower()\n",
    "        matches = re.findall(r\"(\\d{1,2})\\+?\\s+(?:years|yrs)\\s+(?:of\\s+)?experience\", combined_text)\n",
    "        if matches:\n",
    "            return max(int(m) for m in matches)\n",
    "\n",
    "    return 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0667a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Optional\n",
    "from dateutil import parser\n",
    "from datetime import datetime\n",
    "\n",
    "def postprocess_resume(resume: dict) -> dict:\n",
    "    \n",
    "    def months_between(start: datetime, end: datetime) -> int:\n",
    "        return (end.year - start.year) * 12 + (end.month - start.month)\n",
    "\n",
    "    def calculate_duration_in_months(start_date: str, end_date: str) -> Optional[int]:\n",
    "        try:\n",
    "            if not start_date.strip():\n",
    "                return None\n",
    "            start = parser.parse(start_date)\n",
    "            end = parser.parse(end_date) if end_date.strip().lower() not in {\"current\", \"present\", \"\"} else datetime.now()\n",
    "            return max(0, months_between(start, end))\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    def remove_duplicate_experiences(experiences: List[Dict]) -> List[Dict]:\n",
    "        seen = set()\n",
    "        result = []\n",
    "        for exp in experiences:\n",
    "            key = (\n",
    "                exp.get(\"job_title\", \"\").lower(),\n",
    "                exp.get(\"company\", \"\").lower(),\n",
    "                exp.get(\"start_date\", \"\"),\n",
    "                exp.get(\"end_date\", \"\")\n",
    "            )\n",
    "            if key not in seen:\n",
    "                seen.add(key)\n",
    "                result.append(exp)\n",
    "        return result\n",
    "\n",
    "    # üõ† Fill in missing durations\n",
    "    for exp in resume.get(\"experience\", []):\n",
    "        if exp.get(\"duration_in_months\") in [None, \"\"]:\n",
    "            exp[\"duration_in_months\"] = calculate_duration_in_months(exp.get(\"start_date\", \"\"), exp.get(\"end_date\", \"\"))\n",
    "\n",
    "    # üßπ Deduplicate experiences\n",
    "    resume[\"experience\"] = remove_duplicate_experiences(resume.get(\"experience\", []))\n",
    "\n",
    "    # üß† Recalculate total experience\n",
    "    resume[\"total_experience_years\"] = estimate_total_experience_years(resume.get(\"experience\", []))\n",
    "\n",
    "    return resume\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7917da",
   "metadata": {},
   "source": [
    "## Define Pydantic Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdbde38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Education(BaseModel):\n",
    "    degree: Optional[str] = None\n",
    "    field: Optional[str] = None\n",
    "    institution: Optional[str] = None\n",
    "    year: Optional[str] = None\n",
    "    gpa: Optional[float] = None\n",
    "\n",
    "class Experience(BaseModel):\n",
    "    job_title: Optional[str] = None\n",
    "    company: Optional[str] = None\n",
    "    start_date: Optional[str] = None\n",
    "    end_date: Optional[str] = None\n",
    "    duration_in_months: Optional[int] = None\n",
    "    description: Optional[List[str]] = None\n",
    "\n",
    "class Certification(BaseModel):\n",
    "    certification: Optional[str] = None\n",
    "    date_issued: Optional[str] = None\n",
    "\n",
    "class Project(BaseModel):\n",
    "    project_title: Optional[str] = None\n",
    "    description: Optional[str] = None\n",
    "    start_date: Optional[str] = None\n",
    "    end_date: Optional[str] = None\n",
    "    url: Optional[str] = None\n",
    "\n",
    "class Language(BaseModel):\n",
    "    language: Optional[str] = None\n",
    "    proficiency: Optional[str] = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fe7c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "from typing import List, Optional\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class ResumeSchema(BaseModel):\n",
    "    resume_id: Optional[str] = None\n",
    "    basics: Optional[dict] = None\n",
    "    summary: Optional[str] = None\n",
    "    education: Optional[List[dict]] = None\n",
    "    experience: Optional[List[dict]] = None\n",
    "    skills: Optional[List[str]] = None\n",
    "    certifications: Optional[List[dict]] = None\n",
    "    projects: Optional[List[dict]] = None\n",
    "    languages: Optional[List[dict]] = None\n",
    "    total_experience_years: Optional[float] = None\n",
    "\n",
    "    @classmethod\n",
    "    def normalize(cls, resume_dict: dict) -> dict:\n",
    "        resume_dict = dict(resume_dict)\n",
    "\n",
    "        # ‚úÖ Clean 'basics' only if valid\n",
    "        basics = resume_dict.get(\"basics\")\n",
    "        if isinstance(basics, dict):\n",
    "            location = basics.get(\"location\", \"\")\n",
    "            resume_dict[\"basics\"] = {\n",
    "                \"name\": basics.get(\"name\", \"\").strip() if basics.get(\"name\", \"\").strip().lower() != \"not visible\" else None,\n",
    "                \"email\": basics.get(\"email\", \"\").strip() if basics.get(\"email\", \"\").strip().lower() != \"not visible\" else None,\n",
    "                \"phone\": basics.get(\"phone\", \"\").strip() if basics.get(\"phone\", \"\").strip().lower() != \"not visible\" else None,\n",
    "                \"location\": \"\" if location.strip().lower() in {\"city, state\", \"not visible\"} else location.strip(),\n",
    "                \"current_title\": basics.get(\"current_title\", basics.get(\"title\", \"\")).strip(),\n",
    "                \"linkedin_url\": basics.get(\"linkedin_url\", \"\").strip() if basics.get(\"linkedin_url\", \"\").strip().lower() != \"not visible\" else None\n",
    "            }\n",
    "\n",
    "        # ‚úÖ Summary fallback\n",
    "        summary = resume_dict.get(\"summary\")\n",
    "        if summary is None:\n",
    "            resume_dict[\"summary\"] = \"\"\n",
    "        elif not summary.strip() and isinstance(resume_dict.get(\"text\"), str):\n",
    "            first_chunk = resume_dict[\"text\"].lower()\n",
    "            match = re.search(r\"(professional summary|summary|profile)\\s*[:\\-]?\\s*(.+?)(\\n{2,}|$)\", first_chunk, re.IGNORECASE | re.DOTALL)\n",
    "            if match:\n",
    "                resume_dict[\"summary\"] = match.group(2).strip()\n",
    "\n",
    "        # ‚úÖ Clean and deduplicate skills\n",
    "        skills_raw = resume_dict.get(\"skills\")\n",
    "        if skills_raw:\n",
    "            skills = set()\n",
    "            for skill in skills_raw:\n",
    "                if isinstance(skill, str):\n",
    "                    parts = re.split(r\"[,/;]| and | or \", skill)\n",
    "                    for part in parts:\n",
    "                        cleaned = part.strip().lower()\n",
    "                        if cleaned and 2 <= len(cleaned.split()) <= 6 and \"not visible\" not in cleaned:\n",
    "                            skills.add(cleaned)\n",
    "            resume_dict[\"skills\"] = sorted(skills)\n",
    "\n",
    "        # ‚úÖ Normalize languages\n",
    "        langs_raw = resume_dict.get(\"languages\")\n",
    "        if langs_raw:\n",
    "            langs = []\n",
    "            for lang in langs_raw:\n",
    "                if isinstance(lang, str) and lang.lower() != \"not visible\":\n",
    "                    langs.append({\"language\": lang.strip(), \"proficiency\": \"\"})\n",
    "                elif isinstance(lang, dict):\n",
    "                    language = lang.get(\"language\", \"\").strip()\n",
    "                    if language.lower() != \"not visible\":\n",
    "                        langs.append({\n",
    "                            \"language\": language,\n",
    "                            \"proficiency\": lang.get(\"proficiency\", \"\").strip()\n",
    "                        })\n",
    "            resume_dict[\"languages\"] = langs\n",
    "\n",
    "        # ‚úÖ Normalize certifications\n",
    "        certs_raw = resume_dict.get(\"certifications\")\n",
    "        if certs_raw:\n",
    "            certs = []\n",
    "            for cert in certs_raw:\n",
    "                if isinstance(cert, str) and cert.lower() != \"not visible\":\n",
    "                    certs.append({\"certification\": cert.strip()})\n",
    "                elif isinstance(cert, dict):\n",
    "                    name = cert.get(\"certification\", \"\").strip()\n",
    "                    if name and name.lower() != \"not visible\":\n",
    "                        certs.append({\n",
    "                            \"certification\": name,\n",
    "                            \"date_issued\": cert.get(\"date_issued\", \"\").strip() if isinstance(cert.get(\"date_issued\"), str) else None\n",
    "                        })\n",
    "            resume_dict[\"certifications\"] = certs\n",
    "\n",
    "        # ‚úÖ Normalize education\n",
    "        edu_raw = resume_dict.get(\"education\")\n",
    "        if edu_raw:\n",
    "            edu_clean = []\n",
    "            for edu in edu_raw:\n",
    "                if not isinstance(edu, dict):\n",
    "                    continue\n",
    "                degree = edu.get(\"degree\", \"\").strip()\n",
    "                field = edu.get(\"field\", \"\").strip()\n",
    "                institution = edu.get(\"institution\", \"\").strip()\n",
    "                year = str(edu.get(\"year\", \"\")).strip()\n",
    "\n",
    "                if not year:\n",
    "                    match_year = re.search(r\"\\b(19|20)\\d{2}\\b\", degree)\n",
    "                    if match_year:\n",
    "                        year = match_year.group(0)\n",
    "\n",
    "                try:\n",
    "                    gpa = float(edu.get(\"gpa\")) if edu.get(\"gpa\") not in [None, \"\"] else None\n",
    "                except (ValueError, TypeError):\n",
    "                    gpa = None\n",
    "\n",
    "                if degree.lower() != \"not visible\" and institution.lower() != \"not visible\":\n",
    "                    edu_clean.append({\n",
    "                        \"degree\": degree,\n",
    "                        \"field\": field,\n",
    "                        \"institution\": institution,\n",
    "                        \"year\": year,\n",
    "                        \"gpa\": gpa\n",
    "                    })\n",
    "            resume_dict[\"education\"] = edu_clean\n",
    "\n",
    "        # ‚úÖ Normalize experience\n",
    "        exp_raw = resume_dict.get(\"experience\")\n",
    "        if exp_raw:\n",
    "            seen_exp = set()\n",
    "            exp_clean = []\n",
    "            for exp in exp_raw:\n",
    "                if not isinstance(exp, dict):\n",
    "                    continue\n",
    "                title = exp.get(\"job_title\", exp.get(\"title\", \"\")).strip()\n",
    "                company = exp.get(\"company\", \"\").strip()\n",
    "                start = exp.get(\"start_date\", \"\").strip()\n",
    "                end = exp.get(\"end_date\", \"\").strip()\n",
    "                key = (title.lower(), company.lower(), start, end)\n",
    "                if key in seen_exp:\n",
    "                    continue\n",
    "                seen_exp.add(key)\n",
    "\n",
    "                description = exp.get(\"description\", [])\n",
    "                if isinstance(description, str):\n",
    "                    description = [line.strip() for line in re.split(r\"[‚Ä¢\\-\\n\\.]+\", description) if line.strip()]\n",
    "                elif not isinstance(description, list):\n",
    "                    description = []\n",
    "\n",
    "                exp_clean.append({\n",
    "                    \"job_title\": title,\n",
    "                    \"company\": company,\n",
    "                    \"start_date\": start,\n",
    "                    \"end_date\": end,\n",
    "                    \"duration_in_months\": int(exp[\"duration_in_months\"]) if isinstance(exp.get(\"duration_in_months\"), (int, float)) else None,\n",
    "                    \"description\": description\n",
    "                })\n",
    "            resume_dict[\"experience\"] = exp_clean\n",
    "\n",
    "        # ‚úÖ Clean projects if any\n",
    "        projects_raw = resume_dict.get(\"projects\")\n",
    "        if projects_raw:\n",
    "            resume_dict[\"projects\"] = [p for p in projects_raw if isinstance(p, dict)]\n",
    "\n",
    "        resume_dict = postprocess_resume(resume_dict)\n",
    "\n",
    "        return resume_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6df1c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pydantic import BaseModel, field_validator\n",
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "class JobDescriptionSchema(BaseModel):\n",
    "    jd_id: str\n",
    "    title: Optional[str] = \"\"\n",
    "    summary: Optional[str] = \"\"\n",
    "    required_experience_years: Optional[str] = None\n",
    "    preferred_degrees: Optional[List[str]] = []\n",
    "    required_skills: Optional[List[str]] = []\n",
    "    optional_skills: Optional[List[str]] = []\n",
    "    certifications: Optional[List[str]] = []\n",
    "    soft_skills: Optional[List[str]] = []\n",
    "    job_location: Optional[str] = \"\"\n",
    "    remote_option: Optional[bool] = False\n",
    "    employment_type: Optional[str] = None\n",
    "    inferred_domain: str = \"unknown\"\n",
    "    \n",
    "    @field_validator(\"required_experience_years\", mode=\"before\")\n",
    "    @classmethod\n",
    "    def convert_experience_to_string(cls, v):\n",
    "        if v is None:\n",
    "            return None\n",
    "        return str(v).strip()\n",
    "\n",
    "    @classmethod\n",
    "    def normalize(cls, jd_dict: dict) -> dict:\n",
    "        jd_dict = dict(jd_dict)\n",
    "\n",
    "        # ‚úÖ Clean jd_id only if it's a valid number-like string\n",
    "        jd_id = jd_dict.get(\"jd_id\") or jd_dict.get(\"job_id\")\n",
    "        if isinstance(jd_id, str) and jd_id.strip().isdigit():\n",
    "            jd_dict[\"jd_id\"] = jd_id.strip()\n",
    "        elif isinstance(jd_id, int):\n",
    "            jd_dict[\"jd_id\"] = str(jd_id)\n",
    "        else:\n",
    "            jd_dict.pop(\"jd_id\", None)\n",
    "\n",
    "        # ‚úÖ Normalize string fields with deduplication protection\n",
    "        for field in [\"title\", \"summary\", \"job_location\", \"employment_type\", \"inferred_domain\"]:\n",
    "            val = jd_dict.get(field)\n",
    "            if isinstance(val, str):\n",
    "                val = val.strip()\n",
    "                # drop if repeated e.g. \"marketing marketing\"\n",
    "                if len(val.split()) > 1 and val.lower().split()[0] == val.lower().split()[-1]:\n",
    "                    jd_dict.pop(field, None)\n",
    "                elif val:\n",
    "                    jd_dict[field] = val\n",
    "                else:\n",
    "                    jd_dict.pop(field, None)\n",
    "            else:\n",
    "                jd_dict.pop(field, None)\n",
    "\n",
    "        # ‚úÖ Deduplicate list fields, clean strings, drop if empty or invalid\n",
    "        for field in [\n",
    "            \"preferred_degrees\", \"required_skills\", \"optional_skills\", \"certifications\", \"soft_skills\"\n",
    "        ]:\n",
    "            value = jd_dict.get(field)\n",
    "            if isinstance(value, list):\n",
    "                cleaned = list(dict.fromkeys(v.strip() for v in value if isinstance(v, str) and v.strip()))\n",
    "                if cleaned:\n",
    "                    jd_dict[field] = cleaned\n",
    "                else:\n",
    "                    jd_dict.pop(field, None)\n",
    "            else:\n",
    "                jd_dict.pop(field, None)\n",
    "\n",
    "        # ‚úÖ Ensure remote_option is valid boolean\n",
    "        remote = jd_dict.get(\"remote_option\")\n",
    "        if not isinstance(remote, bool):\n",
    "            jd_dict.pop(\"remote_option\", None)\n",
    "\n",
    "        # ‚úÖ Normalize experience years with fallback from summary/title\n",
    "        val = jd_dict.get(\"required_experience_years\")\n",
    "        if val in [None, \"\", 0, 0.0]:\n",
    "            text = f\"{jd_dict.get('summary', '')} {jd_dict.get('title', '')}\".lower()\n",
    "            match = re.search(r\"(\\d{1,2})\\s*[-‚Äì]?\\s*(\\d{1,2})?\\s*(\\+)?\\s*(years|yrs)\", text)\n",
    "            if match:\n",
    "                jd_dict[\"required_experience_years\"] = float(match.group(1))\n",
    "            else:\n",
    "                jd_dict.pop(\"required_experience_years\", None)\n",
    "        else:\n",
    "            try:\n",
    "                jd_dict[\"required_experience_years\"] = float(val)\n",
    "            except Exception:\n",
    "                jd_dict.pop(\"required_experience_years\", None)\n",
    "\n",
    "        return jd_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3121578",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import get_origin, get_args, Union\n",
    "from pydantic import BaseModel\n",
    "\n",
    "def generate_example_structure(model_class) -> dict:\n",
    "    def default_for_type(field_type):\n",
    "        origin = get_origin(field_type)\n",
    "        args = get_args(field_type)\n",
    "\n",
    "        if origin is list:\n",
    "            return []\n",
    "        elif origin is Union and type(None) in args:\n",
    "            non_none_types = [arg for arg in args if arg is not type(None)]\n",
    "            return default_for_type(non_none_types[0]) if non_none_types else \"\"\n",
    "        elif field_type is str:\n",
    "            return \"\"\n",
    "        elif field_type in [float, int]:\n",
    "            return 0.0\n",
    "        elif isinstance(field_type, type) and issubclass(field_type, BaseModel):\n",
    "            return generate_example_structure(field_type)\n",
    "        else:\n",
    "            return \"\"\n",
    "\n",
    "    return {\n",
    "        field_name: default_for_type(field.annotation)\n",
    "        for field_name, field in model_class.model_fields.items()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caa93b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "import json\n",
    "from typing import Optional, Type\n",
    "\n",
    "@lru_cache(maxsize=4)\n",
    "def get_schema_str(schema_model: Optional[Type]) -> str:\n",
    "    \"\"\"\n",
    "    Returns a cached JSON schema string for a given Pydantic model.\n",
    "    Uses LRU cache to avoid recomputing the schema for every call.\n",
    "    \"\"\"\n",
    "    if schema_model is None:\n",
    "        return \"{}\"\n",
    "    example = generate_example_structure(schema_model)\n",
    "    return json.dumps(example, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4633d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Type\n",
    "import json\n",
    "import re\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Cache containers\n",
    "SCHEMA_STR_CACHE: Dict[str, str] = {}\n",
    "PROMPT_TEMPLATE_PARTS: Dict[str, tuple] = {}\n",
    "\n",
    "def cache_prompt_parts(name: str, template: str):\n",
    "    \"\"\"Split template into static parts around {text} placeholder.\"\"\"\n",
    "    if name not in PROMPT_TEMPLATE_PARTS:\n",
    "        parts = re.split(r\"{text}\", template)\n",
    "        if len(parts) != 2:\n",
    "            raise ValueError(f\"Invalid template: missing '{{text}}' placeholder ‚Üí {template}\")\n",
    "        PROMPT_TEMPLATE_PARTS[name] = (parts[0], parts[1])\n",
    "\n",
    "def render_prompt(name: str, text: str, schema_str: str):\n",
    "    \"\"\"Use cached prompt parts for fast prompt construction.\"\"\"\n",
    "    prefix, suffix = PROMPT_TEMPLATE_PARTS[name]\n",
    "    return f\"{prefix}{text}{suffix.replace('{schema}', schema_str)}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274f774b",
   "metadata": {},
   "source": [
    "##  Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbbed0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESUME_PROMPT_TEMPLATE = \"\"\"\n",
    "You are a JSON extractor for a partial resume chunk.\n",
    "\n",
    "Return a **VALID JSON object** with only fields that are **clearly visible and complete** in this chunk.\n",
    "\n",
    "{schema}\n",
    "\n",
    "‚ùå STRICTLY AVOID:\n",
    "- Fabricating or guessing\n",
    "- Assuming data from other chunks\n",
    "- Placeholder/fake/default values:\n",
    "  - \"John Doe\", \"City, State\", \"555-555-5555\", \"johndoe@email.com\", \"resume_1\", \"12345\"\n",
    "  - \"N/A\", \"Not Visible\", \"Unknown\"\n",
    "\n",
    "‚úÖ INCLUDE ONLY IF CLEARLY VISIBLE:\n",
    "- basics: Only valid fields like name, email, phone, title.\n",
    "- summary: Include 1‚Äì3 sentences near the top (within first 30 lines) that describe the candidate‚Äôs background, qualifications, or career focus. Accept even if not labeled ‚ÄúSummary‚Äù. Skip if it‚Äôs clearly just a skill list or job bullet.\n",
    "- experience: Include jobs only if job_title and company are present AND at least one of start_date, end_date, or a readable date string (e.g., \"Aug 2013 ‚Äì Current\") is present. Description must be bullet-like lines.\n",
    "- education: Include only if degree, field, and institution are present. Year is optional.\n",
    "- skills: Real tools/tech only (1‚Äì3 words), no traits/fragments.\n",
    "- certifications: Named credentials only (e.g., ‚ÄúCPA‚Äù).\n",
    "- languages: Real languages only (e.g., ‚ÄúEnglish‚Äù, ‚ÄúSpanish‚Äù).\n",
    "- total_experience_years: Only if 2+ valid experience entries with job_title, company, and at least one date are visible.\n",
    "\n",
    "‚ö†Ô∏è If no valid fields are visible, return: {{}}\n",
    "‚ö†Ô∏è Output ONLY valid JSON (no markdown, explanation, or fallback placeholders)\n",
    "\n",
    "====================\n",
    "\\\"\\\"\\\"{text}\\\"\\\"\\\"\n",
    "====================\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bcae69",
   "metadata": {},
   "outputs": [],
   "source": [
    "JD_PROMPT_TEMPLATE = \"\"\"\n",
    "You are a structured JSON parser working on **partial job descriptions** (chunked input).\n",
    "Each chunk may contain incomplete or partial data ‚Äî only return fields that are fully visible and valid in the current chunk.\n",
    "\n",
    "Extract a single valid JSON object matching this schema:\n",
    "{schema}\n",
    "\n",
    "Instructions:\n",
    "- ONLY include a field if you clearly see its valid value in the current chunk. Do NOT fabricate or repeat fields from earlier chunks.\n",
    "- If a field like \"jd_id\", \"job_location\", or \"employment_type\" is not present in this chunk, leave it out entirely.\n",
    "- NEVER guess values or default them unless explicitly instructed.\n",
    "\n",
    "Field Guidance:\n",
    "- \"jd_id\": Use only the exact job_id visible in the current chunk (e.g. 3899527256). Do NOT guess or reuse.\n",
    "- \"title\": Use job title if available. No repetition or combining with roles.\n",
    "- \"summary\": First 1‚Äì2 sentences introducing the role.\n",
    "- \"required_experience_years\":\n",
    "  ‚Ä¢ Extract from phrases like \"3‚Äì5 years\", \"5+ years\", etc.\n",
    "  ‚Ä¢ Use the **lower value** if range.\n",
    "  ‚Ä¢ If not found but title has \"Senior\", \"Mid\", or \"Junior\", infer as 5, 3, or 0.\n",
    "  ‚Ä¢ Otherwise, do NOT include this field.\n",
    "\n",
    "- \"preferred_degrees\": Only if clearly stated (e.g. \"Bachelor‚Äôs in Marketing\").\n",
    "- \"optional_skills\": If marked as preferred or nice-to-have.\n",
    "- \"certifications\": Only named ones like \"PMP\", \"Google Ads Certified\".\n",
    "- \"job_location\": Only if explicitly stated in the chunk.\n",
    "- \"remote_option\": true if this chunk says \"remote\", \"WFH\", \"hybrid\"; else leave it out.\n",
    "- \"employment_type\": Use only if visible (e.g. \"full-time\", \"contract\").\n",
    "- \"inferred_domain\": Return a **single domain noun** like \"marketing\", \"software\", \"finance\". Never guess if unsure.\n",
    "- \"required_skills\":\n",
    "  ‚Ä¢ Short verb‚Äìnoun skills like \"build provider relationships\", \"conduct territory planning\". \n",
    "  ‚Ä¢ Include only multi-word task phrases, known tools (e.g. \"TikTok\", \"Adobe Suite\"), or specific techniques.\n",
    "  ‚Ä¢ Do NOT include vague one-word entries like \"developing\", \"implementing\", \"clients\", \"platforms\", or \"content\" unless they're clearly paired with an action.\n",
    "  ‚Ä¢ Avoid full sentences or overly broad/general phrases.\n",
    "\n",
    "- \"soft_skills\":\n",
    "  ‚Ä¢ Only extract actual personality or behavior traits (e.g., \"team player\", \"detail-oriented\").\n",
    "  ‚Ä¢ Do NOT infer from vague phrases like \"maintain relationships\" or \"excellent communication\".\n",
    "  \n",
    "- Skip any hallucinated domain-generic skills like \"optimize ad performance\" unless chunk explicitly includes it.\n",
    "\n",
    "\n",
    "Output Rules:\n",
    "- Return ONLY a valid JSON object. No markdown, comments, labels, or extra text.\n",
    "- Output must begin with {{ and end with }}.\n",
    "\n",
    "====================\n",
    "{text}\n",
    "====================\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeb4f67",
   "metadata": {},
   "source": [
    "##  Inference + Validation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d521772",
   "metadata": {},
   "source": [
    "### Generate Raw LLM Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef16308",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_llm_output(prompt: str, max_new_tokens: int = 1024) -> str:\n",
    "    \"\"\"\n",
    "    Run LLM using generate() and return decoded output.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        inputs = llm_pipeline.tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(llm_pipeline.model.device)\n",
    "        generated_ids = llm_pipeline.model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            pad_token_id=llm_pipeline.tokenizer.eos_token_id,\n",
    "            do_sample=False\n",
    "        )\n",
    "        return llm_pipeline.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"LLM generation failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94efd107",
   "metadata": {},
   "source": [
    "### Sanitize Output: Strip Prompt, Fix Cutoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc2466d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def clean_json_string(raw: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans LLM output to extract and sanitize the most likely valid JSON string.\n",
    "    - Removes ASCII and Unicode control characters\n",
    "    - Normalizes smart quotes, dashes, BOM, and other confusables\n",
    "    - Extracts the largest balanced {...} or [...] block\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Replace invisible ASCII control characters\n",
    "    raw = re.sub(r\"[\\x00-\\x1F\\x7F]\", \" \", raw)\n",
    "\n",
    "    # 2. Strip BOM and normalize typographic characters\n",
    "    raw = raw.replace(\"\\ufeff\", \"\")\n",
    "\n",
    "    replacements = {\n",
    "        \"‚Äú\": '\"', \"‚Äù\": '\"', \"‚Äò\": \"'\", \"‚Äô\": \"'\", \"¬¥\": \"'\",\n",
    "        \"¬´\": '\"', \"¬ª\": '\"', \"‚Äê\": \"-\", \"‚Äì\": \"-\", \"‚Äî\": \"-\",\n",
    "        \"‚Ä¶\": \"...\", \"\\u00a0\": \" \"  # non-breaking space\n",
    "    }\n",
    "    for bad, good in replacements.items():\n",
    "        raw = raw.replace(bad, good)\n",
    "\n",
    "    # 3. Remove any remaining control Unicode categories (Cc, Cf, etc.)\n",
    "    raw = \"\".join(c for c in raw if unicodedata.category(c)[0] != \"C\")\n",
    "\n",
    "    # 4. Extract best-matching balanced JSON candidate\n",
    "    start_curly = raw.find(\"{\")\n",
    "    start_square = raw.find(\"[\")\n",
    "    end_curly = raw.rfind(\"}\")\n",
    "    end_square = raw.rfind(\"]\")\n",
    "\n",
    "    candidates = []\n",
    "    if 0 <= start_curly < end_curly:\n",
    "        candidates.append(raw[start_curly:end_curly + 1])\n",
    "    if 0 <= start_square < end_square:\n",
    "        candidates.append(raw[start_square:end_square + 1])\n",
    "\n",
    "    for candidate in candidates:\n",
    "        if is_brace_balanced(candidate):\n",
    "            return candidate.strip()\n",
    "\n",
    "    # Fallback: return whole cleaned string\n",
    "    return raw.strip()\n",
    "\n",
    "def is_brace_balanced(s: str) -> bool:\n",
    "    \"\"\"\n",
    "    Returns True if the string has balanced {} and [] braces.\n",
    "    \"\"\"\n",
    "    stack = []\n",
    "    pair = {']': '[', '}': '{'}\n",
    "    for c in s:\n",
    "        if c in \"{[\":\n",
    "            stack.append(c)\n",
    "        elif c in \"}]\" and (not stack or stack[-1] != pair[c]):\n",
    "            return False\n",
    "        elif c in \"}]\" and stack:\n",
    "            stack.pop()\n",
    "    return not stack\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0018aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_llm_output(raw_output: str, prompt: str = \"\") -> str:\n",
    "    \"\"\"\n",
    "    Removes echoed prompt and trims raw output to valid JSON block.\n",
    "    Then performs cleaning and balancing.\n",
    "    \"\"\"\n",
    "    # 1. Remove prompt echo\n",
    "    cleaned = raw_output.replace(prompt, \"\").strip()\n",
    "\n",
    "    # 2. Trim to first {...} block\n",
    "    json_start = cleaned.find(\"{\")\n",
    "    if json_start != -1:\n",
    "        cleaned = cleaned[json_start:]\n",
    "        last_brace = cleaned.rfind(\"}\")\n",
    "        if last_brace != -1:\n",
    "            cleaned = cleaned[:last_brace + 1]\n",
    "\n",
    "    # 3. Final sanitization\n",
    "    return clean_json_string(cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8d3f67",
   "metadata": {},
   "source": [
    "### Regex-based JSON Block Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abedf72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_fix_fallback(raw_output: str) -> Optional[dict]:\n",
    "    \"\"\"\n",
    "    Best-effort fallback to recover a JSON object from malformed text.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cleaned = raw_output.strip()\n",
    "\n",
    "        json_start = cleaned.find(\"{\")\n",
    "        if json_start != -1:\n",
    "            cleaned = cleaned[json_start:]\n",
    "\n",
    "        last_brace = cleaned.rfind(\"}\")\n",
    "        if last_brace != -1:\n",
    "            cleaned = cleaned[:last_brace+1]\n",
    "\n",
    "        return json.loads(cleaned)\n",
    "    except Exception:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb189464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "import json5\n",
    "\n",
    "def extract_json_block(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts the first valid JSON object from a text using the `regex` module and parses with `json5`.\n",
    "    This is more robust than standard `json` and can handle trailing commas, comments, etc.\n",
    "    \"\"\"\n",
    "    # Recursive regex pattern to find balanced curly braces (non-greedy)\n",
    "    pattern = r'(\\{(?:[^{}]|(?R))*\\})'\n",
    "\n",
    "    for match in regex.finditer(pattern, text, flags=regex.DOTALL):\n",
    "        json_candidate = match.group(1)\n",
    "        try:\n",
    "            return json5.loads(json_candidate)\n",
    "        except json5.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "    raise ValueError(\"‚ùå No valid JSON object found using regex and json5.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0de79d7",
   "metadata": {},
   "source": [
    "### Final Orchestrator: Fault-Tolerant Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd5b4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(text: str) -> int:\n",
    "    return len(text.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f422921b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List\n",
    "\n",
    "def split_resume_text_into_chunks(\n",
    "    text: str,\n",
    "    chunk_size: int = 1500,\n",
    "    overlap: int = 250,\n",
    "    min_chunk_size: int = 800,\n",
    "    max_sentences_per_chunk: int = 40\n",
    ") -> List[str]:\n",
    "    SECTION_HEADERS = [\n",
    "        \"Skills\", \"Work History\", \"Experience\", \"Education\", \"Certifications\",\n",
    "        \"Projects\", \"Professional Summary\", \"Summary\", \"Highlights\"\n",
    "    ]\n",
    "\n",
    "    sentences = re.split(r'(?<=[.?!])\\s+', text.strip())\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    sentence_count = 0\n",
    "\n",
    "    def add_chunk(chunk: str):\n",
    "        if chunk and len(chunk.strip()) >= min_chunk_size:\n",
    "            chunks.append(chunk.strip())\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if any(sentence.startswith(header) for header in SECTION_HEADERS):\n",
    "            add_chunk(current_chunk)\n",
    "            current_chunk = sentence\n",
    "            sentence_count = 1\n",
    "            continue\n",
    "\n",
    "        if len(current_chunk) + len(sentence) + 1 > chunk_size or sentence_count >= max_sentences_per_chunk:\n",
    "            add_chunk(current_chunk)\n",
    "            current_chunk = sentence\n",
    "            sentence_count = 1\n",
    "        else:\n",
    "            current_chunk = f\"{current_chunk} {sentence}\".strip()\n",
    "            sentence_count += 1\n",
    "\n",
    "    add_chunk(current_chunk)\n",
    "\n",
    "    # Overlap\n",
    "    if overlap > 0 and len(chunks) > 1:\n",
    "        return [\n",
    "            f\"{chunks[i - 1][-overlap:]} {chunks[i]}\".strip() if i > 0 else chunks[i]\n",
    "            for i in range(len(chunks))\n",
    "        ]\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f40df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List\n",
    "\n",
    "def split_jd_text_into_chunks(text: str, chunk_size: int = 1000, overlap: int = 200) -> List[str]:\n",
    "    \"\"\"\n",
    "    Splits JD text into coherent chunks based on line breaks and block sections.\n",
    "    Uses character length, not sentence splitting.\n",
    "    \"\"\"\n",
    "    lines = [line.strip() for line in text.strip().splitlines() if line.strip()]\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    for line in lines:\n",
    "        if len(current_chunk) + len(line) + 1 > chunk_size:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            # Start next chunk with last `overlap` chars from current\n",
    "            current_chunk = current_chunk[-overlap:] + \"\\n\" + line\n",
    "        else:\n",
    "            current_chunk += \"\\n\" + line\n",
    "\n",
    "    if current_chunk.strip():\n",
    "        chunks.append(current_chunk.strip())\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8321354",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "from collections import OrderedDict\n",
    "from copy import deepcopy\n",
    "\n",
    "def merge_jsons_resume(json_list: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    FIELD_ORDER = [\n",
    "        \"resume_id\", \"basics\", \"summary\", \"education\", \"experience\",\n",
    "        \"skills\", \"certifications\", \"projects\", \"languages\", \"total_experience_years\"\n",
    "    ]\n",
    "    dedupe_list_keys = {\"skills\", \"certifications\", \"projects\", \"languages\"}\n",
    "    preserve_keys = {\"resume_id\"}\n",
    "    skip_values = {\n",
    "        \"\", \"not visible\", \"n/a\", \"unknown\", \"city, state\",\n",
    "        \"john doe\", \"linkedin.com/in/johndoe\", \"johndoe@email.com\", \"555-555-5555\", \"et\"\n",
    "    }\n",
    "\n",
    "    def is_clean(val: Any) -> bool:\n",
    "        return isinstance(val, str) and val.strip().lower() not in skip_values\n",
    "\n",
    "    def try_extract_summary(text: str) -> str:\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        lines = text.strip().splitlines()\n",
    "        candidate_lines = lines[:30]\n",
    "        blob = \" \".join(line.strip() for line in candidate_lines if line.strip())\n",
    "        sentences = [s.strip() for s in blob.split(\".\") if len(s.strip().split()) > 6]\n",
    "        if 1 <= len(sentences) <= 3:\n",
    "            return \". \".join(sentences) + \".\"\n",
    "        return \"\"\n",
    "\n",
    "    merged: Dict[str, Any] = {}\n",
    "    seen_edu = set()\n",
    "    seen_exp = set()\n",
    "\n",
    "    sorted_parts = sorted(json_list, key=lambda x: x.get(\"_chunk_index\", 0))\n",
    "\n",
    "    for part in sorted_parts:\n",
    "        for key, value in part.items():\n",
    "            if key == \"_chunk_index\":\n",
    "                continue\n",
    "\n",
    "            if key in preserve_keys:\n",
    "                merged.setdefault(key, deepcopy(value))\n",
    "\n",
    "            elif key == \"summary\":\n",
    "                if is_clean(value) and not merged.get(\"summary\"):\n",
    "                    merged[\"summary\"] = value.strip()\n",
    "\n",
    "            elif key == \"basics\":\n",
    "                if isinstance(value, dict):\n",
    "                    merged.setdefault(\"basics\", {})\n",
    "                    for k, v in value.items():\n",
    "                        if is_clean(v) and not merged[\"basics\"].get(k):\n",
    "                            merged[\"basics\"][k] = v.strip()\n",
    "\n",
    "            elif key == \"education\":\n",
    "                if isinstance(value, list):\n",
    "                    merged.setdefault(\"education\", [])\n",
    "                    for item in value:\n",
    "                        if isinstance(item, dict):\n",
    "                            degree = item.get(\"degree\", \"\").strip().lower()\n",
    "                            field = item.get(\"field\", \"\").strip().lower()\n",
    "                            institution = item.get(\"institution\", \"\").strip().lower()\n",
    "                            if not degree or not field or not institution:\n",
    "                                continue\n",
    "                            edu_key = (degree, field, institution)\n",
    "                            if edu_key not in seen_edu:\n",
    "                                seen_edu.add(edu_key)\n",
    "                                merged[\"education\"].append(deepcopy(item))\n",
    "\n",
    "            elif key == \"experience\":\n",
    "                if isinstance(value, list):\n",
    "                    merged.setdefault(\"experience\", [])\n",
    "                    for exp in value:\n",
    "                        if isinstance(exp, dict):\n",
    "                            title = exp.get(\"job_title\", \"\").strip()\n",
    "                            company = exp.get(\"company\", \"\").strip()\n",
    "                            start = exp.get(\"start_date\", \"\").strip()\n",
    "                            end = exp.get(\"end_date\", \"\").strip()\n",
    "                            if not (title and company and (start or end)):\n",
    "                                continue\n",
    "                            exp_key = (title.lower(), company.lower(), start, end)\n",
    "                            if exp_key not in seen_exp:\n",
    "                                seen_exp.add(exp_key)\n",
    "                                merged[\"experience\"].append(deepcopy(exp))\n",
    "\n",
    "            elif key in dedupe_list_keys:\n",
    "                if isinstance(value, list):\n",
    "                    merged.setdefault(key, [])\n",
    "                    for item in value:\n",
    "                        if isinstance(item, dict):\n",
    "                            if all(not is_clean(str(v)) for v in item.values()):\n",
    "                                continue\n",
    "                            merged[key].append(deepcopy(item))\n",
    "                        elif isinstance(item, str) and is_clean(item):\n",
    "                            merged[key].append(item.strip())\n",
    "\n",
    "            elif key == \"text\":\n",
    "                # Fallback for summary from top of text\n",
    "                if \"summary\" not in merged:\n",
    "                    inferred = try_extract_summary(value)\n",
    "                    if is_clean(inferred):\n",
    "                        merged[\"summary\"] = inferred\n",
    "\n",
    "            elif isinstance(value, (str, int, float)) and is_clean(str(value)) and not merged.get(key):\n",
    "                merged[key] = deepcopy(value)\n",
    "\n",
    "    # Deduplicate flat list fields like skills\n",
    "    for key in dedupe_list_keys:\n",
    "        if key in merged and isinstance(merged[key], list):\n",
    "            seen = set()\n",
    "            deduped = []\n",
    "            for item in merged[key]:\n",
    "                fingerprint = str(item).strip().lower()\n",
    "                if fingerprint and fingerprint not in seen:\n",
    "                    seen.add(fingerprint)\n",
    "                    deduped.append(item)\n",
    "            merged[key] = deduped\n",
    "\n",
    "    # Ensure all required fields are present in correct order\n",
    "    ordered = OrderedDict()\n",
    "    for field in FIELD_ORDER:\n",
    "        default = [] if field in {\"education\", \"experience\", \"skills\", \"certifications\", \"projects\", \"languages\"} else \"\"\n",
    "        ordered[field] = merged.get(field, deepcopy(default))\n",
    "\n",
    "    return ordered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9653d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "GENERIC_PLACEHOLDER_VALUES = {\n",
    "    \"not visible\", \"n/a\", \"unknown\", \"city, state\", \"resume_1\", \"resume_123\", \"resume123\",\n",
    "    \"john doe\", \"jane smith\", \"linkedin.com/in/johndoe\", \"linkedin.com/in/example\",\n",
    "    \"johndoe@email.com\", \"johndoe@example.com\", \"555-555-5555\", \"123-456-7890\"\n",
    "}\n",
    "\n",
    "def is_placeholder(value: str) -> bool:\n",
    "    val = value.strip().lower()\n",
    "    return (\n",
    "        val in GENERIC_PLACEHOLDER_VALUES\n",
    "        or val.endswith(\"@example.com\") or val.endswith(\"@email.com\")\n",
    "        or re.match(r\"^resume_\\d+$\", val)\n",
    "        or re.match(r\"^[a-z0-9\\-]{36}$\", val)  # UUID pattern\n",
    "    )\n",
    "\n",
    "def cleanup_resume_json(resume: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    # üîπ basics field\n",
    "    if \"basics\" in resume and isinstance(resume[\"basics\"], dict):\n",
    "        resume[\"basics\"] = {\n",
    "            k: v for k, v in resume[\"basics\"].items()\n",
    "            if isinstance(v, str) and v.strip() and not is_placeholder(v)\n",
    "        }\n",
    "        if not resume[\"basics\"]:\n",
    "            del resume[\"basics\"]\n",
    "\n",
    "    # üîπ summary\n",
    "    if \"summary\" in resume and (\n",
    "        not isinstance(resume[\"summary\"], str)\n",
    "        or is_placeholder(resume[\"summary\"])\n",
    "        or re.search(r\"\\b\\d+\\+?\\s+years?\\s+of\\s+experience\\b\", resume[\"summary\"].lower())\n",
    "    ):\n",
    "        resume.pop(\"summary\", None)\n",
    "\n",
    "    # üîπ skills\n",
    "    if \"skills\" in resume:\n",
    "        cleaned = set()\n",
    "        for s in resume[\"skills\"]:\n",
    "            if isinstance(s, str):\n",
    "                val = s.strip().lower()\n",
    "                if val and not is_placeholder(val) and 2 <= len(val.split()) <= 6:\n",
    "                    cleaned.add(val)\n",
    "        resume[\"skills\"] = sorted(cleaned)\n",
    "        if not resume[\"skills\"]:\n",
    "            resume.pop(\"skills\", None)\n",
    "\n",
    "    # üîπ experience\n",
    "    if \"experience\" in resume:\n",
    "        cleaned_exp = []\n",
    "        for exp in resume[\"experience\"]:\n",
    "            if isinstance(exp, dict):\n",
    "                if all(\n",
    "                    not is_placeholder(str(exp.get(field, \"\")).lower())\n",
    "                    for field in [\"job_title\", \"company\"]\n",
    "                ):\n",
    "                    cleaned_exp.append(exp)\n",
    "        resume[\"experience\"] = cleaned_exp\n",
    "        if not resume[\"experience\"]:\n",
    "            resume.pop(\"experience\", None)\n",
    "\n",
    "    # üîπ certifications\n",
    "    if \"certifications\" in resume:\n",
    "        resume[\"certifications\"] = [\n",
    "            c for c in resume[\"certifications\"]\n",
    "            if isinstance(c, dict) and not is_placeholder(c.get(\"certification\", \"\"))\n",
    "        ]\n",
    "        if not resume[\"certifications\"]:\n",
    "            resume.pop(\"certifications\", None)\n",
    "\n",
    "    # üîπ total_experience_years\n",
    "    if \"total_experience_years\" in resume:\n",
    "        if not isinstance(resume[\"total_experience_years\"], (int, float)) or resume[\"total_experience_years\"] == 0:\n",
    "            resume.pop(\"total_experience_years\", None)\n",
    "\n",
    "    return resume\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc86093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_jd_chunk(chunk: dict) -> bool:\n",
    "    if not isinstance(chunk, dict):\n",
    "        return False\n",
    "    if \"title\" not in chunk and \"summary\" not in chunk and \"required_skills\" not in chunk:\n",
    "        return False\n",
    "    if \"You are a\" in str(chunk) or \"schema\" in str(chunk):\n",
    "        return False\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9547adff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "import copy\n",
    "\n",
    "def merge_jsons_jd(json_list: List[Dict]) -> Dict:\n",
    "    merged = {}\n",
    "    \n",
    "    preserve_keys = {\"jd_id\"}\n",
    "    replace_once_keys = {\"job_location\", \"employment_type\", \"inferred_domain\", \"summary\", \"title\"}\n",
    "    dedupe_list_keys = {\n",
    "        \"preferred_degrees\", \"required_skills\", \"optional_skills\", \"certifications\", \"soft_skills\"\n",
    "    }\n",
    "\n",
    "    for part in json_list:\n",
    "        if not is_valid_jd_chunk(part):\n",
    "            continue\n",
    "\n",
    "        for key, value in part.items():\n",
    "            if key in preserve_keys:\n",
    "                merged.setdefault(key, copy.deepcopy(value))\n",
    "\n",
    "            elif key in replace_once_keys:\n",
    "                if key not in merged and isinstance(value, str) and value.strip():\n",
    "                    merged[key] = value.strip()\n",
    "\n",
    "            elif isinstance(value, list):\n",
    "                merged.setdefault(key, []).extend(copy.deepcopy(value))\n",
    "\n",
    "            elif key not in merged:\n",
    "                merged[key] = copy.deepcopy(value)\n",
    "\n",
    "    # Deduplicate lists\n",
    "    for key in dedupe_list_keys:\n",
    "        if key in merged and isinstance(merged[key], list):\n",
    "            merged[key] = list(dict.fromkeys(\n",
    "                [v.strip() for v in merged[key] if isinstance(v, str) and v.strip()]\n",
    "            ))\n",
    "\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea617359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_jsons(json_list: List[Dict], is_resume: bool) -> Dict:\n",
    "    return merge_jsons_resume(json_list) if is_resume else merge_jsons_jd(json_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c8f52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_text_json_pair(output_dir: str, record_id: str, text: str, parsed_json: Dict, is_resume=True):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    fname = f\"{record_id}_{'resume' if is_resume else 'jd'}_pair.json\"\n",
    "    with open(Path(output_dir) / fname, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\n",
    "            \"text\": text,\n",
    "            \"parsed_json\": parsed_json\n",
    "        }, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306296e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from typing import Dict, Optional, Type\n",
    "\n",
    "def inject_ids(parsed: Dict, schema_model: Optional[Type]) -> Dict:\n",
    "    \"\"\"\n",
    "    Injects a UUID as `resume_id` or `jd_id` based on the schema model name.\n",
    "    \"\"\"\n",
    "    if not schema_model:\n",
    "        print(\"‚ö†Ô∏è No schema model provided for ID injection.\")\n",
    "        return parsed\n",
    "    schema_name = schema_model.__name__.lower()\n",
    "    if schema_name.startswith(\"resume\"):\n",
    "        parsed[\"resume_id\"] = str(uuid.uuid4())\n",
    "    elif schema_name.startswith(\"jobdescription\") and not parsed.get(\"jd_id\", \"\").strip():\n",
    "        parsed[\"jd_id\"] = str(uuid.uuid4())\n",
    "    return parsed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94558974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "def extract_structured_json(\n",
    "    text: str,\n",
    "    prompt_template: str,\n",
    "    schema_model: Optional[type] = None,\n",
    "    max_new_tokens: int = 1024, \n",
    "    validate: bool = True,\n",
    "    is_resume: bool = True,\n",
    "    chunk_size: int = 1000,     # üîß Larger chunks\n",
    "    overlap: int = 200,\n",
    "    record_id: Optional[str] = None \n",
    ") -> dict:\n",
    "\n",
    "    # ‚úÖ Retrieve cached schema string\n",
    "    schema_str = get_schema_str(schema_model)\n",
    "\n",
    "    # ‚úÖ Choose chunking strategy\n",
    "    if is_resume:\n",
    "        chunks = split_resume_text_into_chunks(text, chunk_size=1500, overlap=overlap)\n",
    "    else:\n",
    "        chunks = split_jd_text_into_chunks(text, chunk_size=chunk_size, overlap=overlap)\n",
    "    merged_result = {}\n",
    "    raw_output = \"\"\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        prompt = prompt_template.format(text=chunk, schema=schema_str)\n",
    "        print(f\"üî¢ Chunk {i+1} ({record_id or 'no-id'}): {len(prompt.split())} tokens\")\n",
    "\n",
    "        try:\n",
    "            response = generate_llm_output(prompt, max_new_tokens=max_new_tokens)\n",
    "            raw_output = sanitize_llm_output(response, prompt)\n",
    "            print(f\"üîç Chunk {i+1} raw output:\\n{raw_output}\")\n",
    "            \n",
    "            # ‚úÖ Robust guard: skip if output is empty or just repeats prompt\n",
    "            if not raw_output.strip() or raw_output.strip() == \"{}\" or raw_output.strip().lower().startswith(\"you are a structured\"):\n",
    "                print(f\"üß™ Chunk {i+1} produced empty output:\\nPrompt:\\n{prompt}\\nResponse:\\n{response}\")\n",
    "                raise ValueError(\"Echoed prompt or empty response\")\n",
    "\n",
    "            parsed = json.loads(raw_output)\n",
    "            merged_result = merge_jsons([merged_result, parsed], is_resume=is_resume)\n",
    "            if is_resume:\n",
    "                merged_result = cleanup_resume_json(merged_result)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Chunk {i+1}: Failed to parse ‚Äì {e}\")\n",
    "            print(\"üß™ Raw output :\\n\", raw_output)\n",
    "            traceback.print_exc(limit=4)\n",
    "\n",
    "            # üîÅ Try JSON fix fallback first\n",
    "            fallback = json_fix_fallback(raw_output)\n",
    "            if fallback:\n",
    "                print(f\"‚úÖ Chunk {i+1}: Fallback JSON recovery successful.\")\n",
    "                merged_result = merge_jsons([merged_result, fallback], is_resume=is_resume)\n",
    "                continue\n",
    "\n",
    "            \n",
    "\n",
    "    if not merged_result:\n",
    "        print(\"‚ö†Ô∏è All chunks failed. Attempting final fallback.\")\n",
    "        try:\n",
    "            fallback = extract_json_block(raw_output)\n",
    "            merged_result = fallback\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"raw_output\": raw_output.strip(),\n",
    "                \"error\": f\"Regex fallback failed: {e}\"\n",
    "            }\n",
    "\n",
    "    merged_result = inject_ids(merged_result, schema_model)\n",
    "\n",
    "    if validate and schema_model:\n",
    "        if hasattr(schema_model, \"normalize\"):\n",
    "            merged_result = schema_model.normalize(merged_result)\n",
    "        schema_model.model_validate(merged_result)\n",
    "\n",
    "    return merged_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f8e0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import ValidationError\n",
    "\n",
    "def pydantic_validate(model_class, data):\n",
    "    \"\"\"\n",
    "    Version-safe validator that supports both Pydantic v1 and v2.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Pydantic v2\n",
    "        return model_class.model_validate(data)\n",
    "    except AttributeError:\n",
    "        # Fallback to Pydantic v1\n",
    "        return model_class.parse_obj(data)\n",
    "\n",
    "\n",
    "def validate_entry(entry, is_resume):\n",
    "    try:\n",
    "        model = ResumeSchema if is_resume else JobDescriptionSchema\n",
    "        if hasattr(model, \"normalize\"):\n",
    "            normalized = model.normalize(entry)\n",
    "        else:\n",
    "            normalized = entry\n",
    "        pydantic_validate(model, normalized)\n",
    "        return True, None\n",
    "    except ValidationError as ve:\n",
    "        return False, str(ve)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143847db",
   "metadata": {},
   "source": [
    "##  Normalize in Batches with Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7325133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_metadata_summary(\n",
    "    output_dir: Path,\n",
    "    is_resume: bool,\n",
    "    input_file: str,\n",
    "    total_records: int,\n",
    "    total_valid: int,\n",
    "    total_invalid: int,\n",
    "    start_index: int,\n",
    "    end_index: int,\n",
    "    timestamp: str,\n",
    "    batch_id: str\n",
    "):\n",
    "    summary = {\n",
    "        \"batch_id\": batch_id,\n",
    "        \"timestamp\": timestamp,\n",
    "        \"input_file\": input_file,\n",
    "        \"input_type\": \"resume\" if is_resume else \"job_description\",\n",
    "        \"records_start_index\": start_index,\n",
    "        \"records_end_index\": end_index,\n",
    "        \"records_total\": total_records,\n",
    "        \"records_valid\": total_valid,\n",
    "        \"records_invalid\": total_invalid,\n",
    "        \"output_dir\": str(output_dir)\n",
    "    }\n",
    "    summary_file = output_dir / f\"meta_{'resumes' if is_resume else 'jds'}_{start_index}_{end_index}_{timestamp}_{batch_id}.json\"\n",
    "    save_json_output(summary, str(summary_file), overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72001714",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_jd_text(record: dict) -> str:\n",
    "    \"\"\"\n",
    "    Constructs a rich text string from all non-empty fields of a JD record.\n",
    "    \"\"\"\n",
    "    jd_parts = []\n",
    "    for k, v in record.items():\n",
    "        label = k.replace('_', ' ').title()\n",
    "\n",
    "        if v is None:\n",
    "            continue\n",
    "\n",
    "        if isinstance(v, list):\n",
    "            if v:\n",
    "                value_str = \", \".join(str(i) for i in v if i)\n",
    "                jd_parts.append(f\"{label}: {value_str}\")\n",
    "        elif isinstance(v, (str, int, float)):\n",
    "            value_str = str(v).strip()\n",
    "            if value_str:\n",
    "                jd_parts.append(f\"{label}: {value_str}\")\n",
    "\n",
    "    return \"\\n\".join(jd_parts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc31dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "import json\n",
    "import os\n",
    "\n",
    "def normalize_batch(\n",
    "    records: List[dict],\n",
    "    start_idx: int,\n",
    "    end_idx: int,\n",
    "    is_resume: bool,\n",
    "    output_dir: Path,\n",
    "    prompt_template,\n",
    "    schema_model\n",
    "):\n",
    "    results, invalids = [], []\n",
    "\n",
    "    pair_dir = output_dir / \"pairs\"\n",
    "    pair_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for i, record in enumerate(records):\n",
    "        idx = start_idx + i\n",
    "        text = record.get(\"Resume_str\", \"\") if is_resume else render_jd_text(record)\n",
    "        record_id = record.get(\"resume_id\") if is_resume else record.get(\"jd_id\")\n",
    "        if not record_id:\n",
    "            record_id = f\"{'resume' if is_resume else 'jd'}_{idx}\"\n",
    "\n",
    "        parsed = extract_structured_json(\n",
    "            text=text,\n",
    "            prompt_template=prompt_template,\n",
    "            schema_model=schema_model,\n",
    "            validate=True,\n",
    "            record_id=record_id,\n",
    "            is_resume=is_resume\n",
    "        )\n",
    "\n",
    "        if \"raw_output\" in parsed or \"error\" in parsed:\n",
    "            invalids.append({\n",
    "                \"record_id\": record_id,\n",
    "                \"input\": text,\n",
    "                \"output\": parsed,\n",
    "                \"error\": parsed.get(\"error\", \"Malformed or unstructured output\")\n",
    "            })\n",
    "        else:\n",
    "            results.append(parsed)\n",
    "            pair_path = pair_dir / f\"{record_id}_pair.json\"\n",
    "            with open(pair_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump({\"text\": text, \"parsed_json\": parsed}, f, indent=2)\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "    batch_id = uuid.uuid4().hex[:6]\n",
    "    prefix = \"resumes\" if is_resume else \"jds\"\n",
    "\n",
    "    if results:\n",
    "        save_json_output(\n",
    "            results,\n",
    "            output_path=output_dir / f\"{prefix}_valid_{start_idx}_{end_idx}_{timestamp}_{batch_id}.json\"\n",
    "        )\n",
    "    if invalids:\n",
    "        save_json_output(\n",
    "            invalids,\n",
    "            output_path=output_dir / f\"{prefix}_invalid_{start_idx}_{end_idx}_{timestamp}_{batch_id}.json\"\n",
    "        )\n",
    "\n",
    "    return results, invalids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154963eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def normalize_file_in_batches(\n",
    "    input_filename: str,\n",
    "    output_dir: Path,\n",
    "    is_resume: bool = True,\n",
    "    input_dir: Path = Path(\"json_outputs\"),\n",
    "    save_every: int = 5,\n",
    "    limit: int = None\n",
    "):\n",
    "    input_path = input_dir / input_filename\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "    checkpoint_file = output_dir / f\"checkpoint_{'resumes' if is_resume else 'jds'}.json\"\n",
    "    start_index = 0\n",
    "    if checkpoint_file.exists():\n",
    "        with open(checkpoint_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            checkpoint = json.load(f)\n",
    "            start_index = checkpoint.get(\"last_index\", 0)\n",
    "            print(f\"üîÅ Resuming from index {start_index}\")\n",
    "\n",
    "    data_to_process = data[start_index:]\n",
    "    if limit is not None:\n",
    "        data_to_process = data_to_process[:limit]\n",
    "\n",
    "    prompt_template = RESUME_PROMPT_TEMPLATE if is_resume else JD_PROMPT_TEMPLATE\n",
    "    schema_model = ResumeSchema if is_resume else JobDescriptionSchema\n",
    "   \n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    total_valid, total_invalid = 0, 0\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "    batch_id = uuid.uuid4().hex[:6]\n",
    "    actual_start = start_index\n",
    "    actual_end = start_index + len(data_to_process)\n",
    "\n",
    "    for i in tqdm(range(0, len(data_to_process), save_every)):\n",
    "        batch = data_to_process[i:i + save_every]\n",
    "        batch_start = start_index + i\n",
    "        batch_end = batch_start + len(batch)\n",
    "\n",
    "        results, invalids = normalize_batch(\n",
    "            records=batch,\n",
    "            start_idx=batch_start,\n",
    "            end_idx=batch_end,\n",
    "            is_resume=is_resume,\n",
    "            output_dir=output_dir,\n",
    "            prompt_template=prompt_template,\n",
    "            schema_model=schema_model,\n",
    "        )\n",
    "\n",
    "        total_valid += len(results)\n",
    "        total_invalid += len(invalids)\n",
    "\n",
    "        with open(checkpoint_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({\"last_index\": batch_end}, f)\n",
    "\n",
    "    save_metadata_summary(\n",
    "        output_dir=output_dir,\n",
    "        is_resume=is_resume,\n",
    "        input_file=input_filename,\n",
    "        total_records=len(data_to_process),\n",
    "        total_valid=total_valid,\n",
    "        total_invalid=total_invalid,\n",
    "        start_index=actual_start,\n",
    "        end_index=actual_end,\n",
    "        timestamp=timestamp,\n",
    "        batch_id=batch_id\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34057ff1",
   "metadata": {},
   "source": [
    "## Run Phase 2 End-to-End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf23348",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_file_in_batches(\n",
    "    input_filename=\"parsed_jds.json\",\n",
    "    input_dir=Path(Config.JSON_OUTPUT_DIR),\n",
    "    output_dir=Path(Config.JSON_OUTPUT_NORMALIZED_DIR),\n",
    "    is_resume=False,\n",
    "    save_every=5,\n",
    "    limit=5  # Set to None to process all records, or specify a limit for testing\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b33fc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_file_in_batches(\n",
    "    input_filename=\"parsed_resumes.json\",\n",
    "    input_dir=Path(Config.JSON_OUTPUT_DIR),\n",
    "    output_dir=Path(Config.JSON_OUTPUT_NORMALIZED_DIR),\n",
    "    is_resume=True,\n",
    "    save_every=5,\n",
    "    limit=1  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7409286b",
   "metadata": {},
   "source": [
    "## Merge normalized files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3940c601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "normalized_dir = Path(Config.JSON_OUTPUT_NORMALIZED_DIR)\n",
    "merged_dir = normalized_dir / \"merged\"\n",
    "\n",
    "merge_json_files(\n",
    "    source_dir=normalized_dir,\n",
    "    output_file=normalized_dir / \"normalized_jds.json\",\n",
    "    pattern=\"jds_valid*.json\",\n",
    "    merged_dir=merged_dir\n",
    ")\n",
    "\n",
    "merge_json_files(\n",
    "    source_dir=normalized_dir,\n",
    "    output_file=normalized_dir / \"normalized_resumes.json\",\n",
    "    pattern=\"resumes_valid*.json\",\n",
    "    merged_dir=merged_dir\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
