{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c8edb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas tqdm pdfplumber python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6d985a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install kaggle kagglehub pandas-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c64fb311",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de233220",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "import zipfile\n",
    "#import pandas as pd\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "def authenticate_kaggle():\n",
    "    \"\"\"Authenticate Kaggle API using kaggle.json file.\"\"\"\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "    return api\n",
    "\n",
    "def download_kaggle_dataset(api, dataset_slug: str, download_dir: str):\n",
    "    \"\"\"\n",
    "    Download the Kaggle dataset if it's not already downloaded and extracted.\n",
    "    \n",
    "    Args:\n",
    "        api (KaggleApi): Authenticated Kaggle API instance\n",
    "        dataset_slug (str): The Kaggle dataset slug (e.g., 'snehaanbhawal/resume-dataset')\n",
    "        download_dir (str): Directory where the dataset will be stored\n",
    "    \"\"\"\n",
    "    os.makedirs(download_dir, exist_ok=True)\n",
    "    \n",
    "    dataset_name = dataset_slug.split(\"/\")[-1]\n",
    "    zip_path = os.path.join(download_dir, f\"{dataset_name}.zip\")\n",
    "    extract_path = os.path.join(download_dir, dataset_name)\n",
    "    \n",
    "    if os.path.exists(extract_path):\n",
    "        print(f\"âœ… Dataset already extracted at '{extract_path}', skipping download.\")\n",
    "        return extract_path\n",
    "\n",
    "    print(f\"â¬‡ï¸ Downloading dataset '{dataset_slug}'...\")\n",
    "    api.dataset_download_files(dataset_slug, path=download_dir, quiet=False)\n",
    "\n",
    "    print(f\"ðŸ“¦ Extracting dataset '{zip_path}'...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_path)\n",
    "    print(f\"âœ… Extraction completed at '{extract_path}'.\")\n",
    "\n",
    "    return extract_path\n",
    "\n",
    "def find_csv_file(root_path: str, target_csv: str) -> str:\n",
    "    \"\"\"\n",
    "    Recursively search for the target CSV inside the extracted dataset directory.\n",
    "    \n",
    "    Args:\n",
    "        root_path (str): Directory to start search from\n",
    "        target_csv (str): Name of the CSV file to find\n",
    "    \n",
    "    Returns:\n",
    "        str: Full path to the CSV file\n",
    "    \"\"\"\n",
    "    for dirpath, _, filenames in os.walk(root_path):\n",
    "        if target_csv in filenames:\n",
    "            full_path = os.path.join(dirpath, target_csv)\n",
    "            print(f\"âœ… Found CSV at: {full_path}\")\n",
    "            return full_path\n",
    "    raise FileNotFoundError(f\"âŒ CSV file '{target_csv}' not found inside '{root_path}'.\")\n",
    "\n",
    "def load_csv_to_dataframe(csv_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a CSV file into a pandas DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        csv_path (str): Path to the CSV file\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Loaded DataFrame\n",
    "    \"\"\"\n",
    "    print(f\"ðŸ“– Loading CSV file '{csv_path}'...\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"âœ… CSV loaded successfully. Shape: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "def download_and_load_kaggle_dataset(api, dataset_slug: str, \n",
    "                                     download_dir: str = \"datasets\", \n",
    "                                     csv_filename: str = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    High-level function to download, extract, find, and load a Kaggle CSV dataset.\n",
    "    \n",
    "    Args:\n",
    "        api (KaggleApi): Authenticated Kaggle API instance\n",
    "        dataset_slug (str): The Kaggle dataset slug\n",
    "        download_dir (str): Local directory to store the datasets\n",
    "        csv_filename (str): Specific CSV file name inside the dataset to load\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Loaded pandas DataFrame\n",
    "    \"\"\"\n",
    "    if csv_filename is None:\n",
    "        raise ValueError(\"You must specify the CSV filename to load.\")\n",
    "\n",
    "    # Download and extract\n",
    "    extract_path = download_kaggle_dataset(api, dataset_slug, download_dir)\n",
    "\n",
    "    # Find the CSV\n",
    "    csv_path = find_csv_file(extract_path, csv_filename)\n",
    "\n",
    "    # Load the CSV\n",
    "    df = load_csv_to_dataframe(csv_path)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def download_and_load_multiple_datasets(dataset_info: list, download_dir: str = \"datasets\") -> dict:\n",
    "    \"\"\"\n",
    "    Load multiple Kaggle datasets at once.\n",
    "    \n",
    "    Args:\n",
    "        dataset_info (list): List of dicts with keys 'slug' and 'csv'\n",
    "        download_dir (str): Directory to store datasets\n",
    "    \n",
    "    Returns:\n",
    "        dict: Mapping from dataset name to DataFrame\n",
    "    \"\"\"\n",
    "    api = authenticate_kaggle()\n",
    "    loaded_datasets = {}\n",
    "\n",
    "    for item in dataset_info:\n",
    "        print(f\"\\nðŸš€ Processing dataset: {item['slug']}\")\n",
    "        df = download_and_load_kaggle_dataset(api, item['slug'], download_dir, item['csv'])\n",
    "        dataset_key = item.get('key', item['slug'].split('/')[-1])\n",
    "        loaded_datasets[dataset_key] = df\n",
    "    \n",
    "    print(\"\\nâœ… All datasets loaded successfully.\")\n",
    "    return loaded_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9286dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_info = [\n",
    "    {\n",
    "        \"slug\": \"snehaanbhawal/resume-dataset\",\n",
    "        \"csv\": \"Resume.csv\",\n",
    "        \"key\": \"resume_data\"\n",
    "    },\n",
    "    {\n",
    "        \"slug\": \"arshkon/linkedin-job-postings\",\n",
    "        \"csv\": \"postings.csv\",\n",
    "        \"key\": \"job_postings\"\n",
    "    }\n",
    "]\n",
    "\n",
    "datasets = download_and_load_multiple_datasets(dataset_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fee501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume Data\n",
    "resume_df = datasets[\"resume_data\"]\n",
    "print(resume_df.shape)\n",
    "resume_df.head()\n",
    "\n",
    "# Job Postings Data\n",
    "job_postings_df = datasets[\"job_postings\"]\n",
    "print(job_postings_df.shape)\n",
    "job_postings_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8781c47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These should point to the actual extracted Kaggle dataset folders\n",
    "OUTPUT_JSON_DIR_RESUME = 'structured_resumes'\n",
    "OUTPUT_JSON_DIR_JOB_POSTINGS = 'structured_job_postings'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63bc7acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure_resume_from_csv(row):\n",
    "    return {\n",
    "        \"basics\": {\n",
    "            \"name\": None,  # We don't have names yet\n",
    "            \"email\": None,\n",
    "            \"phone\": None,\n",
    "            \"location\": None,\n",
    "            \"summary\": None,\n",
    "            \"category\": row.get('Category', '')  # Use dataset label\n",
    "        },\n",
    "        \"education\": [],\n",
    "        \"experience\": [],\n",
    "        \"skills\": [],\n",
    "        \"certifications\": [],\n",
    "        \"projects\": [],\n",
    "        \"raw_text\": row.get('Resume_str', '')  # Full resume text\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfc601f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure_jd_from_csv(row):\n",
    "    return {\n",
    "        \"basics\": {\n",
    "            \"title\": row.get('title', ''),\n",
    "            \"company\": row.get('company_name', ''),\n",
    "            \"location\": row.get('location', '')\n",
    "        },\n",
    "        \"description\": row.get('description', ''),\n",
    "        \"skills_description\": row.get('skills_desc', ''),\n",
    "        \"job_posted_date\": row.get('posted_date', ''),\n",
    "        \"job_id\": row.get('job_id', ''),\n",
    "        \"seniority_level\": row.get('formatted_experience_level', ''),\n",
    "        \"employment_type\": row.get('formatted_work_type', ''),\n",
    "        \"job_function\": row.get('job_function', '')\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d6d652",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2484 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2484/2484 [00:00<00:00, 15680.68it/s]\n"
     ]
    }
   ],
   "source": [
    "structured_resumes = []\n",
    "\n",
    "for idx, row in tqdm(resume_df.iterrows(), total=len(resume_df)):\n",
    "    resume_obj = structure_resume_from_csv(row)\n",
    "    structured_resumes.append(resume_obj)\n",
    "\n",
    "# Save parsed resumes\n",
    "with open(os.path.join(OUTPUT_JSON_DIR_RESUME, \"structured_resumes.json\"), \"w\") as f:\n",
    "    json.dump(structured_resumes, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a933531a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 123849/123849 [00:09<00:00, 12997.69it/s]\n"
     ]
    }
   ],
   "source": [
    "structured_jds = []\n",
    "\n",
    "for idx, row in tqdm(job_postings_df.iterrows(), total=len(job_postings_df)):\n",
    "    jd_obj = structure_jd_from_csv(row)\n",
    "    structured_jds.append(jd_obj)\n",
    "\n",
    "# Save parsed JDs\n",
    "with open(os.path.join(OUTPUT_JSON_DIR_JOB_POSTINGS, \"structured_jds.json\"), \"w\") as f:\n",
    "    json.dump(structured_jds, f, indent=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
