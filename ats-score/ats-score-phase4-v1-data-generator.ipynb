{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01c8bad6",
   "metadata": {},
   "source": [
    "# Global setup and package installation used in most phases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d08acab",
   "metadata": {},
   "source": [
    "## Colab + GPU Detection Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4b5631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def is_running_in_colab():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "def get_available_gpu_memory_gb():\n",
    "    try:\n",
    "        output = subprocess.check_output(\n",
    "            [\"nvidia-smi\", \"--query-gpu=memory.free\", \"--format=csv,nounits,noheader\"],\n",
    "            encoding=\"utf-8\"\n",
    "        )\n",
    "        free_mem_mb = int(output.strip().split(\"\\n\")[0])\n",
    "        return free_mem_mb / 1024\n",
    "    except Exception:\n",
    "        return 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934d3e82",
   "metadata": {},
   "source": [
    "## install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d72196",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_running_in_colab():\n",
    "    # Install the required packages\n",
    "    !pip install kagglehub pandas\n",
    "    !pip install -q transformers accelerate bitsandbytes sentencepiece pydantic huggingface_hub xformers\n",
    "    !pip install regex json5\n",
    "    !pip install sentence-transformers scikit-learn\n",
    "    !pip install rapidfuzz unidecode\n",
    "\n",
    "else:\n",
    "    %pip install kagglehub pandas\n",
    "    %pip install -q transformers accelerate sentencepiece pydantic huggingface_hub xformers\n",
    "    #%pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu121\n",
    "    #%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "    %pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128\n",
    "    %pip install -U bitsandbytes\n",
    "    %pip install regex json5\n",
    "    %pip install sentence-transformers scikit-learn\n",
    "    %pip install rapidfuzz unidecode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e36d02",
   "metadata": {},
   "source": [
    "## Login to huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea132993",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# Set your token here securely or prompt for it in Colab\n",
    "# Recommended: store in Colab secrets or environment variable\n",
    "HF_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "\n",
    "if not HF_TOKEN:\n",
    "    if is_running_in_colab():\n",
    "        # If running in Colab, use the Colab secrets\n",
    "        try:\n",
    "            from google.colab import userdata\n",
    "            HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "            if not HF_TOKEN:\n",
    "                raise ValueError(\"‚ö†Ô∏è Hugging Face token not found in Colab secrets.\")\n",
    "            print(\"üîë Hugging Face token found in Colab secrets.\")\n",
    "        except ImportError:\n",
    "            print(\"‚ö†Ô∏è Unable to authenticate in Colab. Please set your Hugging Face token manually.\")\n",
    "    else:\n",
    "        # Prompt for token if not set in environment\n",
    "        print(\"üîë Please enter your Hugging Face token:\")\n",
    "        # For Colab or local prompt input\n",
    "        HF_TOKEN = input(\"üîë Enter your Hugging Face token: \").strip()\n",
    "\n",
    "login(token=HF_TOKEN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a919779",
   "metadata": {},
   "source": [
    "## Setup Kaggle Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6319924",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def setup_kaggle_credentials():\n",
    "    kaggle_path = os.path.expanduser('~/.kaggle/kaggle.json')\n",
    "    if not os.path.exists(kaggle_path):\n",
    "        from google.colab import files\n",
    "        print(\"üìÇ Upload kaggle.json file...\")\n",
    "        uploaded = files.upload()\n",
    "        os.makedirs(os.path.dirname(kaggle_path), exist_ok=True)\n",
    "        for filename in uploaded.keys():\n",
    "            shutil.move(filename, kaggle_path)\n",
    "        os.chmod(kaggle_path, 0o600)\n",
    "        print(f\"‚úÖ Kaggle credentials setup at {kaggle_path}\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Kaggle credentials already exist at {kaggle_path}\")\n",
    "\n",
    "setup_kaggle_credentials()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdc73bb",
   "metadata": {},
   "source": [
    "##  Load Qwen-Instruct with Fallback to Quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d8bcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "import torch\n",
    "\n",
    "def load_model_pipeline(model_name: str, hf_token: str):\n",
    "    has_cuda = torch.cuda.is_available()\n",
    "    free_mem = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3) if has_cuda else 0\n",
    "    print(f\"üíª CUDA: {has_cuda} | GPU Memory: {free_mem:.2f} GB\")\n",
    "\n",
    "    device_map = {\"\": 0} if has_cuda else \"cpu\"\n",
    "    use_4bit = has_cuda and free_mem < 24\n",
    "\n",
    "    # Set quantization config\n",
    "    quant_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True if use_4bit else False,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    ) if use_4bit else None\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token, trust_remote_code=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # ‚úÖ Fix warning about pad_token\n",
    "\n",
    "    # Load model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device_map,\n",
    "        quantization_config=quant_config,\n",
    "        torch_dtype=torch.float16 if not quant_config else None,\n",
    "        trust_remote_code=True,\n",
    "        token=hf_token\n",
    "    )\n",
    "\n",
    "    print(f\"‚úÖ Model loaded on {next(model.parameters()).device}\")\n",
    "    return pipeline(\"text-generation\", model=model, tokenizer=tokenizer, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8285479",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_pipeline = load_model_pipeline(\n",
    "    model_name=\"Qwen/Qwen2-7B-Instruct\",\n",
    "    hf_token=HF_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738cae0b",
   "metadata": {},
   "source": [
    "# Global utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1ec563",
   "metadata": {},
   "source": [
    "### Utility to save json to a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e091036",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "# üì¶ Save JSON Output with Safety\n",
    "def save_json_output(data, output_path: str, indent: int = 4, overwrite: bool = True):\n",
    "    output_dir = os.path.dirname(output_path)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    if os.path.exists(output_path):\n",
    "        if overwrite:\n",
    "            os.remove(output_path)\n",
    "        else:\n",
    "            raise FileExistsError(f\"File {output_path} already exists and overwrite=False.\")\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=indent, ensure_ascii=False)\n",
    "\n",
    "    print(f\"‚úÖ Saved output to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f6db83",
   "metadata": {},
   "source": [
    "### Utility to load file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e7aa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import json\n",
    "\n",
    "# üìÇ Load normalized JSON data\n",
    "def load_json_file(file_path: str) -> Any:\n",
    "    if not os.path.isfile(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5a9e6f",
   "metadata": {},
   "source": [
    "### Utility to save jsonl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5a9af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_jsonl_line(record: dict, output_file: str):\n",
    "    \"\"\"\n",
    "    Appends a single JSON object as a line to a .jsonl file.\n",
    "    \"\"\"\n",
    "    with open(output_file, \"a\", encoding=\"utf-8\") as f:\n",
    "        json.dump(record, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89959de6",
   "metadata": {},
   "source": [
    "### Configurations  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6519d739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# üõ† CONFIGURATION\n",
    "# ==============================\n",
    "\n",
    "class Config:\n",
    "    JSON_OUTPUT_DIR = \"json_outputs_all_data\"\n",
    "    JSON_OUTPUT_NORMALIZED_DIR = \"json_outputs_all_data/normalized\"\n",
    "    JSON_OUTPUT_NORMALIZED_JD = \"json_outputs_all_data/normalized/jd\"\n",
    "    JSON_OUTPUT_NORMALIZED_RESUME = \"json_outputs_all_data/normalized/resume\"\n",
    "    JSON_OUTPUT_SCORING_DIR = \"json_outputs_all_data/scoring\"\n",
    "    JSON_OUTPUT_SCORING_SPLIT_DIR = \"json_outputs_all_data/scoring/split\"\n",
    "    JSON_OUTPUT_SCORING_FT_DATA = \"json_outputs_all_data/scoring/FT_data\"\n",
    "    JSON_OUTPUT_FINE_TUNE_SCORE = \"json_outputs_all_data/fine-tune/scored\"\n",
    "    JSON_OUTPUT_FINE_TUNE_RECORD = \"json_outputs_all_data/fine-tune/record\"\n",
    "    JSON_OUTPUT_FINE_TUNE_TEST_DATA = \"json_outputs_all_data/fine-tune/test-data\"\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84403281",
   "metadata": {},
   "source": [
    "# Generate 30K core sample semantic data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635c4f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple, Set\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "\n",
    "# === 1. FILTERING ===\n",
    "def filter_pairs_by_score(pairs: List[Dict], threshold: float) -> List[Dict]:\n",
    "    return [p for p in pairs if p.get(\"resume_jd_similarity\", 0.0) >= threshold]\n",
    "\n",
    "\n",
    "# === 2. GROUPING ===\n",
    "def group_pairs_by_quality_and_resume(pairs: List[Dict]) -> Tuple[\n",
    "    Dict[str, List[Dict]],\n",
    "    Dict[int, Dict[str, List[Dict]]],\n",
    "    Dict[str, List[Dict]]\n",
    "]:\n",
    "    quality_buckets = defaultdict(list)\n",
    "    resume_to_qualities = defaultdict(lambda: defaultdict(list))\n",
    "    domain_coverage = defaultdict(list)\n",
    "\n",
    "    for p in pairs:\n",
    "        q = p.get(\"semantic_match_label\", \"unknown\").lower()\n",
    "        rid = p[\"resume_id\"]\n",
    "        res_dom = p.get(\"resume_domain\", \"unknown\").lower()\n",
    "        jd_dom = p.get(\"jd_domain\", \"unknown\").lower()\n",
    "\n",
    "        quality_buckets[q].append(p)\n",
    "        resume_to_qualities[rid][q].append(p)\n",
    "        domain_coverage[res_dom].append(p)\n",
    "        domain_coverage[jd_dom].append(p)\n",
    "\n",
    "    return quality_buckets, resume_to_qualities, domain_coverage\n",
    "\n",
    "\n",
    "# === 3. RESUME-BALANCED SAMPLING ===\n",
    "def resume_balanced_sampling(\n",
    "    resume_to_qualities: Dict[int, Dict[str, List[Dict]]],\n",
    "    target_qualities: List[str],\n",
    "    seen_pairs: Set[Tuple[int, int]]\n",
    ") -> Dict[str, List[Dict]]:\n",
    "    selected = defaultdict(list)\n",
    "    resume_count = 0\n",
    "\n",
    "    for resume_id, qmap in resume_to_qualities.items():\n",
    "        used = 0\n",
    "        for q in target_qualities:\n",
    "            if q in qmap:\n",
    "                pair = random.choice(qmap[q])\n",
    "                key = (pair[\"resume_id\"], pair[\"jd_id\"])\n",
    "                if key not in seen_pairs:\n",
    "                    selected[q].append(pair)\n",
    "                    seen_pairs.add(key)\n",
    "                    used += 1\n",
    "        if used > 0:\n",
    "            resume_count += 1\n",
    "\n",
    "    return selected, resume_count\n",
    "\n",
    "\n",
    "# === 4. DOMAIN QUOTA ENFORCEMENT ===\n",
    "def enforce_domain_quota(\n",
    "    quality_buckets: Dict[str, List[Dict]],\n",
    "    seen_pairs: Set[Tuple[int, int]],\n",
    "    target_counts: Dict[str, int],\n",
    "    current_counts: Dict[str, int],\n",
    "    min_per_domain: int\n",
    ") -> Dict[str, List[Dict]]:\n",
    "    selected = defaultdict(list)\n",
    "    domain_pair_cache = defaultdict(list)\n",
    "\n",
    "    for q in quality_buckets:\n",
    "        for p in quality_buckets[q]:\n",
    "            d1 = p.get(\"resume_domain\", \"unknown\").lower()\n",
    "            d2 = p.get(\"jd_domain\", \"unknown\").lower()\n",
    "            domain_pair_cache[(q, d1)].append(p)\n",
    "            domain_pair_cache[(q, d2)].append(p)\n",
    "\n",
    "    for (q, domain), group in domain_pair_cache.items():\n",
    "        if current_counts[q] >= target_counts[q]:\n",
    "            continue  # already filled this quality\n",
    "\n",
    "        candidates = [p for p in group if (p[\"resume_id\"], p[\"jd_id\"]) not in seen_pairs]\n",
    "        room_left = target_counts[q] - current_counts[q]\n",
    "        take_count = min(min_per_domain, len(candidates), room_left)\n",
    "        sampled = random.sample(candidates, take_count)\n",
    "\n",
    "        for p in sampled:\n",
    "            key = (p[\"resume_id\"], p[\"jd_id\"])\n",
    "            if key not in seen_pairs:\n",
    "                selected[q].append(p)\n",
    "                seen_pairs.add(key)\n",
    "                current_counts[q] += 1\n",
    "                if current_counts[q] >= target_counts[q]:\n",
    "                    break  # stop sampling more of this quality\n",
    "\n",
    "    return selected\n",
    "\n",
    "\n",
    "\n",
    "# === 5. FILL REMAINING WITH DIVERSITY ===\n",
    "def fill_remaining_by_diverse_domains(\n",
    "    quality_buckets: Dict[str, List[Dict]],\n",
    "    selected_by_quality: Dict[str, List[Dict]],\n",
    "    target_counts: Dict[str, int],\n",
    "    seen_pairs: Set[Tuple[int, int]]\n",
    ") -> Dict[str, List[Dict]]:\n",
    "    for q in target_counts:\n",
    "        remaining = target_counts[q] - len(selected_by_quality[q])\n",
    "        if remaining <= 0:\n",
    "            continue\n",
    "\n",
    "        available = [\n",
    "            p for p in quality_buckets[q]\n",
    "            if (p[\"resume_id\"], p[\"jd_id\"]) not in seen_pairs\n",
    "        ]\n",
    "\n",
    "        # Bucket by domain\n",
    "        domain_groups = defaultdict(list)\n",
    "        for p in available:\n",
    "            domain_groups[p.get(\"resume_domain\", \"unknown\").lower()].append(p)\n",
    "            domain_groups[p.get(\"jd_domain\", \"unknown\").lower()].append(p)\n",
    "\n",
    "        sampled = []\n",
    "        domain_cycle = list(domain_groups.keys())\n",
    "        random.shuffle(domain_cycle)\n",
    "\n",
    "        while remaining > 0 and domain_cycle:\n",
    "            domain = domain_cycle.pop(0)\n",
    "            group = domain_groups[domain]\n",
    "            group = [p for p in group if (p[\"resume_id\"], p[\"jd_id\"]) not in seen_pairs]\n",
    "            if not group:\n",
    "                continue\n",
    "            chosen = random.choice(group)\n",
    "            sampled.append(chosen)\n",
    "            seen_pairs.add((chosen[\"resume_id\"], chosen[\"jd_id\"]))\n",
    "            domain_groups[domain].remove(chosen)\n",
    "            domain_cycle.append(domain)\n",
    "            remaining -= 1\n",
    "\n",
    "        selected_by_quality[q].extend(sampled)\n",
    "\n",
    "    return selected_by_quality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a9c6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 6. MAIN WRAPPER ===\n",
    "def generate_balanced_sample(\n",
    "    all_pairs: List[Dict],\n",
    "    target_counts: Dict[str, int],\n",
    "    score_threshold: float = 0.1,\n",
    "    min_per_domain: int = 50\n",
    ") -> List[Dict]:\n",
    "    print(f\"üîç Total pairs in input: {len(all_pairs)}\")\n",
    "    filtered = filter_pairs_by_score(all_pairs, score_threshold)\n",
    "    print(f\"‚úÖ After score ‚â• {score_threshold}: {len(filtered)}\")\n",
    "\n",
    "    quality_buckets, resume_to_qualities, domain_coverage = group_pairs_by_quality_and_resume(filtered)\n",
    "    print(\"üìä Match quality counts:\")\n",
    "    for q in target_counts:\n",
    "        print(f\"  {q.upper():<7}: {len(quality_buckets[q])}\")\n",
    "    print(f\"üë• Unique resumes: {len(resume_to_qualities)}\")\n",
    "\n",
    "    seen_pairs = set()\n",
    "\n",
    "    # Resume-balanced\n",
    "    selected_by_quality, resume_count = resume_balanced_sampling(resume_to_qualities, list(target_counts.keys()), seen_pairs)\n",
    "    print(f\"üë§ Resume-balanced resumes: {resume_count}\")\n",
    "\n",
    "    # Domain quotas\n",
    "    current_counts = {q: len(selected_by_quality[q]) for q in target_counts}\n",
    "    domain_quota_selected = enforce_domain_quota(\n",
    "        quality_buckets,\n",
    "        seen_pairs,\n",
    "        target_counts,\n",
    "        current_counts,\n",
    "        min_per_domain\n",
    "    )\n",
    "\n",
    "    for q in target_counts:\n",
    "        selected_by_quality[q].extend(domain_quota_selected.get(q, []))\n",
    "\n",
    "    # Fill remaining\n",
    "    selected_by_quality = fill_remaining_by_diverse_domains(quality_buckets, selected_by_quality, target_counts, seen_pairs)\n",
    "\n",
    "    # Final merge\n",
    "    final_sample = []\n",
    "    print(\"\\nüì¶ Final sampled count:\")\n",
    "    for q in target_counts:\n",
    "        group = selected_by_quality[q]\n",
    "        print(f\"  {q.upper():<7}: {len(group)}\")\n",
    "        final_sample.extend(group)\n",
    "\n",
    "    print(f\"\\nüéØ Total selected: {len(final_sample)}\")\n",
    "\n",
    "    # Domain coverage\n",
    "    resume_domains = [p.get(\"resume_domain\", \"unknown\").lower() for p in final_sample]\n",
    "    jd_domains = [p.get(\"jd_domain\", \"unknown\").lower() for p in final_sample]\n",
    "    print(\"\\nüìä Resume domain coverage (top 10):\")\n",
    "    for dom, count in Counter(resume_domains).most_common(10):\n",
    "        print(f\"  {dom:<30} {count}\")\n",
    "\n",
    "    print(\"\\nüìä JD domain coverage (top 10):\")\n",
    "    for dom, count in Counter(jd_domains).most_common(10):\n",
    "        print(f\"  {dom:<30} {count}\")\n",
    "\n",
    "    return final_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e4a037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ Define how many samples per match type\n",
    "target_counts = {\n",
    "    \"strong\": 20000,\n",
    "    \"medium\": 25000,\n",
    "    \"weak\": 15000\n",
    "}\n",
    "\n",
    "# üì• Load the filtered list only\n",
    "input_path = os.path.join(Config.JSON_OUTPUT_SCORING_DIR, 'semantic_relevance_scores.json')\n",
    "full_data  = load_json_file(input_path)\n",
    "relevance_data = full_data.get(\"semantic_relevance_scores\", [])\n",
    "\n",
    "# üß† Generate the balanced subset\n",
    "sampled = generate_balanced_sample(\n",
    "    all_pairs=relevance_data,\n",
    "    target_counts=target_counts,\n",
    "    score_threshold=0.2\n",
    ")\n",
    "\n",
    "# Wrap in valid JSON object structure\n",
    "relevance_wrapped = {\n",
    "    \"semantic_relevance_scores\": sampled\n",
    "}\n",
    "\n",
    "# üíæ Save the sampled subset\n",
    "relevance_map_file = os.path.join(Config.JSON_OUTPUT_SCORING_DIR, 'relevant_pairs.json')\n",
    "save_json_output(relevance_wrapped, relevance_map_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58eb1907",
   "metadata": {},
   "source": [
    "# Generate Normal distributed 30K samples file from scores files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cee4704",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e51a660",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCORING_FOLDER = Config.JSON_OUTPUT_SCORING_DIR  # path to scoring files\n",
    "OUTPUT_FILE = os.path.join(Config.JSON_OUTPUT_SCORING_FT_DATA, \"fine_tuning_filenames.json\")  # JSON_OUTPUT_SCORING_FT_DATA\n",
    "TARGET_COUNTS = {\"strong\": 10000, \"medium\": 15000, \"weak\": 5000}\n",
    "QUALITY_BUCKETS = list(TARGET_COUNTS.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b2a396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_filename(filename):\n",
    "    # Handles timestamps with both date and time like: 20250616_160025\n",
    "    match = re.match(r\"^(\\d+)_(\\d+)_(strong|medium|weak)_\\d{8}_\\d{6}_([\\d.]+)\\.json$\", filename)\n",
    "    if match:\n",
    "        resume_id, jd_id, quality, score = match.groups()\n",
    "        return {\n",
    "            \"filename\": filename,\n",
    "            \"resume_id\": resume_id,\n",
    "            \"jd_id\": jd_id,\n",
    "            \"quality\": quality,\n",
    "            \"score\": float(score)\n",
    "        }\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29245d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def load_scoring_files(folder):\n",
    "    all_meta = []\n",
    "    debug_limit = 5  # Only print debug for this many files\n",
    "    debug_count = 0\n",
    "\n",
    "    for fname in os.listdir(folder):\n",
    "        if not fname.endswith(\".json\"):\n",
    "            continue\n",
    "\n",
    "        # üß† Extract core metadata from filename\n",
    "        meta = parse_filename(fname)\n",
    "        if not meta:\n",
    "            print(f\"‚ö†Ô∏è Skipping unrecognized filename format: {fname}\")\n",
    "            continue\n",
    "\n",
    "        path = os.path.join(folder, fname)\n",
    "        try:\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            # ‚úÖ Add extra fields from inside the JSON\n",
    "            meta[\"domain\"] = data.get(\"domain\", \"UNKNOWN\")\n",
    "\n",
    "            # üîÑ Use filename 'quality' as fallback for missing fields\n",
    "            match_quality = data.get(\"match_quality\", meta[\"quality\"])\n",
    "            semantic_match_label = data.get(\"semantic_match_label\", meta[\"quality\"])\n",
    "\n",
    "            meta[\"match_quality\"] = str(match_quality).strip().lower()\n",
    "            meta[\"semantic_match_label\"] = str(semantic_match_label).strip().lower()\n",
    "\n",
    "            # üêõ Debug print for first few entries\n",
    "            #if debug_count < debug_limit:\n",
    "            #    print(f\"üîç File: {fname}\")\n",
    "            #    print(f\"    ‚û§ match_quality: {meta['match_quality']}\")\n",
    "            #    print(f\"    ‚û§ semantic_match_label: {meta['semantic_match_label']}\")\n",
    "            #    print(f\"    ‚û§ domain: {meta['domain']}\")\n",
    "            #    debug_count += 1\n",
    "\n",
    "            all_meta.append(meta)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading {fname}: {e}\")\n",
    "\n",
    "    return all_meta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec370e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_label(meta_list):\n",
    "    return {\n",
    "        \"strong\": [m for m in meta_list if m[\"match_quality\"] == \"strong\"],\n",
    "        \"strong_semantic\": [m for m in meta_list if m[\"semantic_match_label\"] == \"strong\" and m[\"match_quality\"] != \"strong\"],\n",
    "        \"medium\": [m for m in meta_list if m[\"match_quality\"] == \"medium\"],\n",
    "        \"weak\": [m for m in meta_list if m[\"match_quality\"] == \"weak\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4af2128",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_by_domain_distribution(meta_list, limit):\n",
    "    if not meta_list:\n",
    "        print(f\"‚ö†Ô∏è No data available to select from for limit {limit}\")\n",
    "        return []\n",
    "\n",
    "    selected = []\n",
    "    domain_buckets = defaultdict(list)\n",
    "    for m in meta_list:\n",
    "        domain_buckets[m[\"domain\"]].append(m)\n",
    "\n",
    "    domain_keys = list(domain_buckets.keys())\n",
    "    if not domain_keys:\n",
    "        print(f\"‚ö†Ô∏è No domains found in the given metadata list.\")\n",
    "        return []\n",
    "\n",
    "    per_domain = max(1, limit // len(domain_keys))\n",
    "    for domain in domain_keys:\n",
    "        random.shuffle(domain_buckets[domain])\n",
    "        selected.extend(domain_buckets[domain][:per_domain])\n",
    "\n",
    "    # Fill remaining if needed\n",
    "    if len(selected) < limit:\n",
    "        remaining = [m for m in meta_list if m[\"filename\"] not in {x[\"filename\"] for x in selected}]\n",
    "        random.shuffle(remaining)\n",
    "        selected.extend(remaining[:limit - len(selected)])\n",
    "    return selected[:limit]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28df00bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_distribution(selected_list, label):\n",
    "    total = len(selected_list)\n",
    "    by_domain = Counter([m[\"domain\"] for m in selected_list])\n",
    "    print(f\"\\nüìä Final count for {label}: {total}\")\n",
    "    for domain, count in by_domain.most_common():\n",
    "        print(f\"  {domain}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b9ed14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    all_meta = load_scoring_files(SCORING_FOLDER)\n",
    "    # Debug: Check quality counts across all files\n",
    "    count_by_quality_field = Counter([m.get(\"match_quality\", \"MISSING\") for m in all_meta])\n",
    "    count_by_semantic_field = Counter([m.get(\"semantic_match_label\", \"MISSING\") for m in all_meta])\n",
    "    print(\"\\nüìã match_quality field distribution:\", count_by_quality_field)\n",
    "    print(\"üìã semantic_match_label field distribution:\", count_by_semantic_field)\n",
    "\n",
    "\n",
    "    grouped = group_by_label(all_meta)\n",
    "    final_selected = []\n",
    "\n",
    "    # Step 1: Strong group\n",
    "    strong_actual = grouped[\"strong\"]\n",
    "    strong_selected = []\n",
    "    \n",
    "    if len(strong_actual) >= TARGET_COUNTS[\"strong\"]:\n",
    "        strong_selected = select_by_domain_distribution(strong_actual, TARGET_COUNTS[\"strong\"])\n",
    "    else:\n",
    "        shortfall = TARGET_COUNTS[\"strong\"] - len(strong_actual)\n",
    "        fallback = grouped[\"strong_semantic\"]\n",
    "        fallback_selected = select_by_domain_distribution(fallback, shortfall)\n",
    "\n",
    "        strong_selected = strong_actual + fallback_selected\n",
    "        print(f\"\\n‚öôÔ∏è Strong fallback logic:\")\n",
    "        print(f\"  ‚úÖ Actual strong available: {len(strong_actual)}\")\n",
    "        print(f\"  ‚ûï Needed fallback: {shortfall}\")\n",
    "        print(f\"  üß© Fallback added from semantic match: {len(fallback_selected)}\")\n",
    "        print(f\"  ‚ùå Still missing: {shortfall - len(fallback_selected)} (if > 0, this means we are under target)\")\n",
    "    log_distribution(strong_selected, \"strong\")\n",
    "    final_selected.extend(strong_selected)\n",
    "    selected_filenames = {m[\"filename\"] for m in final_selected}\n",
    "\n",
    "    # Step 2: Medium group (exclude used)\n",
    "    medium_pool = [m for m in grouped[\"medium\"] if m[\"filename\"] not in selected_filenames]\n",
    "    medium_selected = select_by_domain_distribution(medium_pool, TARGET_COUNTS[\"medium\"])\n",
    "    log_distribution(medium_selected, \"medium\")\n",
    "    final_selected.extend(medium_selected)\n",
    "    selected_filenames.update(m[\"filename\"] for m in medium_selected)\n",
    "\n",
    "    # Step 3: Weak group (exclude used)\n",
    "    weak_pool = [m for m in grouped[\"weak\"] if m[\"filename\"] not in selected_filenames]\n",
    "    weak_selected = select_by_domain_distribution(weak_pool, TARGET_COUNTS[\"weak\"])\n",
    "    log_distribution(weak_selected, \"weak\")\n",
    "    final_selected.extend(weak_selected)\n",
    "\n",
    "    # Save output\n",
    "    filenames = [m[\"filename\"] for m in final_selected]\n",
    "    save_json_output(filenames, OUTPUT_FILE, indent=2)\n",
    "    print(f\"\\n‚úÖ Saved final list to {OUTPUT_FILE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08ef8be",
   "metadata": {},
   "source": [
    "# Copy scored files to fine-tune/score folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d350f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths\n",
    "SCORING_FOLDER = Path(Config.JSON_OUTPUT_SCORING_DIR)  # Source folder\n",
    "FT_JSON_FILE =  os.path.join(Config.JSON_OUTPUT_SCORING_FT_DATA, \"fine_tuning_filenames.json\") #Path(\"fine_tuning_filenames.json\")  # File with selected filenames\n",
    "TARGET_FOLDER = Path(Config.JSON_OUTPUT_FINE_TUNE_SCORE)  # Destination\n",
    "\n",
    "# Ensure output folder exists\n",
    "TARGET_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load filenames\n",
    "#with open(FT_JSON_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "#    filenames = json.load(f)\n",
    "    \n",
    "filenames = load_json_file(FT_JSON_FILE)\n",
    "\n",
    "print(f\"üìÑ Loaded {len(filenames)} filenames from {FT_JSON_FILE}\")\n",
    "\n",
    "# Copy files\n",
    "copied = 0\n",
    "missing = []\n",
    "for fname in filenames:\n",
    "    src_path = SCORING_FOLDER / fname\n",
    "    dest_path = TARGET_FOLDER / fname\n",
    "\n",
    "    if src_path.exists():\n",
    "        shutil.copy2(src_path, dest_path)\n",
    "        copied += 1\n",
    "    else:\n",
    "        missing.append(fname)\n",
    "\n",
    "print(f\"\\n‚úÖ Copied {copied} files to {TARGET_FOLDER}\")\n",
    "if missing:\n",
    "    print(f\"‚ö†Ô∏è {len(missing)} files were missing in {SCORING_FOLDER}:\")\n",
    "    for m in missing[:5]:\n",
    "        print(\"  -\", m)\n",
    "    if len(missing) > 5:\n",
    "        print(f\"  ...and {len(missing) - 5} more.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054a3833",
   "metadata": {},
   "source": [
    "# Create SFT Record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea7aed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Set, Dict\n",
    "from glob import glob\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949b5ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîÅ Find resume/JD JSON by prefix\n",
    "def find_file_with_prefix(directory: Path, prefix: str) -> Path:\n",
    "    files = list(directory.glob(f\"{prefix}_*.json\"))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No matching file for prefix: {prefix}\")\n",
    "    return files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a339b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# üß† Determine optimal max_workers for I/O-bound tasks\n",
    "def get_optimal_max_workers(io_bound: bool = True, cap: int = 32) -> int:\n",
    "    cores = os.cpu_count() or 4  # fallback if undetectable\n",
    "    if io_bound:\n",
    "        return min(cap, 4 * cores)  # I/O-bound: more threads\n",
    "    else:\n",
    "        return min(cap, cores)      # CPU-bound: 1 thread per core\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b71497e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "def load_first_record_from_file(path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Load the first JSON object from a file that contains either:\n",
    "    - a single dictionary (direct object)\n",
    "    - a list with one dictionary [{...}]\n",
    "    \n",
    "    Raises ValueError if the list has more than one item.\n",
    "    \"\"\"\n",
    "    data = load_json_file(path)\n",
    "\n",
    "    if isinstance(data, list):\n",
    "        if len(data) == 1:\n",
    "            return data[0]\n",
    "        raise ValueError(f\"File {path} contains list with {len(data)} items ‚Äî expected only one.\")\n",
    "    \n",
    "    elif isinstance(data, dict):\n",
    "        return data\n",
    "    \n",
    "    else:\n",
    "        raise TypeError(f\"File {path} must contain a dict or single-record list, got {type(data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb0aed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_file(\n",
    "    fname: str,\n",
    "    scoring_dir: Path,\n",
    "    resume_dir: Path,\n",
    "    jd_dir: Path,\n",
    "    output_dir: Path,\n",
    "    checkpoint_set: Set[str]\n",
    ") -> str:\n",
    "    try:\n",
    "        scoring_path = scoring_dir / fname\n",
    "        record = load_json_file(str(scoring_path))\n",
    "\n",
    "        uid = f\"{record['resume_id']}_{record['jd_id']}\"\n",
    "        if uid in checkpoint_set:\n",
    "            return None\n",
    "\n",
    "        resume_path = find_file_with_prefix(resume_dir, f\"resumes_{record['resume_id']}\")\n",
    "        jd_path = find_file_with_prefix(jd_dir, f\"jds_{record['jd_id']}\")\n",
    "        #resume_input = load_json_file(str(resume_path))[\"input_text\"]\n",
    "        #jd_input = load_json_file(str(jd_path))[\"input_text\"]\n",
    "        resume_data = load_first_record_from_file(str(resume_path))\n",
    "        jd_data = load_first_record_from_file(str(jd_path))\n",
    "\n",
    "\n",
    "        resume_input = resume_data[\"input_text\"]\n",
    "        jd_input = jd_data[\"input_text\"]\n",
    "\n",
    "\n",
    "        #input_text = f\"{resume_input.strip()}\\n{jd_input.strip()}\"\n",
    "        input_text = (\n",
    "            f\"<DOMAIN>{record.get('domain', '').strip()}</DOMAIN>\\n\"\n",
    "            f\"<RESUME_START>\\n{resume_input.strip()}\\n<RESUME_END>\\n\\n\"\n",
    "            f\"<JD_START>\\n{jd_input.strip()}\\n<JD_END>\"\n",
    "        )\n",
    "\n",
    "        section_scores = record[\"section_scores\"]\n",
    "        output_scores = {\n",
    "            \"final_ats_score\": record[\"final_ats_score\"],\n",
    "            **{section: round(value[\"score\"], 3) for section, value in section_scores.items()}\n",
    "        }\n",
    "\n",
    "        sft_data = {\n",
    "            \"input\": input_text,\n",
    "            \"output\": output_scores\n",
    "        }\n",
    "\n",
    "        match_quality = record.get(\"semantic_match_label\", \"unknown\")\n",
    "        output_path = output_dir / f\"{record['resume_id']}_{record['jd_id']}_{match_quality}.json\"\n",
    "        save_json_output(sft_data, str(output_path))\n",
    "\n",
    "        return uid\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing {fname}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446699a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sft_record_parallel(\n",
    "    scoring_file: Path,\n",
    "    scoring_dir: Path,\n",
    "    resume_dir: Path,\n",
    "    jd_dir: Path,\n",
    "    output_dir: Path,\n",
    "    checkpoint_file: Path,\n",
    "    max_workers: int = None\n",
    "):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    filenames = load_json_file(str(scoring_file))  # list of filenames\n",
    "    checkpoint_set = set()\n",
    "\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        checkpoint_set = set(load_json_file(str(checkpoint_file)).get(\"processed\", []))\n",
    "\n",
    "    updated_checkpoint = list(checkpoint_set)\n",
    "    max_workers = max_workers or get_optimal_max_workers(io_bound=True)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [\n",
    "            executor.submit(\n",
    "                process_single_file, fname, scoring_dir, resume_dir, jd_dir, output_dir, checkpoint_set\n",
    "            )\n",
    "            for fname in filenames\n",
    "        ]\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                updated_checkpoint.append(result)\n",
    "\n",
    "    save_json_output({\"processed\": updated_checkpoint}, str(checkpoint_file))\n",
    "    print(f\"‚úÖ All records processed with {max_workers} workers.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef23dd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_sft_record_parallel(\n",
    "    scoring_file=os.path.join(Config.JSON_OUTPUT_SCORING_FT_DATA, \"fine_tuning_filenames.json\"),\n",
    "    scoring_dir=Path(Config.JSON_OUTPUT_FINE_TUNE_SCORE),\n",
    "    resume_dir=Path(Config.JSON_OUTPUT_NORMALIZED_RESUME),\n",
    "    jd_dir=Path(Config.JSON_OUTPUT_NORMALIZED_JD),\n",
    "    output_dir=Path(Config.JSON_OUTPUT_FINE_TUNE_RECORD),\n",
    "    checkpoint_file=os.path.join(Config.JSON_OUTPUT_FINE_TUNE_RECORD, \"sft_checkpoint.json\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26192565",
   "metadata": {},
   "source": [
    "# Create training_data.jsonl from individual records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec12df72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def merge_sft_json_to_jsonl_notebook(input_dir: str, output_file: str):\n",
    "    \"\"\"\n",
    "    Merges individual SFT JSON files into a newline-delimited JSONL file.\n",
    "\n",
    "    Uses existing utility:\n",
    "    - load_json_file(): safely loads a .json file\n",
    "    - save_jsonl_line(): appends a record to JSONL\n",
    "    \"\"\"\n",
    "    input_path = Path(input_dir)\n",
    "    output_path = Path(output_file)\n",
    "\n",
    "    if not input_path.exists():\n",
    "        raise FileNotFoundError(f\"‚ùå Input directory not found: {input_dir}\")\n",
    "    \n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)  # üõ†Ô∏è Ensure directory exists\n",
    "\n",
    "\n",
    "    # Clear previous output if exists\n",
    "    if output_path.exists():\n",
    "        os.remove(output_path)\n",
    "\n",
    "    json_files = sorted(input_path.glob(\"*.json\"))\n",
    "    total_written = 0\n",
    "\n",
    "    for file in tqdm(json_files, desc=\"üîÑ Merging JSON files\"):\n",
    "        try:\n",
    "            record = load_json_file(str(file))\n",
    "            if \"input\" in record and \"output\" in record:\n",
    "                save_jsonl_line(record, str(output_path))\n",
    "                total_written += 1\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Skipped {file.name}: missing 'input' or 'output' field\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error reading {file.name}: {e}\")\n",
    "\n",
    "    print(f\"\\n‚úÖ Successfully merged {total_written} files into {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026239a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_sft_json_to_jsonl_notebook(\n",
    "    input_dir=Path(Config.JSON_OUTPUT_FINE_TUNE_RECORD), # \"json_outputs_all_data/fine-tune/record\",\n",
    "    output_file=os.path.join(Config.JSON_OUTPUT_FINE_TUNE_TEST_DATA, \"training_data.jsonl\"), #\"json_outputs_all_data/fine-tune/training_data.jsonl\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68da3e9f",
   "metadata": {},
   "source": [
    "##  Split Merged .jsonl into 80/20 Train/Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688c22c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def extract_match_quality(input_text: str) -> str:\n",
    "    match = re.search(r\"<MATCH_QUALITY>(.*?)</MATCH_QUALITY>\", input_text)\n",
    "    return match.group(1).strip().lower() if match else \"unknown\"\n",
    "\n",
    "def stratified_split_jsonl(input_jsonl: str, train_path: str, eval_path: str, split_ratio: float = 0.8, seed: int = 42):\n",
    "    \"\"\"\n",
    "    Perform stratified 80/20 split by <MATCH_QUALITY> tag in the 'input' field.\n",
    "    \"\"\"\n",
    "    input_path = Path(input_jsonl)\n",
    "    if not input_path.exists():\n",
    "        raise FileNotFoundError(f\"‚ùå File not found: {input_jsonl}\")\n",
    "\n",
    "    stratified_data = defaultdict(list)\n",
    "\n",
    "    # Group records by match quality\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in tqdm(f, desc=\"üìÇ Reading and grouping records\"):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                record = json.loads(line)\n",
    "                match_quality = extract_match_quality(record[\"input\"])\n",
    "                stratified_data[match_quality].append(line)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Failed to process line: {e}\")\n",
    "\n",
    "    # Split each group and collect\n",
    "    train_records, eval_records = [], []\n",
    "    random.seed(seed)\n",
    "\n",
    "    for label, records in stratified_data.items():\n",
    "        random.shuffle(records)\n",
    "        split_index = int(len(records) * split_ratio)\n",
    "        train_records.extend(records[:split_index])\n",
    "        eval_records.extend(records[split_index:])\n",
    "        print(f\"üîπ {label}: {len(records)} ‚Üí train={split_index}, eval={len(records) - split_index}\")\n",
    "\n",
    "    # Save outputs\n",
    "    Path(train_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    Path(eval_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with open(train_path, \"w\", encoding=\"utf-8\") as f_train:\n",
    "        f_train.write(\"\\n\".join(train_records) + \"\\n\")\n",
    "\n",
    "    with open(eval_path, \"w\", encoding=\"utf-8\") as f_eval:\n",
    "        f_eval.write(\"\\n\".join(eval_records) + \"\\n\")\n",
    "\n",
    "    print(f\"\\n‚úÖ Stratified split complete: {len(train_records)} train / {len(eval_records)} eval\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8971ee11",
   "metadata": {},
   "outputs": [],
   "source": [
    "stratified_split_jsonl(\n",
    "    input_jsonl=os.path.join(Config.JSON_OUTPUT_FINE_TUNE_TEST_DATA, \"training_data.jsonl\"), #\"json_outputs_all_data/fine-tune/training_data.jsonl\",\n",
    "    train_path=os.path.join(Config.JSON_OUTPUT_FINE_TUNE_TEST_DATA, \"train.jsonl\"), #\"json_outputs_all_data/fine-tune/train.jsonl\",\n",
    "    eval_path=os.path.join(Config.JSON_OUTPUT_FINE_TUNE_TEST_DATA, \"eval.jsonl\")  #\"json_outputs_all_data/fine-tune/eval.jsonl\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5349b2fc",
   "metadata": {},
   "source": [
    "# create local debugging test data of 100 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba32eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Setup\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from typing import Any\n",
    "import json\n",
    "\n",
    "# === CONFIG ===\n",
    "input_dir = Path(Config.JSON_OUTPUT_FINE_TUNE_RECORD)  #Path(\"json_outputs_all_data/fine-tune/record\")\n",
    "train_file =os.path.join(Config.JSON_OUTPUT_FINE_TUNE_TEST_DATA, \"train_local.jsonl\") \n",
    "eval_file = os.path.join(Config.JSON_OUTPUT_FINE_TUNE_TEST_DATA, \"eval_local.jsonl\")  \n",
    "sample_count = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de965c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset output files\n",
    "for file in [train_file, eval_file]:\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)\n",
    "\n",
    "# Collect labeled files\n",
    "buckets = defaultdict(list)\n",
    "for file in input_dir.glob(\"*.json\"):\n",
    "    parts = file.stem.split(\"_\")\n",
    "    if len(parts) >= 3:\n",
    "        label = parts[2].lower()\n",
    "        if label in {\"strong\", \"medium\", \"weak\"}:\n",
    "            buckets[label].append(file)\n",
    "\n",
    "# Show counts\n",
    "for label, files in buckets.items():\n",
    "    print(f\"{label}: {len(files)} files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4efbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ Stratified Sampling: Split into Train and Eval\n",
    "target_ratios = {\"strong\": 0.3, \"medium\": 0.5, \"weak\": 0.2}\n",
    "train_count = int(sample_count * 0.8)\n",
    "eval_count = sample_count - train_count\n",
    "\n",
    "split_ratio = train_count / sample_count  # 0.8\n",
    "\n",
    "train_written = 0\n",
    "eval_written = 0\n",
    "\n",
    "for label, label_ratio in target_ratios.items():\n",
    "    n_total = int(sample_count * label_ratio)\n",
    "    n_train = int(n_total * split_ratio)\n",
    "    n_eval = n_total - n_train\n",
    "\n",
    "    candidates = buckets[label]\n",
    "    selected = random.sample(candidates, min(n_total, len(candidates)))\n",
    "    train_samples = selected[:n_train]\n",
    "    eval_samples = selected[n_train:n_train + n_eval]\n",
    "\n",
    "    for path in train_samples:\n",
    "        try:\n",
    "            data = load_json_file(str(path))\n",
    "            if \"input\" in data and \"output\" in data:\n",
    "                save_jsonl_line(data, train_file)\n",
    "                train_written += 1\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Train error in {path.name}: {e}\")\n",
    "\n",
    "    for path in eval_samples:\n",
    "        try:\n",
    "            data = load_json_file(str(path))\n",
    "            if \"input\" in data and \"output\" in data:\n",
    "                save_jsonl_line(data, eval_file)\n",
    "                eval_written += 1\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Eval error in {path.name}: {e}\")\n",
    "\n",
    "print(f\"‚úÖ Wrote {train_written} examples to {train_file}\")\n",
    "print(f\"‚úÖ Wrote {eval_written} examples to {eval_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
