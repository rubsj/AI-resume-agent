{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01c8bad6",
   "metadata": {},
   "source": [
    "# Global setup and package installation used in most phases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d08acab",
   "metadata": {},
   "source": [
    "## Colab + GPU Detection Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae4b5631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def is_running_in_colab():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "def get_available_gpu_memory_gb():\n",
    "    try:\n",
    "        output = subprocess.check_output(\n",
    "            [\"nvidia-smi\", \"--query-gpu=memory.free\", \"--format=csv,nounits,noheader\"],\n",
    "            encoding=\"utf-8\"\n",
    "        )\n",
    "        free_mem_mb = int(output.strip().split(\"\\n\")[0])\n",
    "        return free_mem_mb / 1024\n",
    "    except Exception:\n",
    "        return 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934d3e82",
   "metadata": {},
   "source": [
    "## install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d72196",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_running_in_colab():\n",
    "    # Install the required packages\n",
    "    !pip install kagglehub pandas\n",
    "    !pip install -q transformers accelerate bitsandbytes sentencepiece pydantic huggingface_hub xformers\n",
    "    !pip install regex json5\n",
    "    !pip install sentence-transformers scikit-learn\n",
    "    !pip install rapidfuzz unidecode\n",
    "\n",
    "else:\n",
    "    %pip install kagglehub pandas\n",
    "    %pip install -q transformers accelerate sentencepiece pydantic huggingface_hub xformers\n",
    "    #%pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu121\n",
    "    #%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "    %pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128\n",
    "    %pip install -U bitsandbytes\n",
    "    %pip install regex json5\n",
    "    %pip install sentence-transformers scikit-learn\n",
    "    %pip install rapidfuzz unidecode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e36d02",
   "metadata": {},
   "source": [
    "## Login to huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea132993",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# Set your token here securely or prompt for it in Colab\n",
    "# Recommended: store in Colab secrets or environment variable\n",
    "HF_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "\n",
    "if not HF_TOKEN:\n",
    "    if is_running_in_colab():\n",
    "        # If running in Colab, use the Colab secrets\n",
    "        try:\n",
    "            from google.colab import userdata\n",
    "            HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "            if not HF_TOKEN:\n",
    "                raise ValueError(\"‚ö†Ô∏è Hugging Face token not found in Colab secrets.\")\n",
    "            print(\"üîë Hugging Face token found in Colab secrets.\")\n",
    "        except ImportError:\n",
    "            print(\"‚ö†Ô∏è Unable to authenticate in Colab. Please set your Hugging Face token manually.\")\n",
    "    else:\n",
    "        # Prompt for token if not set in environment\n",
    "        print(\"üîë Please enter your Hugging Face token:\")\n",
    "        # For Colab or local prompt input\n",
    "        HF_TOKEN = input(\"üîë Enter your Hugging Face token: \").strip()\n",
    "\n",
    "login(token=HF_TOKEN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a919779",
   "metadata": {},
   "source": [
    "## Setup Kaggle Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6319924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Kaggle credentials already exist at C:\\Users\\rubyj/.kaggle/kaggle.json\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "def setup_kaggle_credentials():\n",
    "    kaggle_path = os.path.expanduser('~/.kaggle/kaggle.json')\n",
    "    if not os.path.exists(kaggle_path):\n",
    "        from google.colab import files\n",
    "        print(\"üìÇ Upload kaggle.json file...\")\n",
    "        uploaded = files.upload()\n",
    "        os.makedirs(os.path.dirname(kaggle_path), exist_ok=True)\n",
    "        for filename in uploaded.keys():\n",
    "            shutil.move(filename, kaggle_path)\n",
    "        os.chmod(kaggle_path, 0o600)\n",
    "        print(f\"‚úÖ Kaggle credentials setup at {kaggle_path}\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Kaggle credentials already exist at {kaggle_path}\")\n",
    "\n",
    "setup_kaggle_credentials()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdc73bb",
   "metadata": {},
   "source": [
    "##  Load Qwen-Instruct with Fallback to Quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d8bcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "import torch\n",
    "\n",
    "def load_model_pipeline(model_name: str, hf_token: str):\n",
    "    has_cuda = torch.cuda.is_available()\n",
    "    free_mem = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3) if has_cuda else 0\n",
    "    print(f\"üíª CUDA: {has_cuda} | GPU Memory: {free_mem:.2f} GB\")\n",
    "\n",
    "    device_map = {\"\": 0} if has_cuda else \"cpu\"\n",
    "    use_4bit = has_cuda and free_mem < 24\n",
    "\n",
    "    # Set quantization config\n",
    "    quant_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True if use_4bit else False,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    ) if use_4bit else None\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token, trust_remote_code=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # ‚úÖ Fix warning about pad_token\n",
    "\n",
    "    # Load model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device_map,\n",
    "        quantization_config=quant_config,\n",
    "        torch_dtype=torch.float16 if not quant_config else None,\n",
    "        trust_remote_code=True,\n",
    "        token=hf_token\n",
    "    )\n",
    "\n",
    "    print(f\"‚úÖ Model loaded on {next(model.parameters()).device}\")\n",
    "    return pipeline(\"text-generation\", model=model, tokenizer=tokenizer, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8285479",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_pipeline = load_model_pipeline(\n",
    "    model_name=\"Qwen/Qwen2-7B-Instruct\",\n",
    "    hf_token=HF_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738cae0b",
   "metadata": {},
   "source": [
    "# Global utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1ec563",
   "metadata": {},
   "source": [
    "### Utility to save json to a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e091036",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "# üì¶ Save JSON Output with Safety\n",
    "def save_json_output(data, output_path: str, indent: int = 4, overwrite: bool = True):\n",
    "    output_dir = os.path.dirname(output_path)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    if os.path.exists(output_path):\n",
    "        if overwrite:\n",
    "            os.remove(output_path)\n",
    "        else:\n",
    "            raise FileExistsError(f\"File {output_path} already exists and overwrite=False.\")\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=indent, ensure_ascii=False)\n",
    "\n",
    "    print(f\"‚úÖ Saved output to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f6db83",
   "metadata": {},
   "source": [
    "### Utility to load file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87e7aa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import json\n",
    "\n",
    "# üìÇ Load normalized JSON data\n",
    "def load_json_file(file_path: str) -> Any:\n",
    "    if not os.path.isfile(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89959de6",
   "metadata": {},
   "source": [
    "### Configurations  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6519d739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# üõ† CONFIGURATION\n",
    "# ==============================\n",
    "\n",
    "class Config:\n",
    "    JSON_OUTPUT_DIR = \"json_outputs_all_data\"\n",
    "    JSON_OUTPUT_NORMALIZED_DIR = \"json_outputs_all_data/normalized\"\n",
    "    JSON_OUTPUT_NORMALIZED_JD = \"json_outputs_all_data/normalized/jd\"\n",
    "    JSON_OUTPUT_NORMALIZED_RESUME = \"json_outputs_all_data/normalized/resume\"\n",
    "    JSON_OUTPUT_SCORING_DIR = \"json_outputs_all_data/scoring\"\n",
    "    JSON_OUTPUT_SCORING_SPLIT_DIR = \"json_outputs_all_data/scoring/split\"\n",
    "    JSON_OUTPUT_SCORING_FT_DATA = \"json_outputs_all_data/scoring/FT_data\"\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84403281",
   "metadata": {},
   "source": [
    "# Generate 30K core sample semantic data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "635c4f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple, Set\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "\n",
    "# === 1. FILTERING ===\n",
    "def filter_pairs_by_score(pairs: List[Dict], threshold: float) -> List[Dict]:\n",
    "    return [p for p in pairs if p.get(\"resume_jd_similarity\", 0.0) >= threshold]\n",
    "\n",
    "\n",
    "# === 2. GROUPING ===\n",
    "def group_pairs_by_quality_and_resume(pairs: List[Dict]) -> Tuple[\n",
    "    Dict[str, List[Dict]],\n",
    "    Dict[int, Dict[str, List[Dict]]],\n",
    "    Dict[str, List[Dict]]\n",
    "]:\n",
    "    quality_buckets = defaultdict(list)\n",
    "    resume_to_qualities = defaultdict(lambda: defaultdict(list))\n",
    "    domain_coverage = defaultdict(list)\n",
    "\n",
    "    for p in pairs:\n",
    "        q = p.get(\"semantic_match_label\", \"unknown\").lower()\n",
    "        rid = p[\"resume_id\"]\n",
    "        res_dom = p.get(\"resume_domain\", \"unknown\").lower()\n",
    "        jd_dom = p.get(\"jd_domain\", \"unknown\").lower()\n",
    "\n",
    "        quality_buckets[q].append(p)\n",
    "        resume_to_qualities[rid][q].append(p)\n",
    "        domain_coverage[res_dom].append(p)\n",
    "        domain_coverage[jd_dom].append(p)\n",
    "\n",
    "    return quality_buckets, resume_to_qualities, domain_coverage\n",
    "\n",
    "\n",
    "# === 3. RESUME-BALANCED SAMPLING ===\n",
    "def resume_balanced_sampling(\n",
    "    resume_to_qualities: Dict[int, Dict[str, List[Dict]]],\n",
    "    target_qualities: List[str],\n",
    "    seen_pairs: Set[Tuple[int, int]]\n",
    ") -> Dict[str, List[Dict]]:\n",
    "    selected = defaultdict(list)\n",
    "    resume_count = 0\n",
    "\n",
    "    for resume_id, qmap in resume_to_qualities.items():\n",
    "        used = 0\n",
    "        for q in target_qualities:\n",
    "            if q in qmap:\n",
    "                pair = random.choice(qmap[q])\n",
    "                key = (pair[\"resume_id\"], pair[\"jd_id\"])\n",
    "                if key not in seen_pairs:\n",
    "                    selected[q].append(pair)\n",
    "                    seen_pairs.add(key)\n",
    "                    used += 1\n",
    "        if used > 0:\n",
    "            resume_count += 1\n",
    "\n",
    "    return selected, resume_count\n",
    "\n",
    "\n",
    "# === 4. DOMAIN QUOTA ENFORCEMENT ===\n",
    "def enforce_domain_quota(\n",
    "    quality_buckets: Dict[str, List[Dict]],\n",
    "    seen_pairs: Set[Tuple[int, int]],\n",
    "    target_counts: Dict[str, int],\n",
    "    current_counts: Dict[str, int],\n",
    "    min_per_domain: int\n",
    ") -> Dict[str, List[Dict]]:\n",
    "    selected = defaultdict(list)\n",
    "    domain_pair_cache = defaultdict(list)\n",
    "\n",
    "    for q in quality_buckets:\n",
    "        for p in quality_buckets[q]:\n",
    "            d1 = p.get(\"resume_domain\", \"unknown\").lower()\n",
    "            d2 = p.get(\"jd_domain\", \"unknown\").lower()\n",
    "            domain_pair_cache[(q, d1)].append(p)\n",
    "            domain_pair_cache[(q, d2)].append(p)\n",
    "\n",
    "    for (q, domain), group in domain_pair_cache.items():\n",
    "        if current_counts[q] >= target_counts[q]:\n",
    "            continue  # already filled this quality\n",
    "\n",
    "        candidates = [p for p in group if (p[\"resume_id\"], p[\"jd_id\"]) not in seen_pairs]\n",
    "        room_left = target_counts[q] - current_counts[q]\n",
    "        take_count = min(min_per_domain, len(candidates), room_left)\n",
    "        sampled = random.sample(candidates, take_count)\n",
    "\n",
    "        for p in sampled:\n",
    "            key = (p[\"resume_id\"], p[\"jd_id\"])\n",
    "            if key not in seen_pairs:\n",
    "                selected[q].append(p)\n",
    "                seen_pairs.add(key)\n",
    "                current_counts[q] += 1\n",
    "                if current_counts[q] >= target_counts[q]:\n",
    "                    break  # stop sampling more of this quality\n",
    "\n",
    "    return selected\n",
    "\n",
    "\n",
    "\n",
    "# === 5. FILL REMAINING WITH DIVERSITY ===\n",
    "def fill_remaining_by_diverse_domains(\n",
    "    quality_buckets: Dict[str, List[Dict]],\n",
    "    selected_by_quality: Dict[str, List[Dict]],\n",
    "    target_counts: Dict[str, int],\n",
    "    seen_pairs: Set[Tuple[int, int]]\n",
    ") -> Dict[str, List[Dict]]:\n",
    "    for q in target_counts:\n",
    "        remaining = target_counts[q] - len(selected_by_quality[q])\n",
    "        if remaining <= 0:\n",
    "            continue\n",
    "\n",
    "        available = [\n",
    "            p for p in quality_buckets[q]\n",
    "            if (p[\"resume_id\"], p[\"jd_id\"]) not in seen_pairs\n",
    "        ]\n",
    "\n",
    "        # Bucket by domain\n",
    "        domain_groups = defaultdict(list)\n",
    "        for p in available:\n",
    "            domain_groups[p.get(\"resume_domain\", \"unknown\").lower()].append(p)\n",
    "            domain_groups[p.get(\"jd_domain\", \"unknown\").lower()].append(p)\n",
    "\n",
    "        sampled = []\n",
    "        domain_cycle = list(domain_groups.keys())\n",
    "        random.shuffle(domain_cycle)\n",
    "\n",
    "        while remaining > 0 and domain_cycle:\n",
    "            domain = domain_cycle.pop(0)\n",
    "            group = domain_groups[domain]\n",
    "            group = [p for p in group if (p[\"resume_id\"], p[\"jd_id\"]) not in seen_pairs]\n",
    "            if not group:\n",
    "                continue\n",
    "            chosen = random.choice(group)\n",
    "            sampled.append(chosen)\n",
    "            seen_pairs.add((chosen[\"resume_id\"], chosen[\"jd_id\"]))\n",
    "            domain_groups[domain].remove(chosen)\n",
    "            domain_cycle.append(domain)\n",
    "            remaining -= 1\n",
    "\n",
    "        selected_by_quality[q].extend(sampled)\n",
    "\n",
    "    return selected_by_quality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63a9c6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 6. MAIN WRAPPER ===\n",
    "def generate_balanced_sample(\n",
    "    all_pairs: List[Dict],\n",
    "    target_counts: Dict[str, int],\n",
    "    score_threshold: float = 0.1,\n",
    "    min_per_domain: int = 50\n",
    ") -> List[Dict]:\n",
    "    print(f\"üîç Total pairs in input: {len(all_pairs)}\")\n",
    "    filtered = filter_pairs_by_score(all_pairs, score_threshold)\n",
    "    print(f\"‚úÖ After score ‚â• {score_threshold}: {len(filtered)}\")\n",
    "\n",
    "    quality_buckets, resume_to_qualities, domain_coverage = group_pairs_by_quality_and_resume(filtered)\n",
    "    print(\"üìä Match quality counts:\")\n",
    "    for q in target_counts:\n",
    "        print(f\"  {q.upper():<7}: {len(quality_buckets[q])}\")\n",
    "    print(f\"üë• Unique resumes: {len(resume_to_qualities)}\")\n",
    "\n",
    "    seen_pairs = set()\n",
    "\n",
    "    # Resume-balanced\n",
    "    selected_by_quality, resume_count = resume_balanced_sampling(resume_to_qualities, list(target_counts.keys()), seen_pairs)\n",
    "    print(f\"üë§ Resume-balanced resumes: {resume_count}\")\n",
    "\n",
    "    # Domain quotas\n",
    "    current_counts = {q: len(selected_by_quality[q]) for q in target_counts}\n",
    "    domain_quota_selected = enforce_domain_quota(\n",
    "        quality_buckets,\n",
    "        seen_pairs,\n",
    "        target_counts,\n",
    "        current_counts,\n",
    "        min_per_domain\n",
    "    )\n",
    "\n",
    "    for q in target_counts:\n",
    "        selected_by_quality[q].extend(domain_quota_selected.get(q, []))\n",
    "\n",
    "    # Fill remaining\n",
    "    selected_by_quality = fill_remaining_by_diverse_domains(quality_buckets, selected_by_quality, target_counts, seen_pairs)\n",
    "\n",
    "    # Final merge\n",
    "    final_sample = []\n",
    "    print(\"\\nüì¶ Final sampled count:\")\n",
    "    for q in target_counts:\n",
    "        group = selected_by_quality[q]\n",
    "        print(f\"  {q.upper():<7}: {len(group)}\")\n",
    "        final_sample.extend(group)\n",
    "\n",
    "    print(f\"\\nüéØ Total selected: {len(final_sample)}\")\n",
    "\n",
    "    # Domain coverage\n",
    "    resume_domains = [p.get(\"resume_domain\", \"unknown\").lower() for p in final_sample]\n",
    "    jd_domains = [p.get(\"jd_domain\", \"unknown\").lower() for p in final_sample]\n",
    "    print(\"\\nüìä Resume domain coverage (top 10):\")\n",
    "    for dom, count in Counter(resume_domains).most_common(10):\n",
    "        print(f\"  {dom:<30} {count}\")\n",
    "\n",
    "    print(\"\\nüìä JD domain coverage (top 10):\")\n",
    "    for dom, count in Counter(jd_domains).most_common(10):\n",
    "        print(f\"  {dom:<30} {count}\")\n",
    "\n",
    "    return final_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62e4a037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Total pairs in input: 2463477\n",
      "‚úÖ After score ‚â• 0.2: 2463477\n",
      "üìä Match quality counts:\n",
      "  STRONG : 15108\n",
      "  MEDIUM : 998172\n",
      "  WEAK   : 1450197\n",
      "üë• Unique resumes: 2079\n",
      "üë§ Resume-balanced resumes: 2079\n",
      "\n",
      "üì¶ Final sampled count:\n",
      "  STRONG : 14993\n",
      "  MEDIUM : 25000\n",
      "  WEAK   : 15000\n",
      "\n",
      "üéØ Total selected: 54993\n",
      "\n",
      "üìä Resume domain coverage (top 10):\n",
      "  chef                           5943\n",
      "  accountant                     5854\n",
      "  sales                          5482\n",
      "  engineering                    5401\n",
      "  finance                        4787\n",
      "  construction                   3686\n",
      "  business-development           3322\n",
      "  banking                        2632\n",
      "  information-technology         2476\n",
      "  consultant                     1929\n",
      "\n",
      "üìä JD domain coverage (top 10):\n",
      "  finance                        7052\n",
      "  food_service                   2497\n",
      "  retail                         2097\n",
      "  construction                   1631\n",
      "  manufacturing                  1087\n",
      "  education                      1077\n",
      "  business                       847\n",
      "  engineering                    771\n",
      "  sales                          763\n",
      "  information technology         728\n",
      "‚úÖ Saved output to json_outputs_all_data/scoring\\relevant_pairs.json\n"
     ]
    }
   ],
   "source": [
    "# üéØ Define how many samples per match type\n",
    "target_counts = {\n",
    "    \"strong\": 20000,\n",
    "    \"medium\": 25000,\n",
    "    \"weak\": 15000\n",
    "}\n",
    "\n",
    "# üì• Load the filtered list only\n",
    "input_path = os.path.join(Config.JSON_OUTPUT_SCORING_DIR, 'semantic_relevance_scores.json')\n",
    "full_data  = load_json_file(input_path)\n",
    "relevance_data = full_data.get(\"semantic_relevance_scores\", [])\n",
    "\n",
    "# üß† Generate the balanced subset\n",
    "sampled = generate_balanced_sample(\n",
    "    all_pairs=relevance_data,\n",
    "    target_counts=target_counts,\n",
    "    score_threshold=0.2\n",
    ")\n",
    "\n",
    "# Wrap in valid JSON object structure\n",
    "relevance_wrapped = {\n",
    "    \"semantic_relevance_scores\": sampled\n",
    "}\n",
    "\n",
    "# üíæ Save the sampled subset\n",
    "relevance_map_file = os.path.join(Config.JSON_OUTPUT_SCORING_DIR, 'relevant_pairs.json')\n",
    "save_json_output(relevance_wrapped, relevance_map_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58eb1907",
   "metadata": {},
   "source": [
    "## Generate Normal distributed 30K samples file from scores files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b9ed14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "SCORING_FOLDER = Config.JSON_OUTPUT_SCORING_DIR  # path to scoring files\n",
    "OUTPUT_FILE = os.path.join(Config.JSON_OUTPUT_SCORING_FT_DATA, \"fine_tuning_filenames.json\")  # JSON_OUTPUT_SCORING_FT_DATA\n",
    "TARGET_COUNTS = {\"strong\": 10000, \"medium\": 15000, \"weak\": 5000}\n",
    "QUALITY_BUCKETS = list(TARGET_COUNTS.keys())\n",
    "\n",
    "def parse_filename(fname):\n",
    "    match = re.match(r\"^(\\d+)_(\\d+)_(strong|medium|weak)_[^_]+_([\\d.]+)\\.json$\", fname)\n",
    "    if not match:\n",
    "        return None\n",
    "    resume_id, jd_id, quality, score = match.groups()\n",
    "    return {\n",
    "        \"filename\": fname,\n",
    "        \"resume_id\": resume_id,\n",
    "        \"jd_id\": jd_id,\n",
    "        \"quality\": quality,\n",
    "        \"score\": float(score)\n",
    "    }\n",
    "\n",
    "def load_scoring_files(folder):\n",
    "    all_meta = []\n",
    "    for fname in os.listdir(folder):\n",
    "        if not fname.endswith(\".json\"):\n",
    "            continue\n",
    "        meta = parse_filename(fname)\n",
    "        if not meta:\n",
    "            continue\n",
    "        path = os.path.join(folder, fname)\n",
    "        try:\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "            meta.update({\n",
    "                \"domain\": data.get(\"domain\", \"UNKNOWN\"),\n",
    "                \"match_quality\": data.get(\"match_quality\", \"\").lower(),\n",
    "                \"semantic_match_label\": data.get(\"semantic_match_label\", \"\").lower()\n",
    "            })\n",
    "            all_meta.append(meta)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Skipping {fname}: {e}\")\n",
    "    return all_meta\n",
    "\n",
    "def group_by_label(meta_list):\n",
    "    return {\n",
    "        \"strong\": [m for m in meta_list if m[\"match_quality\"] == \"strong\"],\n",
    "        \"strong_semantic\": [m for m in meta_list if m[\"semantic_match_label\"] == \"strong\" and m[\"match_quality\"] != \"strong\"],\n",
    "        \"medium\": [m for m in meta_list if m[\"match_quality\"] == \"medium\"],\n",
    "        \"weak\": [m for m in meta_list if m[\"match_quality\"] == \"weak\"]\n",
    "    }\n",
    "\n",
    "def select_by_domain_distribution(meta_list, limit):\n",
    "    selected = []\n",
    "    domain_buckets = defaultdict(list)\n",
    "    for m in meta_list:\n",
    "        domain_buckets[m[\"domain\"]].append(m)\n",
    "\n",
    "    # Equal or proportionally balanced selection\n",
    "    domain_keys = list(domain_buckets.keys())\n",
    "    per_domain = max(1, limit // len(domain_keys))\n",
    "    for domain in domain_keys:\n",
    "        random.shuffle(domain_buckets[domain])\n",
    "        selected.extend(domain_buckets[domain][:per_domain])\n",
    "\n",
    "    if len(selected) < limit:\n",
    "        remaining = [m for m in meta_list if m[\"filename\"] not in {x[\"filename\"] for x in selected}]\n",
    "        random.shuffle(remaining)\n",
    "        selected.extend(remaining[:limit - len(selected)])\n",
    "    return selected[:limit]\n",
    "\n",
    "def log_distribution(selected_list, label):\n",
    "    total = len(selected_list)\n",
    "    by_domain = Counter([m[\"domain\"] for m in selected_list])\n",
    "    print(f\"\\nüìä Final count for {label}: {total}\")\n",
    "    for domain, count in by_domain.most_common():\n",
    "        print(f\"  {domain}: {count}\")\n",
    "\n",
    "def main():\n",
    "    all_meta = load_scoring_files(SCORING_FOLDER)\n",
    "    grouped = group_by_label(all_meta)\n",
    "    final_selected = []\n",
    "\n",
    "    # Step 1: Strong group\n",
    "    strong_actual = grouped[\"strong\"]\n",
    "    if len(strong_actual) >= TARGET_COUNTS[\"strong\"]:\n",
    "        strong_selected = select_by_domain_distribution(strong_actual, TARGET_COUNTS[\"strong\"])\n",
    "    else:\n",
    "        shortfall = TARGET_COUNTS[\"strong\"] - len(strong_actual)\n",
    "        fallback = grouped[\"strong_semantic\"]\n",
    "        fallback_selected = select_by_domain_distribution(fallback, shortfall)\n",
    "        strong_selected = strong_actual + fallback_selected\n",
    "    log_distribution(strong_selected, \"strong\")\n",
    "    final_selected.extend(strong_selected)\n",
    "    selected_filenames = {m[\"filename\"] for m in final_selected}\n",
    "\n",
    "    # Step 2: Medium group (exclude used)\n",
    "    medium_pool = [m for m in grouped[\"medium\"] if m[\"filename\"] not in selected_filenames]\n",
    "    medium_selected = select_by_domain_distribution(medium_pool, TARGET_COUNTS[\"medium\"])\n",
    "    log_distribution(medium_selected, \"medium\")\n",
    "    final_selected.extend(medium_selected)\n",
    "    selected_filenames.update(m[\"filename\"] for m in medium_selected)\n",
    "\n",
    "    # Step 3: Weak group (exclude used)\n",
    "    weak_pool = [m for m in grouped[\"weak\"] if m[\"filename\"] not in selected_filenames]\n",
    "    weak_selected = select_by_domain_distribution(weak_pool, TARGET_COUNTS[\"weak\"])\n",
    "    log_distribution(weak_selected, \"weak\")\n",
    "    final_selected.extend(weak_selected)\n",
    "\n",
    "    # Save output\n",
    "    filenames = [m[\"filename\"] for m in final_selected]\n",
    "    with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(filenames, f, indent=2)\n",
    "    print(f\"\\n‚úÖ Saved final list to {OUTPUT_FILE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
