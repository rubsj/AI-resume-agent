{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01c8bad6",
   "metadata": {},
   "source": [
    "# Global setup and package installation used in most phases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d08acab",
   "metadata": {},
   "source": [
    "## Colab + GPU Detection Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae4b5631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def is_running_in_colab():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "def get_available_gpu_memory_gb():\n",
    "    try:\n",
    "        output = subprocess.check_output(\n",
    "            [\"nvidia-smi\", \"--query-gpu=memory.free\", \"--format=csv,nounits,noheader\"],\n",
    "            encoding=\"utf-8\"\n",
    "        )\n",
    "        free_mem_mb = int(output.strip().split(\"\\n\")[0])\n",
    "        return free_mem_mb / 1024\n",
    "    except Exception:\n",
    "        return 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934d3e82",
   "metadata": {},
   "source": [
    "## install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3d72196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kagglehub\n",
      "  Downloading kagglehub-0.3.12-py3-none-any.whl (67 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.0/68.0 KB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pandas\n",
      "  Downloading pandas-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from kagglehub) (25.0)\n",
      "Collecting requests\n",
      "  Downloading requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.8/64.8 KB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyyaml\n",
      "  Downloading PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (751 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m751.2/751.2 KB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 KB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting numpy>=1.22.4\n",
      "  Downloading numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tzdata>=2022.7\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 KB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pytz>=2020.1\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 KB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2025.6.15-py3-none-any.whl (157 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.7/157.7 KB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting idna<4,>=2.5\n",
      "  Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 KB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting charset_normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.5/149.5 KB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.8/129.8 KB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytz, urllib3, tzdata, tqdm, pyyaml, numpy, idna, charset_normalizer, certifi, requests, pandas, kagglehub\n",
      "Successfully installed certifi-2025.6.15 charset_normalizer-3.4.2 idna-3.10 kagglehub-0.3.12 numpy-2.2.6 pandas-2.3.0 pytz-2025.2 pyyaml-6.0.2 requests-2.32.4 tqdm-4.67.1 tzdata-2025.2 urllib3-2.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: torch in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (2.7.0)\n",
      "Collecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.20.1%2Bcu121-cp310-cp310-linux_x86_64.whl (7.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hCollecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.5.1%2Bcu121-cp310-cp310-linux_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: fsspec in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from torch) (2025.5.1)\n",
      "Requirement already satisfied: triton==3.3.0 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from torch) (3.3.0)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from torch) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: jinja2 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: networkx in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from torch) (9.5.1.17)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from torch) (4.14.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from triton==3.3.0->torch) (59.6.0)\n",
      "Collecting pillow!=8.3.*,>=5.3.0\n",
      "  Downloading https://download.pytorch.org/whl/pillow-11.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m110.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from torchvision) (2.2.6)\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torch-2.5.1%2Bcu121-cp310-cp310-linux_x86_64.whl (780.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m780.4/780.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of <Python from Requires-Python> to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.20.0%2Bcu121-cp310-cp310-linux_x86_64.whl (7.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torch-2.5.0%2Bcu121-cp310-cp310-linux_x86_64.whl (780.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m780.4/780.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.19.1%2Bcu121-cp310-cp310-linux_x86_64.whl (7.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torch-2.4.1%2Bcu121-cp310-cp310-linux_x86_64.whl (798.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.9/798.9 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.19.0%2Bcu121-cp310-cp310-linux_x86_64.whl (7.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m93.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torch-2.4.0%2Bcu121-cp310-cp310-linux_x86_64.whl (799.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m799.1/799.1 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.18.1%2Bcu121-cp310-cp310-linux_x86_64.whl (7.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m98.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torch-2.3.1%2Bcu121-cp310-cp310-linux_x86_64.whl (781.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.0/781.0 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.18.0%2Bcu121-cp310-cp310-linux_x86_64.whl (7.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m93.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torch-2.3.0%2Bcu121-cp310-cp310-linux_x86_64.whl (781.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.0/781.0 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.17.2%2Bcu121-cp310-cp310-linux_x86_64.whl (7.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torch-2.2.2%2Bcu121-cp310-cp310-linux_x86_64.whl (757.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m757.3/757.3 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.17.1%2Bcu121-cp310-cp310-linux_x86_64.whl (7.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m110.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torch-2.2.1%2Bcu121-cp310-cp310-linux_x86_64.whl (757.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m757.3/757.3 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of <Python from Requires-Python> to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.17.0%2Bcu121-cp310-cp310-linux_x86_64.whl (7.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torch-2.2.0%2Bcu121-cp310-cp310-linux_x86_64.whl (757.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m757.3/757.3 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from torchvision) (2.32.4)\n",
      "Collecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.16.2%2Bcu121-cp310-cp310-linux_x86_64.whl (6.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torch-2.1.2%2Bcu121-cp310-cp310-linux_x86_64.whl (2200.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 GB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.16.1%2Bcu121-cp310-cp310-linux_x86_64.whl (6.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torch-2.1.1%2Bcu121-cp310-cp310-linux_x86_64.whl (2200.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 GB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.16.0%2Bcu121-cp310-cp310-linux_x86_64.whl (7.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torch-2.1.0%2Bcu121-cp310-cp310-linux_x86_64.whl (2200.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 GB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/torchvision-0.2.0-py2.py3-none-any.whl (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.8/48.8 KB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from torchvision) (1.17.0)\n",
      "INFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.5.0%2Bcu121-cp310-cp310-linux_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m112.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.4.1%2Bcu121-cp310-cp310-linux_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.4.0%2Bcu121-cp310-cp310-linux_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m92.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.3.1%2Bcu121-cp310-cp310-linux_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m115.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.3.0%2Bcu121-cp310-cp310-linux_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m92.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.2.2%2Bcu121-cp310-cp310-linux_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.2.1%2Bcu121-cp310-cp310-linux_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.2.0%2Bcu121-cp310-cp310-linux_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.1.2%2Bcu121-cp310-cp310-linux_x86_64.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.1.1%2Bcu121-cp310-cp310-linux_x86_64.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.1.0%2Bcu121-cp310-cp310-linux_x86_64.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m116.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "Collecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/torchvision-0.1.6-py3-none-any.whl (16 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "INFO: pip is looking at multiple versions of triton to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting triton==3.3.0\n",
      "  Downloading https://download.pytorch.org/whl/triton-3.3.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.4/156.4 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "INFO: pip is looking at multiple versions of nvidia-nvtx-cu12 to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of nvidia-nvjitlink-cu12 to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of nvidia-nccl-cu12 to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of nvidia-cusparselt-cu12 to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of nvidia-cusparse-cu12 to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of nvidia-cusolver-cu12 to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of nvidia-curand-cu12 to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of nvidia-cufile-cu12 to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of nvidia-cufft-cu12 to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of nvidia-cudnn-cu12 to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of nvidia-cuda-runtime-cu12 to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of nvidia-cuda-nvrtc-cu12 to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of nvidia-cuda-cupti-cu12 to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of nvidia-cublas-cu12 to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting nvidia-curand-cu12==10.3.2.106\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.21.5\n",
      "  Downloading https://download.pytorch.org/whl/nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12==12.1.105\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting sympy==1.13.1\n",
      "  Downloading https://download.pytorch.org/whl/sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m117.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting triton==3.1.0\n",
      "  Downloading https://download.pytorch.org/whl/triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 KB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m102.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 KB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Installing collected packages: triton, sympy, pillow, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchvision, torchaudio\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.3.0\n",
      "    Uninstalling triton-3.3.0:\n",
      "      Successfully uninstalled triton-3.3.0\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.14.0\n",
      "    Uninstalling sympy-1.14.0:\n",
      "      Successfully uninstalled sympy-1.14.0\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
      "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.26.2\n",
      "    Uninstalling nvidia-nccl-cu12-2.26.2:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.26.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
      "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.5.1.17\n",
      "    Uninstalling nvidia-cudnn-cu12-9.5.1.17:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.5.1.17\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.7.0\n",
      "    Uninstalling torch-2.7.0:\n",
      "      Successfully uninstalled torch-2.7.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "xformers 0.0.30 requires torch==2.7.0, but you have torch 2.5.1+cu121 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.21.5 nvidia-nvtx-cu12-12.1.105 pillow-11.0.0 sympy-1.13.1 torch-2.5.1+cu121 torchaudio-2.5.1+cu121 torchvision-0.20.1+cu121 triton-3.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl (67.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch<3,>=2.2 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from bitsandbytes) (2.5.1+cu121)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from bitsandbytes) (2.2.6)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from torch<3,>=2.2->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from torch<3,>=2.2->bitsandbytes) (2.21.5)\n",
      "Requirement already satisfied: networkx in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from torch<3,>=2.2->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: fsspec in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from torch<3,>=2.2->bitsandbytes) (2025.5.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from torch<3,>=2.2->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from torch<3,>=2.2->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from torch<3,>=2.2->bitsandbytes) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from torch<3,>=2.2->bitsandbytes) (12.1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from torch<3,>=2.2->bitsandbytes) (4.14.0)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from torch<3,>=2.2->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from torch<3,>=2.2->bitsandbytes) (12.1.0.106)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from torch<3,>=2.2->bitsandbytes) (3.1.0)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from torch<3,>=2.2->bitsandbytes) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from torch<3,>=2.2->bitsandbytes) (11.4.5.107)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from torch<3,>=2.2->bitsandbytes) (3.18.0)\n",
      "Requirement already satisfied: jinja2 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from torch<3,>=2.2->bitsandbytes) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<3,>=2.2->bitsandbytes) (12.6.85)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n",
      "Installing collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.46.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: regex in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (2024.11.6)\n",
      "Collecting json5\n",
      "  Downloading json5-0.12.0-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: json5\n",
      "Successfully installed json5-0.12.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-4.1.0-py3-none-any.whl (345 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.7/345.7 KB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing_extensions>=4.5.0 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from sentence-transformers) (4.14.0)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.7/37.7 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from sentence-transformers) (2.5.1+cu121)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from sentence-transformers) (4.52.4)\n",
      "Requirement already satisfied: Pillow in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from sentence-transformers) (0.33.0)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from scikit-learn) (2.2.6)\n",
      "Collecting threadpoolctl>=3.1.0\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Collecting joblib>=1.2.0\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 KB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec>=2023.5.0 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.5.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
      "Requirement already satisfied: jinja2 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (11.4.5.107)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.0)\n",
      "Requirement already satisfied: networkx in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.6.85)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.6.15)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn, sentence-transformers\n",
      "Successfully installed joblib-1.5.1 scikit-learn-1.7.0 scipy-1.15.3 sentence-transformers-4.1.0 threadpoolctl-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting rapidfuzz\n",
      "  Downloading rapidfuzz-3.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hCollecting unidecode\n",
      "  Downloading Unidecode-1.4.0-py3-none-any.whl (235 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.8/235.8 KB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: unidecode, rapidfuzz\n",
      "Successfully installed rapidfuzz-3.13.0 unidecode-1.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "if is_running_in_colab():\n",
    "    # Install the required packages\n",
    "    !pip install kagglehub pandas\n",
    "    !pip install -q transformers accelerate bitsandbytes sentencepiece pydantic huggingface_hub xformers\n",
    "    !pip install regex json5\n",
    "    !pip install sentence-transformers scikit-learn\n",
    "    !pip install rapidfuzz unidecode\n",
    "\n",
    "else:\n",
    "    %pip install kagglehub pandas\n",
    "    %pip install -q transformers accelerate sentencepiece pydantic huggingface_hub xformers\n",
    "    #%pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu121\n",
    "    %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "    #%pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128\n",
    "    %pip install -U bitsandbytes\n",
    "    %pip install regex json5\n",
    "    %pip install sentence-transformers scikit-learn\n",
    "    %pip install rapidfuzz unidecode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e36d02",
   "metadata": {},
   "source": [
    "## Login to huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea132993",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# Set your token here securely or prompt for it in Colab\n",
    "# Recommended: store in Colab secrets or environment variable\n",
    "HF_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "\n",
    "if not HF_TOKEN:\n",
    "    if is_running_in_colab():\n",
    "        # If running in Colab, use the Colab secrets\n",
    "        try:\n",
    "            from google.colab import userdata\n",
    "            HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "            if not HF_TOKEN:\n",
    "                raise ValueError(\"⚠️ Hugging Face token not found in Colab secrets.\")\n",
    "            print(\"🔑 Hugging Face token found in Colab secrets.\")\n",
    "        except ImportError:\n",
    "            print(\"⚠️ Unable to authenticate in Colab. Please set your Hugging Face token manually.\")\n",
    "    else:\n",
    "        # Prompt for token if not set in environment\n",
    "        print(\"🔑 Please enter your Hugging Face token:\")\n",
    "        # For Colab or local prompt input\n",
    "        HF_TOKEN = input(\"🔑 Enter your Hugging Face token: \").strip()\n",
    "\n",
    "login(token=HF_TOKEN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a919779",
   "metadata": {},
   "source": [
    "## Setup Kaggle Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6319924",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def setup_kaggle_credentials():\n",
    "    kaggle_path = os.path.expanduser('~/.kaggle/kaggle.json')\n",
    "    if not os.path.exists(kaggle_path):\n",
    "        from google.colab import files\n",
    "        print(\"📂 Upload kaggle.json file...\")\n",
    "        uploaded = files.upload()\n",
    "        os.makedirs(os.path.dirname(kaggle_path), exist_ok=True)\n",
    "        for filename in uploaded.keys():\n",
    "            shutil.move(filename, kaggle_path)\n",
    "        os.chmod(kaggle_path, 0o600)\n",
    "        print(f\"✅ Kaggle credentials setup at {kaggle_path}\")\n",
    "    else:\n",
    "        print(f\"✅ Kaggle credentials already exist at {kaggle_path}\")\n",
    "\n",
    "setup_kaggle_credentials()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdc73bb",
   "metadata": {},
   "source": [
    "##  Load Qwen-Instruct with Fallback to Quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d8bcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "import torch\n",
    "\n",
    "def load_model_pipeline(model_name: str, hf_token: str):\n",
    "    has_cuda = torch.cuda.is_available()\n",
    "    free_mem = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3) if has_cuda else 0\n",
    "    print(f\"💻 CUDA: {has_cuda} | GPU Memory: {free_mem:.2f} GB\")\n",
    "\n",
    "    device_map = {\"\": 0} if has_cuda else \"cpu\"\n",
    "    use_4bit = has_cuda and free_mem < 24\n",
    "\n",
    "    # Set quantization config\n",
    "    quant_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True if use_4bit else False,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    ) if use_4bit else None\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token, trust_remote_code=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # ✅ Fix warning about pad_token\n",
    "\n",
    "    # Load model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device_map,\n",
    "        quantization_config=quant_config,\n",
    "        torch_dtype=torch.float16 if not quant_config else None,\n",
    "        trust_remote_code=True,\n",
    "        token=hf_token\n",
    "    )\n",
    "\n",
    "    print(f\"✅ Model loaded on {next(model.parameters()).device}\")\n",
    "    return pipeline(\"text-generation\", model=model, tokenizer=tokenizer, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8285479",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_pipeline = load_model_pipeline(\n",
    "    model_name=\"Qwen/Qwen2-7B-Instruct\",\n",
    "    hf_token=HF_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738cae0b",
   "metadata": {},
   "source": [
    "# Global utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1ec563",
   "metadata": {},
   "source": [
    "### Utility to save json to a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e091036",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "# 📦 Save JSON Output with Safety\n",
    "def save_json_output(data, output_path: str, indent: int = 4, overwrite: bool = True):\n",
    "    output_dir = os.path.dirname(output_path)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    if os.path.exists(output_path):\n",
    "        if overwrite:\n",
    "            os.remove(output_path)\n",
    "        else:\n",
    "            raise FileExistsError(f\"File {output_path} already exists and overwrite=False.\")\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=indent, ensure_ascii=False)\n",
    "\n",
    "    print(f\"✅ Saved output to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f6db83",
   "metadata": {},
   "source": [
    "### Utility to load file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87e7aa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import json\n",
    "\n",
    "# 📂 Load normalized JSON data\n",
    "def load_json_file(file_path: str) -> Any:\n",
    "    if not os.path.isfile(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5a9e6f",
   "metadata": {},
   "source": [
    "### Utility to save jsonl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c5a9af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_jsonl_line(record: dict, output_file: str):\n",
    "    \"\"\"\n",
    "    Appends a single JSON object as a line to a .jsonl file.\n",
    "    \"\"\"\n",
    "    with open(output_file, \"a\", encoding=\"utf-8\") as f:\n",
    "        json.dump(record, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89959de6",
   "metadata": {},
   "source": [
    "### Configurations  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6519d739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 🛠 CONFIGURATION\n",
    "# ==============================\n",
    "\n",
    "class Config:\n",
    "    JSON_OUTPUT_DIR = \"json_outputs_all_data\"\n",
    "    JSON_OUTPUT_NORMALIZED_DIR = \"json_outputs_all_data/normalized\"\n",
    "    JSON_OUTPUT_NORMALIZED_JD = \"json_outputs_all_data/normalized/jd\"\n",
    "    JSON_OUTPUT_NORMALIZED_RESUME = \"json_outputs_all_data/normalized/resume\"\n",
    "    JSON_OUTPUT_SCORING_DIR = \"json_outputs_all_data/scoring\"\n",
    "    JSON_OUTPUT_SCORING_SPLIT_DIR = \"json_outputs_all_data/scoring/split\"\n",
    "    JSON_OUTPUT_SCORING_FT_DATA = \"json_outputs_all_data/scoring/FT_data\"\n",
    "    JSON_OUTPUT_FINE_TUNE_SCORE = \"json_outputs_all_data/fine-tune/scored\"\n",
    "    JSON_OUTPUT_FINE_TUNE_RECORD = \"json_outputs_all_data/fine-tune/record\"\n",
    "    JSON_OUTPUT_FINE_TUNE_TEST_DATA = \"json_outputs_all_data/fine-tune/test-data\"\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84403281",
   "metadata": {},
   "source": [
    "# Generate 30K core sample semantic data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635c4f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple, Set\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "\n",
    "# === 1. FILTERING ===\n",
    "def filter_pairs_by_score(pairs: List[Dict], threshold: float) -> List[Dict]:\n",
    "    return [p for p in pairs if p.get(\"resume_jd_similarity\", 0.0) >= threshold]\n",
    "\n",
    "\n",
    "# === 2. GROUPING ===\n",
    "def group_pairs_by_quality_and_resume(pairs: List[Dict]) -> Tuple[\n",
    "    Dict[str, List[Dict]],\n",
    "    Dict[int, Dict[str, List[Dict]]],\n",
    "    Dict[str, List[Dict]]\n",
    "]:\n",
    "    quality_buckets = defaultdict(list)\n",
    "    resume_to_qualities = defaultdict(lambda: defaultdict(list))\n",
    "    domain_coverage = defaultdict(list)\n",
    "\n",
    "    for p in pairs:\n",
    "        q = p.get(\"semantic_match_label\", \"unknown\").lower()\n",
    "        rid = p[\"resume_id\"]\n",
    "        res_dom = p.get(\"resume_domain\", \"unknown\").lower()\n",
    "        jd_dom = p.get(\"jd_domain\", \"unknown\").lower()\n",
    "\n",
    "        quality_buckets[q].append(p)\n",
    "        resume_to_qualities[rid][q].append(p)\n",
    "        domain_coverage[res_dom].append(p)\n",
    "        domain_coverage[jd_dom].append(p)\n",
    "\n",
    "    return quality_buckets, resume_to_qualities, domain_coverage\n",
    "\n",
    "\n",
    "# === 3. RESUME-BALANCED SAMPLING ===\n",
    "def resume_balanced_sampling(\n",
    "    resume_to_qualities: Dict[int, Dict[str, List[Dict]]],\n",
    "    target_qualities: List[str],\n",
    "    seen_pairs: Set[Tuple[int, int]]\n",
    ") -> Dict[str, List[Dict]]:\n",
    "    selected = defaultdict(list)\n",
    "    resume_count = 0\n",
    "\n",
    "    for resume_id, qmap in resume_to_qualities.items():\n",
    "        used = 0\n",
    "        for q in target_qualities:\n",
    "            if q in qmap:\n",
    "                pair = random.choice(qmap[q])\n",
    "                key = (pair[\"resume_id\"], pair[\"jd_id\"])\n",
    "                if key not in seen_pairs:\n",
    "                    selected[q].append(pair)\n",
    "                    seen_pairs.add(key)\n",
    "                    used += 1\n",
    "        if used > 0:\n",
    "            resume_count += 1\n",
    "\n",
    "    return selected, resume_count\n",
    "\n",
    "\n",
    "# === 4. DOMAIN QUOTA ENFORCEMENT ===\n",
    "def enforce_domain_quota(\n",
    "    quality_buckets: Dict[str, List[Dict]],\n",
    "    seen_pairs: Set[Tuple[int, int]],\n",
    "    target_counts: Dict[str, int],\n",
    "    current_counts: Dict[str, int],\n",
    "    min_per_domain: int\n",
    ") -> Dict[str, List[Dict]]:\n",
    "    selected = defaultdict(list)\n",
    "    domain_pair_cache = defaultdict(list)\n",
    "\n",
    "    for q in quality_buckets:\n",
    "        for p in quality_buckets[q]:\n",
    "            d1 = p.get(\"resume_domain\", \"unknown\").lower()\n",
    "            d2 = p.get(\"jd_domain\", \"unknown\").lower()\n",
    "            domain_pair_cache[(q, d1)].append(p)\n",
    "            domain_pair_cache[(q, d2)].append(p)\n",
    "\n",
    "    for (q, domain), group in domain_pair_cache.items():\n",
    "        if current_counts[q] >= target_counts[q]:\n",
    "            continue  # already filled this quality\n",
    "\n",
    "        candidates = [p for p in group if (p[\"resume_id\"], p[\"jd_id\"]) not in seen_pairs]\n",
    "        room_left = target_counts[q] - current_counts[q]\n",
    "        take_count = min(min_per_domain, len(candidates), room_left)\n",
    "        sampled = random.sample(candidates, take_count)\n",
    "\n",
    "        for p in sampled:\n",
    "            key = (p[\"resume_id\"], p[\"jd_id\"])\n",
    "            if key not in seen_pairs:\n",
    "                selected[q].append(p)\n",
    "                seen_pairs.add(key)\n",
    "                current_counts[q] += 1\n",
    "                if current_counts[q] >= target_counts[q]:\n",
    "                    break  # stop sampling more of this quality\n",
    "\n",
    "    return selected\n",
    "\n",
    "\n",
    "\n",
    "# === 5. FILL REMAINING WITH DIVERSITY ===\n",
    "def fill_remaining_by_diverse_domains(\n",
    "    quality_buckets: Dict[str, List[Dict]],\n",
    "    selected_by_quality: Dict[str, List[Dict]],\n",
    "    target_counts: Dict[str, int],\n",
    "    seen_pairs: Set[Tuple[int, int]]\n",
    ") -> Dict[str, List[Dict]]:\n",
    "    for q in target_counts:\n",
    "        remaining = target_counts[q] - len(selected_by_quality[q])\n",
    "        if remaining <= 0:\n",
    "            continue\n",
    "\n",
    "        available = [\n",
    "            p for p in quality_buckets[q]\n",
    "            if (p[\"resume_id\"], p[\"jd_id\"]) not in seen_pairs\n",
    "        ]\n",
    "\n",
    "        # Bucket by domain\n",
    "        domain_groups = defaultdict(list)\n",
    "        for p in available:\n",
    "            domain_groups[p.get(\"resume_domain\", \"unknown\").lower()].append(p)\n",
    "            domain_groups[p.get(\"jd_domain\", \"unknown\").lower()].append(p)\n",
    "\n",
    "        sampled = []\n",
    "        domain_cycle = list(domain_groups.keys())\n",
    "        random.shuffle(domain_cycle)\n",
    "\n",
    "        while remaining > 0 and domain_cycle:\n",
    "            domain = domain_cycle.pop(0)\n",
    "            group = domain_groups[domain]\n",
    "            group = [p for p in group if (p[\"resume_id\"], p[\"jd_id\"]) not in seen_pairs]\n",
    "            if not group:\n",
    "                continue\n",
    "            chosen = random.choice(group)\n",
    "            sampled.append(chosen)\n",
    "            seen_pairs.add((chosen[\"resume_id\"], chosen[\"jd_id\"]))\n",
    "            domain_groups[domain].remove(chosen)\n",
    "            domain_cycle.append(domain)\n",
    "            remaining -= 1\n",
    "\n",
    "        selected_by_quality[q].extend(sampled)\n",
    "\n",
    "    return selected_by_quality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a9c6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 6. MAIN WRAPPER ===\n",
    "def generate_balanced_sample(\n",
    "    all_pairs: List[Dict],\n",
    "    target_counts: Dict[str, int],\n",
    "    score_threshold: float = 0.1,\n",
    "    min_per_domain: int = 50\n",
    ") -> List[Dict]:\n",
    "    print(f\"🔍 Total pairs in input: {len(all_pairs)}\")\n",
    "    filtered = filter_pairs_by_score(all_pairs, score_threshold)\n",
    "    print(f\"✅ After score ≥ {score_threshold}: {len(filtered)}\")\n",
    "\n",
    "    quality_buckets, resume_to_qualities, domain_coverage = group_pairs_by_quality_and_resume(filtered)\n",
    "    print(\"📊 Match quality counts:\")\n",
    "    for q in target_counts:\n",
    "        print(f\"  {q.upper():<7}: {len(quality_buckets[q])}\")\n",
    "    print(f\"👥 Unique resumes: {len(resume_to_qualities)}\")\n",
    "\n",
    "    seen_pairs = set()\n",
    "\n",
    "    # Resume-balanced\n",
    "    selected_by_quality, resume_count = resume_balanced_sampling(resume_to_qualities, list(target_counts.keys()), seen_pairs)\n",
    "    print(f\"👤 Resume-balanced resumes: {resume_count}\")\n",
    "\n",
    "    # Domain quotas\n",
    "    current_counts = {q: len(selected_by_quality[q]) for q in target_counts}\n",
    "    domain_quota_selected = enforce_domain_quota(\n",
    "        quality_buckets,\n",
    "        seen_pairs,\n",
    "        target_counts,\n",
    "        current_counts,\n",
    "        min_per_domain\n",
    "    )\n",
    "\n",
    "    for q in target_counts:\n",
    "        selected_by_quality[q].extend(domain_quota_selected.get(q, []))\n",
    "\n",
    "    # Fill remaining\n",
    "    selected_by_quality = fill_remaining_by_diverse_domains(quality_buckets, selected_by_quality, target_counts, seen_pairs)\n",
    "\n",
    "    # Final merge\n",
    "    final_sample = []\n",
    "    print(\"\\n📦 Final sampled count:\")\n",
    "    for q in target_counts:\n",
    "        group = selected_by_quality[q]\n",
    "        print(f\"  {q.upper():<7}: {len(group)}\")\n",
    "        final_sample.extend(group)\n",
    "\n",
    "    print(f\"\\n🎯 Total selected: {len(final_sample)}\")\n",
    "\n",
    "    # Domain coverage\n",
    "    resume_domains = [p.get(\"resume_domain\", \"unknown\").lower() for p in final_sample]\n",
    "    jd_domains = [p.get(\"jd_domain\", \"unknown\").lower() for p in final_sample]\n",
    "    print(\"\\n📊 Resume domain coverage (top 10):\")\n",
    "    for dom, count in Counter(resume_domains).most_common(10):\n",
    "        print(f\"  {dom:<30} {count}\")\n",
    "\n",
    "    print(\"\\n📊 JD domain coverage (top 10):\")\n",
    "    for dom, count in Counter(jd_domains).most_common(10):\n",
    "        print(f\"  {dom:<30} {count}\")\n",
    "\n",
    "    return final_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e4a037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎯 Define how many samples per match type\n",
    "target_counts = {\n",
    "    \"strong\": 20000,\n",
    "    \"medium\": 25000,\n",
    "    \"weak\": 15000\n",
    "}\n",
    "\n",
    "# 📥 Load the filtered list only\n",
    "input_path = os.path.join(Config.JSON_OUTPUT_SCORING_DIR, 'semantic_relevance_scores.json')\n",
    "full_data  = load_json_file(input_path)\n",
    "relevance_data = full_data.get(\"semantic_relevance_scores\", [])\n",
    "\n",
    "# 🧠 Generate the balanced subset\n",
    "sampled = generate_balanced_sample(\n",
    "    all_pairs=relevance_data,\n",
    "    target_counts=target_counts,\n",
    "    score_threshold=0.2\n",
    ")\n",
    "\n",
    "# Wrap in valid JSON object structure\n",
    "relevance_wrapped = {\n",
    "    \"semantic_relevance_scores\": sampled\n",
    "}\n",
    "\n",
    "# 💾 Save the sampled subset\n",
    "relevance_map_file = os.path.join(Config.JSON_OUTPUT_SCORING_DIR, 'relevant_pairs.json')\n",
    "save_json_output(relevance_wrapped, relevance_map_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58eb1907",
   "metadata": {},
   "source": [
    "# Generate Normal distributed 30K samples file from scores files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cee4704",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e51a660",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCORING_FOLDER = Config.JSON_OUTPUT_SCORING_DIR  # path to scoring files\n",
    "OUTPUT_FILE = os.path.join(Config.JSON_OUTPUT_SCORING_FT_DATA, \"fine_tuning_filenames.json\")  # JSON_OUTPUT_SCORING_FT_DATA\n",
    "TARGET_COUNTS = {\"strong\": 10000, \"medium\": 15000, \"weak\": 5000}\n",
    "QUALITY_BUCKETS = list(TARGET_COUNTS.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b2a396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_filename(filename):\n",
    "    # Handles timestamps with both date and time like: 20250616_160025\n",
    "    match = re.match(r\"^(\\d+)_(\\d+)_(strong|medium|weak)_\\d{8}_\\d{6}_([\\d.]+)\\.json$\", filename)\n",
    "    if match:\n",
    "        resume_id, jd_id, quality, score = match.groups()\n",
    "        return {\n",
    "            \"filename\": filename,\n",
    "            \"resume_id\": resume_id,\n",
    "            \"jd_id\": jd_id,\n",
    "            \"quality\": quality,\n",
    "            \"score\": float(score)\n",
    "        }\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29245d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def load_scoring_files(folder):\n",
    "    all_meta = []\n",
    "    debug_limit = 5  # Only print debug for this many files\n",
    "    debug_count = 0\n",
    "\n",
    "    for fname in os.listdir(folder):\n",
    "        if not fname.endswith(\".json\"):\n",
    "            continue\n",
    "\n",
    "        # 🧠 Extract core metadata from filename\n",
    "        meta = parse_filename(fname)\n",
    "        if not meta:\n",
    "            print(f\"⚠️ Skipping unrecognized filename format: {fname}\")\n",
    "            continue\n",
    "\n",
    "        path = os.path.join(folder, fname)\n",
    "        try:\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            # ✅ Add extra fields from inside the JSON\n",
    "            meta[\"domain\"] = data.get(\"domain\", \"UNKNOWN\")\n",
    "\n",
    "            # 🔄 Use filename 'quality' as fallback for missing fields\n",
    "            match_quality = data.get(\"match_quality\", meta[\"quality\"])\n",
    "            semantic_match_label = data.get(\"semantic_match_label\", meta[\"quality\"])\n",
    "\n",
    "            meta[\"match_quality\"] = str(match_quality).strip().lower()\n",
    "            meta[\"semantic_match_label\"] = str(semantic_match_label).strip().lower()\n",
    "\n",
    "            # 🐛 Debug print for first few entries\n",
    "            #if debug_count < debug_limit:\n",
    "            #    print(f\"🔍 File: {fname}\")\n",
    "            #    print(f\"    ➤ match_quality: {meta['match_quality']}\")\n",
    "            #    print(f\"    ➤ semantic_match_label: {meta['semantic_match_label']}\")\n",
    "            #    print(f\"    ➤ domain: {meta['domain']}\")\n",
    "            #    debug_count += 1\n",
    "\n",
    "            all_meta.append(meta)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading {fname}: {e}\")\n",
    "\n",
    "    return all_meta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec370e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_label(meta_list):\n",
    "    return {\n",
    "        \"strong\": [m for m in meta_list if m[\"match_quality\"] == \"strong\"],\n",
    "        \"strong_semantic\": [m for m in meta_list if m[\"semantic_match_label\"] == \"strong\" and m[\"match_quality\"] != \"strong\"],\n",
    "        \"medium\": [m for m in meta_list if m[\"match_quality\"] == \"medium\"],\n",
    "        \"weak\": [m for m in meta_list if m[\"match_quality\"] == \"weak\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4af2128",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_by_domain_distribution(meta_list, limit):\n",
    "    if not meta_list:\n",
    "        print(f\"⚠️ No data available to select from for limit {limit}\")\n",
    "        return []\n",
    "\n",
    "    selected = []\n",
    "    domain_buckets = defaultdict(list)\n",
    "    for m in meta_list:\n",
    "        domain_buckets[m[\"domain\"]].append(m)\n",
    "\n",
    "    domain_keys = list(domain_buckets.keys())\n",
    "    if not domain_keys:\n",
    "        print(f\"⚠️ No domains found in the given metadata list.\")\n",
    "        return []\n",
    "\n",
    "    per_domain = max(1, limit // len(domain_keys))\n",
    "    for domain in domain_keys:\n",
    "        random.shuffle(domain_buckets[domain])\n",
    "        selected.extend(domain_buckets[domain][:per_domain])\n",
    "\n",
    "    # Fill remaining if needed\n",
    "    if len(selected) < limit:\n",
    "        remaining = [m for m in meta_list if m[\"filename\"] not in {x[\"filename\"] for x in selected}]\n",
    "        random.shuffle(remaining)\n",
    "        selected.extend(remaining[:limit - len(selected)])\n",
    "    return selected[:limit]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28df00bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_distribution(selected_list, label):\n",
    "    total = len(selected_list)\n",
    "    by_domain = Counter([m[\"domain\"] for m in selected_list])\n",
    "    print(f\"\\n📊 Final count for {label}: {total}\")\n",
    "    for domain, count in by_domain.most_common():\n",
    "        print(f\"  {domain}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b9ed14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    all_meta = load_scoring_files(SCORING_FOLDER)\n",
    "    # Debug: Check quality counts across all files\n",
    "    count_by_quality_field = Counter([m.get(\"match_quality\", \"MISSING\") for m in all_meta])\n",
    "    count_by_semantic_field = Counter([m.get(\"semantic_match_label\", \"MISSING\") for m in all_meta])\n",
    "    print(\"\\n📋 match_quality field distribution:\", count_by_quality_field)\n",
    "    print(\"📋 semantic_match_label field distribution:\", count_by_semantic_field)\n",
    "\n",
    "\n",
    "    grouped = group_by_label(all_meta)\n",
    "    final_selected = []\n",
    "\n",
    "    # Step 1: Strong group\n",
    "    strong_actual = grouped[\"strong\"]\n",
    "    strong_selected = []\n",
    "    \n",
    "    if len(strong_actual) >= TARGET_COUNTS[\"strong\"]:\n",
    "        strong_selected = select_by_domain_distribution(strong_actual, TARGET_COUNTS[\"strong\"])\n",
    "    else:\n",
    "        shortfall = TARGET_COUNTS[\"strong\"] - len(strong_actual)\n",
    "        fallback = grouped[\"strong_semantic\"]\n",
    "        fallback_selected = select_by_domain_distribution(fallback, shortfall)\n",
    "\n",
    "        strong_selected = strong_actual + fallback_selected\n",
    "        print(f\"\\n⚙️ Strong fallback logic:\")\n",
    "        print(f\"  ✅ Actual strong available: {len(strong_actual)}\")\n",
    "        print(f\"  ➕ Needed fallback: {shortfall}\")\n",
    "        print(f\"  🧩 Fallback added from semantic match: {len(fallback_selected)}\")\n",
    "        print(f\"  ❌ Still missing: {shortfall - len(fallback_selected)} (if > 0, this means we are under target)\")\n",
    "    log_distribution(strong_selected, \"strong\")\n",
    "    final_selected.extend(strong_selected)\n",
    "    selected_filenames = {m[\"filename\"] for m in final_selected}\n",
    "\n",
    "    # Step 2: Medium group (exclude used)\n",
    "    medium_pool = [m for m in grouped[\"medium\"] if m[\"filename\"] not in selected_filenames]\n",
    "    medium_selected = select_by_domain_distribution(medium_pool, TARGET_COUNTS[\"medium\"])\n",
    "    log_distribution(medium_selected, \"medium\")\n",
    "    final_selected.extend(medium_selected)\n",
    "    selected_filenames.update(m[\"filename\"] for m in medium_selected)\n",
    "\n",
    "    # Step 3: Weak group (exclude used)\n",
    "    weak_pool = [m for m in grouped[\"weak\"] if m[\"filename\"] not in selected_filenames]\n",
    "    weak_selected = select_by_domain_distribution(weak_pool, TARGET_COUNTS[\"weak\"])\n",
    "    log_distribution(weak_selected, \"weak\")\n",
    "    final_selected.extend(weak_selected)\n",
    "\n",
    "    # Save output\n",
    "    filenames = [m[\"filename\"] for m in final_selected]\n",
    "    save_json_output(filenames, OUTPUT_FILE, indent=2)\n",
    "    print(f\"\\n✅ Saved final list to {OUTPUT_FILE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08ef8be",
   "metadata": {},
   "source": [
    "# Copy scored files to fine-tune/score folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d350f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths\n",
    "SCORING_FOLDER = Path(Config.JSON_OUTPUT_SCORING_DIR)  # Source folder\n",
    "FT_JSON_FILE =  os.path.join(Config.JSON_OUTPUT_SCORING_FT_DATA, \"fine_tuning_filenames.json\") #Path(\"fine_tuning_filenames.json\")  # File with selected filenames\n",
    "TARGET_FOLDER = Path(Config.JSON_OUTPUT_FINE_TUNE_SCORE)  # Destination\n",
    "\n",
    "# Ensure output folder exists\n",
    "TARGET_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load filenames\n",
    "#with open(FT_JSON_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "#    filenames = json.load(f)\n",
    "    \n",
    "filenames = load_json_file(FT_JSON_FILE)\n",
    "\n",
    "print(f\"📄 Loaded {len(filenames)} filenames from {FT_JSON_FILE}\")\n",
    "\n",
    "# Copy files\n",
    "copied = 0\n",
    "missing = []\n",
    "for fname in filenames:\n",
    "    src_path = SCORING_FOLDER / fname\n",
    "    dest_path = TARGET_FOLDER / fname\n",
    "\n",
    "    if src_path.exists():\n",
    "        shutil.copy2(src_path, dest_path)\n",
    "        copied += 1\n",
    "    else:\n",
    "        missing.append(fname)\n",
    "\n",
    "print(f\"\\n✅ Copied {copied} files to {TARGET_FOLDER}\")\n",
    "if missing:\n",
    "    print(f\"⚠️ {len(missing)} files were missing in {SCORING_FOLDER}:\")\n",
    "    for m in missing[:5]:\n",
    "        print(\"  -\", m)\n",
    "    if len(missing) > 5:\n",
    "        print(f\"  ...and {len(missing) - 5} more.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054a3833",
   "metadata": {},
   "source": [
    "# Create SFT Record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea7aed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Set, Dict\n",
    "from glob import glob\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949b5ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔁 Find resume/JD JSON by prefix\n",
    "def find_file_with_prefix(directory: Path, prefix: str) -> Path:\n",
    "    files = list(directory.glob(f\"{prefix}_*.json\"))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No matching file for prefix: {prefix}\")\n",
    "    return files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a339b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 🧠 Determine optimal max_workers for I/O-bound tasks\n",
    "def get_optimal_max_workers(io_bound: bool = True, cap: int = 32) -> int:\n",
    "    cores = os.cpu_count() or 4  # fallback if undetectable\n",
    "    if io_bound:\n",
    "        return min(cap, 4 * cores)  # I/O-bound: more threads\n",
    "    else:\n",
    "        return min(cap, cores)      # CPU-bound: 1 thread per core\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b71497e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "def load_first_record_from_file(path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Load the first JSON object from a file that contains either:\n",
    "    - a single dictionary (direct object)\n",
    "    - a list with one dictionary [{...}]\n",
    "    \n",
    "    Raises ValueError if the list has more than one item.\n",
    "    \"\"\"\n",
    "    data = load_json_file(path)\n",
    "\n",
    "    if isinstance(data, list):\n",
    "        if len(data) == 1:\n",
    "            return data[0]\n",
    "        raise ValueError(f\"File {path} contains list with {len(data)} items — expected only one.\")\n",
    "    \n",
    "    elif isinstance(data, dict):\n",
    "        return data\n",
    "    \n",
    "    else:\n",
    "        raise TypeError(f\"File {path} must contain a dict or single-record list, got {type(data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb0aed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_file(\n",
    "    fname: str,\n",
    "    scoring_dir: Path,\n",
    "    resume_dir: Path,\n",
    "    jd_dir: Path,\n",
    "    output_dir: Path,\n",
    "    checkpoint_set: Set[str]\n",
    ") -> str:\n",
    "    try:\n",
    "        scoring_path = scoring_dir / fname\n",
    "        record = load_json_file(str(scoring_path))\n",
    "\n",
    "        uid = f\"{record['resume_id']}_{record['jd_id']}\"\n",
    "        if uid in checkpoint_set:\n",
    "            return None\n",
    "\n",
    "        resume_path = find_file_with_prefix(resume_dir, f\"resumes_{record['resume_id']}\")\n",
    "        jd_path = find_file_with_prefix(jd_dir, f\"jds_{record['jd_id']}\")\n",
    "        #resume_input = load_json_file(str(resume_path))[\"input_text\"]\n",
    "        #jd_input = load_json_file(str(jd_path))[\"input_text\"]\n",
    "        resume_data = load_first_record_from_file(str(resume_path))\n",
    "        jd_data = load_first_record_from_file(str(jd_path))\n",
    "\n",
    "\n",
    "        resume_input = resume_data[\"input_text\"]\n",
    "        jd_input = jd_data[\"input_text\"]\n",
    "\n",
    "\n",
    "        #input_text = f\"{resume_input.strip()}\\n{jd_input.strip()}\"\n",
    "        input_text = (\n",
    "            f\"<DOMAIN>{record.get('domain', '').strip()}</DOMAIN>\\n\"\n",
    "            f\"<RESUME_START>\\n{resume_input.strip()}\\n<RESUME_END>\\n\\n\"\n",
    "            f\"<JD_START>\\n{jd_input.strip()}\\n<JD_END>\"\n",
    "        )\n",
    "\n",
    "        section_scores = record[\"section_scores\"]\n",
    "        output_scores = {\n",
    "            \"final_ats_score\": record[\"final_ats_score\"],\n",
    "            **{section: round(value[\"score\"], 3) for section, value in section_scores.items()}\n",
    "        }\n",
    "\n",
    "        sft_data = {\n",
    "            \"input\": input_text,\n",
    "            \"output\": output_scores\n",
    "        }\n",
    "\n",
    "        match_quality = record.get(\"semantic_match_label\", \"unknown\")\n",
    "        output_path = output_dir / f\"{record['resume_id']}_{record['jd_id']}_{match_quality}.json\"\n",
    "        save_json_output(sft_data, str(output_path))\n",
    "\n",
    "        return uid\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {fname}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446699a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sft_record_parallel(\n",
    "    scoring_file: Path,\n",
    "    scoring_dir: Path,\n",
    "    resume_dir: Path,\n",
    "    jd_dir: Path,\n",
    "    output_dir: Path,\n",
    "    checkpoint_file: Path,\n",
    "    max_workers: int = None\n",
    "):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    filenames = load_json_file(str(scoring_file))  # list of filenames\n",
    "    checkpoint_set = set()\n",
    "\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        checkpoint_set = set(load_json_file(str(checkpoint_file)).get(\"processed\", []))\n",
    "\n",
    "    updated_checkpoint = list(checkpoint_set)\n",
    "    max_workers = max_workers or get_optimal_max_workers(io_bound=True)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [\n",
    "            executor.submit(\n",
    "                process_single_file, fname, scoring_dir, resume_dir, jd_dir, output_dir, checkpoint_set\n",
    "            )\n",
    "            for fname in filenames\n",
    "        ]\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                updated_checkpoint.append(result)\n",
    "\n",
    "    save_json_output({\"processed\": updated_checkpoint}, str(checkpoint_file))\n",
    "    print(f\"✅ All records processed with {max_workers} workers.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef23dd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_sft_record_parallel(\n",
    "    scoring_file=os.path.join(Config.JSON_OUTPUT_SCORING_FT_DATA, \"fine_tuning_filenames.json\"),\n",
    "    scoring_dir=Path(Config.JSON_OUTPUT_FINE_TUNE_SCORE),\n",
    "    resume_dir=Path(Config.JSON_OUTPUT_NORMALIZED_RESUME),\n",
    "    jd_dir=Path(Config.JSON_OUTPUT_NORMALIZED_JD),\n",
    "    output_dir=Path(Config.JSON_OUTPUT_FINE_TUNE_RECORD),\n",
    "    checkpoint_file=os.path.join(Config.JSON_OUTPUT_FINE_TUNE_RECORD, \"sft_checkpoint.json\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26192565",
   "metadata": {},
   "source": [
    "# Create training_data.jsonl from individual records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec12df72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def merge_sft_json_to_jsonl_notebook(input_dir: str, output_file: str):\n",
    "    \"\"\"\n",
    "    Merges individual SFT JSON files into a newline-delimited JSONL file.\n",
    "\n",
    "    Uses existing utility:\n",
    "    - load_json_file(): safely loads a .json file\n",
    "    - save_jsonl_line(): appends a record to JSONL\n",
    "    \"\"\"\n",
    "    input_path = Path(input_dir)\n",
    "    output_path = Path(output_file)\n",
    "\n",
    "    if not input_path.exists():\n",
    "        raise FileNotFoundError(f\"❌ Input directory not found: {input_dir}\")\n",
    "    \n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)  # 🛠️ Ensure directory exists\n",
    "\n",
    "\n",
    "    # Clear previous output if exists\n",
    "    if output_path.exists():\n",
    "        os.remove(output_path)\n",
    "\n",
    "    json_files = sorted(input_path.glob(\"*.json\"))\n",
    "    total_written = 0\n",
    "\n",
    "    for file in tqdm(json_files, desc=\"🔄 Merging JSON files\"):\n",
    "        try:\n",
    "            record = load_json_file(str(file))\n",
    "            if \"input\" in record and \"output\" in record:\n",
    "                save_jsonl_line(record, str(output_path))\n",
    "                total_written += 1\n",
    "            else:\n",
    "                print(f\"⚠️ Skipped {file.name}: missing 'input' or 'output' field\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error reading {file.name}: {e}\")\n",
    "\n",
    "    print(f\"\\n✅ Successfully merged {total_written} files into {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "026239a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔄 Merging JSON files: 100%|██████████| 29994/29994 [00:07<00:00, 3840.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Successfully merged 29994 files into json_outputs_all_data/fine-tune/test-data/training_data.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "merge_sft_json_to_jsonl_notebook(\n",
    "    input_dir=Path(Config.JSON_OUTPUT_FINE_TUNE_RECORD), # \"json_outputs_all_data/fine-tune/record\",\n",
    "    output_file=os.path.join(Config.JSON_OUTPUT_FINE_TUNE_TEST_DATA, \"training_data.jsonl\"), #\"json_outputs_all_data/fine-tune/training_data.jsonl\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68da3e9f",
   "metadata": {},
   "source": [
    "##  Split Merged .jsonl into 80/20 Train/Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "688c22c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_match_quality(input_text: str) -> str:\n",
    "    match = re.search(r\"<MATCH_QUALITY>(.*?)</MATCH_QUALITY>\", input_text)\n",
    "    return match.group(1).strip().lower() if match else \"unknown\"\n",
    "\n",
    "def stratified_split_jsonl(input_jsonl: str, train_path: str, eval_path: str, split_ratio: float = 0.8, seed: int = 42):\n",
    "    \"\"\"\n",
    "    Perform stratified 80/20 split by <MATCH_QUALITY> tag in the 'input' field.\n",
    "    \"\"\"\n",
    "    input_path = Path(input_jsonl)\n",
    "    if not input_path.exists():\n",
    "        raise FileNotFoundError(f\"❌ File not found: {input_jsonl}\")\n",
    "\n",
    "    stratified_data = defaultdict(list)\n",
    "\n",
    "    # Group records by match quality\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in tqdm(f, desc=\"📂 Reading and grouping records\"):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                record = json.loads(line)\n",
    "                match_quality = extract_match_quality(record[\"input\"])\n",
    "                stratified_data[match_quality].append(line)\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Failed to process line: {e}\")\n",
    "\n",
    "    # Split each group and collect\n",
    "    train_records, eval_records = [], []\n",
    "    random.seed(seed)\n",
    "\n",
    "    for label, records in stratified_data.items():\n",
    "        random.shuffle(records)\n",
    "        split_index = int(len(records) * split_ratio)\n",
    "        train_records.extend(records[:split_index])\n",
    "        eval_records.extend(records[split_index:])\n",
    "        print(f\"🔹 {label}: {len(records)} → train={split_index}, eval={len(records) - split_index}\")\n",
    "\n",
    "    # Save outputs\n",
    "    Path(train_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    Path(eval_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with open(train_path, \"w\", encoding=\"utf-8\") as f_train:\n",
    "        f_train.write(\"\\n\".join(train_records) + \"\\n\")\n",
    "\n",
    "    with open(eval_path, \"w\", encoding=\"utf-8\") as f_eval:\n",
    "        f_eval.write(\"\\n\".join(eval_records) + \"\\n\")\n",
    "\n",
    "    print(f\"\\n✅ Stratified split complete: {len(train_records)} train / {len(eval_records)} eval\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8971ee11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "📂 Reading and grouping records: 29994it [00:02, 13817.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 unknown: 29994 → train=23995, eval=5999\n",
      "\n",
      "✅ Stratified split complete: 23995 train / 5999 eval\n"
     ]
    }
   ],
   "source": [
    "stratified_split_jsonl(\n",
    "    input_jsonl=os.path.join(Config.JSON_OUTPUT_FINE_TUNE_TEST_DATA, \"training_data.jsonl\"), #\"json_outputs_all_data/fine-tune/training_data.jsonl\",\n",
    "    train_path=os.path.join(Config.JSON_OUTPUT_FINE_TUNE_TEST_DATA, \"train.jsonl\"), #\"json_outputs_all_data/fine-tune/train.jsonl\",\n",
    "    eval_path=os.path.join(Config.JSON_OUTPUT_FINE_TUNE_TEST_DATA, \"eval.jsonl\")  #\"json_outputs_all_data/fine-tune/eval.jsonl\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5349b2fc",
   "metadata": {},
   "source": [
    "# create local debugging test data of 100 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba32eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📦 Setup\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from typing import Any\n",
    "import json\n",
    "\n",
    "# === CONFIG ===\n",
    "input_dir = Path(Config.JSON_OUTPUT_FINE_TUNE_RECORD)  #Path(\"json_outputs_all_data/fine-tune/record\")\n",
    "train_file =os.path.join(Config.JSON_OUTPUT_FINE_TUNE_TEST_DATA, \"train_local.jsonl\") \n",
    "eval_file = os.path.join(Config.JSON_OUTPUT_FINE_TUNE_TEST_DATA, \"eval_local.jsonl\")  \n",
    "sample_count = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de965c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset output files\n",
    "for file in [train_file, eval_file]:\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)\n",
    "\n",
    "# Collect labeled files\n",
    "buckets = defaultdict(list)\n",
    "for file in input_dir.glob(\"*.json\"):\n",
    "    parts = file.stem.split(\"_\")\n",
    "    if len(parts) >= 3:\n",
    "        label = parts[2].lower()\n",
    "        if label in {\"strong\", \"medium\", \"weak\"}:\n",
    "            buckets[label].append(file)\n",
    "\n",
    "# Show counts\n",
    "for label, files in buckets.items():\n",
    "    print(f\"{label}: {len(files)} files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4efbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎯 Stratified Sampling: Split into Train and Eval\n",
    "target_ratios = {\"strong\": 0.3, \"medium\": 0.5, \"weak\": 0.2}\n",
    "train_count = int(sample_count * 0.8)\n",
    "eval_count = sample_count - train_count\n",
    "\n",
    "split_ratio = train_count / sample_count  # 0.8\n",
    "\n",
    "train_written = 0\n",
    "eval_written = 0\n",
    "\n",
    "for label, label_ratio in target_ratios.items():\n",
    "    n_total = int(sample_count * label_ratio)\n",
    "    n_train = int(n_total * split_ratio)\n",
    "    n_eval = n_total - n_train\n",
    "\n",
    "    candidates = buckets[label]\n",
    "    selected = random.sample(candidates, min(n_total, len(candidates)))\n",
    "    train_samples = selected[:n_train]\n",
    "    eval_samples = selected[n_train:n_train + n_eval]\n",
    "\n",
    "    for path in train_samples:\n",
    "        try:\n",
    "            data = load_json_file(str(path))\n",
    "            if \"input\" in data and \"output\" in data:\n",
    "                save_jsonl_line(data, train_file)\n",
    "                train_written += 1\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Train error in {path.name}: {e}\")\n",
    "\n",
    "    for path in eval_samples:\n",
    "        try:\n",
    "            data = load_json_file(str(path))\n",
    "            if \"input\" in data and \"output\" in data:\n",
    "                save_jsonl_line(data, eval_file)\n",
    "                eval_written += 1\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Eval error in {path.name}: {e}\")\n",
    "\n",
    "print(f\"✅ Wrote {train_written} examples to {train_file}\")\n",
    "print(f\"✅ Wrote {eval_written} examples to {eval_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
