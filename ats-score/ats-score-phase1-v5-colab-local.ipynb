{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad6598b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "# -------------------------------\n",
    "# CONFIGURATION\n",
    "# -------------------------------\n",
    "\n",
    "class Config:\n",
    "    # Directories\n",
    "    DATASET_DOWNLOAD_DIR = \"datasets\"\n",
    "    #json outputs\n",
    "    JSON_OUTPUT_DIR = \"json_outputs\"\n",
    "    # Cleanup flag\n",
    "    AUTO_CLEANUP = False\n",
    "    # Kaggle API token location\n",
    "    KAGGLE_JSON_PATH = os.path.expanduser('~/.kaggle/kaggle.json')\n",
    "    \n",
    "    @staticmethod\n",
    "    def setup_kaggle_credentials():\n",
    "        \"\"\"Setup Kaggle credentials for Colab or local\"\"\"\n",
    "        if not os.path.exists(Config.KAGGLE_JSON_PATH):\n",
    "            from google.colab import files\n",
    "            print(\"üìÇ Upload kaggle.json file...\")\n",
    "            uploaded = files.upload()\n",
    "            os.makedirs(os.path.dirname(Config.KAGGLE_JSON_PATH), exist_ok=True)\n",
    "            for filename in uploaded.keys():\n",
    "                shutil.move(filename, Config.KAGGLE_JSON_PATH)\n",
    "            os.chmod(Config.KAGGLE_JSON_PATH, 0o600)\n",
    "            print(f\"‚úÖ Kaggle credentials setup at {Config.KAGGLE_JSON_PATH}\")\n",
    "        else:\n",
    "            print(f\"‚úÖ Kaggle credentials already exist at {Config.KAGGLE_JSON_PATH}\")\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# DOWNLOADER\n",
    "# -------------------------------\n",
    "\n",
    "class DatasetDownloader:\n",
    "    @staticmethod\n",
    "    def download_and_extract(dataset_path: str) -> tuple[str, str]:\n",
    "        os.makedirs(Config.DATASET_DOWNLOAD_DIR, exist_ok=True)\n",
    "        \n",
    "        dataset_slug = dataset_path.split(\"/\")[-1]\n",
    "        extract_folder_path = os.path.join(Config.DATASET_DOWNLOAD_DIR, dataset_slug)\n",
    "        zip_filename = f\"{dataset_slug}.zip\"\n",
    "        zip_path = os.path.join(Config.DATASET_DOWNLOAD_DIR, zip_filename)\n",
    "\n",
    "        # If already extracted, skip download and extraction\n",
    "        if os.path.exists(extract_folder_path) and any(Path(extract_folder_path).rglob('*.csv')):\n",
    "            print(f\"‚ö° Dataset folder already exists at '{extract_folder_path}', skipping download and extraction.\")\n",
    "            return extract_folder_path, zip_filename\n",
    "        \n",
    "        print(f\"‚¨áÔ∏è Downloading dataset: {dataset_path} ...\")\n",
    "        !kaggle datasets download -d {dataset_path} -p {Config.DATASET_DOWNLOAD_DIR}\n",
    "        \n",
    "        if not os.path.exists(zip_path):\n",
    "            raise FileNotFoundError(f\"‚ùå Zip file '{zip_filename}' not found after download!\")\n",
    "\n",
    "        os.makedirs(extract_folder_path, exist_ok=True)\n",
    "        \n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_folder_path)\n",
    "        \n",
    "        print(f\"‚úÖ Downloaded and extracted to '{extract_folder_path}'.\")\n",
    "        return extract_folder_path, zip_filename\n",
    "\n",
    "# -------------------------------\n",
    "# LOADER\n",
    "# -------------------------------\n",
    "\n",
    "class DatasetLoader:\n",
    "    @staticmethod\n",
    "    def load_csv(dataset_folder: str, target_csv_name: str) -> pd.DataFrame:\n",
    "        # Walk and find the CSV\n",
    "        print(f\"üîç Searching for '{target_csv_name}' inside {dataset_folder}...\")\n",
    "        if not os.path.exists(dataset_folder):\n",
    "            raise FileNotFoundError(f\"‚ùå Dataset folder '{dataset_folder}' does not exist!\")\n",
    "        \n",
    "        for root, _, files in os.walk(dataset_folder):\n",
    "            for file in files:\n",
    "                if file.lower() == target_csv_name.lower():\n",
    "                    csv_path = os.path.join(root, file)\n",
    "                    df = pd.read_csv(csv_path)\n",
    "                    print(f\"‚úÖ Loaded CSV with shape {df.shape}\")\n",
    "                    return df\n",
    "        raise FileNotFoundError(f\"‚ùå CSV file '{target_csv_name}' not found inside extracted dataset!\")\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# PROCESSOR\n",
    "# -------------------------------\n",
    "\n",
    "class DatasetProcessor:\n",
    "    @staticmethod\n",
    "    def filter_fields(df: pd.DataFrame, allowed_fields: List[str]) -> pd.DataFrame:\n",
    "        missing_fields = [field for field in allowed_fields if field not in df.columns]\n",
    "        if missing_fields:\n",
    "            raise ValueError(f\"‚ùå Fields {missing_fields} not found in dataset!\")\n",
    "        \n",
    "        filtered_df = df[allowed_fields]\n",
    "        print(f\"‚úÖ Filtered columns: {list(filtered_df.columns)}\")\n",
    "        return filtered_df\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_to_json(df: pd.DataFrame, output_json_name: str):\n",
    "        # Ensure JSON output directory exists\n",
    "        os.makedirs(Config.JSON_OUTPUT_DIR, exist_ok=True)\n",
    "        \n",
    "        output_json_path = os.path.join(Config.JSON_OUTPUT_DIR, output_json_name)\n",
    "        \n",
    "        # Check if file already exists, remove it before writing\n",
    "        if os.path.exists(output_json_path):\n",
    "            print(f\"‚ö†Ô∏è JSON file '{output_json_name}' already exists. Deleting and re-creating it.\")\n",
    "            os.remove(output_json_path)\n",
    "        \n",
    "        # Save DataFrame to JSON\n",
    "        print(f\"üìù Saving DataFrame to JSON: {output_json_name}...\")\n",
    "        df.to_json(output_json_path, orient='records', lines=True)\n",
    "        print(f\"‚úÖ JSON file saved to '{output_json_path}'.\")\n",
    "\n",
    "# -------------------------------\n",
    "# CLEANER\n",
    "# -------------------------------\n",
    "\n",
    "class Cleaner:\n",
    "    @staticmethod\n",
    "    def cleanup_dataset_artifacts(extracted_folder_path: str, zip_filename: str):\n",
    "        # Delete extracted folder\n",
    "        if os.path.exists(extracted_folder_path):\n",
    "            shutil.rmtree(extracted_folder_path)\n",
    "            print(f\"üßπ Folder '{extracted_folder_path}' has been deleted successfully.\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Folder '{extracted_folder_path}' does not exist, skipping folder deletion.\")\n",
    "        \n",
    "        # Delete zip file\n",
    "        zip_path = os.path.join(Config.DATASET_DOWNLOAD_DIR, zip_filename)\n",
    "        if os.path.exists(zip_path):\n",
    "            os.remove(zip_path)\n",
    "            print(f\"üóëÔ∏è Zip file '{zip_path}' has been deleted successfully.\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Zip file '{zip_path}' does not exist, skipping zip deletion.\")\n",
    "\n",
    "# -------------------------------\n",
    "# MAIN FLOW\n",
    "# -------------------------------\n",
    "\n",
    "def process_dataset(dataset_path: str, target_csv_name: str, allowed_fields: List[str], output_json_name: str):\n",
    "    # Download and extract dataset\n",
    "    extracted_folder, zip_filename = DatasetDownloader.download_and_extract(dataset_path)\n",
    "    # Load the CSV file\n",
    "    df = DatasetLoader.load_csv(extracted_folder, target_csv_name)\n",
    "    # Filter the columns based on allowed fields\n",
    "    filtered_df = DatasetProcessor.filter_fields(df, allowed_fields)\n",
    "    # Save to JSON in the json_outputs folder\n",
    "    DatasetProcessor.save_to_json(filtered_df, output_json_name)\n",
    "    \n",
    "    if Config.AUTO_CLEANUP:\n",
    "        Cleaner.cleanup_dataset_artifacts(extracted_folder, zip_filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f892a33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Kaggle credentials already exist at C:\\Users\\rubyj/.kaggle/kaggle.json\n",
      "‚ö° Dataset folder already exists at 'datasets\\resume-dataset', skipping download and extraction.\n",
      "üîç Searching for 'Resume.csv' inside datasets\\resume-dataset...\n",
      "‚úÖ Loaded CSV with shape (2484, 4)\n",
      "‚úÖ Filtered columns: ['Category', 'Resume_str']\n",
      "‚ö†Ô∏è JSON file 'resume_data.json' already exists. Deleting and re-creating it.\n",
      "üìù Saving DataFrame to JSON: resume_data.json...\n",
      "‚úÖ JSON file saved to 'json_outputs\\resume_data.json'.\n",
      "‚ö° Dataset folder already exists at 'datasets\\linkedin-job-postings', skipping download and extraction.\n",
      "üîç Searching for 'postings.csv' inside datasets\\linkedin-job-postings...\n",
      "‚úÖ Loaded CSV with shape (123849, 31)\n",
      "‚úÖ Filtered columns: ['title', 'company_name', 'location', 'description', 'skills_desc', 'job_id', 'formatted_experience_level', 'formatted_work_type']\n",
      "‚ö†Ô∏è JSON file 'job_postings.json' already exists. Deleting and re-creating it.\n",
      "üìù Saving DataFrame to JSON: job_postings.json...\n",
      "‚úÖ JSON file saved to 'json_outputs\\job_postings.json'.\n"
     ]
    }
   ],
   "source": [
    "# First run Kaggle setup\n",
    "Config.setup_kaggle_credentials()\n",
    "\n",
    "# Process Resume Dataset\n",
    "process_dataset(\n",
    "    dataset_path=\"snehaanbhawal/resume-dataset\",\n",
    "    target_csv_name=\"Resume.csv\",\n",
    "    allowed_fields=[\"Category\", \"Resume_str\"],\n",
    "    output_json_name=\"resume_data.json\"\n",
    ")\n",
    "\n",
    "# Process Job Postings Dataset\n",
    "process_dataset(\n",
    "    dataset_path=\"arshkon/linkedin-job-postings\",\n",
    "    target_csv_name=\"postings.csv\",\n",
    "    allowed_fields=[\"title\", \"company_name\", \"location\", \"description\", \"skills_desc\", \"job_id\" , \"formatted_experience_level\", \"formatted_work_type\"],\n",
    "    output_json_name=\"job_postings.json\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
