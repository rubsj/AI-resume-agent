{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f630d895",
   "metadata": {},
   "source": [
    "# Global setup and package installation used in most phases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969fcea6",
   "metadata": {},
   "source": [
    "## Colab + GPU Detection Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7cf79fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def is_running_in_colab():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "def get_available_gpu_memory_gb():\n",
    "    try:\n",
    "        output = subprocess.check_output(\n",
    "            [\"nvidia-smi\", \"--query-gpu=memory.free\", \"--format=csv,nounits,noheader\"],\n",
    "            encoding=\"utf-8\"\n",
    "        )\n",
    "        free_mem_mb = int(output.strip().split(\"\\n\")[0])\n",
    "        return free_mem_mb / 1024\n",
    "    except Exception:\n",
    "        return 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66be54d4",
   "metadata": {},
   "source": [
    "## install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942fba10",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_running_in_colab():\n",
    "    # Install the required packages\n",
    "    !pip install kagglehub pandas\n",
    "    !pip install -q transformers accelerate bitsandbytes sentencepiece pydantic huggingface_hub xformers\n",
    "    !pip install langid\n",
    "else:\n",
    "    %pip install kagglehub pandas\n",
    "    %pip install -q transformers accelerate sentencepiece pydantic huggingface_hub xformers\n",
    "    #%pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu121\n",
    "    #%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "    %pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128\n",
    "    %pip install -U bitsandbytes\n",
    "    %pip install langid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4a9f61",
   "metadata": {},
   "source": [
    "## Login to huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1235aef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# Set your token here securely or prompt for it in Colab\n",
    "# Recommended: store in Colab secrets or environment variable\n",
    "HF_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "\n",
    "if not HF_TOKEN:\n",
    "    if is_running_in_colab():\n",
    "        # If running in Colab, use the Colab secrets\n",
    "        try:\n",
    "            from google.colab import userdata\n",
    "            HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "            if not HF_TOKEN:\n",
    "                raise ValueError(\"⚠️ Hugging Face token not found in Colab secrets.\")\n",
    "            print(\"🔑 Hugging Face token found in Colab secrets.\")\n",
    "        except ImportError:\n",
    "            print(\"⚠️ Unable to authenticate in Colab. Please set your Hugging Face token manually.\")\n",
    "    else:\n",
    "        # Prompt for token if not set in environment\n",
    "        print(\"🔑 Please enter your Hugging Face token:\")\n",
    "        # For Colab or local prompt input\n",
    "        HF_TOKEN = input(\"🔑 Enter your Hugging Face token: \").strip()\n",
    "\n",
    "login(token=HF_TOKEN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b423735e",
   "metadata": {},
   "source": [
    "## Setup Kaggle Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "409818e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Kaggle credentials already exist at C:\\Users\\rubyj/.kaggle/kaggle.json\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "def setup_kaggle_credentials():\n",
    "    kaggle_path = os.path.expanduser('~/.kaggle/kaggle.json')\n",
    "    if not os.path.exists(kaggle_path):\n",
    "        from google.colab import files\n",
    "        print(\"📂 Upload kaggle.json file...\")\n",
    "        uploaded = files.upload()\n",
    "        os.makedirs(os.path.dirname(kaggle_path), exist_ok=True)\n",
    "        for filename in uploaded.keys():\n",
    "            shutil.move(filename, kaggle_path)\n",
    "        os.chmod(kaggle_path, 0o600)\n",
    "        print(f\"✅ Kaggle credentials setup at {kaggle_path}\")\n",
    "    else:\n",
    "        print(f\"✅ Kaggle credentials already exist at {kaggle_path}\")\n",
    "\n",
    "setup_kaggle_credentials()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d384b2f",
   "metadata": {},
   "source": [
    "## Mount Google Drive (Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1517ea98",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_running_in_colab():\n",
    "   from google.colab import drive\n",
    "   drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb849be",
   "metadata": {},
   "source": [
    "# Phase 1: First Steps Notebook — Data Ingestion + Minimal Parsing\n",
    "1. Load Resume and JD datasets\n",
    "2. Minimal Parsing into JSON Structure\n",
    "3. Save structured JSON for Phase 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb756b8f",
   "metadata": {},
   "source": [
    "## Util Classes and methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43abc8ac",
   "metadata": {},
   "source": [
    "### Configurations  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1e827a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 🛠 CONFIGURATION\n",
    "# ==============================\n",
    "\n",
    "class Config:\n",
    "    DATASET_DOWNLOAD_DIR = \"datasets\"\n",
    "    JSON_OUTPUT_DIR = \"json_outputs_sample_data\"\n",
    "    AUTO_CLEANUP = False\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a537a5",
   "metadata": {},
   "source": [
    "### Downloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89480102",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================\n",
    "# DOWNLOADER\n",
    "# ==============================\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "class DatasetDownloader:\n",
    "    @staticmethod\n",
    "    def download_and_extract(dataset_path: str) -> tuple[str, str]:\n",
    "        os.makedirs(Config.DATASET_DOWNLOAD_DIR, exist_ok=True)\n",
    "        dataset_slug = dataset_path.split(\"/\")[-1]\n",
    "        extract_folder_path = os.path.join(Config.DATASET_DOWNLOAD_DIR, dataset_slug)\n",
    "        zip_filename = f\"{dataset_slug}.zip\"\n",
    "        zip_path = os.path.join(Config.DATASET_DOWNLOAD_DIR, zip_filename)\n",
    "\n",
    "        if os.path.exists(extract_folder_path) and any(Path(extract_folder_path).rglob(\"*.csv\")):\n",
    "            print(f\"⚡ Dataset folder already exists at '{extract_folder_path}', skipping download and extraction.\")\n",
    "            return extract_folder_path, zip_filename\n",
    "\n",
    "        print(f\"⬇️ Downloading dataset: {dataset_path} ...\")\n",
    "        !kaggle datasets download -d {dataset_path} -p {Config.DATASET_DOWNLOAD_DIR}\n",
    "\n",
    "        if not os.path.exists(zip_path):\n",
    "            raise FileNotFoundError(f\"❌ Zip file '{zip_filename}' not found after download!\")\n",
    "\n",
    "        os.makedirs(extract_folder_path, exist_ok=True)\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_folder_path)\n",
    "\n",
    "        print(f\"✅ Downloaded and extracted to '{extract_folder_path}'.\")\n",
    "        return extract_folder_path, zip_filename\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1170df00",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2c06119",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================\n",
    "# LOADER\n",
    "# ==============================\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class DatasetLoader:\n",
    "    @staticmethod\n",
    "    def load_csv(dataset_folder: str, target_csv_name: str) -> pd.DataFrame:\n",
    "        print(f\"🔍 Searching for '{target_csv_name}' inside {dataset_folder}...\")\n",
    "        if not os.path.exists(dataset_folder):\n",
    "            raise FileNotFoundError(f\"❌ Dataset folder '{dataset_folder}' does not exist!\")\n",
    "\n",
    "        for root, _, files in os.walk(dataset_folder):\n",
    "            for file in files:\n",
    "                if file.lower() == target_csv_name.lower():\n",
    "                    csv_path = os.path.join(root, file)\n",
    "                    df = pd.read_csv(csv_path)\n",
    "                    print(f\"✅ Loaded CSV with shape {df.shape}\")\n",
    "                    return df\n",
    "\n",
    "        raise FileNotFoundError(f\"❌ CSV file '{target_csv_name}' not found inside extracted dataset!\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_all_linkedin_jd_files(dataset_folder: str) -> dict:\n",
    "        print(\"📦 Loading all relevant CSVs for JD processing...\")\n",
    "        return {\n",
    "            \"postings\": DatasetLoader.load_csv(dataset_folder, \"postings.csv\"),\n",
    "            \"skills\": DatasetLoader.load_csv(dataset_folder, \"skills.csv\"),\n",
    "            \"industries\": DatasetLoader.load_csv(dataset_folder, \"industries.csv\"),\n",
    "            \"job_skills\": DatasetLoader.load_csv(dataset_folder, \"job_skills.csv\"),\n",
    "            \"job_industries\": DatasetLoader.load_csv(dataset_folder, \"job_industries.csv\"),\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a80a5a",
   "metadata": {},
   "source": [
    "### Data Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08247c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================\n",
    "# PROCESSOR\n",
    "# ==============================\n",
    "import os\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from functools import lru_cache\n",
    "import langid\n",
    "\n",
    "\n",
    "class DatasetProcessor:\n",
    "    @staticmethod\n",
    "    def filter_fields(df: pd.DataFrame, allowed_fields: List[str]) -> pd.DataFrame:\n",
    "        missing_fields = [field for field in allowed_fields if field not in df.columns]\n",
    "        if missing_fields:\n",
    "            raise ValueError(f\"❌ Fields {missing_fields} not found in dataset!\")\n",
    "\n",
    "        filtered_df = df[allowed_fields]\n",
    "        print(f\"✅ Filtered columns: {list(filtered_df.columns)}\")\n",
    "        return filtered_df\n",
    "\n",
    "    @staticmethod\n",
    "    def save_to_json(df: pd.DataFrame, output_json_name: str):\n",
    "        os.makedirs(Config.JSON_OUTPUT_DIR, exist_ok=True)\n",
    "        output_path = os.path.join(Config.JSON_OUTPUT_DIR, output_json_name)\n",
    "\n",
    "        if os.path.exists(output_path):\n",
    "            os.remove(output_path)\n",
    "            print(f\"🗑️ Existing JSON '{output_path}' deleted.\")\n",
    "\n",
    "        df.to_json(output_path, orient='records', lines=True, force_ascii=False)\n",
    "        print(f\"✅ Data saved to JSON at '{output_path}'\")\n",
    "        \n",
    "    @staticmethod\n",
    "    def detect_languages_langid_cached(texts: List[str]) -> List[str]:\n",
    "        @lru_cache(maxsize=10000)\n",
    "        def detect_lang(text: str) -> str:\n",
    "            return langid.classify(text)[0] if isinstance(text, str) else \"unknown\"\n",
    "        return [detect_lang(text) for text in texts]    \n",
    "\n",
    "    @staticmethod\n",
    "    def print_row_stats(initial: int, deduped: int, enriched: int, quality: int, final: int):\n",
    "        print(\"\\n📊 Row Reduction Summary:\")\n",
    "        print(f\"🔹 Initial rows in 'postings.csv':     {initial}\")\n",
    "        print(f\"🔹 After deduplication:               {deduped}\")\n",
    "        print(f\"🔹 After domain/industry enrichment: {enriched}\")\n",
    "        print(f\"🔹 After quality filtering:          {quality}\")\n",
    "        print(f\"🔹 After language filtering:         {final}\")\n",
    "        \n",
    "    @staticmethod\n",
    "    def _deduplicate_postings(postings: pd.DataFrame) -> pd.DataFrame:\n",
    "        return postings.dropna(subset=[\"description\"]).drop_duplicates(subset=[\"description\"]).copy()\n",
    "\n",
    "    @staticmethod\n",
    "    def _enrich_industry(postings: pd.DataFrame, job_industries: pd.DataFrame, industries: pd.DataFrame) -> pd.DataFrame:\n",
    "        job_industry_map = (\n",
    "            job_industries\n",
    "            .groupby(\"job_id\")[\"industry_id\"]\n",
    "            .agg(lambda x: x.value_counts().index[0])\n",
    "            .reset_index()\n",
    "        )\n",
    "        postings = postings.merge(job_industry_map, on=\"job_id\", how=\"left\")\n",
    "        postings = postings.merge(industries, on=\"industry_id\", how=\"left\")\n",
    "        postings.rename(columns={\"industry_name\": \"industry\"}, inplace=True)\n",
    "        return postings\n",
    "\n",
    "    @staticmethod\n",
    "    def _enrich_skills_description(postings: pd.DataFrame, job_skills: pd.DataFrame, skills: pd.DataFrame) -> tuple[pd.DataFrame, int]:\n",
    "        filled_skills_desc_count = 0\n",
    "        try:\n",
    "            if \"skill_abr\" in job_skills.columns and \"skill_abr\" in skills.columns:\n",
    "                job_skills = job_skills.merge(skills, on=\"skill_abr\", how=\"left\")\n",
    "                skill_map = (\n",
    "                    job_skills\n",
    "                    .dropna(subset=[\"skill_name\"])\n",
    "                    .groupby(\"job_id\")[\"skill_name\"]\n",
    "                    .agg(lambda x: \", \".join(sorted(set(x))))\n",
    "                    .reset_index()\n",
    "                    .rename(columns={\"skill_name\": \"merged_skills_text\"})\n",
    "                )\n",
    "                postings = postings.merge(skill_map, on=\"job_id\", how=\"left\")\n",
    "\n",
    "                def fill_skills_desc(row):\n",
    "                    nonlocal filled_skills_desc_count\n",
    "                    if pd.isna(row[\"skills_desc\"]) or str(row[\"skills_desc\"]).strip() == \"\":\n",
    "                        if pd.notna(row[\"merged_skills_text\"]):\n",
    "                            filled_skills_desc_count += 1\n",
    "                            return row[\"merged_skills_text\"]\n",
    "                    return row[\"skills_desc\"]\n",
    "\n",
    "                postings[\"skills_desc\"] = postings.apply(fill_skills_desc, axis=1)\n",
    "                postings.drop(columns=[\"merged_skills_text\"], inplace=True)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Skill enrichment skipped due to error: {e}\")\n",
    "        return postings, filled_skills_desc_count\n",
    "\n",
    "    @staticmethod\n",
    "    def _filter_postings(postings: pd.DataFrame) -> pd.DataFrame:\n",
    "        postings = postings[postings[\"description\"].str.len() > 300]\n",
    "        if \"formatted_work_type\" in postings.columns:\n",
    "            postings[\"formatted_work_type\"] = postings[\"formatted_work_type\"].str.lower()\n",
    "            postings = postings[postings[\"formatted_work_type\"].isin([\"full-time\", \"permanent\"])]\n",
    "\n",
    "        postings = postings[postings[\"skills_desc\"].notna() & (postings[\"skills_desc\"].str.strip() != \"\")]\n",
    "        postings = postings[postings[\"industry\"].notna() & (postings[\"industry\"].str.strip() != \"\")]\n",
    "        return postings\n",
    "\n",
    "    @staticmethod\n",
    "    def _filter_language(postings: pd.DataFrame) -> pd.DataFrame:\n",
    "        print(\"🌍 Detecting languages using langid...\")\n",
    "        postings[\"lang\"] = DatasetProcessor.detect_languages_langid_cached(postings[\"description\"].tolist())\n",
    "        postings = postings[postings[\"lang\"] == \"en\"]\n",
    "        return postings.drop(columns=[\"lang\"])\n",
    "\n",
    "    @staticmethod\n",
    "    def process_and_save_enriched_jds(jd_data: dict) -> pd.DataFrame:\n",
    "        initial_rows = len(jd_data[\"postings\"])\n",
    "\n",
    "        postings = DatasetProcessor._deduplicate_postings(jd_data[\"postings\"])\n",
    "        rows_after_dedup = len(postings)\n",
    "\n",
    "        postings = DatasetProcessor._enrich_industry(postings, jd_data[\"job_industries\"], jd_data[\"industries\"])\n",
    "        postings, filled_skills_desc_count = DatasetProcessor._enrich_skills_description(postings, jd_data[\"job_skills\"], jd_data[\"skills\"])\n",
    "        print(f\"🧠 Enriched 'skills_desc' from skills.csv for {filled_skills_desc_count} records.\")\n",
    "        rows_before_quality = len(postings)\n",
    "\n",
    "        postings = DatasetProcessor._filter_postings(postings)\n",
    "        rows_after_quality = len(postings)\n",
    "\n",
    "        postings = DatasetProcessor._filter_language(postings)\n",
    "        final_rows = len(postings)\n",
    "\n",
    "        DatasetProcessor.print_row_stats(\n",
    "            initial_rows,\n",
    "            rows_after_dedup,\n",
    "            rows_before_quality,\n",
    "            rows_after_quality,\n",
    "            final_rows\n",
    "        )\n",
    "\n",
    "        return postings.reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fc4fb6",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7245255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# CLEANER\n",
    "# ==============================\n",
    "class Cleaner:\n",
    "    @staticmethod\n",
    "    def cleanup_dataset_artifacts(extracted_folder_path: str, zip_filename: str):\n",
    "        if os.path.exists(extracted_folder_path):\n",
    "            shutil.rmtree(extracted_folder_path)\n",
    "            print(f\"🧹 Folder '{extracted_folder_path}' has been deleted successfully.\")\n",
    "\n",
    "        zip_path = os.path.join(Config.DATASET_DOWNLOAD_DIR, zip_filename)\n",
    "        if os.path.exists(zip_path):\n",
    "            os.remove(zip_path)\n",
    "            print(f\"🗑️ Zip file '{zip_path}' has been deleted successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a2525b",
   "metadata": {},
   "source": [
    "### Hybrid Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8d85253",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================\n",
    "# HYBRID LOADER\n",
    "# ==============================\n",
    "try:\n",
    "    import kagglehub\n",
    "    from kagglehub import KaggleDatasetAdapter\n",
    "except ImportError:\n",
    "    kagglehub = None\n",
    "\n",
    "class HybridDatasetLoader:\n",
    "    @staticmethod\n",
    "    def load_dataset(dataset_path: str, file_name: str) -> pd.DataFrame:\n",
    "        if kagglehub:\n",
    "            try:\n",
    "                print(f\"📥 Trying KaggleHub for {dataset_path}...\")\n",
    "                df = kagglehub.dataset_load(KaggleDatasetAdapter.PANDAS, dataset_path, file_name)\n",
    "                print(f\"✅ Loaded using KaggleHub: shape = {df.shape}\")\n",
    "                return df\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ KaggleHub failed: {e}\\nFalling back to ZIP-based loader.\")\n",
    "\n",
    "        extracted_folder, _ = DatasetDownloader.download_and_extract(dataset_path)\n",
    "        return DatasetLoader.load_csv(extracted_folder, file_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18dbb61",
   "metadata": {},
   "source": [
    "### Filter high quality JDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0147abe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JobDescriptionFilter:\n",
    "    @staticmethod\n",
    "    def filter_required_fields(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        return df[\n",
    "            df[\"title\"].notna() & df[\"title\"].str.strip().ne(\"\") &\n",
    "            df[\"industry\"].notna() & df[\"industry\"].str.strip().ne(\"\") &\n",
    "            df[\"description\"].notna() & df[\"description\"].str.strip().ne(\"\")\n",
    "        ]\n",
    "\n",
    "    @staticmethod\n",
    "    def filter_structured_description(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        return df[df[\"description\"].str.contains(\"responsibilit|requirement|qualif\", case=False, na=False)]\n",
    "\n",
    "    @staticmethod\n",
    "    def filter_min_description_length(df: pd.DataFrame, min_words: int = 150) -> pd.DataFrame:\n",
    "        df[\"word_count\"] = df[\"description\"].str.split().str.len()\n",
    "        filtered = df[df[\"word_count\"] >= min_words].copy()\n",
    "        filtered.drop(columns=[\"word_count\"], inplace=True)\n",
    "        return filtered\n",
    "\n",
    "    @staticmethod\n",
    "    def filter_min_skills(df: pd.DataFrame, min_skills: int = 2) -> pd.DataFrame:\n",
    "        df[\"skill_count\"] = df[\"skills_desc\"].str.split(\",\").str.len()\n",
    "        filtered = df[df[\"skill_count\"] >= min_skills].copy()\n",
    "        filtered.drop(columns=[\"skill_count\"], inplace=True)\n",
    "        return filtered\n",
    "\n",
    "    @staticmethod\n",
    "    def filter_top_industries(df: pd.DataFrame, top_n: int = 10) -> pd.DataFrame:\n",
    "        top_industries = df[\"industry\"].value_counts().nlargest(top_n).index\n",
    "        return df[df[\"industry\"].isin(top_industries)].copy()\n",
    "\n",
    "    @staticmethod\n",
    "    def deduplicate_by_title_and_description(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df[\"title_desc\"] = df[\"title\"].str.lower() + \"|\" + df[\"description\"].str.lower()\n",
    "        deduped = df.drop_duplicates(subset=\"title_desc\").copy()\n",
    "        deduped.drop(columns=[\"title_desc\"], inplace=True)\n",
    "        return deduped\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_all_filters(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        original = len(df)\n",
    "\n",
    "        df = JobDescriptionFilter.filter_required_fields(df)\n",
    "        f1 = len(df)\n",
    "\n",
    "        df = JobDescriptionFilter.filter_structured_description(df)\n",
    "        f2 = len(df)\n",
    "        \n",
    "        df = JobDescriptionFilter.deduplicate_by_title_and_description(df)\n",
    "        f3 = len(df)\n",
    "\n",
    "        df = JobDescriptionFilter.filter_min_skills(df, min_skills=2)\n",
    "        f4 = len(df)\n",
    "        \n",
    "        df = JobDescriptionFilter.filter_min_description_length(df, min_words=300)\n",
    "        f5 = len(df)\n",
    "\n",
    "        df = JobDescriptionFilter.filter_top_industries(df, top_n=30)\n",
    "        f6 = len(df)\n",
    "\n",
    " \n",
    "\n",
    "        print(\"\\n📊 High-Quality JD Filter Reduction Summary:\")\n",
    "        print(f\"🔹 Initial records:           {original}\")\n",
    "        print(f\"🔹 After required fields:     {f1}\")\n",
    "        print(f\"🔹 After structure check:     {f2}\")\n",
    "        print(f\"🔹 After deduplication:       {f3}\")\n",
    "        print(f\"🔹 After min skills (2):      {f4}\")\n",
    "        print(f\"🔹 After min length (300):    {f5}\")\n",
    "        print(f\"🔹 After top industries (30): {f6}\")\n",
    "       \n",
    "\n",
    "        return df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b1fd6842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_jds_top_industries(df: pd.DataFrame, top_n: int = 20, per_industry: int = 2) -> pd.DataFrame:\n",
    "    top_industries = df[\"industry\"].value_counts().nlargest(top_n).index\n",
    "    df_top = df[df[\"industry\"].isin(top_industries)]\n",
    "    sample_df = (\n",
    "        df_top.groupby(\"industry\", group_keys=False)\n",
    "        .apply(lambda x: x.sample(n=min(len(x), per_industry), random_state=42))\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    print(f\"🧪 Sampled {len(sample_df)} JDs — {per_industry} per top-{top_n} industries.\")\n",
    "    return sample_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4ed1ea",
   "metadata": {},
   "source": [
    "### Load Resume and JD datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7bc2aa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Resume Dataset Loader (with caching)\n",
    "# ==============================\n",
    "def load_resume_dataset(dataset_path: str = \"snehaanbhawal/resume-dataset\", target_csv_name: str = \"Resume.csv\") -> pd.DataFrame:\n",
    "    if not hasattr(load_resume_dataset, \"_cache\"):\n",
    "        print(\"📥 Loading resume dataset for the first time...\")\n",
    "        load_resume_dataset._cache = HybridDatasetLoader.load_dataset(dataset_path, target_csv_name)\n",
    "    else:\n",
    "        print(\"✅ Using cached resume dataset.\")\n",
    "    \n",
    "    return load_resume_dataset._cache\n",
    "\n",
    "# ==============================\n",
    "# Job Description Dataset Loader (with caching)\n",
    "# ==============================\n",
    "def load_job_description_dataset(dataset_path: str = \"arshkon/linkedin-job-postings\") -> dict:\n",
    "    if not hasattr(load_job_description_dataset, \"_cache\"):\n",
    "        print(\"📥 Loading all job description CSVs from Kaggle dataset...\")\n",
    "\n",
    "        def hybrid_load(csv_name: str) -> pd.DataFrame:\n",
    "            return HybridDatasetLoader.load_dataset(dataset_path, csv_name)\n",
    "\n",
    "        load_job_description_dataset._cache = {\n",
    "            \"postings\": hybrid_load(\"postings.csv\"),\n",
    "            \"skills\": hybrid_load(\"skills.csv\"),\n",
    "            \"industries\": hybrid_load(\"industries.csv\"),\n",
    "            \"job_skills\": hybrid_load(\"job_skills.csv\"),\n",
    "            \"job_industries\": hybrid_load(\"job_industries.csv\"),\n",
    "        }\n",
    "    else:\n",
    "        print(\"✅ Using cached job description dataset files.\")\n",
    "    \n",
    "    return load_job_description_dataset._cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff68a05",
   "metadata": {},
   "source": [
    "### JD Dataset Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "31443703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save_enriched_jd_dataset(\n",
    "    dataset_path: str ,\n",
    "    output_json_name: str,\n",
    "    allowed_fields: List[str] \n",
    "):\n",
    "    # 🔄 Load enriched multi-source JD data using HybridLoader\n",
    "    jd_data = load_job_description_dataset(dataset_path)\n",
    "\n",
    "    # Process (returns enriched DataFrame)\n",
    "    enriched_df = DatasetProcessor.process_and_save_enriched_jds(jd_data)\n",
    "    \n",
    "    high_quality_df = JobDescriptionFilter.apply_all_filters(enriched_df)\n",
    "\n",
    "    # Filter only allowed fields\n",
    "    final_df = DatasetProcessor.filter_fields(high_quality_df, allowed_fields)\n",
    "\n",
    "    # Save\n",
    "    DatasetProcessor.save_to_json(final_df, output_json_name)\n",
    "    \n",
    "    # cleanup dataset\n",
    "    dataset_slug = dataset_path.split(\"/\")[-1]\n",
    "    extracted_folder = os.path.join(Config.DATASET_DOWNLOAD_DIR, dataset_slug)\n",
    "    zip_filename = f\"{dataset_slug}.zip\"\n",
    "    if Config.AUTO_CLEANUP:\n",
    "        Cleaner.cleanup_dataset_artifacts(extracted_folder, zip_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "55dda0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save_enriched_jd_dataset_sample(\n",
    "    dataset_path: str,\n",
    "    output_json_name: str,\n",
    "    allowed_fields: List[str],\n",
    "    top_n: int = 20,\n",
    "    per_industry: int = 2\n",
    "):\n",
    "    # 🔄 Load enriched multi-source JD data using HybridLoader\n",
    "    jd_data = load_job_description_dataset(dataset_path)\n",
    "\n",
    "    # Process (returns enriched DataFrame)\n",
    "    enriched_df = DatasetProcessor.process_and_save_enriched_jds(jd_data)\n",
    "    \n",
    "    high_quality_df = JobDescriptionFilter.apply_all_filters(enriched_df)\n",
    "\n",
    "    # Filter only allowed fields\n",
    "    final_df = DatasetProcessor.filter_fields(high_quality_df, allowed_fields)\n",
    "\n",
    "    # Sample\n",
    "    sampled_df = sample_jds_top_industries(final_df, top_n=top_n, per_industry=per_industry)\n",
    "\n",
    "    # Save\n",
    "    DatasetProcessor.save_to_json(sampled_df, output_json_name)\n",
    "    \n",
    "    # cleanup dataset\n",
    "    dataset_slug = dataset_path.split(\"/\")[-1]\n",
    "    extracted_folder = os.path.join(Config.DATASET_DOWNLOAD_DIR, dataset_slug)\n",
    "    zip_filename = f\"{dataset_slug}.zip\"\n",
    "    if Config.AUTO_CLEANUP:\n",
    "        Cleaner.cleanup_dataset_artifacts(extracted_folder, zip_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e917e9d6",
   "metadata": {},
   "source": [
    "### Resume Dataset Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "23ea4be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Resume Filtering (5 per category)\n",
    "# ==============================\n",
    "def filter_resumes_by_category(resume_df: pd.DataFrame, top_n: int = 5) -> pd.DataFrame:\n",
    "    if 'Category' not in resume_df.columns:\n",
    "        raise ValueError(\"❌ Resume dataset does not contain 'Category' column.\")\n",
    "\n",
    "    filtered_resumes = (\n",
    "        resume_df\n",
    "        .dropna(subset=['Category'])\n",
    "        .groupby('Category', group_keys=False)\n",
    "        .apply(lambda group: group.head(top_n))\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    print(f\"✅ Filtered {len(filtered_resumes)} resumes (top {top_n} from each category).\")\n",
    "    return filtered_resumes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "766a42bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================\n",
    "# MAIN FLOW\n",
    "# ==============================\n",
    "\n",
    "def process_dataset_resume(dataset_path: str, target_csv_name: str, allowed_fields: List[str], output_json_name: str):\n",
    "    df = load_resume_dataset(dataset_path, target_csv_name)\n",
    "    # Add resume_str length filter (3000–8000 characters)\n",
    "    df = df[df[\"Resume_str\"].str.len().between(3000, 8000)]\n",
    "    filtered_df = DatasetProcessor.filter_fields(df, allowed_fields)\n",
    "    DatasetProcessor.save_to_json(filtered_df, output_json_name)\n",
    "\n",
    "    dataset_slug = dataset_path.split(\"/\")[-1]\n",
    "    extracted_folder = os.path.join(Config.DATASET_DOWNLOAD_DIR, dataset_slug)\n",
    "    zip_filename = f\"{dataset_slug}.zip\"\n",
    "    if Config.AUTO_CLEANUP:\n",
    "        Cleaner.cleanup_dataset_artifacts(extracted_folder, zip_filename)\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Save Filtered Resumes\n",
    "# ==============================\n",
    "def process_and_save_sample_resumes(dataset_path: str, target_csv_name: str, allowed_fields: List[str], output_json_name: str):\n",
    "    resume_df = load_resume_dataset(dataset_path, target_csv_name)\n",
    "    # Add resume_str length filter (3000–8000 characters)\n",
    "    resume_df = resume_df[resume_df[\"Resume_str\"].str.len().between(3000, 8000)]\n",
    "    \n",
    "    df = filter_resumes_by_category(resume_df, top_n=2)\n",
    "    filtered_df = DatasetProcessor.filter_fields(df, allowed_fields)\n",
    "    DatasetProcessor.save_to_json(filtered_df, output_json_name)\n",
    "    \n",
    "    dataset_slug = dataset_path.split(\"/\")[-1]\n",
    "    extracted_folder = os.path.join(Config.DATASET_DOWNLOAD_DIR, dataset_slug)\n",
    "    zip_filename = f\"{dataset_slug}.zip\"\n",
    "    if Config.AUTO_CLEANUP:\n",
    "        Cleaner.cleanup_dataset_artifacts(extracted_folder, zip_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60577088",
   "metadata": {},
   "source": [
    "## Login and do the processing of Resume and JD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "491107cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Loading resume dataset for the first time...\n",
      "📥 Trying KaggleHub for snehaanbhawal/resume-dataset...\n",
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.11), please consider upgrading to the latest version (0.3.12).\n",
      "⚠️ KaggleHub failed: 404 Client Error.\n",
      "\n",
      "Resource not found at URL: https://www.kaggle.com/datasets/snehaanbhawal/resume-dataset/versions/1\n",
      "The server reported the following issues: Data not found\n",
      "Please make sure you specified the correct resource identifiers.\n",
      "Falling back to ZIP-based loader.\n",
      "⚡ Dataset folder already exists at 'datasets\\resume-dataset', skipping download and extraction.\n",
      "🔍 Searching for 'Resume.csv' inside datasets\\resume-dataset...\n",
      "✅ Loaded CSV with shape (2484, 4)\n",
      "✅ Filtered 48 resumes (top 2 from each category).\n",
      "✅ Filtered columns: ['ID', 'Category', 'Resume_str']\n",
      "🗑️ Existing JSON 'json_outputs_sample_data\\parsed_resumes.json' deleted.\n",
      "✅ Data saved to JSON at 'json_outputs_sample_data\\parsed_resumes.json'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rubyj\\AppData\\Local\\Temp\\ipykernel_564\\2194308232.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda group: group.head(top_n))\n"
     ]
    }
   ],
   "source": [
    "process_and_save_sample_resumes(\n",
    "    dataset_path=\"snehaanbhawal/resume-dataset\",\n",
    "    target_csv_name=\"Resume.csv\",\n",
    "    allowed_fields=[\"ID\", \"Category\", \"Resume_str\"],\n",
    "    output_json_name=\"parsed_resumes.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bb5022",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Process Resume Dataset\n",
    "process_dataset_resume(\n",
    "    dataset_path=\"snehaanbhawal/resume-dataset\",\n",
    "    target_csv_name=\"Resume.csv\",\n",
    "    allowed_fields=[\"ID\", \"Category\", \"Resume_str\"],\n",
    "    output_json_name=\"parsed_resumes.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8258ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_and_save_enriched_jd_dataset(\n",
    "    dataset_path=\"arshkon/linkedin-job-postings\",\n",
    "    output_json_name=\"parsed_jds.json\",\n",
    "    allowed_fields=[\"job_id\", \"title\", \"industry\", \"company_name\", \"location\",  \"skills_desc\",\"formatted_experience_level\", \"formatted_work_type\", \"description\"] \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15ce25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_and_save_enriched_jd_dataset_sample(\n",
    "    dataset_path=\"arshkon/linkedin-job-postings\",\n",
    "    output_json_name=\"parsed_jds.json\",\n",
    "    allowed_fields=[\"job_id\", \"title\", \"industry\", \"company_name\", \"location\",  \"skills_desc\",\"formatted_experience_level\", \"formatted_work_type\", \"description\"],\n",
    "    top_n=30,\n",
    "    per_industry=2\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
