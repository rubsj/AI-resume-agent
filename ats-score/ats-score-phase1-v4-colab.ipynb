{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60883f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Detected environment: Local Machine\n",
      "‚úÖ Kaggle API credentials found (Local Machine).\n",
      "‚¨áÔ∏è Downloading dataset: snehaanbhawal/resume-dataset ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/snehaanbhawal/resume-dataset\n",
      "License(s): CC0-1.0\n",
      "‚úÖ Downloaded and extracted to 'datasets\\resume-dataset'.\n",
      "‚úÖ Found CSV: datasets\\resume-dataset\\Resume\\Resume.csv\n",
      "‚úÖ Loaded CSV 'Resume.csv' with shape (2484, 4).\n",
      "üßπ Retained fields: ['Category', 'Resume_str']\n",
      "üóëÔ∏è Old JSON file 'resume_data.json' deleted.\n",
      "‚úÖ Saved JSON to 'json_outputs\\resume_data.json'.\n",
      "üßπ Folder 'datasets\\resume-dataset' has been deleted successfully.\n",
      "üóëÔ∏è Zip file 'datasets\\resume-dataset.zip' has been deleted successfully.\n",
      "‚¨áÔ∏è Downloading dataset: arshkon/linkedin-job-postings ...\n",
      "Dataset URL: https://www.kaggle.com/datasets/arshkon/linkedin-job-postings\n",
      "License(s): CC-BY-SA-4.0\n",
      "‚úÖ Downloaded and extracted to 'datasets\\linkedin-job-postings'.\n",
      "‚úÖ Found CSV: datasets\\linkedin-job-postings\\postings.csv\n",
      "‚úÖ Loaded CSV 'postings.csv' with shape (123849, 31).\n",
      "‚ö†Ô∏è Warning: Allowed fields missing from dataset: ['posted_date', 'job_function']\n",
      "üßπ Retained fields: ['title', 'company_name', 'location', 'description', 'skills_desc', 'job_id', 'formatted_experience_level', 'formatted_work_type']\n",
      "üóëÔ∏è Old JSON file 'job_postings.json' deleted.\n",
      "‚úÖ Saved JSON to 'json_outputs\\job_postings.json'.\n",
      "üßπ Folder 'datasets\\linkedin-job-postings' has been deleted successfully.\n",
      "üóëÔ∏è Zip file 'datasets\\linkedin-job-postings.zip' has been deleted successfully.\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# üõ† CONFIGURATION\n",
    "# ==============================\n",
    "AUTO_CLEANUP = True  # Set to True for automatic folder cleanup after JSON creation\n",
    "DATASET_DOWNLOAD_DIR = \"datasets\"\n",
    "JSON_OUTPUT_DIR = \"json_outputs\"\n",
    "\n",
    "# ==============================\n",
    "# üì¶ Install and Import libraries\n",
    "# ==============================\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"üß† Detected environment: Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"üß† Detected environment: Local Machine\")\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "# ==============================\n",
    "# üì• Kaggle Authentication\n",
    "# ==============================\n",
    "def authenticate_kaggle():\n",
    "    \"\"\"\n",
    "    Authenticate with Kaggle based on environment (Colab vs Local).\n",
    "    \"\"\"\n",
    "    if IN_COLAB:\n",
    "        from google.colab import files\n",
    "        print(\"üì• Please upload your kaggle.json file...\")\n",
    "        uploaded = files.upload()\n",
    "\n",
    "        os.makedirs('/root/.kaggle', exist_ok=True)\n",
    "        with open('/root/.kaggle/kaggle.json', 'wb') as f:\n",
    "            f.write(uploaded['kaggle.json'])\n",
    "        \n",
    "        os.chmod('/root/.kaggle/kaggle.json', 600)\n",
    "        print(\"‚úÖ Kaggle API credentials set up successfully (Colab).\")\n",
    "    else:\n",
    "        kaggle_path = os.path.expanduser('~/.kaggle/kaggle.json')\n",
    "        if not os.path.exists(kaggle_path):\n",
    "            raise FileNotFoundError(f\"‚ùå kaggle.json not found at {kaggle_path}. Please place your Kaggle API key there.\")\n",
    "        print(\"‚úÖ Kaggle API credentials found (Local Machine).\")\n",
    "\n",
    "# ==============================\n",
    "# ‚öôÔ∏è Helper Functions\n",
    "# ==============================\n",
    "def download_and_extract_dataset(dataset_path: str, download_dir: str = DATASET_DOWNLOAD_DIR) -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Download and extract a Kaggle dataset.\n",
    "    Extract into a subfolder named after the ZIP file.\n",
    "    Returns (extracted_folder_path, zip_filename).\n",
    "    \"\"\"\n",
    "    os.makedirs(download_dir, exist_ok=True)\n",
    "    print(f\"‚¨áÔ∏è Downloading dataset: {dataset_path} ...\")\n",
    "    !kaggle datasets download -d {dataset_path} -p {download_dir}\n",
    "    \n",
    "    zip_files = [f for f in os.listdir(download_dir) if f.endswith('.zip')]\n",
    "    if not zip_files:\n",
    "        raise FileNotFoundError(\"‚ùå No zip file found after download!\")\n",
    "    \n",
    "    zip_filename = zip_files[0]\n",
    "    zip_path = os.path.join(download_dir, zip_filename)\n",
    "    \n",
    "    extract_folder_name = zip_filename.replace(\".zip\", \"\")\n",
    "    extract_folder_path = os.path.join(download_dir, extract_folder_name)\n",
    "    os.makedirs(extract_folder_path, exist_ok=True)\n",
    "\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_folder_path)\n",
    "    \n",
    "    print(f\"‚úÖ Downloaded and extracted to '{extract_folder_path}'.\")\n",
    "    \n",
    "    return extract_folder_path, zip_filename\n",
    "\n",
    "\n",
    "def find_csv_file(root_dir: str, target_csv_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Walks through root_dir and finds the full path to the target CSV.\n",
    "    \"\"\"\n",
    "    for root, dirs, files in os.walk(root_dir):\n",
    "        for file in files:\n",
    "            if file.lower() == target_csv_name.lower():\n",
    "                found_path = os.path.join(root, file)\n",
    "                print(f\"‚úÖ Found CSV: {found_path}\")\n",
    "                return found_path\n",
    "    raise FileNotFoundError(f\"‚ùå CSV file '{target_csv_name}' not found in '{root_dir}'!\")\n",
    "\n",
    "def load_csv(dataset_dir: str, csv_filename: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load CSV file into pandas DataFrame after walking the directory tree.\n",
    "    \"\"\"\n",
    "    csv_path = find_csv_file(dataset_dir, csv_filename)\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"‚úÖ Loaded CSV '{csv_filename}' with shape {df.shape}.\")\n",
    "    return df\n",
    "\n",
    "def filter_fields(df: pd.DataFrame, allowed_fields: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Keep only allowed fields from DataFrame.\n",
    "    \"\"\"\n",
    "    missing = [field for field in allowed_fields if field not in df.columns]\n",
    "    if missing:\n",
    "        print(f\"‚ö†Ô∏è Warning: Allowed fields missing from dataset: {missing}\")\n",
    "    \n",
    "    df_filtered = df[[col for col in allowed_fields if col in df.columns]]\n",
    "    print(f\"üßπ Retained fields: {df_filtered.columns.tolist()}\")\n",
    "    return df_filtered\n",
    "\n",
    "def save_to_json(df: pd.DataFrame, output_filename: str, output_dir: str = JSON_OUTPUT_DIR):\n",
    "    \"\"\"\n",
    "    Save DataFrame to JSON records.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = os.path.join(output_dir, output_filename)\n",
    "    \n",
    "    if os.path.exists(output_path):\n",
    "        os.remove(output_path)\n",
    "        print(f\"üóëÔ∏è Old JSON file '{output_filename}' deleted.\")\n",
    "\n",
    "    df.to_json(output_path, orient=\"records\", lines=True, force_ascii=False)\n",
    "    print(f\"‚úÖ Saved JSON to '{output_path}'.\")\n",
    "\n",
    "def cleanup_dataset_artifacts(extracted_folder_path: str, zip_filename: str, download_dir: str = DATASET_DOWNLOAD_DIR):\n",
    "    \"\"\"\n",
    "    Delete both the extracted dataset folder and the associated zip file.\n",
    "    \"\"\"\n",
    "    # Delete extracted folder\n",
    "    if os.path.exists(extracted_folder_path):\n",
    "        shutil.rmtree(extracted_folder_path)\n",
    "        print(f\"üßπ Folder '{extracted_folder_path}' has been deleted successfully.\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Folder '{extracted_folder_path}' does not exist, skipping folder deletion.\")\n",
    "    \n",
    "    # Delete zip file\n",
    "    zip_path = os.path.join(download_dir, zip_filename)\n",
    "    if os.path.exists(zip_path):\n",
    "        os.remove(zip_path)\n",
    "        print(f\"üóëÔ∏è Zip file '{zip_path}' has been deleted successfully.\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Zip file '{zip_path}' does not exist, skipping zip deletion.\")\n",
    "\n",
    "# ==============================\n",
    "# üî• Kaggle Authentication\n",
    "# ==============================\n",
    "authenticate_kaggle()\n",
    "\n",
    "# ==============================\n",
    "# üìÑ Process Resume Dataset\n",
    "# ==============================\n",
    "resume_dataset_path = \"snehaanbhawal/resume-dataset\"\n",
    "resume_csv_name = \"Resume.csv\"\n",
    "resume_allowed_fields = [\"Category\", \"Resume_str\"]\n",
    "resume_json_name = \"resume_data.json\"\n",
    "\n",
    "resume_extracted_folder, resume_zip_filename = download_and_extract_dataset(resume_dataset_path)\n",
    "resume_df = load_csv(resume_extracted_folder, resume_csv_name)\n",
    "resume_filtered_df = filter_fields(resume_df, resume_allowed_fields)\n",
    "save_to_json(resume_filtered_df, resume_json_name)\n",
    "\n",
    "if AUTO_CLEANUP:\n",
    "    cleanup_dataset_artifacts(resume_extracted_folder, resume_zip_filename)\n",
    "\n",
    "# ==============================\n",
    "# üìÑ Process LinkedIn Job Postings Dataset\n",
    "# ==============================\n",
    "job_dataset_path = \"arshkon/linkedin-job-postings\"\n",
    "job_csv_name = \"postings.csv\"\n",
    "job_allowed_fields = [\"title\", \"company_name\", \"location\", \"description\", \"skills_desc\", \"posted_date\", \"job_id\" , \"formatted_experience_level\", \"formatted_work_type\", \"job_function\"]\n",
    "job_json_name = \"job_postings.json\"\n",
    "\n",
    "job_extracted_folder, job_zip_filename = download_and_extract_dataset(job_dataset_path)\n",
    "job_df = load_csv(job_extracted_folder, job_csv_name)\n",
    "job_filtered_df = filter_fields(job_df, job_allowed_fields)\n",
    "save_to_json(job_filtered_df, job_json_name)\n",
    "\n",
    "if AUTO_CLEANUP:\n",
    "    cleanup_dataset_artifacts(job_extracted_folder, job_zip_filename)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
