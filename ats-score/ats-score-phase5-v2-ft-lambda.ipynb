{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a62a7e73",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80839423",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install kagglehub pandas\n",
    "%pip install -q transformers peft datasets accelerate bitsandbytes sentencepiece pydantic huggingface_hub xformers\n",
    "%pip install optuna\n",
    "\n",
    "#%pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu121\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "#%pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e991874",
   "metadata": {},
   "source": [
    "## Configurations  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ec2f4fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 🛠 CONFIGURATION\n",
    "# ==============================\n",
    "\n",
    "class Config:\n",
    "    JSON_OUTPUT_DIR = \"json_outputs_all_data\"\n",
    "    JSON_OUTPUT_NORMALIZED_DIR = \"json_outputs_all_data/normalized\"\n",
    "    JSON_OUTPUT_NORMALIZED_JD = \"json_outputs_all_data/normalized/jd\"\n",
    "    JSON_OUTPUT_NORMALIZED_RESUME = \"json_outputs_all_data/normalized/resume\"\n",
    "    JSON_OUTPUT_SCORING_DIR = \"json_outputs_all_data/scoring\"\n",
    "    JSON_OUTPUT_SCORING_SPLIT_DIR = \"json_outputs_all_data/scoring/split\"\n",
    "    JSON_OUTPUT_SCORING_FT_DATA = \"json_outputs_all_data/scoring/FT_data\"\n",
    "    JSON_OUTPUT_FINE_TUNE_SCORE = \"json_outputs_all_data/fine-tune/scored\"\n",
    "    JSON_OUTPUT_FINE_TUNE_RECORD = \"json_outputs_all_data/fine-tune/record\"\n",
    "    JSON_OUTPUT_FINE_TUNE_TEST_DATA = \"json_outputs_all_data/fine-tune/test-data\"\n",
    "    JSON_OUTPUT_FINE_TUNE_OUTPUT = \"json_outputs_all_data/fine-tune/optuna_output\"\n",
    "    JSON_OUTPUT_FINE_TUNE_MODEL = \"json_outputs_all_data/fine-tune/model\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecca7f7f",
   "metadata": {},
   "source": [
    "## Login to huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4f223e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# Set your token here securely or prompt for it in Colab\n",
    "# Recommended: store in Colab secrets or environment variable\n",
    "HF_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "\n",
    "if not HF_TOKEN:\n",
    "    # Prompt for token if not set in environment\n",
    "    print(\"🔑 Please enter your Hugging Face token:\")\n",
    "    # For Colab or local prompt input\n",
    "    HF_TOKEN = input(\"🔑 Enter your Hugging Face token: \").strip()\n",
    "\n",
    "login(token=HF_TOKEN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cd1793",
   "metadata": {},
   "source": [
    "# Full Fine-Tuning on Lambda with Optuna, LR Scheduler, Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7002494a",
   "metadata": {},
   "source": [
    "### Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22f8df89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import os\n",
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, DataCollatorForLanguageModeling\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb6a859",
   "metadata": {},
   "source": [
    "### Paths & Basic Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fab610c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"Qwen/Qwen2-7B-Instruct\"\n",
    "\n",
    "\n",
    "# ✅ Paths\n",
    "train_path =os.path.join(Config.JSON_OUTPUT_FINE_TUNE_TEST_DATA, \"train.jsonl\") \n",
    "eval_path = os.path.join(Config.JSON_OUTPUT_FINE_TUNE_TEST_DATA, \"eval.jsonl\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66d88d8",
   "metadata": {},
   "source": [
    "### Load Tokenizer & Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9f92ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "data = load_dataset(\"json\", data_files={\"train\": train_path, \"validation\": eval_path})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4d9e07",
   "metadata": {},
   "source": [
    "### Tokenization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cba89c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(example):\n",
    "    prompt = f\"<|im_start|>user\\n{example['input']}<|im_end|>\\n<|im_start|>assistant\\n{example['output']}<|im_end|>\"\n",
    "    tokens = tokenizer(prompt, padding=\"max_length\", truncation=True, max_length=1024)\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
    "    return tokens\n",
    "\n",
    "\n",
    "tokenized_data = data.map(tokenize, remove_columns=data[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591ca392",
   "metadata": {},
   "source": [
    "### Define Optuna Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0088158",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    from peft import prepare_model_for_kbit_training\n",
    "\n",
    "    # Hyperparameter suggestions\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 5e-5, 5e-4, log=True)\n",
    "    num_train_epochs = trial.suggest_int(\"num_train_epochs\", 2, 4)\n",
    "    lora_r = trial.suggest_categorical(\"lora_r\", [4, 8, 16])\n",
    "    lora_alpha = trial.suggest_categorical(\"lora_alpha\", [16, 32, 64])\n",
    "\n",
    "    # Load base model with bitsandbytes quantization (no meta tensor error)\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    base_model = prepare_model_for_kbit_training(base_model)\n",
    "\n",
    "    # Apply LoRA\n",
    "    lora_config = LoraConfig(\n",
    "        r=lora_r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM\n",
    "    )\n",
    "    model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "    # Output directories for current trial\n",
    "    output_dir = os.path.join(Config.JSON_OUTPUT_FINE_TUNE_OUTPUT, f\"optuna_trial_{trial.number}\")\n",
    "    logging_dir = os.path.join(output_dir, \"logs\")\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=2,\n",
    "        per_device_eval_batch_size=2,\n",
    "        gradient_accumulation_steps=8,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=learning_rate,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        bf16=True,\n",
    "        load_best_model_at_end=True,\n",
    "        report_to=\"none\",\n",
    "        save_total_limit=1,\n",
    "        logging_dir=logging_dir,\n",
    "        logging_steps=10,\n",
    "    )\n",
    "\n",
    "    # Prepare trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_data[\"train\"],\n",
    "        eval_dataset=tokenized_data[\"validation\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    "    )\n",
    "\n",
    "    # Training and evaluation\n",
    "    trainer.train()\n",
    "    trial.set_user_attr(\"best_model_path\", trainer.state.best_model_checkpoint or output_dir)\n",
    "    eval_metrics = trainer.evaluate()\n",
    "    return eval_metrics[\"eval_loss\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97fb436",
   "metadata": {},
   "source": [
    "### Launch Optuna Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc53aec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-24 03:06:12,954] A new study created in memory with name: no-name-d8fbb683-ed2c-439c-a685-0b3cfe68b5f2\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.40s/it]\n",
      "/tmp/ipykernel_9503/3243740175.py:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6000' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6000/6000 13:36:39, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.937700</td>\n",
       "      <td>1.863273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.669700</td>\n",
       "      <td>1.700749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.683100</td>\n",
       "      <td>1.627911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.671900</td>\n",
       "      <td>1.596733</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3000/3000 15:38]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-24 16:58:54,497] Trial 0 finished with value: 1.5967330932617188 and parameters: {'learning_rate': 0.0004189916968040283, 'num_train_epochs': 4, 'lora_r': 4, 'lora_alpha': 32}. Best is trial 0 with value: 1.5967330932617188.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.41s/it]\n",
      "/tmp/ipykernel_9503/3243740175.py:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "/home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4500' max='4500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4500/4500 10:12:15, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.006100</td>\n",
       "      <td>1.937912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.730800</td>\n",
       "      <td>1.770430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.772300</td>\n",
       "      <td>1.719054</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3000/3000 15:38]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-25 03:27:11,265] Trial 1 finished with value: 1.7190537452697754 and parameters: {'learning_rate': 0.0002965589109933222, 'num_train_epochs': 3, 'lora_r': 4, 'lora_alpha': 32}. Best is trial 0 with value: 1.5967330932617188.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.13s/it]\n",
      "/tmp/ipykernel_9503/3243740175.py:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "/home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6000' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6000/6000 13:37:07, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.340900</td>\n",
       "      <td>2.266978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.165800</td>\n",
       "      <td>2.188035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.194100</td>\n",
       "      <td>2.137888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.138300</td>\n",
       "      <td>2.118460</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3000/3000 15:39]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-25 17:20:19,104] Trial 2 finished with value: 2.118459939956665 and parameters: {'learning_rate': 5.3421168996654035e-05, 'num_train_epochs': 4, 'lora_r': 4, 'lora_alpha': 32}. Best is trial 0 with value: 1.5967330932617188.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.17s/it]\n",
      "/tmp/ipykernel_9503/3243740175.py:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "/home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3000/3000 6:48:22, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.014400</td>\n",
       "      <td>1.940913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.770900</td>\n",
       "      <td>1.817408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3000/3000 15:39]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-26 00:24:42,263] Trial 3 finished with value: 1.8174083232879639 and parameters: {'learning_rate': 0.0002247059971759653, 'num_train_epochs': 2, 'lora_r': 4, 'lora_alpha': 64}. Best is trial 0 with value: 1.5967330932617188.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.05s/it]\n",
      "/tmp/ipykernel_9503/3243740175.py:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "/home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3000/3000 6:47:55, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.563300</td>\n",
       "      <td>1.477697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.267400</td>\n",
       "      <td>1.278966</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3000/3000 15:36]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-26 07:28:35,931] Trial 4 finished with value: 1.2789655923843384 and parameters: {'learning_rate': 0.00032850708239078144, 'num_train_epochs': 2, 'lora_r': 16, 'lora_alpha': 64}. Best is trial 4 with value: 1.2789655923843384.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.11s/it]\n",
      "/tmp/ipykernel_9503/3243740175.py:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "/home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1636' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1636/3000 3:40:32 < 3:04:06, 0.12 it/s, Epoch 1.09/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.177500</td>\n",
       "      <td>2.109524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "print(\"✅ Best hyperparameters:\")\n",
    "print(study.best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21368235",
   "metadata": {},
   "source": [
    "### Resume Optuna trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b8cd28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-27 15:28:15,819] Using an existing study with name 'resume_lora_ft' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Detected completed trials: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      "🔁 Running 2 more trials (out of 10)\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import os\n",
    "import json\n",
    "from peft import prepare_model_for_kbit_training, get_peft_model, LoraConfig, TaskType\n",
    "from transformers import AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling, EarlyStoppingCallback\n",
    "#from transformers.utils import logging\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# Setup\n",
    "completed_trials = set()\n",
    "base_output_dir = Config.JSON_OUTPUT_FINE_TUNE_OUTPUT\n",
    "\n",
    "# Detect completed trials based on output folders\n",
    "for entry in os.listdir(base_output_dir):\n",
    "    if entry.startswith(\"optuna_trial_\"):\n",
    "        try:\n",
    "            trial_id = int(entry.replace(\"optuna_trial_\", \"\"))\n",
    "            trial_path = os.path.join(base_output_dir, entry)\n",
    "\n",
    "            # 🔍 Search for checkpoint-*/trainer_state.json\n",
    "            for subdir in sorted(os.listdir(trial_path), reverse=True):\n",
    "                if subdir.startswith(\"checkpoint-\"):\n",
    "                    checkpoint_path = os.path.join(trial_path, subdir, \"trainer_state.json\")\n",
    "                    if os.path.exists(checkpoint_path):\n",
    "                        with open(checkpoint_path) as f:\n",
    "                            state = json.load(f)\n",
    "                            if state.get(\"best_model_checkpoint\", None):\n",
    "                                completed_trials.add(trial_id)\n",
    "                        break  # Stop after finding the first valid checkpoint\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "print(f\"✅ Detected completed trials: {sorted(completed_trials)}\")\n",
    "\n",
    "# ⛏️ Define objective\n",
    "def objective(trial):\n",
    "    trial_number = trial.number\n",
    "    if trial_number in completed_trials:\n",
    "        print(f\"⏩ Skipping trial {trial_number} (already completed)\")\n",
    "        raise optuna.TrialPruned()\n",
    "\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 5e-5, 5e-4, log=True)\n",
    "    num_train_epochs = trial.suggest_int(\"num_train_epochs\", 2, 4)\n",
    "    lora_r = trial.suggest_categorical(\"lora_r\", [4, 8, 16])\n",
    "    lora_alpha = trial.suggest_categorical(\"lora_alpha\", [16, 32, 64])\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    base_model = prepare_model_for_kbit_training(base_model)\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=lora_r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "    )\n",
    "    model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "    output_dir = os.path.join(base_output_dir, f\"optuna_trial_{trial_number}\")\n",
    "    logging_dir = os.path.join(output_dir, \"logs\")\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=2,\n",
    "        per_device_eval_batch_size=2,\n",
    "        gradient_accumulation_steps=8,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=learning_rate,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        bf16=True,\n",
    "        load_best_model_at_end=True,\n",
    "        report_to=\"none\",\n",
    "        save_total_limit=1,\n",
    "        logging_dir=logging_dir,\n",
    "        logging_steps=10,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_data[\"train\"],\n",
    "        eval_dataset=tokenized_data[\"validation\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    "    )\n",
    "\n",
    "    print(f\"🎯 Trial {trial_number} | LR: {learning_rate:.2e}, Epochs: {num_train_epochs}, R: {lora_r}, Alpha: {lora_alpha}\")\n",
    "\n",
    "    trainer.train()\n",
    "    trial.set_user_attr(\"best_model_path\", trainer.state.best_model_checkpoint or output_dir)\n",
    "    eval_metrics = trainer.evaluate()\n",
    "    return eval_metrics[\"eval_loss\"]\n",
    "\n",
    "# 📈 Resume Study\n",
    "storage_path = f\"sqlite:///{os.path.join(base_output_dir, 'optuna_study.db')}\"\n",
    "study = optuna.create_study(direction=\"minimize\", study_name=\"resume_lora_ft\", storage=storage_path, load_if_exists=True)\n",
    "\n",
    "# 🔁 Continue from last completed\n",
    "n_trials_total = 10\n",
    "n_remaining = n_trials_total - len(completed_trials)\n",
    "print(f\"🔁 Running {n_remaining} more trials (out of {n_trials_total})\")\n",
    "\n",
    "#study.optimize(objective, n_trials=n_remaining)\n",
    "\n",
    "#print(\"🏁 Final Best Hyperparameters:\")\n",
    "#print(study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31f1a4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Injected 6 completed trials into Optuna study\n"
     ]
    }
   ],
   "source": [
    "from optuna.trial import TrialState\n",
    "\n",
    "# --- Restore completed trials into Optuna Study ---\n",
    "for trial_id in completed_trials:\n",
    "    trial_dir = os.path.join(base_output_dir, f\"optuna_trial_{trial_id}\")\n",
    "    \n",
    "    # Extract saved config from TrainingArguments\n",
    "    trainer_state_path = None\n",
    "    for subdir in os.listdir(trial_dir):\n",
    "        if subdir.startswith(\"checkpoint-\"):\n",
    "            state_path = os.path.join(trial_dir, subdir, \"trainer_state.json\")\n",
    "            if os.path.exists(state_path):\n",
    "                trainer_state_path = state_path\n",
    "                break\n",
    "\n",
    "    if not trainer_state_path:\n",
    "        continue\n",
    "\n",
    "    with open(trainer_state_path) as f:\n",
    "        trainer_state = json.load(f)\n",
    "\n",
    "    # Extract saved best checkpoint path\n",
    "    best_model_path = trainer_state.get(\"best_model_checkpoint\", trial_dir)\n",
    "    hparams_path = os.path.join(best_model_path, \"hparams.json\")\n",
    "    if not os.path.exists(hparams_path):\n",
    "        continue\n",
    "\n",
    "    with open(hparams_path) as f:\n",
    "        hparams = json.load(f)\n",
    "\n",
    "    # Manually register this trial\n",
    "    trial = study.ask(\n",
    "        {\n",
    "            \"learning_rate\": float(hparams.get(\"learning_rate\", 2e-4)),\n",
    "            \"num_train_epochs\": int(hparams.get(\"num_train_epochs\", 3)),\n",
    "            \"lora_r\": int(hparams.get(\"lora_r\", 8)),\n",
    "            \"lora_alpha\": int(hparams.get(\"lora_alpha\", 32)),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Log final loss (fallback if missing)\n",
    "    eval_loss = trainer_state.get(\"log_history\", [{}])[-1].get(\"eval_loss\", 2.0)\n",
    "    \n",
    "    # Manually tell Optuna the result\n",
    "    study.tell(trial, eval_loss, state=TrialState.COMPLETE)\n",
    "\n",
    "print(f\"✅ Injected {len(completed_trials)} completed trials into Optuna study\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84a92a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4, 5, 6, 7}\n"
     ]
    }
   ],
   "source": [
    "print(completed_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dda52550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔁 Running 2 more trials (out of 10 total)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.29s/it]\n",
      "/tmp/ipykernel_99891/3353892731.py:94: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Trial 10 | LR: 4.97e-04, Epochs: 3, R: 8, Alpha: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4500' max='4500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4500/4500 10:11:35, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.688800</td>\n",
       "      <td>1.593021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.390200</td>\n",
       "      <td>1.388636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.332500</td>\n",
       "      <td>1.297847</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3000/3000 15:37]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-28 01:58:06,657] Trial 10 finished with value: 1.297846794128418 and parameters: {'learning_rate': 0.0004966883624204826, 'num_train_epochs': 3, 'lora_r': 8, 'lora_alpha': 64}. Best is trial 10 with value: 1.297846794128418.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.30s/it]\n",
      "/tmp/ipykernel_99891/3353892731.py:94: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Trial 11 | LR: 1.00e-04, Epochs: 2, R: 8, Alpha: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3000/3000 6:47:54, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.231900</td>\n",
       "      <td>2.159543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.016700</td>\n",
       "      <td>2.052743</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/AI-resume-agent/myenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3000/3000 15:38]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-28 09:02:02,534] Trial 11 finished with value: 2.0527429580688477 and parameters: {'learning_rate': 0.0001001872627709479, 'num_train_epochs': 2, 'lora_r': 8, 'lora_alpha': 32}. Best is trial 10 with value: 1.297846794128418.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏁 Final Best Hyperparameters:\n",
      "{'learning_rate': 0.0004966883624204826, 'num_train_epochs': 3, 'lora_r': 8, 'lora_alpha': 64}\n"
     ]
    }
   ],
   "source": [
    "# Continue tuning only remaining trials\n",
    "n_remaining = 10 - len(completed_trials)\n",
    "print(f\"🔁 Running {n_remaining} more trials (out of 10 total)\")\n",
    "study.optimize(objective, n_trials=n_remaining)\n",
    "\n",
    "print(\"🏁 Final Best Hyperparameters:\")\n",
    "print(study.best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b1a394",
   "metadata": {},
   "source": [
    "###  Save Final Best Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13651478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏆 Best model saved at: json_outputs_all_data/fine-tune/optuna_output/optuna_trial_10/checkpoint-4500\n"
     ]
    }
   ],
   "source": [
    "best_trial = study.best_trial\n",
    "best_model_path = best_trial.user_attrs.get(\"best_model_path\")\n",
    "print(f\"🏆 Best model saved at: {best_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "30c7c071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Best model path: json_outputs_all_data/fine-tune/optuna_output/optuna_trial_10/checkpoint-4500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.40it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('json_outputs_all_data/fine-tune/model/tokenizer_config.json',\n",
       " 'json_outputs_all_data/fine-tune/model/special_tokens_map.json',\n",
       " 'json_outputs_all_data/fine-tune/model/chat_template.jinja',\n",
       " 'json_outputs_all_data/fine-tune/model/vocab.json',\n",
       " 'json_outputs_all_data/fine-tune/model/merges.txt',\n",
       " 'json_outputs_all_data/fine-tune/model/added_tokens.json',\n",
       " 'json_outputs_all_data/fine-tune/model/tokenizer.json')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_trial_number = study.best_trial.number\n",
    "#best_model_path = f\"optuna_output/{best_trial_number}\"\n",
    "best_model_path = study.best_trial.user_attrs.get(\"best_model_path\")\n",
    "print(\"✅ Best model path:\", best_model_path)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(best_model_path)\n",
    "model.save_pretrained(Config.JSON_OUTPUT_FINE_TUNE_MODEL)\n",
    "tokenizer.save_pretrained(Config.JSON_OUTPUT_FINE_TUNE_MODEL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb93d0c",
   "metadata": {},
   "source": [
    "## Save to hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c45f5474",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "adapter_model.safetensors: 100%|██████████| 10.1M/10.1M [00:00<00:00, 12.5MB/s]\n",
      "tokenizer.json: 100%|██████████| 11.4M/11.4M [00:00<00:00, 14.3MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/rubsj/Qwen2-Resume-ATS/commit/b2c29ebf448f72c22c07d1e322d68625de6d1d6e', commit_message='Upload tokenizer', commit_description='', oid='b2c29ebf448f72c22c07d1e322d68625de6d1d6e', pr_url=None, repo_url=RepoUrl('https://huggingface.co/rubsj/Qwen2-Resume-ATS', endpoint='https://huggingface.co', repo_type='model', repo_id='rubsj/Qwen2-Resume-ATS'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi, HfFolder\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# ✅ Optional: Login (only needed once per environment)\n",
    "# from huggingface_hub import login\n",
    "# login(\"hf_your_access_token\")\n",
    "\n",
    "# Set model path and repo name\n",
    "model_path = Config.JSON_OUTPUT_FINE_TUNE_MODEL\n",
    "repo_name = \"rubsj/Qwen2-Resume-ATS\"  # customize this\n",
    "\n",
    "# Push model and tokenizer to HF hub\n",
    "model.push_to_hub(repo_name, private=True)\n",
    "tokenizer.push_to_hub(repo_name, private=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b961a3ae",
   "metadata": {},
   "source": [
    "## Create ZIP for Lambda Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fba74aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model zipped at: json_outputs_all_data/fine-tune/model.zip\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "zip_path = f\"{Config.JSON_OUTPUT_FINE_TUNE_MODEL}.zip\"\n",
    "shutil.make_archive(base_name=Config.JSON_OUTPUT_FINE_TUNE_MODEL, format='zip', root_dir=Config.JSON_OUTPUT_FINE_TUNE_MODEL)\n",
    "print(f\"✅ Model zipped at: {zip_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "38f4c0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Hugging Face Model URL: https://huggingface.co/rubsj/Qwen2-Resume-ATS\n",
      "✅ Local zip ready for download: json_outputs_all_data/fine-tune/model.zip\n"
     ]
    }
   ],
   "source": [
    "print(f\"✅ Hugging Face Model URL: https://huggingface.co/{repo_name}\")\n",
    "print(f\"✅ Local zip ready for download: {zip_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "53f0fc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model zipped at: json_outputs_all_data/fine-tune/optuna_output.zip\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "zip_path = f\"{Config.JSON_OUTPUT_FINE_TUNE_OUTPUT}.zip\"\n",
    "shutil.make_archive(base_name=Config.JSON_OUTPUT_FINE_TUNE_OUTPUT, format='zip', root_dir=Config.JSON_OUTPUT_FINE_TUNE_OUTPUT)\n",
    "print(f\"✅ Model zipped at: {zip_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bd91c725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Database size: 0.11 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "db_path = os.path.join(Config.JSON_OUTPUT_FINE_TUNE_OUTPUT, \"optuna_study.db\")\n",
    "size_in_bytes = os.path.getsize(db_path)\n",
    "size_in_mb = size_in_bytes / (1024 * 1024)\n",
    "print(f\"📦 Database size: {size_in_mb:.2f} MB\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
