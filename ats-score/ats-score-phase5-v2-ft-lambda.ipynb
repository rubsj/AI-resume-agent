{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a62a7e73",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80839423",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install kagglehub pandas\n",
    "%pip install -q transformers peft datasets accelerate bitsandbytes sentencepiece pydantic huggingface_hub xformers\n",
    "%pip install optuna\n",
    "\n",
    "#%pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu121\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "#%pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e991874",
   "metadata": {},
   "source": [
    "## Configurations  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2f4fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# ðŸ›  CONFIGURATION\n",
    "# ==============================\n",
    "\n",
    "class Config:\n",
    "    JSON_OUTPUT_DIR = \"json_outputs_all_data\"\n",
    "    JSON_OUTPUT_NORMALIZED_DIR = \"json_outputs_all_data/normalized\"\n",
    "    JSON_OUTPUT_NORMALIZED_JD = \"json_outputs_all_data/normalized/jd\"\n",
    "    JSON_OUTPUT_NORMALIZED_RESUME = \"json_outputs_all_data/normalized/resume\"\n",
    "    JSON_OUTPUT_SCORING_DIR = \"json_outputs_all_data/scoring\"\n",
    "    JSON_OUTPUT_SCORING_SPLIT_DIR = \"json_outputs_all_data/scoring/split\"\n",
    "    JSON_OUTPUT_SCORING_FT_DATA = \"json_outputs_all_data/scoring/FT_data\"\n",
    "    JSON_OUTPUT_FINE_TUNE_SCORE = \"json_outputs_all_data/fine-tune/scored\"\n",
    "    JSON_OUTPUT_FINE_TUNE_RECORD = \"json_outputs_all_data/fine-tune/record\"\n",
    "    JSON_OUTPUT_FINE_TUNE_TEST_DATA = \"json_outputs_all_data/fine-tune/test-data\"\n",
    "    JSON_OUTPUT_FINE_TUNE_OUTPUT = \"json_outputs_all_data/fine-tune/optuna_output\"\n",
    "    JSON_OUTPUT_FINE_TUNE_MODEL = \"json_outputs_all_data/fine-tune/model\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecca7f7f",
   "metadata": {},
   "source": [
    "## Login to huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f223e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# Set your token here securely or prompt for it in Colab\n",
    "# Recommended: store in Colab secrets or environment variable\n",
    "HF_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "\n",
    "if not HF_TOKEN:\n",
    "    # Prompt for token if not set in environment\n",
    "    print(\"ðŸ”‘ Please enter your Hugging Face token:\")\n",
    "    # For Colab or local prompt input\n",
    "    HF_TOKEN = input(\"ðŸ”‘ Enter your Hugging Face token: \").strip()\n",
    "\n",
    "login(token=HF_TOKEN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cd1793",
   "metadata": {},
   "source": [
    "# Full Fine-Tuning on Lambda with Optuna, LR Scheduler, Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7002494a",
   "metadata": {},
   "source": [
    "### Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f8df89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import os\n",
    "from optuna.integration import TrainerCallback\n",
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, DataCollatorForLanguageModeling\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb6a859",
   "metadata": {},
   "source": [
    "### Paths & Basic Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fab610c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"Qwen/Qwen2-7B-Instruct\"\n",
    "\n",
    "\n",
    "# âœ… Paths\n",
    "train_path =os.path.join(Config.JSON_OUTPUT_FINE_TUNE_TEST_DATA, \"train.jsonl\") \n",
    "eval_path = os.path.join(Config.JSON_OUTPUT_FINE_TUNE_TEST_DATA, \"eval.jsonl\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66d88d8",
   "metadata": {},
   "source": [
    "### Load Tokenizer & Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f92ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "data = load_dataset(\"json\", data_files={\"train\": train_path, \"validation\": eval_path})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4d9e07",
   "metadata": {},
   "source": [
    "### Tokenization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba89c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(example):\n",
    "    prompt = f\"<|im_start|>user\\n{example['input']}<|im_end|>\\n<|im_start|>assistant\\n{example['output']}<|im_end|>\"\n",
    "    tokens = tokenizer(prompt, padding=\"max_length\", truncation=True, max_length=1024)\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
    "    return tokens\n",
    "\n",
    "\n",
    "tokenized_data = data.map(tokenize, remove_columns=data[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591ca392",
   "metadata": {},
   "source": [
    "### Define Optuna Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81e7a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 5e-5, 5e-4)\n",
    "    num_train_epochs = trial.suggest_int(\"num_train_epochs\", 2, 4)\n",
    "    lora_r = trial.suggest_categorical(\"lora_r\", [4, 8, 16])\n",
    "    lora_alpha = trial.suggest_categorical(\"lora_alpha\", [16, 32, 64])\n",
    "\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=lora_r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(base_model, lora_config)\n",
    "    output_dir = os.path.join(Config.JSON_OUTPUT_FINE_TUNE_OUTPUT, f\"optuna_trial_{trial.number}\")\n",
    "    logging_dir = os.path.join(output_dir, \"logs\")\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir\n",
    "        per_device_train_batch_size=2,\n",
    "        per_device_eval_batch_size=2,\n",
    "        gradient_accumulation_steps=8,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=learning_rate,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        bf16=True,\n",
    "        load_best_model_at_end=True,\n",
    "        report_to=\"none\",\n",
    "        save_total_limit=1,\n",
    "        logging_dir=logging_dir,\n",
    "        logging_steps=10,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_data[\"train\"],\n",
    "        eval_dataset=tokenized_data[\"eval\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trial.set_user_attr(\"best_model_path\", trainer.state.best_model_checkpoint)\n",
    "    eval_metrics = trainer.evaluate()\n",
    "    return eval_metrics[\"eval_loss\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97fb436",
   "metadata": {},
   "source": [
    "### Launch Optuna Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc53aec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "print(\"âœ… Best hyperparameters:\")\n",
    "print(study.best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b1a394",
   "metadata": {},
   "source": [
    "###  Save Final Best Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c7c071",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial_number = study.best_trial.number\n",
    "#best_model_path = f\"optuna_output/{best_trial_number}\"\n",
    "best_model_path = study.best_trial.user_attrs.get(\"best_model_path\")\n",
    "print(\"âœ… Best model path:\", best_model_path)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(best_model_path)\n",
    "model.save_pretrained(Config.JSON_OUTPUT_FINE_TUNE_MODEL)\n",
    "tokenizer.save_pretrained(Config.JSON_OUTPUT_FINE_TUNE_MODEL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb93d0c",
   "metadata": {},
   "source": [
    "## Save to hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45f5474",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, HfFolder\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# âœ… Optional: Login (only needed once per environment)\n",
    "# from huggingface_hub import login\n",
    "# login(\"hf_your_access_token\")\n",
    "\n",
    "# Set model path and repo name\n",
    "model_path = Config.JSON_OUTPUT_FINE_TUNE_MODEL\n",
    "repo_name = \"rubsj/Qwen2-Resume-ATS\"  # customize this\n",
    "\n",
    "# Push model and tokenizer to HF hub\n",
    "model.push_to_hub(repo_name)\n",
    "tokenizer.push_to_hub(repo_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b961a3ae",
   "metadata": {},
   "source": [
    "## Create ZIP for Lambda Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba74aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "zip_path = f\"{Config.JSON_OUTPUT_FINE_TUNE_MODEL}.zip\"\n",
    "shutil.make_archive(base_name=Config.JSON_OUTPUT_FINE_TUNE_MODEL, format='zip', root_dir=Config.JSON_OUTPUT_FINE_TUNE_MODEL)\n",
    "print(f\"âœ… Model zipped at: {zip_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
