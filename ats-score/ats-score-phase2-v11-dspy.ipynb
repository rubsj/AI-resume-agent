{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e558f9c",
   "metadata": {},
   "source": [
    "# Global setup and package installation used in most phases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4582433",
   "metadata": {},
   "source": [
    "## Colab + GPU Detection Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "909ac010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def is_running_in_colab():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "def get_available_gpu_memory_gb():\n",
    "    try:\n",
    "        output = subprocess.check_output(\n",
    "            [\"nvidia-smi\", \"--query-gpu=memory.free\", \"--format=csv,nounits,noheader\"],\n",
    "            encoding=\"utf-8\"\n",
    "        )\n",
    "        free_mem_mb = int(output.strip().split(\"\\n\")[0])\n",
    "        return free_mem_mb / 1024\n",
    "    except Exception:\n",
    "        return 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746cd125",
   "metadata": {},
   "source": [
    "## install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989d573e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_running_in_colab():\n",
    "    # Install the required packages\n",
    "    !pip install kagglehub pandas\n",
    "    !pip install -q transformers accelerate bitsandbytes sentencepiece pydantic huggingface_hub xformers\n",
    "    !pip install regex json5\n",
    "    !pip install dspy  sentence-transformers transformers accelerate scikit-learn -q\n",
    "\n",
    "else:\n",
    "    %pip install kagglehub pandas\n",
    "    %pip install -q transformers accelerate sentencepiece pydantic huggingface_hub xformers\n",
    "    #%pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu121\n",
    "    #%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "    %pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128\n",
    "    %pip install -U bitsandbytes\n",
    "    %pip install regex json5\n",
    "    # üì¶ INSTALL REQUIREMENTS\n",
    "    %pip install dspy  sentence-transformers transformers accelerate scikit-learn -q\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d7833b",
   "metadata": {},
   "source": [
    "## Login to huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0faca7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# Set your token here securely or prompt for it in Colab\n",
    "# Recommended: store in Colab secrets or environment variable\n",
    "HF_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "\n",
    "if not HF_TOKEN:\n",
    "    if is_running_in_colab():\n",
    "        # If running in Colab, use the Colab secrets\n",
    "        try:\n",
    "            from google.colab import userdata\n",
    "            HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "            if not HF_TOKEN:\n",
    "                raise ValueError(\"‚ö†Ô∏è Hugging Face token not found in Colab secrets.\")\n",
    "            print(\"üîë Hugging Face token found in Colab secrets.\")\n",
    "        except ImportError:\n",
    "            print(\"‚ö†Ô∏è Unable to authenticate in Colab. Please set your Hugging Face token manually.\")\n",
    "    else:\n",
    "        # Prompt for token if not set in environment\n",
    "        print(\"üîë Please enter your Hugging Face token:\")\n",
    "        # For Colab or local prompt input\n",
    "        HF_TOKEN = input(\"üîë Enter your Hugging Face token: \").strip()\n",
    "\n",
    "login(token=HF_TOKEN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639e756d",
   "metadata": {},
   "source": [
    "## Setup Kaggle Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d427d355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Kaggle credentials already exist at C:\\Users\\rubyj/.kaggle/kaggle.json\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "def setup_kaggle_credentials():\n",
    "    kaggle_path = os.path.expanduser('~/.kaggle/kaggle.json')\n",
    "    if not os.path.exists(kaggle_path):\n",
    "        from google.colab import files\n",
    "        print(\"üìÇ Upload kaggle.json file...\")\n",
    "        uploaded = files.upload()\n",
    "        os.makedirs(os.path.dirname(kaggle_path), exist_ok=True)\n",
    "        for filename in uploaded.keys():\n",
    "            shutil.move(filename, kaggle_path)\n",
    "        os.chmod(kaggle_path, 0o600)\n",
    "        print(f\"‚úÖ Kaggle credentials setup at {kaggle_path}\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Kaggle credentials already exist at {kaggle_path}\")\n",
    "\n",
    "setup_kaggle_credentials()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f301c89",
   "metadata": {},
   "source": [
    "##  Load mistral-Instruct with Fallback to Quantized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8df421",
   "metadata": {},
   "source": [
    "### SimpleLM-Like Class for DSPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9c8166b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy import BaseLM\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "class SimpleLM(BaseLM):\n",
    "    def __init__(self, model_name, tokenizer, model, max_new_tokens=1024):\n",
    "        super().__init__(model=model_name)\n",
    "        self.tokenizer = tokenizer\n",
    "        self._model = model         # internal model\n",
    "        self.model = model_name     # string for DSPy to parse with .split()\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "        self.kwargs = {}\n",
    "\n",
    "    def __call__(self, messages, **kwargs):\n",
    "        # DSPy passes `messages=[{\"role\": \"user\", \"content\": \"...\"}]`\n",
    "        if isinstance(messages, list):\n",
    "            prompt = \"\\n\".join([m[\"content\"] for m in messages])\n",
    "        else:\n",
    "            raise ValueError(\"Expected messages as a list of dicts with 'content'.\")\n",
    "\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self._model.device)\n",
    "        outputs = self._model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=self.max_new_tokens,\n",
    "            do_sample=False,\n",
    "            pad_token_id=self.tokenizer.pad_token_id,\n",
    "            eos_token_id=self.tokenizer.eos_token_id\n",
    "        )\n",
    "        output_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "        return output_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc01fa07",
   "metadata": {},
   "source": [
    "### Use It in Your DSPy Project (with Quantized Mistral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4fc325a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíª CUDA: True | GPU Memory: 15.92 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed553005a8274b3ab6560ffab5cb2f1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from dspy import configure\n",
    "\n",
    "# üîß Load quantized or full model manually\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "hf_token = HF_TOKEN\n",
    "\n",
    "has_cuda = torch.cuda.is_available()\n",
    "free_mem = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3) if has_cuda else 0\n",
    "print(f\"üíª CUDA: {has_cuda} | GPU Memory: {free_mem:.2f} GB\")\n",
    "\n",
    "device_map = {\"\": 0} if has_cuda else \"cpu\"\n",
    "use_4bit = has_cuda and free_mem < 24\n",
    "\n",
    "# Set quantization config\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True if use_4bit else False,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ") if use_4bit else None\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=device_map,\n",
    "    quantization_config=quant_config,\n",
    "    torch_dtype=torch.float16 if not quant_config else None,\n",
    "    token=hf_token,\n",
    "    trust_remote_code=True\n",
    ").eval()\n",
    "\n",
    "# ‚úÖ Plug into DSPy\n",
    "lm = SimpleLM(model_name=model_name, tokenizer=tokenizer, model=model, max_new_tokens=1024)\n",
    "configure(lm=lm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1bb74a",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41aa3b5f",
   "metadata": {},
   "source": [
    "### Configurations  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "faa14e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# üõ† CONFIGURATION\n",
    "# ==============================\n",
    "\n",
    "class Config:\n",
    "    DATASET_DOWNLOAD_DIR = \"datasets\"\n",
    "    JSON_OUTPUT_DIR = \"json_outputs_run3\"\n",
    "    JSON_OUTPUT_NORMALIZED_DIR = \"json_outputs_run3/normalized\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1248c8e3",
   "metadata": {},
   "source": [
    "## Utility to save json to a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "808269cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "# üì¶ Save JSON Output with Safety\n",
    "def save_json_output(data, output_path: str, indent: int = 4, overwrite: bool = True):\n",
    "    output_dir = os.path.dirname(output_path)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    if os.path.exists(output_path):\n",
    "        if overwrite:\n",
    "            os.remove(output_path)\n",
    "        else:\n",
    "            raise FileExistsError(f\"File {output_path} already exists and overwrite=False.\")\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=indent, ensure_ascii=False)\n",
    "\n",
    "    print(f\"‚úÖ Saved output to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b305bf",
   "metadata": {},
   "source": [
    "## Utility to load file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cf51ab",
   "metadata": {},
   "source": [
    "### load_ndjson_file() (for resume/jd input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "82200a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def load_ndjson_file(file_path: Path) -> List[dict]:\n",
    "    if not os.path.isfile(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return [json.loads(line) for line in file if line.strip()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0034749",
   "metadata": {},
   "source": [
    "### load_json_file() (for checkpoint & metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "72b1aa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def load_json_file(file_path: Path) -> dict:\n",
    "    if not os.path.isfile(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715c141c",
   "metadata": {},
   "source": [
    "# Phase 2 -\tParse resume/JD into JSON structured scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e91d41f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì• IMPORTS\n",
    "import re\n",
    "import json\n",
    "from typing import List, Dict\n",
    "from pathlib import Path\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from dspy import Signature, InputField, OutputField, Predict\n",
    "from dspy.teleprompt import Teleprompter\n",
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74cd976",
   "metadata": {},
   "source": [
    "## DSPy Signature and Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "25dabff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResumeChunkSig(Signature):\n",
    "    resume_chunk = InputField()\n",
    "    section_type = InputField()\n",
    "    extracted_json = OutputField(desc=\"Return structured resume fields as JSON inside 'extracted_json'.\")\n",
    "\n",
    "class ExtractResumeChunk(Predict):\n",
    "    signature = ResumeChunkSig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc54568",
   "metadata": {},
   "source": [
    "## NLP-Based Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6bc2660a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "SECTION_LABELS = {\n",
    "    \"summary\": \"Candidate summary or profile\",\n",
    "    \"work_history\": \"Jobs, roles, experience\",\n",
    "    \"education\": \"Degrees and academic background\",\n",
    "    \"skills\": \"Technical and soft skills\",\n",
    "    \"certifications\": \"Certifications and licenses\",\n",
    "    \"projects\": \"Project experience\",\n",
    "    \"others\": \"Languages, awards, publications\"\n",
    "}\n",
    "label_embeddings = {label: embedder.encode(desc) for label, desc in SECTION_LABELS.items()}\n",
    "\n",
    "def clean_resume_text(text: str) -> str:\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)\n",
    "    text = re.sub(r'\\n{2,}', '\\n\\n', text)\n",
    "    return text.strip()\n",
    "\n",
    "def classify_paragraph(text: str, threshold=0.6) -> str:\n",
    "    para_embedding = embedder.encode(text)\n",
    "    sims = {\n",
    "        label: cosine_similarity([para_embedding], [label_embeddings[label]])[0][0]\n",
    "        for label in SECTION_LABELS\n",
    "    }\n",
    "    best_label, score = max(sims.items(), key=lambda x: x[1])\n",
    "    return best_label if score >= threshold else \"unknown\"\n",
    "\n",
    "def chunk_resume_with_nlp(text: str, min_len=100) -> List[Dict[str, str]]:\n",
    "    text = clean_resume_text(text)\n",
    "    paragraphs = [p.strip() for p in re.split(r\"\\n\\s*\\n\", text) if len(p.strip()) >= 40]\n",
    "    chunks = []\n",
    "    for para in paragraphs:\n",
    "        label = classify_paragraph(para)\n",
    "        chunks.append({\"chunk\": para, \"section_type\": label})\n",
    "    # Fallback merge\n",
    "    merged = []\n",
    "    for chunk in chunks:\n",
    "        if len(chunk[\"chunk\"]) < min_len or chunk[\"section_type\"] == \"unknown\":\n",
    "            if merged:\n",
    "                merged[-1][\"chunk\"] += \"\\n\\n\" + chunk[\"chunk\"]\n",
    "            else:\n",
    "                merged.append(chunk)\n",
    "        else:\n",
    "            merged.append(chunk)\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8946e531",
   "metadata": {},
   "source": [
    "## Chunk ‚Üí Extract ‚Üí Merge Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "480a8ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "INLINE_DEMO_JSON = '''Example:\n",
    "{\n",
    "  \"extracted_json\": {\n",
    "    \"summary\": \"Experienced backend engineer with 10+ years...\",\n",
    "    \"education\": [\n",
    "      {\"degree\": \"MS\", \"field\": \"Computer Science\", \"institution\": \"XYZ University\"}\n",
    "    ],\n",
    "    \"skills\": [\"Java\", \"React\", \"Spring Boot\", \"SQL\", \"Docker\"]\n",
    "  }\n",
    "}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8a7fcfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_resume_chunks(resume_text: str) -> List[Dict]:\n",
    "    raw_chunks = chunk_resume_with_nlp(resume_text)\n",
    "    predictor = ExtractResumeChunk(signature=ResumeChunkSig)\n",
    "\n",
    "    parsed_chunks = []\n",
    "    for chunk in raw_chunks:\n",
    "        full_prompt = f\"\"\"\n",
    "You are a resume parser that returns structured JSON in this format:\n",
    "\n",
    "{INLINE_DEMO_JSON}\n",
    "\n",
    "Do not add any explanation. Return only the JSON object.\n",
    "\n",
    "Section type: {chunk[\"section_type\"]}\n",
    "Resume content:\n",
    "{chunk[\"chunk\"]}\n",
    "\"\"\"\n",
    "        try:\n",
    "            print(\"üì§ Prompt:\\n\", full_prompt)\n",
    "            result = predictor(\n",
    "                resume_chunk=full_prompt,\n",
    "                section_type=chunk[\"section_type\"]\n",
    "            )\n",
    "            print(\"üì• LLM Response:\\n\", result)\n",
    "            parsed_chunks.append(result.extracted_json)\n",
    "        except Exception as e:\n",
    "            print(\"‚ö†Ô∏è Parse failed, using fallback:\", e)\n",
    "            print(\"Raw chunk:\", chunk[\"chunk\"])\n",
    "            parsed_chunks.append({\"fallback\": chunk[\"chunk\"]})\n",
    "\n",
    "    return parsed_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "139d3b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def merge_chunks(parsed_chunks: List[Dict]) -> Dict:\n",
    "    merged = {\n",
    "        \"summary\": \"\", \"work_history\": [], \"education\": [],\n",
    "        \"skills\": [], \"certifications\": [], \"projects\": [], \"others\": []\n",
    "    }\n",
    "    for chunk in parsed_chunks:\n",
    "        for key in merged:\n",
    "            if isinstance(merged[key], list):\n",
    "                merged[key].extend(chunk.get(key, []))\n",
    "            elif isinstance(merged[key], str) and chunk.get(key):\n",
    "                merged[key] += \"\\n\" + chunk.get(key)\n",
    "    return merged\n",
    "\n",
    "def parse_resume_to_json(resume_text: str, output_path: Path = None):\n",
    "    parsed_chunks = extract_resume_chunks(resume_text)\n",
    "    final_json = merge_chunks(parsed_chunks)\n",
    "    if output_path:\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(output_path, \"w\") as f:\n",
    "            json.dump(final_json, f, indent=2)\n",
    "    return final_json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5726ffea",
   "metadata": {},
   "source": [
    "## Test with Sample Resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a42a2874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì§ Prompt:\n",
      " \n",
      "You are a resume parser that returns structured JSON in this format:\n",
      "\n",
      "Example:\n",
      "{\n",
      "  \"extracted_json\": {\n",
      "    \"summary\": \"Experienced backend engineer with 10+ years...\",\n",
      "    \"education\": [\n",
      "      {\"degree\": \"MS\", \"field\": \"Computer Science\", \"institution\": \"XYZ University\"}\n",
      "    ],\n",
      "    \"skills\": [\"Java\", \"React\", \"Spring Boot\", \"SQL\", \"Docker\"]\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "Do not add any explanation. Return only the JSON object.\n",
      "\n",
      "Section type: unknown\n",
      "Resume content:\n",
      "SUMMARY\n",
      "Experienced software engineer with 10+ years...\n",
      "\n",
      "PROFESSIONAL EXPERIENCE\n",
      "Senior Software Engineer, ABC Corp\n",
      "Built microservices in Java and deployed on Kubernetes.\n",
      "\n",
      "EDUCATION\n",
      "MS in Computer Science, XYZ University\n",
      "\n",
      "SKILLS\n",
      "Java, React, Spring Boot, SQL, Docker\n",
      "\n",
      "‚ö†Ô∏è Parse failed, using fallback: LM response cannot be serialized to a JSON object.\n",
      "\n",
      "Adapter JSONAdapter failed to parse the LM response. \n",
      "\n",
      "LM Response: Y \n",
      "\n",
      "Expected to find output fields in the LM response: [extracted_json] \n",
      "\n",
      "\n",
      "Raw chunk: SUMMARY\n",
      "Experienced software engineer with 10+ years...\n",
      "\n",
      "PROFESSIONAL EXPERIENCE\n",
      "Senior Software Engineer, ABC Corp\n",
      "Built microservices in Java and deployed on Kubernetes.\n",
      "\n",
      "EDUCATION\n",
      "MS in Computer Science, XYZ University\n",
      "\n",
      "SKILLS\n",
      "Java, React, Spring Boot, SQL, Docker\n",
      "{\n",
      "  \"summary\": \"\",\n",
      "  \"work_history\": [],\n",
      "  \"education\": [],\n",
      "  \"skills\": [],\n",
      "  \"certifications\": [],\n",
      "  \"projects\": [],\n",
      "  \"others\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "sample_resume = \"\"\"\n",
    "John Doe\n",
    "\n",
    "SUMMARY\n",
    "Experienced software engineer with 10+ years...\n",
    "\n",
    "PROFESSIONAL EXPERIENCE\n",
    "Senior Software Engineer, ABC Corp\n",
    "Built microservices in Java and deployed on Kubernetes.\n",
    "\n",
    "EDUCATION\n",
    "MS in Computer Science, XYZ University\n",
    "\n",
    "SKILLS\n",
    "Java, React, Spring Boot, SQL, Docker\n",
    "\"\"\"\n",
    "\n",
    "result = parse_resume_to_json(sample_resume)\n",
    "print(json.dumps(result, indent=2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
