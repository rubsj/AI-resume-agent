{
  "best_global_step": 4500,
  "best_metric": 1.6764800548553467,
  "best_model_checkpoint": "json_outputs_all_data/fine-tune/optuna_output/optuna_trial_7/checkpoint-4500",
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 4500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.006667777962993832,
      "grad_norm": 0.14890141785144806,
      "learning_rate": 0.00013731772211993004,
      "loss": 2.6809,
      "step": 10
    },
    {
      "epoch": 0.013335555925987664,
      "grad_norm": 0.1682848185300827,
      "learning_rate": 0.00013701196010229493,
      "loss": 2.5418,
      "step": 20
    },
    {
      "epoch": 0.020003333888981498,
      "grad_norm": 0.16939596831798553,
      "learning_rate": 0.00013670619808465982,
      "loss": 2.4749,
      "step": 30
    },
    {
      "epoch": 0.026671111851975328,
      "grad_norm": 0.1541059911251068,
      "learning_rate": 0.0001364004360670247,
      "loss": 2.4801,
      "step": 40
    },
    {
      "epoch": 0.03333888981496916,
      "grad_norm": 0.1684541255235672,
      "learning_rate": 0.0001360946740493896,
      "loss": 2.4199,
      "step": 50
    },
    {
      "epoch": 0.040006667777962995,
      "grad_norm": 0.19293814897537231,
      "learning_rate": 0.00013578891203175448,
      "loss": 2.4041,
      "step": 60
    },
    {
      "epoch": 0.046674445740956826,
      "grad_norm": 0.1754937469959259,
      "learning_rate": 0.00013548315001411936,
      "loss": 2.3778,
      "step": 70
    },
    {
      "epoch": 0.053342223703950656,
      "grad_norm": 0.17932069301605225,
      "learning_rate": 0.00013517738799648425,
      "loss": 2.4296,
      "step": 80
    },
    {
      "epoch": 0.06001000166694449,
      "grad_norm": 0.19488225877285004,
      "learning_rate": 0.00013487162597884914,
      "loss": 2.4018,
      "step": 90
    },
    {
      "epoch": 0.06667777962993832,
      "grad_norm": 0.2119607776403427,
      "learning_rate": 0.00013456586396121402,
      "loss": 2.3863,
      "step": 100
    },
    {
      "epoch": 0.07334555759293215,
      "grad_norm": 0.21489539742469788,
      "learning_rate": 0.00013426010194357888,
      "loss": 2.3917,
      "step": 110
    },
    {
      "epoch": 0.08001333555592599,
      "grad_norm": 0.21849481761455536,
      "learning_rate": 0.00013395433992594377,
      "loss": 2.4057,
      "step": 120
    },
    {
      "epoch": 0.08668111351891981,
      "grad_norm": 0.20718789100646973,
      "learning_rate": 0.00013364857790830868,
      "loss": 2.349,
      "step": 130
    },
    {
      "epoch": 0.09334889148191365,
      "grad_norm": 0.20428325235843658,
      "learning_rate": 0.00013334281589067357,
      "loss": 2.38,
      "step": 140
    },
    {
      "epoch": 0.10001666944490749,
      "grad_norm": 0.2313920110464096,
      "learning_rate": 0.00013303705387303843,
      "loss": 2.3665,
      "step": 150
    },
    {
      "epoch": 0.10668444740790131,
      "grad_norm": 0.24323666095733643,
      "learning_rate": 0.00013273129185540332,
      "loss": 2.3435,
      "step": 160
    },
    {
      "epoch": 0.11335222537089515,
      "grad_norm": 0.2264997661113739,
      "learning_rate": 0.0001324255298377682,
      "loss": 2.3083,
      "step": 170
    },
    {
      "epoch": 0.12002000333388899,
      "grad_norm": 0.2418995201587677,
      "learning_rate": 0.0001321197678201331,
      "loss": 2.3387,
      "step": 180
    },
    {
      "epoch": 0.12668778129688282,
      "grad_norm": 0.24367974698543549,
      "learning_rate": 0.00013181400580249798,
      "loss": 2.3327,
      "step": 190
    },
    {
      "epoch": 0.13335555925987663,
      "grad_norm": 0.2369595766067505,
      "learning_rate": 0.00013150824378486286,
      "loss": 2.3067,
      "step": 200
    },
    {
      "epoch": 0.14002333722287047,
      "grad_norm": 0.23197944462299347,
      "learning_rate": 0.00013120248176722775,
      "loss": 2.3426,
      "step": 210
    },
    {
      "epoch": 0.1466911151858643,
      "grad_norm": 0.2597339451313019,
      "learning_rate": 0.00013089671974959264,
      "loss": 2.3652,
      "step": 220
    },
    {
      "epoch": 0.15335889314885814,
      "grad_norm": 0.26860013604164124,
      "learning_rate": 0.00013059095773195752,
      "loss": 2.2972,
      "step": 230
    },
    {
      "epoch": 0.16002667111185198,
      "grad_norm": 0.2573777735233307,
      "learning_rate": 0.0001302851957143224,
      "loss": 2.2998,
      "step": 240
    },
    {
      "epoch": 0.16669444907484582,
      "grad_norm": 0.26193803548812866,
      "learning_rate": 0.0001299794336966873,
      "loss": 2.3067,
      "step": 250
    },
    {
      "epoch": 0.17336222703783963,
      "grad_norm": 0.3026279807090759,
      "learning_rate": 0.00012967367167905218,
      "loss": 2.3027,
      "step": 260
    },
    {
      "epoch": 0.18003000500083347,
      "grad_norm": 0.2654014229774475,
      "learning_rate": 0.00012936790966141704,
      "loss": 2.3707,
      "step": 270
    },
    {
      "epoch": 0.1866977829638273,
      "grad_norm": 0.2932984530925751,
      "learning_rate": 0.00012906214764378193,
      "loss": 2.4113,
      "step": 280
    },
    {
      "epoch": 0.19336556092682114,
      "grad_norm": 0.2716967761516571,
      "learning_rate": 0.00012875638562614684,
      "loss": 2.3181,
      "step": 290
    },
    {
      "epoch": 0.20003333888981498,
      "grad_norm": 0.26371678709983826,
      "learning_rate": 0.00012845062360851173,
      "loss": 2.3257,
      "step": 300
    },
    {
      "epoch": 0.2067011168528088,
      "grad_norm": 0.2836880385875702,
      "learning_rate": 0.0001281448615908766,
      "loss": 2.2943,
      "step": 310
    },
    {
      "epoch": 0.21336889481580262,
      "grad_norm": 0.27266925573349,
      "learning_rate": 0.00012783909957324148,
      "loss": 2.3254,
      "step": 320
    },
    {
      "epoch": 0.22003667277879646,
      "grad_norm": 0.3016430139541626,
      "learning_rate": 0.00012753333755560636,
      "loss": 2.2774,
      "step": 330
    },
    {
      "epoch": 0.2267044507417903,
      "grad_norm": 0.28626251220703125,
      "learning_rate": 0.00012722757553797125,
      "loss": 2.2805,
      "step": 340
    },
    {
      "epoch": 0.23337222870478413,
      "grad_norm": 0.3232156038284302,
      "learning_rate": 0.00012692181352033614,
      "loss": 2.2924,
      "step": 350
    },
    {
      "epoch": 0.24004000666777797,
      "grad_norm": 0.2840157449245453,
      "learning_rate": 0.00012661605150270102,
      "loss": 2.3281,
      "step": 360
    },
    {
      "epoch": 0.24670778463077178,
      "grad_norm": 0.30075007677078247,
      "learning_rate": 0.0001263102894850659,
      "loss": 2.3488,
      "step": 370
    },
    {
      "epoch": 0.25337556259376565,
      "grad_norm": 0.3461194634437561,
      "learning_rate": 0.0001260045274674308,
      "loss": 2.3115,
      "step": 380
    },
    {
      "epoch": 0.2600433405567595,
      "grad_norm": 0.3324766159057617,
      "learning_rate": 0.00012569876544979569,
      "loss": 2.2839,
      "step": 390
    },
    {
      "epoch": 0.26671111851975327,
      "grad_norm": 0.30844178795814514,
      "learning_rate": 0.00012539300343216057,
      "loss": 2.2978,
      "step": 400
    },
    {
      "epoch": 0.2733788964827471,
      "grad_norm": 0.36373192071914673,
      "learning_rate": 0.00012508724141452546,
      "loss": 2.261,
      "step": 410
    },
    {
      "epoch": 0.28004667444574094,
      "grad_norm": 0.32322004437446594,
      "learning_rate": 0.00012478147939689035,
      "loss": 2.2459,
      "step": 420
    },
    {
      "epoch": 0.2867144524087348,
      "grad_norm": 0.31445181369781494,
      "learning_rate": 0.0001244757173792552,
      "loss": 2.2529,
      "step": 430
    },
    {
      "epoch": 0.2933822303717286,
      "grad_norm": 0.34348493814468384,
      "learning_rate": 0.00012416995536162012,
      "loss": 2.2825,
      "step": 440
    },
    {
      "epoch": 0.30005000833472245,
      "grad_norm": 0.28809767961502075,
      "learning_rate": 0.000123864193343985,
      "loss": 2.3044,
      "step": 450
    },
    {
      "epoch": 0.3067177862977163,
      "grad_norm": 0.30844709277153015,
      "learning_rate": 0.0001235584313263499,
      "loss": 2.3095,
      "step": 460
    },
    {
      "epoch": 0.3133855642607101,
      "grad_norm": 0.3224761486053467,
      "learning_rate": 0.00012325266930871478,
      "loss": 2.3189,
      "step": 470
    },
    {
      "epoch": 0.32005334222370396,
      "grad_norm": 0.31005606055259705,
      "learning_rate": 0.00012294690729107964,
      "loss": 2.2558,
      "step": 480
    },
    {
      "epoch": 0.3267211201866978,
      "grad_norm": 0.35779422521591187,
      "learning_rate": 0.00012264114527344453,
      "loss": 2.2749,
      "step": 490
    },
    {
      "epoch": 0.33338889814969164,
      "grad_norm": 0.371035099029541,
      "learning_rate": 0.0001223353832558094,
      "loss": 2.3017,
      "step": 500
    },
    {
      "epoch": 0.3400566761126855,
      "grad_norm": 0.4202171266078949,
      "learning_rate": 0.0001220296212381743,
      "loss": 2.2799,
      "step": 510
    },
    {
      "epoch": 0.34672445407567926,
      "grad_norm": 0.3745732307434082,
      "learning_rate": 0.0001217238592205392,
      "loss": 2.2403,
      "step": 520
    },
    {
      "epoch": 0.3533922320386731,
      "grad_norm": 0.3609609305858612,
      "learning_rate": 0.00012141809720290409,
      "loss": 2.2707,
      "step": 530
    },
    {
      "epoch": 0.36006001000166693,
      "grad_norm": 0.3822038769721985,
      "learning_rate": 0.00012111233518526896,
      "loss": 2.21,
      "step": 540
    },
    {
      "epoch": 0.36672778796466077,
      "grad_norm": 0.4059678018093109,
      "learning_rate": 0.00012080657316763385,
      "loss": 2.2245,
      "step": 550
    },
    {
      "epoch": 0.3733955659276546,
      "grad_norm": 0.34499308466911316,
      "learning_rate": 0.00012050081114999873,
      "loss": 2.1435,
      "step": 560
    },
    {
      "epoch": 0.38006334389064844,
      "grad_norm": 0.40822046995162964,
      "learning_rate": 0.00012019504913236362,
      "loss": 2.2425,
      "step": 570
    },
    {
      "epoch": 0.3867311218536423,
      "grad_norm": 0.3543068766593933,
      "learning_rate": 0.0001198892871147285,
      "loss": 2.2569,
      "step": 580
    },
    {
      "epoch": 0.3933988998166361,
      "grad_norm": 0.3722819685935974,
      "learning_rate": 0.00011958352509709338,
      "loss": 2.2489,
      "step": 590
    },
    {
      "epoch": 0.40006667777962995,
      "grad_norm": 0.3767715394496918,
      "learning_rate": 0.00011927776307945828,
      "loss": 2.2137,
      "step": 600
    },
    {
      "epoch": 0.4067344557426238,
      "grad_norm": 0.38535407185554504,
      "learning_rate": 0.00011897200106182317,
      "loss": 2.2249,
      "step": 610
    },
    {
      "epoch": 0.4134022337056176,
      "grad_norm": 0.4167516529560089,
      "learning_rate": 0.00011866623904418804,
      "loss": 2.2062,
      "step": 620
    },
    {
      "epoch": 0.4200700116686114,
      "grad_norm": 0.4402514100074768,
      "learning_rate": 0.00011836047702655293,
      "loss": 2.2264,
      "step": 630
    },
    {
      "epoch": 0.42673778963160525,
      "grad_norm": 0.42795661091804504,
      "learning_rate": 0.00011805471500891781,
      "loss": 2.2236,
      "step": 640
    },
    {
      "epoch": 0.4334055675945991,
      "grad_norm": 0.39900052547454834,
      "learning_rate": 0.0001177489529912827,
      "loss": 2.2497,
      "step": 650
    },
    {
      "epoch": 0.4400733455575929,
      "grad_norm": 0.3710564076900482,
      "learning_rate": 0.00011744319097364757,
      "loss": 2.2297,
      "step": 660
    },
    {
      "epoch": 0.44674112352058676,
      "grad_norm": 0.5186956524848938,
      "learning_rate": 0.00011713742895601247,
      "loss": 2.16,
      "step": 670
    },
    {
      "epoch": 0.4534089014835806,
      "grad_norm": 0.40200281143188477,
      "learning_rate": 0.00011683166693837736,
      "loss": 2.2226,
      "step": 680
    },
    {
      "epoch": 0.46007667944657443,
      "grad_norm": 0.5256277322769165,
      "learning_rate": 0.00011652590492074225,
      "loss": 2.1734,
      "step": 690
    },
    {
      "epoch": 0.46674445740956827,
      "grad_norm": 0.4720330834388733,
      "learning_rate": 0.00011622014290310712,
      "loss": 2.1783,
      "step": 700
    },
    {
      "epoch": 0.4734122353725621,
      "grad_norm": 0.47253087162971497,
      "learning_rate": 0.00011591438088547201,
      "loss": 2.2309,
      "step": 710
    },
    {
      "epoch": 0.48008001333555594,
      "grad_norm": 0.45919060707092285,
      "learning_rate": 0.0001156086188678369,
      "loss": 2.1402,
      "step": 720
    },
    {
      "epoch": 0.4867477912985498,
      "grad_norm": 0.4388178586959839,
      "learning_rate": 0.00011530285685020178,
      "loss": 2.2141,
      "step": 730
    },
    {
      "epoch": 0.49341556926154356,
      "grad_norm": 0.491678923368454,
      "learning_rate": 0.00011499709483256666,
      "loss": 2.1987,
      "step": 740
    },
    {
      "epoch": 0.5000833472245374,
      "grad_norm": 0.4554983377456665,
      "learning_rate": 0.00011469133281493156,
      "loss": 2.2011,
      "step": 750
    },
    {
      "epoch": 0.5067511251875313,
      "grad_norm": 0.45336228609085083,
      "learning_rate": 0.00011438557079729644,
      "loss": 2.2019,
      "step": 760
    },
    {
      "epoch": 0.5134189031505251,
      "grad_norm": 0.7578649520874023,
      "learning_rate": 0.00011407980877966133,
      "loss": 2.1195,
      "step": 770
    },
    {
      "epoch": 0.520086681113519,
      "grad_norm": 0.45343947410583496,
      "learning_rate": 0.0001137740467620262,
      "loss": 2.1697,
      "step": 780
    },
    {
      "epoch": 0.5267544590765127,
      "grad_norm": 0.5478682518005371,
      "learning_rate": 0.00011346828474439109,
      "loss": 2.1892,
      "step": 790
    },
    {
      "epoch": 0.5334222370395065,
      "grad_norm": 0.5055353045463562,
      "learning_rate": 0.00011316252272675598,
      "loss": 2.1771,
      "step": 800
    },
    {
      "epoch": 0.5400900150025004,
      "grad_norm": 0.4844587743282318,
      "learning_rate": 0.00011285676070912086,
      "loss": 2.2444,
      "step": 810
    },
    {
      "epoch": 0.5467577929654942,
      "grad_norm": 0.4401673972606659,
      "learning_rate": 0.00011255099869148574,
      "loss": 2.1417,
      "step": 820
    },
    {
      "epoch": 0.5534255709284881,
      "grad_norm": 0.5362762212753296,
      "learning_rate": 0.00011224523667385064,
      "loss": 2.1615,
      "step": 830
    },
    {
      "epoch": 0.5600933488914819,
      "grad_norm": 0.4467063248157501,
      "learning_rate": 0.00011193947465621552,
      "loss": 2.1818,
      "step": 840
    },
    {
      "epoch": 0.5667611268544758,
      "grad_norm": 0.5883049964904785,
      "learning_rate": 0.00011163371263858041,
      "loss": 2.1407,
      "step": 850
    },
    {
      "epoch": 0.5734289048174696,
      "grad_norm": 0.535122275352478,
      "learning_rate": 0.0001113279506209453,
      "loss": 2.1666,
      "step": 860
    },
    {
      "epoch": 0.5800966827804634,
      "grad_norm": 0.4811178743839264,
      "learning_rate": 0.00011102218860331017,
      "loss": 2.1894,
      "step": 870
    },
    {
      "epoch": 0.5867644607434572,
      "grad_norm": 0.6371428966522217,
      "learning_rate": 0.00011071642658567506,
      "loss": 2.0874,
      "step": 880
    },
    {
      "epoch": 0.5934322387064511,
      "grad_norm": 0.5003959536552429,
      "learning_rate": 0.00011041066456803994,
      "loss": 2.1554,
      "step": 890
    },
    {
      "epoch": 0.6001000166694449,
      "grad_norm": 0.5021778345108032,
      "learning_rate": 0.00011010490255040482,
      "loss": 2.1367,
      "step": 900
    },
    {
      "epoch": 0.6067677946324388,
      "grad_norm": 0.5792333483695984,
      "learning_rate": 0.00010979914053276972,
      "loss": 2.0848,
      "step": 910
    },
    {
      "epoch": 0.6134355725954326,
      "grad_norm": 0.8482128977775574,
      "learning_rate": 0.0001094933785151346,
      "loss": 2.1678,
      "step": 920
    },
    {
      "epoch": 0.6201033505584264,
      "grad_norm": 0.5405132174491882,
      "learning_rate": 0.00010918761649749949,
      "loss": 2.1977,
      "step": 930
    },
    {
      "epoch": 0.6267711285214203,
      "grad_norm": 0.5384297966957092,
      "learning_rate": 0.00010888185447986438,
      "loss": 2.1966,
      "step": 940
    },
    {
      "epoch": 0.633438906484414,
      "grad_norm": 0.6831470727920532,
      "learning_rate": 0.00010857609246222925,
      "loss": 2.1184,
      "step": 950
    },
    {
      "epoch": 0.6401066844474079,
      "grad_norm": 0.6719112992286682,
      "learning_rate": 0.00010827033044459414,
      "loss": 2.1471,
      "step": 960
    },
    {
      "epoch": 0.6467744624104017,
      "grad_norm": 0.7284618020057678,
      "learning_rate": 0.00010796456842695902,
      "loss": 2.1477,
      "step": 970
    },
    {
      "epoch": 0.6534422403733956,
      "grad_norm": 0.600933849811554,
      "learning_rate": 0.00010765880640932392,
      "loss": 2.103,
      "step": 980
    },
    {
      "epoch": 0.6601100183363894,
      "grad_norm": 0.6041619181632996,
      "learning_rate": 0.0001073530443916888,
      "loss": 2.0873,
      "step": 990
    },
    {
      "epoch": 0.6667777962993833,
      "grad_norm": 0.8040493726730347,
      "learning_rate": 0.00010704728237405368,
      "loss": 2.1862,
      "step": 1000
    },
    {
      "epoch": 0.6734455742623771,
      "grad_norm": 0.5739309787750244,
      "learning_rate": 0.00010674152035641857,
      "loss": 2.0635,
      "step": 1010
    },
    {
      "epoch": 0.680113352225371,
      "grad_norm": 0.7331593036651611,
      "learning_rate": 0.00010643575833878346,
      "loss": 2.0967,
      "step": 1020
    },
    {
      "epoch": 0.6867811301883647,
      "grad_norm": 0.6238492727279663,
      "learning_rate": 0.00010612999632114833,
      "loss": 2.1632,
      "step": 1030
    },
    {
      "epoch": 0.6934489081513585,
      "grad_norm": 0.5004650354385376,
      "learning_rate": 0.00010582423430351322,
      "loss": 2.0894,
      "step": 1040
    },
    {
      "epoch": 0.7001166861143524,
      "grad_norm": 0.7454332709312439,
      "learning_rate": 0.0001055184722858781,
      "loss": 2.0571,
      "step": 1050
    },
    {
      "epoch": 0.7067844640773462,
      "grad_norm": 0.8403486013412476,
      "learning_rate": 0.000105212710268243,
      "loss": 2.0418,
      "step": 1060
    },
    {
      "epoch": 0.7134522420403401,
      "grad_norm": 1.180720567703247,
      "learning_rate": 0.00010490694825060788,
      "loss": 2.1324,
      "step": 1070
    },
    {
      "epoch": 0.7201200200033339,
      "grad_norm": 0.7071352601051331,
      "learning_rate": 0.00010460118623297277,
      "loss": 2.0945,
      "step": 1080
    },
    {
      "epoch": 0.7267877979663278,
      "grad_norm": 0.7691003084182739,
      "learning_rate": 0.00010429542421533765,
      "loss": 2.1197,
      "step": 1090
    },
    {
      "epoch": 0.7334555759293215,
      "grad_norm": 0.783906102180481,
      "learning_rate": 0.00010398966219770254,
      "loss": 2.0841,
      "step": 1100
    },
    {
      "epoch": 0.7401233538923154,
      "grad_norm": 0.7236565351486206,
      "learning_rate": 0.00010368390018006741,
      "loss": 2.0537,
      "step": 1110
    },
    {
      "epoch": 0.7467911318553092,
      "grad_norm": 0.6321107745170593,
      "learning_rate": 0.0001033781381624323,
      "loss": 2.1623,
      "step": 1120
    },
    {
      "epoch": 0.7534589098183031,
      "grad_norm": 0.7345169186592102,
      "learning_rate": 0.00010307237614479719,
      "loss": 2.1168,
      "step": 1130
    },
    {
      "epoch": 0.7601266877812969,
      "grad_norm": 0.6817699074745178,
      "learning_rate": 0.00010276661412716209,
      "loss": 2.0534,
      "step": 1140
    },
    {
      "epoch": 0.7667944657442907,
      "grad_norm": 0.7981348633766174,
      "learning_rate": 0.00010246085210952696,
      "loss": 2.0427,
      "step": 1150
    },
    {
      "epoch": 0.7734622437072846,
      "grad_norm": 0.6184855699539185,
      "learning_rate": 0.00010215509009189185,
      "loss": 2.0663,
      "step": 1160
    },
    {
      "epoch": 0.7801300216702783,
      "grad_norm": 0.9511963725090027,
      "learning_rate": 0.00010184932807425673,
      "loss": 2.0424,
      "step": 1170
    },
    {
      "epoch": 0.7867977996332722,
      "grad_norm": 0.6203067302703857,
      "learning_rate": 0.00010154356605662162,
      "loss": 2.0645,
      "step": 1180
    },
    {
      "epoch": 0.793465577596266,
      "grad_norm": 0.856206476688385,
      "learning_rate": 0.00010123780403898649,
      "loss": 2.0949,
      "step": 1190
    },
    {
      "epoch": 0.8001333555592599,
      "grad_norm": 0.7839303612709045,
      "learning_rate": 0.00010093204202135138,
      "loss": 2.018,
      "step": 1200
    },
    {
      "epoch": 0.8068011335222537,
      "grad_norm": 0.7736765742301941,
      "learning_rate": 0.00010062628000371627,
      "loss": 2.0961,
      "step": 1210
    },
    {
      "epoch": 0.8134689114852476,
      "grad_norm": 0.9560636878013611,
      "learning_rate": 0.00010032051798608117,
      "loss": 1.9972,
      "step": 1220
    },
    {
      "epoch": 0.8201366894482414,
      "grad_norm": 0.7159772515296936,
      "learning_rate": 0.00010001475596844604,
      "loss": 2.0912,
      "step": 1230
    },
    {
      "epoch": 0.8268044674112353,
      "grad_norm": 0.7430920600891113,
      "learning_rate": 9.970899395081093e-05,
      "loss": 2.017,
      "step": 1240
    },
    {
      "epoch": 0.833472245374229,
      "grad_norm": 0.8431878685951233,
      "learning_rate": 9.940323193317581e-05,
      "loss": 2.0673,
      "step": 1250
    },
    {
      "epoch": 0.8401400233372228,
      "grad_norm": 1.414790153503418,
      "learning_rate": 9.90974699155407e-05,
      "loss": 2.066,
      "step": 1260
    },
    {
      "epoch": 0.8468078013002167,
      "grad_norm": 0.9411066174507141,
      "learning_rate": 9.879170789790557e-05,
      "loss": 2.0143,
      "step": 1270
    },
    {
      "epoch": 0.8534755792632105,
      "grad_norm": 0.7803811430931091,
      "learning_rate": 9.848594588027046e-05,
      "loss": 2.0723,
      "step": 1280
    },
    {
      "epoch": 0.8601433572262044,
      "grad_norm": 0.799075186252594,
      "learning_rate": 9.818018386263536e-05,
      "loss": 2.0744,
      "step": 1290
    },
    {
      "epoch": 0.8668111351891982,
      "grad_norm": 0.9601827263832092,
      "learning_rate": 9.787442184500025e-05,
      "loss": 2.0239,
      "step": 1300
    },
    {
      "epoch": 0.8734789131521921,
      "grad_norm": 0.6612789630889893,
      "learning_rate": 9.756865982736513e-05,
      "loss": 2.025,
      "step": 1310
    },
    {
      "epoch": 0.8801466911151858,
      "grad_norm": 1.719248652458191,
      "learning_rate": 9.726289780973001e-05,
      "loss": 1.9879,
      "step": 1320
    },
    {
      "epoch": 0.8868144690781797,
      "grad_norm": 1.5061562061309814,
      "learning_rate": 9.69571357920949e-05,
      "loss": 2.01,
      "step": 1330
    },
    {
      "epoch": 0.8934822470411735,
      "grad_norm": 0.6468364596366882,
      "learning_rate": 9.665137377445978e-05,
      "loss": 1.9966,
      "step": 1340
    },
    {
      "epoch": 0.9001500250041674,
      "grad_norm": 0.8309290409088135,
      "learning_rate": 9.634561175682465e-05,
      "loss": 1.9811,
      "step": 1350
    },
    {
      "epoch": 0.9068178029671612,
      "grad_norm": 0.7969707250595093,
      "learning_rate": 9.603984973918954e-05,
      "loss": 2.0032,
      "step": 1360
    },
    {
      "epoch": 0.913485580930155,
      "grad_norm": 0.8917474150657654,
      "learning_rate": 9.573408772155444e-05,
      "loss": 1.9188,
      "step": 1370
    },
    {
      "epoch": 0.9201533588931489,
      "grad_norm": 0.7396261692047119,
      "learning_rate": 9.542832570391933e-05,
      "loss": 2.0125,
      "step": 1380
    },
    {
      "epoch": 0.9268211368561426,
      "grad_norm": 1.2486835718154907,
      "learning_rate": 9.512256368628422e-05,
      "loss": 2.0971,
      "step": 1390
    },
    {
      "epoch": 0.9334889148191365,
      "grad_norm": 1.2751836776733398,
      "learning_rate": 9.481680166864909e-05,
      "loss": 2.0221,
      "step": 1400
    },
    {
      "epoch": 0.9401566927821303,
      "grad_norm": 0.7843127846717834,
      "learning_rate": 9.451103965101398e-05,
      "loss": 2.0449,
      "step": 1410
    },
    {
      "epoch": 0.9468244707451242,
      "grad_norm": 0.8091475367546082,
      "learning_rate": 9.420527763337886e-05,
      "loss": 2.0228,
      "step": 1420
    },
    {
      "epoch": 0.953492248708118,
      "grad_norm": 0.6422589421272278,
      "learning_rate": 9.389951561574375e-05,
      "loss": 2.0999,
      "step": 1430
    },
    {
      "epoch": 0.9601600266711119,
      "grad_norm": 1.4359222650527954,
      "learning_rate": 9.359375359810862e-05,
      "loss": 2.013,
      "step": 1440
    },
    {
      "epoch": 0.9668278046341057,
      "grad_norm": 1.390404462814331,
      "learning_rate": 9.328799158047352e-05,
      "loss": 1.9465,
      "step": 1450
    },
    {
      "epoch": 0.9734955825970996,
      "grad_norm": 1.209550380706787,
      "learning_rate": 9.298222956283841e-05,
      "loss": 1.934,
      "step": 1460
    },
    {
      "epoch": 0.9801633605600933,
      "grad_norm": 1.4740809202194214,
      "learning_rate": 9.26764675452033e-05,
      "loss": 1.9667,
      "step": 1470
    },
    {
      "epoch": 0.9868311385230871,
      "grad_norm": 1.4286482334136963,
      "learning_rate": 9.237070552756817e-05,
      "loss": 1.9975,
      "step": 1480
    },
    {
      "epoch": 0.993498916486081,
      "grad_norm": 0.853011965751648,
      "learning_rate": 9.206494350993306e-05,
      "loss": 1.916,
      "step": 1490
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.0717512369155884,
      "learning_rate": 9.175918149229794e-05,
      "loss": 2.0483,
      "step": 1500
    },
    {
      "epoch": 1.0,
      "eval_loss": 1.9856573343276978,
      "eval_runtime": 938.1715,
      "eval_samples_per_second": 6.394,
      "eval_steps_per_second": 3.198,
      "step": 1500
    },
    {
      "epoch": 1.0066677779629938,
      "grad_norm": 1.3470380306243896,
      "learning_rate": 9.145341947466283e-05,
      "loss": 1.9288,
      "step": 1510
    },
    {
      "epoch": 1.0133355559259876,
      "grad_norm": 0.9357823729515076,
      "learning_rate": 9.11476574570277e-05,
      "loss": 1.9394,
      "step": 1520
    },
    {
      "epoch": 1.0200033338889816,
      "grad_norm": 1.0276525020599365,
      "learning_rate": 9.08418954393926e-05,
      "loss": 1.9571,
      "step": 1530
    },
    {
      "epoch": 1.0266711118519753,
      "grad_norm": 0.8422480225563049,
      "learning_rate": 9.053613342175749e-05,
      "loss": 1.9926,
      "step": 1540
    },
    {
      "epoch": 1.0333388898149691,
      "grad_norm": 0.6464714407920837,
      "learning_rate": 9.023037140412238e-05,
      "loss": 2.0059,
      "step": 1550
    },
    {
      "epoch": 1.040006667777963,
      "grad_norm": 0.8247736096382141,
      "learning_rate": 8.992460938648725e-05,
      "loss": 1.9504,
      "step": 1560
    },
    {
      "epoch": 1.046674445740957,
      "grad_norm": 0.6925612688064575,
      "learning_rate": 8.961884736885214e-05,
      "loss": 1.9642,
      "step": 1570
    },
    {
      "epoch": 1.0533422237039507,
      "grad_norm": 1.0318986177444458,
      "learning_rate": 8.931308535121702e-05,
      "loss": 1.8785,
      "step": 1580
    },
    {
      "epoch": 1.0600100016669445,
      "grad_norm": 1.1904841661453247,
      "learning_rate": 8.900732333358191e-05,
      "loss": 1.8885,
      "step": 1590
    },
    {
      "epoch": 1.0666777796299383,
      "grad_norm": 1.079735517501831,
      "learning_rate": 8.87015613159468e-05,
      "loss": 1.9802,
      "step": 1600
    },
    {
      "epoch": 1.073345557592932,
      "grad_norm": 0.8895419836044312,
      "learning_rate": 8.839579929831168e-05,
      "loss": 1.9079,
      "step": 1610
    },
    {
      "epoch": 1.080013335555926,
      "grad_norm": 1.428966760635376,
      "learning_rate": 8.809003728067657e-05,
      "loss": 1.9875,
      "step": 1620
    },
    {
      "epoch": 1.0866811135189198,
      "grad_norm": 0.6613536477088928,
      "learning_rate": 8.778427526304146e-05,
      "loss": 1.9757,
      "step": 1630
    },
    {
      "epoch": 1.0933488914819136,
      "grad_norm": 0.7411365509033203,
      "learning_rate": 8.747851324540633e-05,
      "loss": 1.887,
      "step": 1640
    },
    {
      "epoch": 1.1000166694449074,
      "grad_norm": 1.9259026050567627,
      "learning_rate": 8.717275122777122e-05,
      "loss": 1.9458,
      "step": 1650
    },
    {
      "epoch": 1.1066844474079014,
      "grad_norm": 0.9527695775032043,
      "learning_rate": 8.68669892101361e-05,
      "loss": 1.8853,
      "step": 1660
    },
    {
      "epoch": 1.1133522253708952,
      "grad_norm": 0.9938172101974487,
      "learning_rate": 8.656122719250099e-05,
      "loss": 1.8592,
      "step": 1670
    },
    {
      "epoch": 1.120020003333889,
      "grad_norm": 1.2837448120117188,
      "learning_rate": 8.625546517486589e-05,
      "loss": 1.9078,
      "step": 1680
    },
    {
      "epoch": 1.1266877812968827,
      "grad_norm": 0.9004368185997009,
      "learning_rate": 8.594970315723076e-05,
      "loss": 1.9306,
      "step": 1690
    },
    {
      "epoch": 1.1333555592598765,
      "grad_norm": 1.2527960538864136,
      "learning_rate": 8.564394113959565e-05,
      "loss": 1.8245,
      "step": 1700
    },
    {
      "epoch": 1.1400233372228705,
      "grad_norm": 1.005386471748352,
      "learning_rate": 8.533817912196054e-05,
      "loss": 1.8896,
      "step": 1710
    },
    {
      "epoch": 1.1466911151858643,
      "grad_norm": 1.303781270980835,
      "learning_rate": 8.503241710432541e-05,
      "loss": 1.8837,
      "step": 1720
    },
    {
      "epoch": 1.153358893148858,
      "grad_norm": 0.8980891704559326,
      "learning_rate": 8.47266550866903e-05,
      "loss": 1.8273,
      "step": 1730
    },
    {
      "epoch": 1.160026671111852,
      "grad_norm": 1.431971788406372,
      "learning_rate": 8.442089306905519e-05,
      "loss": 1.9309,
      "step": 1740
    },
    {
      "epoch": 1.1666944490748459,
      "grad_norm": 0.6724970936775208,
      "learning_rate": 8.411513105142007e-05,
      "loss": 1.8305,
      "step": 1750
    },
    {
      "epoch": 1.1733622270378397,
      "grad_norm": 0.8838610649108887,
      "learning_rate": 8.380936903378497e-05,
      "loss": 1.8505,
      "step": 1760
    },
    {
      "epoch": 1.1800300050008334,
      "grad_norm": 1.0542221069335938,
      "learning_rate": 8.350360701614985e-05,
      "loss": 1.8982,
      "step": 1770
    },
    {
      "epoch": 1.1866977829638272,
      "grad_norm": 1.3874162435531616,
      "learning_rate": 8.319784499851473e-05,
      "loss": 1.8172,
      "step": 1780
    },
    {
      "epoch": 1.1933655609268212,
      "grad_norm": 0.9177318215370178,
      "learning_rate": 8.289208298087962e-05,
      "loss": 1.841,
      "step": 1790
    },
    {
      "epoch": 1.200033338889815,
      "grad_norm": 0.9117442965507507,
      "learning_rate": 8.25863209632445e-05,
      "loss": 1.8953,
      "step": 1800
    },
    {
      "epoch": 1.2067011168528088,
      "grad_norm": 0.9390110969543457,
      "learning_rate": 8.228055894560938e-05,
      "loss": 1.9101,
      "step": 1810
    },
    {
      "epoch": 1.2133688948158026,
      "grad_norm": 0.9252817630767822,
      "learning_rate": 8.197479692797427e-05,
      "loss": 1.8171,
      "step": 1820
    },
    {
      "epoch": 1.2200366727787966,
      "grad_norm": 1.036450743675232,
      "learning_rate": 8.166903491033915e-05,
      "loss": 1.8915,
      "step": 1830
    },
    {
      "epoch": 1.2267044507417904,
      "grad_norm": 1.0398904085159302,
      "learning_rate": 8.136327289270405e-05,
      "loss": 1.8869,
      "step": 1840
    },
    {
      "epoch": 1.2333722287047841,
      "grad_norm": 1.0857857465744019,
      "learning_rate": 8.105751087506893e-05,
      "loss": 1.8644,
      "step": 1850
    },
    {
      "epoch": 1.240040006667778,
      "grad_norm": 1.4571828842163086,
      "learning_rate": 8.075174885743381e-05,
      "loss": 1.8769,
      "step": 1860
    },
    {
      "epoch": 1.2467077846307717,
      "grad_norm": 1.0287326574325562,
      "learning_rate": 8.04459868397987e-05,
      "loss": 1.8249,
      "step": 1870
    },
    {
      "epoch": 1.2533755625937657,
      "grad_norm": 1.0021708011627197,
      "learning_rate": 8.014022482216359e-05,
      "loss": 1.9023,
      "step": 1880
    },
    {
      "epoch": 1.2600433405567595,
      "grad_norm": 1.1753360033035278,
      "learning_rate": 7.983446280452846e-05,
      "loss": 1.8689,
      "step": 1890
    },
    {
      "epoch": 1.2667111185197533,
      "grad_norm": 0.9797338247299194,
      "learning_rate": 7.952870078689335e-05,
      "loss": 1.9376,
      "step": 1900
    },
    {
      "epoch": 1.273378896482747,
      "grad_norm": 1.0041354894638062,
      "learning_rate": 7.922293876925825e-05,
      "loss": 1.8932,
      "step": 1910
    },
    {
      "epoch": 1.280046674445741,
      "grad_norm": 1.2076143026351929,
      "learning_rate": 7.891717675162313e-05,
      "loss": 1.9224,
      "step": 1920
    },
    {
      "epoch": 1.2867144524087348,
      "grad_norm": 1.699418544769287,
      "learning_rate": 7.861141473398801e-05,
      "loss": 1.8946,
      "step": 1930
    },
    {
      "epoch": 1.2933822303717286,
      "grad_norm": 1.2782257795333862,
      "learning_rate": 7.83056527163529e-05,
      "loss": 1.7733,
      "step": 1940
    },
    {
      "epoch": 1.3000500083347224,
      "grad_norm": 1.3319779634475708,
      "learning_rate": 7.799989069871778e-05,
      "loss": 1.796,
      "step": 1950
    },
    {
      "epoch": 1.3067177862977162,
      "grad_norm": 1.2624270915985107,
      "learning_rate": 7.769412868108267e-05,
      "loss": 1.847,
      "step": 1960
    },
    {
      "epoch": 1.3133855642607102,
      "grad_norm": 1.3568285703659058,
      "learning_rate": 7.738836666344754e-05,
      "loss": 1.8053,
      "step": 1970
    },
    {
      "epoch": 1.320053342223704,
      "grad_norm": 1.0245040655136108,
      "learning_rate": 7.708260464581243e-05,
      "loss": 1.89,
      "step": 1980
    },
    {
      "epoch": 1.3267211201866977,
      "grad_norm": 1.4254754781723022,
      "learning_rate": 7.677684262817733e-05,
      "loss": 1.7501,
      "step": 1990
    },
    {
      "epoch": 1.3333888981496917,
      "grad_norm": 0.9763301014900208,
      "learning_rate": 7.647108061054221e-05,
      "loss": 1.8882,
      "step": 2000
    },
    {
      "epoch": 1.3400566761126855,
      "grad_norm": 1.1155035495758057,
      "learning_rate": 7.616531859290709e-05,
      "loss": 1.8075,
      "step": 2010
    },
    {
      "epoch": 1.3467244540756793,
      "grad_norm": 1.1941840648651123,
      "learning_rate": 7.585955657527197e-05,
      "loss": 1.7788,
      "step": 2020
    },
    {
      "epoch": 1.353392232038673,
      "grad_norm": 0.8120251297950745,
      "learning_rate": 7.555379455763686e-05,
      "loss": 1.7726,
      "step": 2030
    },
    {
      "epoch": 1.3600600100016669,
      "grad_norm": 1.061883807182312,
      "learning_rate": 7.524803254000175e-05,
      "loss": 1.8406,
      "step": 2040
    },
    {
      "epoch": 1.3667277879646607,
      "grad_norm": 1.2014557123184204,
      "learning_rate": 7.494227052236662e-05,
      "loss": 1.6936,
      "step": 2050
    },
    {
      "epoch": 1.3733955659276547,
      "grad_norm": 1.1453673839569092,
      "learning_rate": 7.463650850473151e-05,
      "loss": 1.7931,
      "step": 2060
    },
    {
      "epoch": 1.3800633438906484,
      "grad_norm": 1.1899642944335938,
      "learning_rate": 7.433074648709641e-05,
      "loss": 1.8776,
      "step": 2070
    },
    {
      "epoch": 1.3867311218536422,
      "grad_norm": 1.2387830018997192,
      "learning_rate": 7.40249844694613e-05,
      "loss": 1.8565,
      "step": 2080
    },
    {
      "epoch": 1.3933988998166362,
      "grad_norm": 1.634663701057434,
      "learning_rate": 7.371922245182617e-05,
      "loss": 1.7265,
      "step": 2090
    },
    {
      "epoch": 1.40006667777963,
      "grad_norm": 0.9120047092437744,
      "learning_rate": 7.341346043419106e-05,
      "loss": 1.9159,
      "step": 2100
    },
    {
      "epoch": 1.4067344557426238,
      "grad_norm": 0.7453147172927856,
      "learning_rate": 7.310769841655594e-05,
      "loss": 1.8879,
      "step": 2110
    },
    {
      "epoch": 1.4134022337056176,
      "grad_norm": 1.4529064893722534,
      "learning_rate": 7.280193639892083e-05,
      "loss": 1.7616,
      "step": 2120
    },
    {
      "epoch": 1.4200700116686114,
      "grad_norm": 0.8200656175613403,
      "learning_rate": 7.24961743812857e-05,
      "loss": 1.9166,
      "step": 2130
    },
    {
      "epoch": 1.4267377896316051,
      "grad_norm": 0.9725787043571472,
      "learning_rate": 7.219041236365059e-05,
      "loss": 1.7695,
      "step": 2140
    },
    {
      "epoch": 1.4334055675945991,
      "grad_norm": 1.2408111095428467,
      "learning_rate": 7.188465034601549e-05,
      "loss": 1.8574,
      "step": 2150
    },
    {
      "epoch": 1.440073345557593,
      "grad_norm": 1.3138426542282104,
      "learning_rate": 7.157888832838038e-05,
      "loss": 1.7959,
      "step": 2160
    },
    {
      "epoch": 1.4467411235205867,
      "grad_norm": 1.1438580751419067,
      "learning_rate": 7.127312631074525e-05,
      "loss": 1.8879,
      "step": 2170
    },
    {
      "epoch": 1.4534089014835807,
      "grad_norm": 0.996818482875824,
      "learning_rate": 7.096736429311014e-05,
      "loss": 1.7781,
      "step": 2180
    },
    {
      "epoch": 1.4600766794465745,
      "grad_norm": 1.1375174522399902,
      "learning_rate": 7.066160227547502e-05,
      "loss": 1.773,
      "step": 2190
    },
    {
      "epoch": 1.4667444574095683,
      "grad_norm": 1.1213793754577637,
      "learning_rate": 7.035584025783991e-05,
      "loss": 1.9176,
      "step": 2200
    },
    {
      "epoch": 1.473412235372562,
      "grad_norm": 1.6988831758499146,
      "learning_rate": 7.005007824020478e-05,
      "loss": 1.8532,
      "step": 2210
    },
    {
      "epoch": 1.4800800133355558,
      "grad_norm": 0.7910037040710449,
      "learning_rate": 6.974431622256968e-05,
      "loss": 1.7635,
      "step": 2220
    },
    {
      "epoch": 1.4867477912985498,
      "grad_norm": 0.9152804017066956,
      "learning_rate": 6.943855420493457e-05,
      "loss": 1.8331,
      "step": 2230
    },
    {
      "epoch": 1.4934155692615436,
      "grad_norm": 0.9226350784301758,
      "learning_rate": 6.913279218729946e-05,
      "loss": 1.795,
      "step": 2240
    },
    {
      "epoch": 1.5000833472245374,
      "grad_norm": 1.3006854057312012,
      "learning_rate": 6.882703016966434e-05,
      "loss": 1.8563,
      "step": 2250
    },
    {
      "epoch": 1.5067511251875314,
      "grad_norm": 1.3358805179595947,
      "learning_rate": 6.852126815202922e-05,
      "loss": 1.7668,
      "step": 2260
    },
    {
      "epoch": 1.5134189031505252,
      "grad_norm": 1.1846323013305664,
      "learning_rate": 6.82155061343941e-05,
      "loss": 1.9066,
      "step": 2270
    },
    {
      "epoch": 1.520086681113519,
      "grad_norm": 1.108085036277771,
      "learning_rate": 6.790974411675899e-05,
      "loss": 1.8052,
      "step": 2280
    },
    {
      "epoch": 1.5267544590765127,
      "grad_norm": 1.4178770780563354,
      "learning_rate": 6.760398209912388e-05,
      "loss": 1.822,
      "step": 2290
    },
    {
      "epoch": 1.5334222370395065,
      "grad_norm": 0.986691415309906,
      "learning_rate": 6.729822008148876e-05,
      "loss": 1.8258,
      "step": 2300
    },
    {
      "epoch": 1.5400900150025003,
      "grad_norm": 1.4478869438171387,
      "learning_rate": 6.699245806385365e-05,
      "loss": 1.9054,
      "step": 2310
    },
    {
      "epoch": 1.546757792965494,
      "grad_norm": 1.2820013761520386,
      "learning_rate": 6.668669604621854e-05,
      "loss": 1.8812,
      "step": 2320
    },
    {
      "epoch": 1.553425570928488,
      "grad_norm": 1.476861834526062,
      "learning_rate": 6.638093402858342e-05,
      "loss": 1.8923,
      "step": 2330
    },
    {
      "epoch": 1.5600933488914819,
      "grad_norm": 1.348183512687683,
      "learning_rate": 6.60751720109483e-05,
      "loss": 1.8335,
      "step": 2340
    },
    {
      "epoch": 1.5667611268544759,
      "grad_norm": 0.9404606819152832,
      "learning_rate": 6.576940999331318e-05,
      "loss": 1.6872,
      "step": 2350
    },
    {
      "epoch": 1.5734289048174697,
      "grad_norm": 2.0837230682373047,
      "learning_rate": 6.546364797567807e-05,
      "loss": 1.7603,
      "step": 2360
    },
    {
      "epoch": 1.5800966827804634,
      "grad_norm": 1.1091251373291016,
      "learning_rate": 6.515788595804296e-05,
      "loss": 1.7272,
      "step": 2370
    },
    {
      "epoch": 1.5867644607434572,
      "grad_norm": 1.2338966131210327,
      "learning_rate": 6.485212394040785e-05,
      "loss": 1.7317,
      "step": 2380
    },
    {
      "epoch": 1.593432238706451,
      "grad_norm": 1.6805166006088257,
      "learning_rate": 6.454636192277273e-05,
      "loss": 1.8244,
      "step": 2390
    },
    {
      "epoch": 1.6001000166694448,
      "grad_norm": 1.1321536302566528,
      "learning_rate": 6.424059990513762e-05,
      "loss": 1.792,
      "step": 2400
    },
    {
      "epoch": 1.6067677946324388,
      "grad_norm": 1.0879573822021484,
      "learning_rate": 6.39348378875025e-05,
      "loss": 1.7204,
      "step": 2410
    },
    {
      "epoch": 1.6134355725954326,
      "grad_norm": 1.5425399541854858,
      "learning_rate": 6.362907586986738e-05,
      "loss": 1.708,
      "step": 2420
    },
    {
      "epoch": 1.6201033505584264,
      "grad_norm": 1.230065941810608,
      "learning_rate": 6.332331385223227e-05,
      "loss": 1.8179,
      "step": 2430
    },
    {
      "epoch": 1.6267711285214204,
      "grad_norm": 1.3187733888626099,
      "learning_rate": 6.301755183459715e-05,
      "loss": 1.8854,
      "step": 2440
    },
    {
      "epoch": 1.6334389064844141,
      "grad_norm": 0.913338840007782,
      "learning_rate": 6.271178981696204e-05,
      "loss": 1.8707,
      "step": 2450
    },
    {
      "epoch": 1.640106684447408,
      "grad_norm": 1.0565099716186523,
      "learning_rate": 6.240602779932693e-05,
      "loss": 1.7927,
      "step": 2460
    },
    {
      "epoch": 1.6467744624104017,
      "grad_norm": 1.4931919574737549,
      "learning_rate": 6.210026578169181e-05,
      "loss": 1.8177,
      "step": 2470
    },
    {
      "epoch": 1.6534422403733955,
      "grad_norm": 0.9038106799125671,
      "learning_rate": 6.17945037640567e-05,
      "loss": 1.8128,
      "step": 2480
    },
    {
      "epoch": 1.6601100183363893,
      "grad_norm": 1.1393464803695679,
      "learning_rate": 6.148874174642159e-05,
      "loss": 1.6685,
      "step": 2490
    },
    {
      "epoch": 1.6667777962993833,
      "grad_norm": 1.0577774047851562,
      "learning_rate": 6.118297972878646e-05,
      "loss": 1.8242,
      "step": 2500
    },
    {
      "epoch": 1.673445574262377,
      "grad_norm": 0.8168011903762817,
      "learning_rate": 6.0877217711151346e-05,
      "loss": 1.8394,
      "step": 2510
    },
    {
      "epoch": 1.680113352225371,
      "grad_norm": 1.1151772737503052,
      "learning_rate": 6.057145569351624e-05,
      "loss": 1.795,
      "step": 2520
    },
    {
      "epoch": 1.6867811301883648,
      "grad_norm": 2.5196895599365234,
      "learning_rate": 6.026569367588112e-05,
      "loss": 1.7281,
      "step": 2530
    },
    {
      "epoch": 1.6934489081513586,
      "grad_norm": 1.923088550567627,
      "learning_rate": 5.9959931658246007e-05,
      "loss": 1.7651,
      "step": 2540
    },
    {
      "epoch": 1.7001166861143524,
      "grad_norm": 1.0189658403396606,
      "learning_rate": 5.965416964061089e-05,
      "loss": 1.9001,
      "step": 2550
    },
    {
      "epoch": 1.7067844640773462,
      "grad_norm": 1.4816863536834717,
      "learning_rate": 5.934840762297578e-05,
      "loss": 1.7018,
      "step": 2560
    },
    {
      "epoch": 1.71345224204034,
      "grad_norm": 1.461768388748169,
      "learning_rate": 5.904264560534066e-05,
      "loss": 1.7678,
      "step": 2570
    },
    {
      "epoch": 1.7201200200033337,
      "grad_norm": 1.27462637424469,
      "learning_rate": 5.873688358770555e-05,
      "loss": 1.7877,
      "step": 2580
    },
    {
      "epoch": 1.7267877979663278,
      "grad_norm": 1.2527124881744385,
      "learning_rate": 5.8431121570070434e-05,
      "loss": 1.8617,
      "step": 2590
    },
    {
      "epoch": 1.7334555759293215,
      "grad_norm": 0.885833203792572,
      "learning_rate": 5.812535955243532e-05,
      "loss": 1.7615,
      "step": 2600
    },
    {
      "epoch": 1.7401233538923155,
      "grad_norm": 0.9843743443489075,
      "learning_rate": 5.78195975348002e-05,
      "loss": 1.7371,
      "step": 2610
    },
    {
      "epoch": 1.7467911318553093,
      "grad_norm": 1.0584251880645752,
      "learning_rate": 5.751383551716509e-05,
      "loss": 1.8051,
      "step": 2620
    },
    {
      "epoch": 1.753458909818303,
      "grad_norm": 0.7974694967269897,
      "learning_rate": 5.720807349952998e-05,
      "loss": 1.7354,
      "step": 2630
    },
    {
      "epoch": 1.7601266877812969,
      "grad_norm": 0.8926520943641663,
      "learning_rate": 5.690231148189486e-05,
      "loss": 1.7268,
      "step": 2640
    },
    {
      "epoch": 1.7667944657442907,
      "grad_norm": 1.136082649230957,
      "learning_rate": 5.659654946425974e-05,
      "loss": 1.6525,
      "step": 2650
    },
    {
      "epoch": 1.7734622437072844,
      "grad_norm": 1.6182888746261597,
      "learning_rate": 5.629078744662463e-05,
      "loss": 1.7739,
      "step": 2660
    },
    {
      "epoch": 1.7801300216702782,
      "grad_norm": 1.1878963708877563,
      "learning_rate": 5.598502542898952e-05,
      "loss": 1.8361,
      "step": 2670
    },
    {
      "epoch": 1.7867977996332722,
      "grad_norm": 0.9770686626434326,
      "learning_rate": 5.56792634113544e-05,
      "loss": 1.624,
      "step": 2680
    },
    {
      "epoch": 1.793465577596266,
      "grad_norm": 1.340289831161499,
      "learning_rate": 5.537350139371929e-05,
      "loss": 1.7082,
      "step": 2690
    },
    {
      "epoch": 1.80013335555926,
      "grad_norm": 1.0371266603469849,
      "learning_rate": 5.506773937608417e-05,
      "loss": 1.7915,
      "step": 2700
    },
    {
      "epoch": 1.8068011335222538,
      "grad_norm": 0.9104135632514954,
      "learning_rate": 5.476197735844906e-05,
      "loss": 1.8305,
      "step": 2710
    },
    {
      "epoch": 1.8134689114852476,
      "grad_norm": 1.3250616788864136,
      "learning_rate": 5.445621534081394e-05,
      "loss": 1.7574,
      "step": 2720
    },
    {
      "epoch": 1.8201366894482414,
      "grad_norm": 1.4024503231048584,
      "learning_rate": 5.415045332317883e-05,
      "loss": 1.6662,
      "step": 2730
    },
    {
      "epoch": 1.8268044674112351,
      "grad_norm": 1.2498736381530762,
      "learning_rate": 5.384469130554371e-05,
      "loss": 1.7325,
      "step": 2740
    },
    {
      "epoch": 1.833472245374229,
      "grad_norm": 1.4009509086608887,
      "learning_rate": 5.35389292879086e-05,
      "loss": 1.7678,
      "step": 2750
    },
    {
      "epoch": 1.8401400233372227,
      "grad_norm": 1.1227467060089111,
      "learning_rate": 5.323316727027348e-05,
      "loss": 1.7601,
      "step": 2760
    },
    {
      "epoch": 1.8468078013002167,
      "grad_norm": 1.4695308208465576,
      "learning_rate": 5.292740525263837e-05,
      "loss": 1.7797,
      "step": 2770
    },
    {
      "epoch": 1.8534755792632105,
      "grad_norm": 1.2259671688079834,
      "learning_rate": 5.262164323500325e-05,
      "loss": 1.7203,
      "step": 2780
    },
    {
      "epoch": 1.8601433572262045,
      "grad_norm": 1.0299605131149292,
      "learning_rate": 5.231588121736814e-05,
      "loss": 1.592,
      "step": 2790
    },
    {
      "epoch": 1.8668111351891983,
      "grad_norm": 1.110947847366333,
      "learning_rate": 5.201011919973302e-05,
      "loss": 1.775,
      "step": 2800
    },
    {
      "epoch": 1.873478913152192,
      "grad_norm": 1.503408670425415,
      "learning_rate": 5.170435718209791e-05,
      "loss": 1.8165,
      "step": 2810
    },
    {
      "epoch": 1.8801466911151858,
      "grad_norm": 1.1599724292755127,
      "learning_rate": 5.139859516446279e-05,
      "loss": 1.7875,
      "step": 2820
    },
    {
      "epoch": 1.8868144690781796,
      "grad_norm": 1.5141520500183105,
      "learning_rate": 5.109283314682768e-05,
      "loss": 1.7418,
      "step": 2830
    },
    {
      "epoch": 1.8934822470411734,
      "grad_norm": 1.0405802726745605,
      "learning_rate": 5.078707112919256e-05,
      "loss": 1.737,
      "step": 2840
    },
    {
      "epoch": 1.9001500250041674,
      "grad_norm": 1.4002310037612915,
      "learning_rate": 5.048130911155745e-05,
      "loss": 1.6626,
      "step": 2850
    },
    {
      "epoch": 1.9068178029671612,
      "grad_norm": 0.8006954789161682,
      "learning_rate": 5.017554709392233e-05,
      "loss": 1.7644,
      "step": 2860
    },
    {
      "epoch": 1.913485580930155,
      "grad_norm": 0.7472752332687378,
      "learning_rate": 4.986978507628722e-05,
      "loss": 1.7182,
      "step": 2870
    },
    {
      "epoch": 1.920153358893149,
      "grad_norm": 1.332733392715454,
      "learning_rate": 4.95640230586521e-05,
      "loss": 1.7014,
      "step": 2880
    },
    {
      "epoch": 1.9268211368561428,
      "grad_norm": 1.2599940299987793,
      "learning_rate": 4.925826104101699e-05,
      "loss": 1.7057,
      "step": 2890
    },
    {
      "epoch": 1.9334889148191365,
      "grad_norm": 0.9936406016349792,
      "learning_rate": 4.895249902338188e-05,
      "loss": 1.6996,
      "step": 2900
    },
    {
      "epoch": 1.9401566927821303,
      "grad_norm": 1.0822620391845703,
      "learning_rate": 4.8646737005746764e-05,
      "loss": 1.7977,
      "step": 2910
    },
    {
      "epoch": 1.946824470745124,
      "grad_norm": 1.2832977771759033,
      "learning_rate": 4.8340974988111644e-05,
      "loss": 1.6148,
      "step": 2920
    },
    {
      "epoch": 1.9534922487081179,
      "grad_norm": 0.8879611492156982,
      "learning_rate": 4.803521297047653e-05,
      "loss": 1.8339,
      "step": 2930
    },
    {
      "epoch": 1.9601600266711119,
      "grad_norm": 1.3133513927459717,
      "learning_rate": 4.772945095284142e-05,
      "loss": 1.7095,
      "step": 2940
    },
    {
      "epoch": 1.9668278046341057,
      "grad_norm": 1.0829344987869263,
      "learning_rate": 4.7423688935206304e-05,
      "loss": 1.7648,
      "step": 2950
    },
    {
      "epoch": 1.9734955825970997,
      "grad_norm": 1.5380847454071045,
      "learning_rate": 4.7117926917571184e-05,
      "loss": 1.6839,
      "step": 2960
    },
    {
      "epoch": 1.9801633605600935,
      "grad_norm": 1.4876227378845215,
      "learning_rate": 4.681216489993607e-05,
      "loss": 1.7374,
      "step": 2970
    },
    {
      "epoch": 1.9868311385230872,
      "grad_norm": 1.4746285676956177,
      "learning_rate": 4.650640288230096e-05,
      "loss": 1.6515,
      "step": 2980
    },
    {
      "epoch": 1.993498916486081,
      "grad_norm": 1.048343300819397,
      "learning_rate": 4.6200640864665844e-05,
      "loss": 1.655,
      "step": 2990
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.0712424516677856,
      "learning_rate": 4.5894878847030724e-05,
      "loss": 1.6922,
      "step": 3000
    },
    {
      "epoch": 2.0,
      "eval_loss": 1.7437509298324585,
      "eval_runtime": 935.6419,
      "eval_samples_per_second": 6.412,
      "eval_steps_per_second": 3.206,
      "step": 3000
    },
    {
      "epoch": 2.006667777962994,
      "grad_norm": 1.3875346183776855,
      "learning_rate": 4.558911682939561e-05,
      "loss": 1.6576,
      "step": 3010
    },
    {
      "epoch": 2.0133355559259876,
      "grad_norm": 1.152463436126709,
      "learning_rate": 4.52833548117605e-05,
      "loss": 1.6989,
      "step": 3020
    },
    {
      "epoch": 2.0200033338889813,
      "grad_norm": 1.5142019987106323,
      "learning_rate": 4.4977592794125385e-05,
      "loss": 1.7279,
      "step": 3030
    },
    {
      "epoch": 2.026671111851975,
      "grad_norm": 1.1510040760040283,
      "learning_rate": 4.4671830776490265e-05,
      "loss": 1.7288,
      "step": 3040
    },
    {
      "epoch": 2.0333388898149694,
      "grad_norm": 1.6991784572601318,
      "learning_rate": 4.436606875885515e-05,
      "loss": 1.6681,
      "step": 3050
    },
    {
      "epoch": 2.040006667777963,
      "grad_norm": 1.1956602334976196,
      "learning_rate": 4.406030674122004e-05,
      "loss": 1.6927,
      "step": 3060
    },
    {
      "epoch": 2.046674445740957,
      "grad_norm": 1.4037926197052002,
      "learning_rate": 4.3754544723584925e-05,
      "loss": 1.6406,
      "step": 3070
    },
    {
      "epoch": 2.0533422237039507,
      "grad_norm": 1.3437398672103882,
      "learning_rate": 4.3448782705949805e-05,
      "loss": 1.7068,
      "step": 3080
    },
    {
      "epoch": 2.0600100016669445,
      "grad_norm": 1.4748594760894775,
      "learning_rate": 4.314302068831469e-05,
      "loss": 1.7012,
      "step": 3090
    },
    {
      "epoch": 2.0666777796299383,
      "grad_norm": 1.0738886594772339,
      "learning_rate": 4.2837258670679586e-05,
      "loss": 1.7498,
      "step": 3100
    },
    {
      "epoch": 2.073345557592932,
      "grad_norm": 1.3873120546340942,
      "learning_rate": 4.2531496653044466e-05,
      "loss": 1.7834,
      "step": 3110
    },
    {
      "epoch": 2.080013335555926,
      "grad_norm": 1.2791938781738281,
      "learning_rate": 4.2225734635409346e-05,
      "loss": 1.6873,
      "step": 3120
    },
    {
      "epoch": 2.0866811135189196,
      "grad_norm": 1.6084460020065308,
      "learning_rate": 4.191997261777423e-05,
      "loss": 1.5514,
      "step": 3130
    },
    {
      "epoch": 2.093348891481914,
      "grad_norm": 1.3316562175750732,
      "learning_rate": 4.1614210600139126e-05,
      "loss": 1.6612,
      "step": 3140
    },
    {
      "epoch": 2.1000166694449076,
      "grad_norm": 1.207045555114746,
      "learning_rate": 4.1308448582504006e-05,
      "loss": 1.7177,
      "step": 3150
    },
    {
      "epoch": 2.1066844474079014,
      "grad_norm": 1.4144295454025269,
      "learning_rate": 4.100268656486889e-05,
      "loss": 1.6091,
      "step": 3160
    },
    {
      "epoch": 2.113352225370895,
      "grad_norm": 0.7900782227516174,
      "learning_rate": 4.069692454723377e-05,
      "loss": 1.7461,
      "step": 3170
    },
    {
      "epoch": 2.120020003333889,
      "grad_norm": 1.0234079360961914,
      "learning_rate": 4.0391162529598666e-05,
      "loss": 1.6168,
      "step": 3180
    },
    {
      "epoch": 2.1266877812968827,
      "grad_norm": 1.1372814178466797,
      "learning_rate": 4.0085400511963546e-05,
      "loss": 1.6715,
      "step": 3190
    },
    {
      "epoch": 2.1333555592598765,
      "grad_norm": 1.351787805557251,
      "learning_rate": 3.977963849432843e-05,
      "loss": 1.6539,
      "step": 3200
    },
    {
      "epoch": 2.1400233372228703,
      "grad_norm": 0.9878564476966858,
      "learning_rate": 3.947387647669332e-05,
      "loss": 1.6685,
      "step": 3210
    },
    {
      "epoch": 2.146691115185864,
      "grad_norm": 1.1744520664215088,
      "learning_rate": 3.916811445905821e-05,
      "loss": 1.6401,
      "step": 3220
    },
    {
      "epoch": 2.1533588931488583,
      "grad_norm": 1.1235789060592651,
      "learning_rate": 3.886235244142309e-05,
      "loss": 1.6681,
      "step": 3230
    },
    {
      "epoch": 2.160026671111852,
      "grad_norm": 1.2417727708816528,
      "learning_rate": 3.8556590423787974e-05,
      "loss": 1.7018,
      "step": 3240
    },
    {
      "epoch": 2.166694449074846,
      "grad_norm": 1.4206407070159912,
      "learning_rate": 3.825082840615286e-05,
      "loss": 1.6164,
      "step": 3250
    },
    {
      "epoch": 2.1733622270378397,
      "grad_norm": 1.4722915887832642,
      "learning_rate": 3.794506638851775e-05,
      "loss": 1.7587,
      "step": 3260
    },
    {
      "epoch": 2.1800300050008334,
      "grad_norm": 1.0120675563812256,
      "learning_rate": 3.763930437088263e-05,
      "loss": 1.6548,
      "step": 3270
    },
    {
      "epoch": 2.186697782963827,
      "grad_norm": 0.99338299036026,
      "learning_rate": 3.7333542353247514e-05,
      "loss": 1.7002,
      "step": 3280
    },
    {
      "epoch": 2.193365560926821,
      "grad_norm": 1.3598476648330688,
      "learning_rate": 3.70277803356124e-05,
      "loss": 1.6887,
      "step": 3290
    },
    {
      "epoch": 2.200033338889815,
      "grad_norm": 1.233922004699707,
      "learning_rate": 3.672201831797729e-05,
      "loss": 1.7484,
      "step": 3300
    },
    {
      "epoch": 2.206701116852809,
      "grad_norm": 1.0236408710479736,
      "learning_rate": 3.641625630034217e-05,
      "loss": 1.7566,
      "step": 3310
    },
    {
      "epoch": 2.213368894815803,
      "grad_norm": 1.1540555953979492,
      "learning_rate": 3.6110494282707054e-05,
      "loss": 1.735,
      "step": 3320
    },
    {
      "epoch": 2.2200366727787966,
      "grad_norm": 1.09479558467865,
      "learning_rate": 3.580473226507194e-05,
      "loss": 1.6538,
      "step": 3330
    },
    {
      "epoch": 2.2267044507417904,
      "grad_norm": 1.3049752712249756,
      "learning_rate": 3.549897024743683e-05,
      "loss": 1.587,
      "step": 3340
    },
    {
      "epoch": 2.233372228704784,
      "grad_norm": 1.3031868934631348,
      "learning_rate": 3.519320822980171e-05,
      "loss": 1.6689,
      "step": 3350
    },
    {
      "epoch": 2.240040006667778,
      "grad_norm": 1.2763195037841797,
      "learning_rate": 3.4887446212166595e-05,
      "loss": 1.7213,
      "step": 3360
    },
    {
      "epoch": 2.2467077846307717,
      "grad_norm": 1.1716018915176392,
      "learning_rate": 3.458168419453148e-05,
      "loss": 1.6505,
      "step": 3370
    },
    {
      "epoch": 2.2533755625937655,
      "grad_norm": 1.0561072826385498,
      "learning_rate": 3.427592217689637e-05,
      "loss": 1.7076,
      "step": 3380
    },
    {
      "epoch": 2.2600433405567593,
      "grad_norm": 1.1513826847076416,
      "learning_rate": 3.397016015926125e-05,
      "loss": 1.668,
      "step": 3390
    },
    {
      "epoch": 2.266711118519753,
      "grad_norm": 1.6741811037063599,
      "learning_rate": 3.3664398141626135e-05,
      "loss": 1.6685,
      "step": 3400
    },
    {
      "epoch": 2.2733788964827473,
      "grad_norm": 0.8604211211204529,
      "learning_rate": 3.335863612399102e-05,
      "loss": 1.7722,
      "step": 3410
    },
    {
      "epoch": 2.280046674445741,
      "grad_norm": 0.9824263453483582,
      "learning_rate": 3.305287410635591e-05,
      "loss": 1.7473,
      "step": 3420
    },
    {
      "epoch": 2.286714452408735,
      "grad_norm": 1.263857364654541,
      "learning_rate": 3.274711208872079e-05,
      "loss": 1.8566,
      "step": 3430
    },
    {
      "epoch": 2.2933822303717286,
      "grad_norm": 0.8778588175773621,
      "learning_rate": 3.2441350071085675e-05,
      "loss": 1.7437,
      "step": 3440
    },
    {
      "epoch": 2.3000500083347224,
      "grad_norm": 1.5629913806915283,
      "learning_rate": 3.213558805345056e-05,
      "loss": 1.6855,
      "step": 3450
    },
    {
      "epoch": 2.306717786297716,
      "grad_norm": 1.145825743675232,
      "learning_rate": 3.182982603581545e-05,
      "loss": 1.6815,
      "step": 3460
    },
    {
      "epoch": 2.31338556426071,
      "grad_norm": 1.1807224750518799,
      "learning_rate": 3.152406401818033e-05,
      "loss": 1.7956,
      "step": 3470
    },
    {
      "epoch": 2.320053342223704,
      "grad_norm": 1.560749888420105,
      "learning_rate": 3.1218302000545216e-05,
      "loss": 1.6521,
      "step": 3480
    },
    {
      "epoch": 2.326721120186698,
      "grad_norm": 1.3022706508636475,
      "learning_rate": 3.09125399829101e-05,
      "loss": 1.7559,
      "step": 3490
    },
    {
      "epoch": 2.3333888981496917,
      "grad_norm": 1.7404121160507202,
      "learning_rate": 3.060677796527499e-05,
      "loss": 1.7467,
      "step": 3500
    },
    {
      "epoch": 2.3400566761126855,
      "grad_norm": 1.2426536083221436,
      "learning_rate": 3.0301015947639876e-05,
      "loss": 1.6872,
      "step": 3510
    },
    {
      "epoch": 2.3467244540756793,
      "grad_norm": 1.0702863931655884,
      "learning_rate": 2.999525393000476e-05,
      "loss": 1.6189,
      "step": 3520
    },
    {
      "epoch": 2.353392232038673,
      "grad_norm": 1.325575590133667,
      "learning_rate": 2.9689491912369646e-05,
      "loss": 1.7544,
      "step": 3530
    },
    {
      "epoch": 2.360060010001667,
      "grad_norm": 1.2366151809692383,
      "learning_rate": 2.938372989473453e-05,
      "loss": 1.6652,
      "step": 3540
    },
    {
      "epoch": 2.3667277879646607,
      "grad_norm": 1.1282331943511963,
      "learning_rate": 2.9077967877099417e-05,
      "loss": 1.7072,
      "step": 3550
    },
    {
      "epoch": 2.3733955659276544,
      "grad_norm": 1.090051293373108,
      "learning_rate": 2.87722058594643e-05,
      "loss": 1.7019,
      "step": 3560
    },
    {
      "epoch": 2.380063343890648,
      "grad_norm": 1.0661653280258179,
      "learning_rate": 2.8466443841829187e-05,
      "loss": 1.5829,
      "step": 3570
    },
    {
      "epoch": 2.3867311218536424,
      "grad_norm": 1.1476504802703857,
      "learning_rate": 2.816068182419407e-05,
      "loss": 1.7029,
      "step": 3580
    },
    {
      "epoch": 2.3933988998166362,
      "grad_norm": 1.6058917045593262,
      "learning_rate": 2.7854919806558957e-05,
      "loss": 1.6342,
      "step": 3590
    },
    {
      "epoch": 2.40006667777963,
      "grad_norm": 1.6740291118621826,
      "learning_rate": 2.754915778892384e-05,
      "loss": 1.4869,
      "step": 3600
    },
    {
      "epoch": 2.406734455742624,
      "grad_norm": 1.3892766237258911,
      "learning_rate": 2.7243395771288727e-05,
      "loss": 1.6603,
      "step": 3610
    },
    {
      "epoch": 2.4134022337056176,
      "grad_norm": 1.4069886207580566,
      "learning_rate": 2.693763375365361e-05,
      "loss": 1.7468,
      "step": 3620
    },
    {
      "epoch": 2.4200700116686114,
      "grad_norm": 1.3502870798110962,
      "learning_rate": 2.6631871736018497e-05,
      "loss": 1.734,
      "step": 3630
    },
    {
      "epoch": 2.426737789631605,
      "grad_norm": 1.288881778717041,
      "learning_rate": 2.632610971838338e-05,
      "loss": 1.7094,
      "step": 3640
    },
    {
      "epoch": 2.433405567594599,
      "grad_norm": 1.3522672653198242,
      "learning_rate": 2.6020347700748268e-05,
      "loss": 1.6109,
      "step": 3650
    },
    {
      "epoch": 2.440073345557593,
      "grad_norm": 0.7712321281433105,
      "learning_rate": 2.571458568311315e-05,
      "loss": 1.7073,
      "step": 3660
    },
    {
      "epoch": 2.446741123520587,
      "grad_norm": 1.2232263088226318,
      "learning_rate": 2.5408823665478038e-05,
      "loss": 1.7323,
      "step": 3670
    },
    {
      "epoch": 2.4534089014835807,
      "grad_norm": 1.1103777885437012,
      "learning_rate": 2.510306164784292e-05,
      "loss": 1.6852,
      "step": 3680
    },
    {
      "epoch": 2.4600766794465745,
      "grad_norm": 1.4366365671157837,
      "learning_rate": 2.4797299630207808e-05,
      "loss": 1.6687,
      "step": 3690
    },
    {
      "epoch": 2.4667444574095683,
      "grad_norm": 1.0844566822052002,
      "learning_rate": 2.449153761257269e-05,
      "loss": 1.6338,
      "step": 3700
    },
    {
      "epoch": 2.473412235372562,
      "grad_norm": 1.163079023361206,
      "learning_rate": 2.4185775594937578e-05,
      "loss": 1.7568,
      "step": 3710
    },
    {
      "epoch": 2.480080013335556,
      "grad_norm": 1.211320400238037,
      "learning_rate": 2.388001357730246e-05,
      "loss": 1.6746,
      "step": 3720
    },
    {
      "epoch": 2.4867477912985496,
      "grad_norm": 1.4146913290023804,
      "learning_rate": 2.357425155966735e-05,
      "loss": 1.7526,
      "step": 3730
    },
    {
      "epoch": 2.4934155692615434,
      "grad_norm": 1.385498046875,
      "learning_rate": 2.3268489542032232e-05,
      "loss": 1.6704,
      "step": 3740
    },
    {
      "epoch": 2.500083347224537,
      "grad_norm": 1.4075905084609985,
      "learning_rate": 2.296272752439712e-05,
      "loss": 1.574,
      "step": 3750
    },
    {
      "epoch": 2.5067511251875314,
      "grad_norm": 0.9219121932983398,
      "learning_rate": 2.2656965506762002e-05,
      "loss": 1.6525,
      "step": 3760
    },
    {
      "epoch": 2.513418903150525,
      "grad_norm": 1.297918677330017,
      "learning_rate": 2.235120348912689e-05,
      "loss": 1.6561,
      "step": 3770
    },
    {
      "epoch": 2.520086681113519,
      "grad_norm": 1.1561216115951538,
      "learning_rate": 2.2045441471491772e-05,
      "loss": 1.6756,
      "step": 3780
    },
    {
      "epoch": 2.5267544590765127,
      "grad_norm": 1.1992127895355225,
      "learning_rate": 2.1739679453856662e-05,
      "loss": 1.7163,
      "step": 3790
    },
    {
      "epoch": 2.5334222370395065,
      "grad_norm": 1.009836196899414,
      "learning_rate": 2.1433917436221542e-05,
      "loss": 1.6304,
      "step": 3800
    },
    {
      "epoch": 2.5400900150025003,
      "grad_norm": 0.8388113975524902,
      "learning_rate": 2.1128155418586433e-05,
      "loss": 1.6996,
      "step": 3810
    },
    {
      "epoch": 2.546757792965494,
      "grad_norm": 1.4192992448806763,
      "learning_rate": 2.0822393400951316e-05,
      "loss": 1.6543,
      "step": 3820
    },
    {
      "epoch": 2.5534255709284883,
      "grad_norm": 1.0261757373809814,
      "learning_rate": 2.0516631383316203e-05,
      "loss": 1.665,
      "step": 3830
    },
    {
      "epoch": 2.560093348891482,
      "grad_norm": 1.3696582317352295,
      "learning_rate": 2.021086936568109e-05,
      "loss": 1.7286,
      "step": 3840
    },
    {
      "epoch": 2.566761126854476,
      "grad_norm": 1.12972092628479,
      "learning_rate": 1.9905107348045973e-05,
      "loss": 1.6603,
      "step": 3850
    },
    {
      "epoch": 2.5734289048174697,
      "grad_norm": 1.4434797763824463,
      "learning_rate": 1.959934533041086e-05,
      "loss": 1.5303,
      "step": 3860
    },
    {
      "epoch": 2.5800966827804634,
      "grad_norm": 1.401375651359558,
      "learning_rate": 1.9293583312775743e-05,
      "loss": 1.7265,
      "step": 3870
    },
    {
      "epoch": 2.5867644607434572,
      "grad_norm": 1.0330326557159424,
      "learning_rate": 1.898782129514063e-05,
      "loss": 1.6374,
      "step": 3880
    },
    {
      "epoch": 2.593432238706451,
      "grad_norm": 1.3965989351272583,
      "learning_rate": 1.8682059277505513e-05,
      "loss": 1.6347,
      "step": 3890
    },
    {
      "epoch": 2.600100016669445,
      "grad_norm": 1.1767688989639282,
      "learning_rate": 1.83762972598704e-05,
      "loss": 1.6134,
      "step": 3900
    },
    {
      "epoch": 2.6067677946324386,
      "grad_norm": 1.14263916015625,
      "learning_rate": 1.8070535242235283e-05,
      "loss": 1.662,
      "step": 3910
    },
    {
      "epoch": 2.6134355725954324,
      "grad_norm": 1.115351915359497,
      "learning_rate": 1.776477322460017e-05,
      "loss": 1.6841,
      "step": 3920
    },
    {
      "epoch": 2.620103350558426,
      "grad_norm": 0.9433474540710449,
      "learning_rate": 1.7459011206965054e-05,
      "loss": 1.7103,
      "step": 3930
    },
    {
      "epoch": 2.6267711285214204,
      "grad_norm": 1.3388417959213257,
      "learning_rate": 1.7153249189329937e-05,
      "loss": 1.7182,
      "step": 3940
    },
    {
      "epoch": 2.633438906484414,
      "grad_norm": 1.0638874769210815,
      "learning_rate": 1.6847487171694824e-05,
      "loss": 1.6062,
      "step": 3950
    },
    {
      "epoch": 2.640106684447408,
      "grad_norm": 1.129753828048706,
      "learning_rate": 1.6541725154059707e-05,
      "loss": 1.6524,
      "step": 3960
    },
    {
      "epoch": 2.6467744624104017,
      "grad_norm": 1.2222555875778198,
      "learning_rate": 1.6235963136424594e-05,
      "loss": 1.5748,
      "step": 3970
    },
    {
      "epoch": 2.6534422403733955,
      "grad_norm": 0.9083068370819092,
      "learning_rate": 1.5930201118789477e-05,
      "loss": 1.6013,
      "step": 3980
    },
    {
      "epoch": 2.6601100183363893,
      "grad_norm": 1.0040380954742432,
      "learning_rate": 1.5624439101154364e-05,
      "loss": 1.5521,
      "step": 3990
    },
    {
      "epoch": 2.6667777962993835,
      "grad_norm": 1.1185156106948853,
      "learning_rate": 1.531867708351925e-05,
      "loss": 1.6189,
      "step": 4000
    },
    {
      "epoch": 2.6734455742623773,
      "grad_norm": 1.354953646659851,
      "learning_rate": 1.5012915065884136e-05,
      "loss": 1.6228,
      "step": 4010
    },
    {
      "epoch": 2.680113352225371,
      "grad_norm": 1.202235221862793,
      "learning_rate": 1.4707153048249021e-05,
      "loss": 1.5981,
      "step": 4020
    },
    {
      "epoch": 2.686781130188365,
      "grad_norm": 1.3447729349136353,
      "learning_rate": 1.4401391030613906e-05,
      "loss": 1.7131,
      "step": 4030
    },
    {
      "epoch": 2.6934489081513586,
      "grad_norm": 1.2730501890182495,
      "learning_rate": 1.4095629012978791e-05,
      "loss": 1.6962,
      "step": 4040
    },
    {
      "epoch": 2.7001166861143524,
      "grad_norm": 1.1006994247436523,
      "learning_rate": 1.3789866995343677e-05,
      "loss": 1.7754,
      "step": 4050
    },
    {
      "epoch": 2.706784464077346,
      "grad_norm": 1.1199945211410522,
      "learning_rate": 1.3484104977708562e-05,
      "loss": 1.6527,
      "step": 4060
    },
    {
      "epoch": 2.71345224204034,
      "grad_norm": 1.4734712839126587,
      "learning_rate": 1.3178342960073447e-05,
      "loss": 1.7263,
      "step": 4070
    },
    {
      "epoch": 2.7201200200033337,
      "grad_norm": 1.2575788497924805,
      "learning_rate": 1.2872580942438332e-05,
      "loss": 1.6833,
      "step": 4080
    },
    {
      "epoch": 2.7267877979663275,
      "grad_norm": 1.1086655855178833,
      "learning_rate": 1.2566818924803217e-05,
      "loss": 1.6237,
      "step": 4090
    },
    {
      "epoch": 2.7334555759293213,
      "grad_norm": 1.1765414476394653,
      "learning_rate": 1.2261056907168102e-05,
      "loss": 1.6085,
      "step": 4100
    },
    {
      "epoch": 2.7401233538923155,
      "grad_norm": 1.1108684539794922,
      "learning_rate": 1.1955294889532987e-05,
      "loss": 1.6812,
      "step": 4110
    },
    {
      "epoch": 2.7467911318553093,
      "grad_norm": 1.1054695844650269,
      "learning_rate": 1.1649532871897874e-05,
      "loss": 1.6544,
      "step": 4120
    },
    {
      "epoch": 2.753458909818303,
      "grad_norm": 1.1922898292541504,
      "learning_rate": 1.1343770854262759e-05,
      "loss": 1.6642,
      "step": 4130
    },
    {
      "epoch": 2.760126687781297,
      "grad_norm": 0.836811900138855,
      "learning_rate": 1.1038008836627644e-05,
      "loss": 1.651,
      "step": 4140
    },
    {
      "epoch": 2.7667944657442907,
      "grad_norm": 1.4976505041122437,
      "learning_rate": 1.073224681899253e-05,
      "loss": 1.6088,
      "step": 4150
    },
    {
      "epoch": 2.7734622437072844,
      "grad_norm": 1.435411810874939,
      "learning_rate": 1.0426484801357414e-05,
      "loss": 1.6803,
      "step": 4160
    },
    {
      "epoch": 2.7801300216702782,
      "grad_norm": 1.4428597688674927,
      "learning_rate": 1.01207227837223e-05,
      "loss": 1.7168,
      "step": 4170
    },
    {
      "epoch": 2.7867977996332725,
      "grad_norm": 1.4303407669067383,
      "learning_rate": 9.814960766087184e-06,
      "loss": 1.7081,
      "step": 4180
    },
    {
      "epoch": 2.7934655775962662,
      "grad_norm": 1.0734224319458008,
      "learning_rate": 9.50919874845207e-06,
      "loss": 1.6427,
      "step": 4190
    },
    {
      "epoch": 2.80013335555926,
      "grad_norm": 1.314979076385498,
      "learning_rate": 9.203436730816955e-06,
      "loss": 1.5566,
      "step": 4200
    },
    {
      "epoch": 2.806801133522254,
      "grad_norm": 1.3486319780349731,
      "learning_rate": 8.89767471318184e-06,
      "loss": 1.7392,
      "step": 4210
    },
    {
      "epoch": 2.8134689114852476,
      "grad_norm": 1.4432209730148315,
      "learning_rate": 8.591912695546725e-06,
      "loss": 1.6107,
      "step": 4220
    },
    {
      "epoch": 2.8201366894482414,
      "grad_norm": 1.1298878192901611,
      "learning_rate": 8.286150677911612e-06,
      "loss": 1.7136,
      "step": 4230
    },
    {
      "epoch": 2.826804467411235,
      "grad_norm": 1.2611202001571655,
      "learning_rate": 7.980388660276497e-06,
      "loss": 1.6015,
      "step": 4240
    },
    {
      "epoch": 2.833472245374229,
      "grad_norm": 0.9984972476959229,
      "learning_rate": 7.674626642641382e-06,
      "loss": 1.5739,
      "step": 4250
    },
    {
      "epoch": 2.8401400233372227,
      "grad_norm": 1.2370787858963013,
      "learning_rate": 7.368864625006266e-06,
      "loss": 1.5501,
      "step": 4260
    },
    {
      "epoch": 2.8468078013002165,
      "grad_norm": 1.1990575790405273,
      "learning_rate": 7.063102607371151e-06,
      "loss": 1.6671,
      "step": 4270
    },
    {
      "epoch": 2.8534755792632103,
      "grad_norm": 1.3590656518936157,
      "learning_rate": 6.757340589736036e-06,
      "loss": 1.5455,
      "step": 4280
    },
    {
      "epoch": 2.8601433572262045,
      "grad_norm": 1.09734046459198,
      "learning_rate": 6.451578572100922e-06,
      "loss": 1.5379,
      "step": 4290
    },
    {
      "epoch": 2.8668111351891983,
      "grad_norm": 1.3920429944992065,
      "learning_rate": 6.145816554465807e-06,
      "loss": 1.6582,
      "step": 4300
    },
    {
      "epoch": 2.873478913152192,
      "grad_norm": 1.104437232017517,
      "learning_rate": 5.8400545368306924e-06,
      "loss": 1.7595,
      "step": 4310
    },
    {
      "epoch": 2.880146691115186,
      "grad_norm": 1.0467820167541504,
      "learning_rate": 5.5342925191955775e-06,
      "loss": 1.6482,
      "step": 4320
    },
    {
      "epoch": 2.8868144690781796,
      "grad_norm": 1.1212899684906006,
      "learning_rate": 5.228530501560463e-06,
      "loss": 1.6084,
      "step": 4330
    },
    {
      "epoch": 2.8934822470411734,
      "grad_norm": 1.1805720329284668,
      "learning_rate": 4.922768483925348e-06,
      "loss": 1.6485,
      "step": 4340
    },
    {
      "epoch": 2.9001500250041676,
      "grad_norm": 1.2161357402801514,
      "learning_rate": 4.617006466290233e-06,
      "loss": 1.5142,
      "step": 4350
    },
    {
      "epoch": 2.9068178029671614,
      "grad_norm": 1.1216516494750977,
      "learning_rate": 4.311244448655118e-06,
      "loss": 1.6339,
      "step": 4360
    },
    {
      "epoch": 2.913485580930155,
      "grad_norm": 1.1950764656066895,
      "learning_rate": 4.005482431020004e-06,
      "loss": 1.6769,
      "step": 4370
    },
    {
      "epoch": 2.920153358893149,
      "grad_norm": 1.1440508365631104,
      "learning_rate": 3.699720413384889e-06,
      "loss": 1.6758,
      "step": 4380
    },
    {
      "epoch": 2.9268211368561428,
      "grad_norm": 0.883542001247406,
      "learning_rate": 3.393958395749774e-06,
      "loss": 1.6759,
      "step": 4390
    },
    {
      "epoch": 2.9334889148191365,
      "grad_norm": 1.1080868244171143,
      "learning_rate": 3.088196378114659e-06,
      "loss": 1.652,
      "step": 4400
    },
    {
      "epoch": 2.9401566927821303,
      "grad_norm": 1.022495150566101,
      "learning_rate": 2.7824343604795443e-06,
      "loss": 1.7128,
      "step": 4410
    },
    {
      "epoch": 2.946824470745124,
      "grad_norm": 1.0218192338943481,
      "learning_rate": 2.4766723428444294e-06,
      "loss": 1.6507,
      "step": 4420
    },
    {
      "epoch": 2.953492248708118,
      "grad_norm": 1.361814022064209,
      "learning_rate": 2.1709103252093153e-06,
      "loss": 1.6283,
      "step": 4430
    },
    {
      "epoch": 2.9601600266711117,
      "grad_norm": 1.3268760442733765,
      "learning_rate": 1.8651483075742e-06,
      "loss": 1.6136,
      "step": 4440
    },
    {
      "epoch": 2.9668278046341054,
      "grad_norm": 1.0214780569076538,
      "learning_rate": 1.5593862899390855e-06,
      "loss": 1.7723,
      "step": 4450
    },
    {
      "epoch": 2.9734955825970997,
      "grad_norm": 0.9572758674621582,
      "learning_rate": 1.2536242723039706e-06,
      "loss": 1.7159,
      "step": 4460
    },
    {
      "epoch": 2.9801633605600935,
      "grad_norm": 0.9046815037727356,
      "learning_rate": 9.478622546688558e-07,
      "loss": 1.6585,
      "step": 4470
    },
    {
      "epoch": 2.9868311385230872,
      "grad_norm": 0.9829444885253906,
      "learning_rate": 6.421002370337411e-07,
      "loss": 1.6526,
      "step": 4480
    },
    {
      "epoch": 2.993498916486081,
      "grad_norm": 1.0751073360443115,
      "learning_rate": 3.3633821939862626e-07,
      "loss": 1.606,
      "step": 4490
    },
    {
      "epoch": 3.0,
      "grad_norm": 1.0739067792892456,
      "learning_rate": 3.057620176351148e-08,
      "loss": 1.7116,
      "step": 4500
    },
    {
      "epoch": 3.0,
      "eval_loss": 1.6764800548553467,
      "eval_runtime": 937.8162,
      "eval_samples_per_second": 6.397,
      "eval_steps_per_second": 3.199,
      "step": 4500
    }
  ],
  "logging_steps": 10,
  "max_steps": 4500,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "EarlyStoppingCallback": {
      "args": {
        "early_stopping_patience": 2,
        "early_stopping_threshold": 0.0
      },
      "attributes": {
        "early_stopping_patience_counter": 0
      }
    },
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 3.1293958618821427e+18,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
