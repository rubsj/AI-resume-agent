{
  "best_global_step": 4500,
  "best_metric": 1.7190537452697754,
  "best_model_checkpoint": "json_outputs_all_data/fine-tune/optuna_output/optuna_trial_1/checkpoint-4500",
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 4500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.006667777962993832,
      "grad_norm": 0.36571037769317627,
      "learning_rate": 0.00029596579317133555,
      "loss": 2.6473,
      "step": 10
    },
    {
      "epoch": 0.013335555925987664,
      "grad_norm": 0.32364198565483093,
      "learning_rate": 0.0002953067733691282,
      "loss": 2.477,
      "step": 20
    },
    {
      "epoch": 0.020003333888981498,
      "grad_norm": 0.30999404191970825,
      "learning_rate": 0.0002946477535669208,
      "loss": 2.4151,
      "step": 30
    },
    {
      "epoch": 0.026671111851975328,
      "grad_norm": 0.30800724029541016,
      "learning_rate": 0.0002939887337647134,
      "loss": 2.4338,
      "step": 40
    },
    {
      "epoch": 0.03333888981496916,
      "grad_norm": 0.3057960569858551,
      "learning_rate": 0.0002933297139625061,
      "loss": 2.3886,
      "step": 50
    },
    {
      "epoch": 0.040006667777962995,
      "grad_norm": 0.3502844572067261,
      "learning_rate": 0.00029267069416029866,
      "loss": 2.3776,
      "step": 60
    },
    {
      "epoch": 0.046674445740956826,
      "grad_norm": 0.3036898374557495,
      "learning_rate": 0.0002920116743580913,
      "loss": 2.3511,
      "step": 70
    },
    {
      "epoch": 0.053342223703950656,
      "grad_norm": 0.33041179180145264,
      "learning_rate": 0.00029135265455588387,
      "loss": 2.4077,
      "step": 80
    },
    {
      "epoch": 0.06001000166694449,
      "grad_norm": 0.34370800852775574,
      "learning_rate": 0.0002906936347536765,
      "loss": 2.377,
      "step": 90
    },
    {
      "epoch": 0.06667777962993832,
      "grad_norm": 0.37762147188186646,
      "learning_rate": 0.00029003461495146914,
      "loss": 2.3613,
      "step": 100
    },
    {
      "epoch": 0.07334555759293215,
      "grad_norm": 0.36856532096862793,
      "learning_rate": 0.0002893755951492617,
      "loss": 2.3677,
      "step": 110
    },
    {
      "epoch": 0.08001333555592599,
      "grad_norm": 0.38518306612968445,
      "learning_rate": 0.00028871657534705435,
      "loss": 2.3813,
      "step": 120
    },
    {
      "epoch": 0.08668111351891981,
      "grad_norm": 0.369098037481308,
      "learning_rate": 0.000288057555544847,
      "loss": 2.3245,
      "step": 130
    },
    {
      "epoch": 0.09334889148191365,
      "grad_norm": 0.3533758521080017,
      "learning_rate": 0.0002873985357426396,
      "loss": 2.3544,
      "step": 140
    },
    {
      "epoch": 0.10001666944490749,
      "grad_norm": 0.3841038942337036,
      "learning_rate": 0.00028673951594043224,
      "loss": 2.342,
      "step": 150
    },
    {
      "epoch": 0.10668444740790131,
      "grad_norm": 0.40689823031425476,
      "learning_rate": 0.0002860804961382248,
      "loss": 2.3159,
      "step": 160
    },
    {
      "epoch": 0.11335222537089515,
      "grad_norm": 0.3960244953632355,
      "learning_rate": 0.00028542147633601745,
      "loss": 2.2773,
      "step": 170
    },
    {
      "epoch": 0.12002000333388899,
      "grad_norm": 0.4079987704753876,
      "learning_rate": 0.0002847624565338101,
      "loss": 2.3108,
      "step": 180
    },
    {
      "epoch": 0.12668778129688282,
      "grad_norm": 0.40078210830688477,
      "learning_rate": 0.00028410343673160266,
      "loss": 2.3029,
      "step": 190
    },
    {
      "epoch": 0.13335555925987663,
      "grad_norm": 0.4260159432888031,
      "learning_rate": 0.0002834444169293953,
      "loss": 2.2746,
      "step": 200
    },
    {
      "epoch": 0.14002333722287047,
      "grad_norm": 0.39434194564819336,
      "learning_rate": 0.00028278539712718793,
      "loss": 2.3069,
      "step": 210
    },
    {
      "epoch": 0.1466911151858643,
      "grad_norm": 0.452265202999115,
      "learning_rate": 0.00028212637732498056,
      "loss": 2.3306,
      "step": 220
    },
    {
      "epoch": 0.15335889314885814,
      "grad_norm": 0.44810786843299866,
      "learning_rate": 0.00028146735752277314,
      "loss": 2.2647,
      "step": 230
    },
    {
      "epoch": 0.16002667111185198,
      "grad_norm": 0.4305063784122467,
      "learning_rate": 0.00028080833772056577,
      "loss": 2.263,
      "step": 240
    },
    {
      "epoch": 0.16669444907484582,
      "grad_norm": 0.4379928410053253,
      "learning_rate": 0.0002801493179183584,
      "loss": 2.2659,
      "step": 250
    },
    {
      "epoch": 0.17336222703783963,
      "grad_norm": 0.5267752408981323,
      "learning_rate": 0.000279490298116151,
      "loss": 2.2622,
      "step": 260
    },
    {
      "epoch": 0.18003000500083347,
      "grad_norm": 0.4154207408428192,
      "learning_rate": 0.0002788312783139436,
      "loss": 2.3364,
      "step": 270
    },
    {
      "epoch": 0.1866977829638273,
      "grad_norm": 0.4696459174156189,
      "learning_rate": 0.00027817225851173625,
      "loss": 2.3725,
      "step": 280
    },
    {
      "epoch": 0.19336556092682114,
      "grad_norm": 0.4376654624938965,
      "learning_rate": 0.0002775132387095289,
      "loss": 2.2821,
      "step": 290
    },
    {
      "epoch": 0.20003333888981498,
      "grad_norm": 0.4483473002910614,
      "learning_rate": 0.00027685421890732146,
      "loss": 2.2842,
      "step": 300
    },
    {
      "epoch": 0.2067011168528088,
      "grad_norm": 0.47988954186439514,
      "learning_rate": 0.0002761951991051141,
      "loss": 2.2528,
      "step": 310
    },
    {
      "epoch": 0.21336889481580262,
      "grad_norm": 0.48429861664772034,
      "learning_rate": 0.0002755361793029067,
      "loss": 2.2806,
      "step": 320
    },
    {
      "epoch": 0.22003667277879646,
      "grad_norm": 0.5162979960441589,
      "learning_rate": 0.0002748771595006993,
      "loss": 2.2323,
      "step": 330
    },
    {
      "epoch": 0.2267044507417903,
      "grad_norm": 0.47944024205207825,
      "learning_rate": 0.00027421813969849193,
      "loss": 2.2357,
      "step": 340
    },
    {
      "epoch": 0.23337222870478413,
      "grad_norm": 0.5847373604774475,
      "learning_rate": 0.00027355911989628457,
      "loss": 2.2405,
      "step": 350
    },
    {
      "epoch": 0.24004000666777797,
      "grad_norm": 0.460659921169281,
      "learning_rate": 0.0002729001000940772,
      "loss": 2.2866,
      "step": 360
    },
    {
      "epoch": 0.24670778463077178,
      "grad_norm": 0.4853246510028839,
      "learning_rate": 0.00027224108029186983,
      "loss": 2.3025,
      "step": 370
    },
    {
      "epoch": 0.25337556259376565,
      "grad_norm": 0.5323846936225891,
      "learning_rate": 0.0002715820604896624,
      "loss": 2.2675,
      "step": 380
    },
    {
      "epoch": 0.2600433405567595,
      "grad_norm": 0.5129333734512329,
      "learning_rate": 0.00027092304068745504,
      "loss": 2.2292,
      "step": 390
    },
    {
      "epoch": 0.26671111851975327,
      "grad_norm": 0.5815289616584778,
      "learning_rate": 0.0002702640208852476,
      "loss": 2.2512,
      "step": 400
    },
    {
      "epoch": 0.2733788964827471,
      "grad_norm": 0.7192515134811401,
      "learning_rate": 0.00026960500108304025,
      "loss": 2.2088,
      "step": 410
    },
    {
      "epoch": 0.28004667444574094,
      "grad_norm": 0.8784112930297852,
      "learning_rate": 0.0002689459812808329,
      "loss": 2.1964,
      "step": 420
    },
    {
      "epoch": 0.2867144524087348,
      "grad_norm": 0.5231346487998962,
      "learning_rate": 0.00026828696147862546,
      "loss": 2.2018,
      "step": 430
    },
    {
      "epoch": 0.2933822303717286,
      "grad_norm": 0.5665676593780518,
      "learning_rate": 0.00026762794167641815,
      "loss": 2.2254,
      "step": 440
    },
    {
      "epoch": 0.30005000833472245,
      "grad_norm": 0.45354023575782776,
      "learning_rate": 0.0002669689218742107,
      "loss": 2.2579,
      "step": 450
    },
    {
      "epoch": 0.3067177862977163,
      "grad_norm": 0.4916379451751709,
      "learning_rate": 0.00026630990207200336,
      "loss": 2.2648,
      "step": 460
    },
    {
      "epoch": 0.3133855642607101,
      "grad_norm": 0.5591432452201843,
      "learning_rate": 0.000265650882269796,
      "loss": 2.2705,
      "step": 470
    },
    {
      "epoch": 0.32005334222370396,
      "grad_norm": 0.503703773021698,
      "learning_rate": 0.00026499186246758857,
      "loss": 2.1921,
      "step": 480
    },
    {
      "epoch": 0.3267211201866978,
      "grad_norm": 0.6676342487335205,
      "learning_rate": 0.0002643328426653812,
      "loss": 2.2161,
      "step": 490
    },
    {
      "epoch": 0.33338889814969164,
      "grad_norm": 0.6028255224227905,
      "learning_rate": 0.00026367382286317383,
      "loss": 2.2422,
      "step": 500
    },
    {
      "epoch": 0.3400566761126855,
      "grad_norm": 0.589493989944458,
      "learning_rate": 0.0002630148030609664,
      "loss": 2.2254,
      "step": 510
    },
    {
      "epoch": 0.34672445407567926,
      "grad_norm": 0.6793193221092224,
      "learning_rate": 0.00026235578325875904,
      "loss": 2.1785,
      "step": 520
    },
    {
      "epoch": 0.3533922320386731,
      "grad_norm": 0.5982882976531982,
      "learning_rate": 0.0002616967634565517,
      "loss": 2.2002,
      "step": 530
    },
    {
      "epoch": 0.36006001000166693,
      "grad_norm": 0.7313753962516785,
      "learning_rate": 0.0002610377436543443,
      "loss": 2.1409,
      "step": 540
    },
    {
      "epoch": 0.36672778796466077,
      "grad_norm": 0.7011795043945312,
      "learning_rate": 0.0002603787238521369,
      "loss": 2.1648,
      "step": 550
    },
    {
      "epoch": 0.3733955659276546,
      "grad_norm": 0.5528658628463745,
      "learning_rate": 0.0002597197040499295,
      "loss": 2.0683,
      "step": 560
    },
    {
      "epoch": 0.38006334389064844,
      "grad_norm": 0.7086460590362549,
      "learning_rate": 0.00025906068424772215,
      "loss": 2.1833,
      "step": 570
    },
    {
      "epoch": 0.3867311218536423,
      "grad_norm": 0.4944664537906647,
      "learning_rate": 0.00025840166444551473,
      "loss": 2.1973,
      "step": 580
    },
    {
      "epoch": 0.3933988998166361,
      "grad_norm": 0.5274134874343872,
      "learning_rate": 0.00025774264464330736,
      "loss": 2.1888,
      "step": 590
    },
    {
      "epoch": 0.40006667777962995,
      "grad_norm": 0.5811222791671753,
      "learning_rate": 0.0002570836248411,
      "loss": 2.1293,
      "step": 600
    },
    {
      "epoch": 0.4067344557426238,
      "grad_norm": 0.606865406036377,
      "learning_rate": 0.00025642460503889263,
      "loss": 2.1729,
      "step": 610
    },
    {
      "epoch": 0.4134022337056176,
      "grad_norm": 0.6375803351402283,
      "learning_rate": 0.0002557655852366852,
      "loss": 2.1331,
      "step": 620
    },
    {
      "epoch": 0.4200700116686114,
      "grad_norm": 0.7105298638343811,
      "learning_rate": 0.00025510656543447784,
      "loss": 2.1632,
      "step": 630
    },
    {
      "epoch": 0.42673778963160525,
      "grad_norm": 0.6721119284629822,
      "learning_rate": 0.00025444754563227047,
      "loss": 2.1481,
      "step": 640
    },
    {
      "epoch": 0.4334055675945991,
      "grad_norm": 0.9817171096801758,
      "learning_rate": 0.00025378852583006305,
      "loss": 2.1738,
      "step": 650
    },
    {
      "epoch": 0.4400733455575929,
      "grad_norm": 0.5903323292732239,
      "learning_rate": 0.0002531295060278557,
      "loss": 2.1644,
      "step": 660
    },
    {
      "epoch": 0.44674112352058676,
      "grad_norm": 0.9297865033149719,
      "learning_rate": 0.0002524704862256483,
      "loss": 2.089,
      "step": 670
    },
    {
      "epoch": 0.4534089014835806,
      "grad_norm": 0.5766723155975342,
      "learning_rate": 0.00025181146642344095,
      "loss": 2.1541,
      "step": 680
    },
    {
      "epoch": 0.46007667944657443,
      "grad_norm": 0.8335058689117432,
      "learning_rate": 0.0002511524466212336,
      "loss": 2.0926,
      "step": 690
    },
    {
      "epoch": 0.46674445740956827,
      "grad_norm": 1.3809469938278198,
      "learning_rate": 0.00025049342681902616,
      "loss": 2.1027,
      "step": 700
    },
    {
      "epoch": 0.4734122353725621,
      "grad_norm": 0.9226062893867493,
      "learning_rate": 0.0002498344070168188,
      "loss": 2.1487,
      "step": 710
    },
    {
      "epoch": 0.48008001333555594,
      "grad_norm": 0.6733959913253784,
      "learning_rate": 0.0002491753872146114,
      "loss": 2.0618,
      "step": 720
    },
    {
      "epoch": 0.4867477912985498,
      "grad_norm": 0.6673468947410583,
      "learning_rate": 0.000248516367412404,
      "loss": 2.1572,
      "step": 730
    },
    {
      "epoch": 0.49341556926154356,
      "grad_norm": 0.8094750046730042,
      "learning_rate": 0.00024785734761019663,
      "loss": 2.1351,
      "step": 740
    },
    {
      "epoch": 0.5000833472245374,
      "grad_norm": 0.862520694732666,
      "learning_rate": 0.00024719832780798926,
      "loss": 2.1081,
      "step": 750
    },
    {
      "epoch": 0.5067511251875313,
      "grad_norm": 0.6917394399642944,
      "learning_rate": 0.0002465393080057819,
      "loss": 2.1393,
      "step": 760
    },
    {
      "epoch": 0.5134189031505251,
      "grad_norm": 0.7135963439941406,
      "learning_rate": 0.0002458802882035745,
      "loss": 2.0456,
      "step": 770
    },
    {
      "epoch": 0.520086681113519,
      "grad_norm": 0.7573021650314331,
      "learning_rate": 0.0002452212684013671,
      "loss": 2.0968,
      "step": 780
    },
    {
      "epoch": 0.5267544590765127,
      "grad_norm": 0.5989375114440918,
      "learning_rate": 0.00024456224859915974,
      "loss": 2.1152,
      "step": 790
    },
    {
      "epoch": 0.5334222370395065,
      "grad_norm": 0.8170872330665588,
      "learning_rate": 0.00024390322879695234,
      "loss": 2.1006,
      "step": 800
    },
    {
      "epoch": 0.5400900150025004,
      "grad_norm": 0.6606876254081726,
      "learning_rate": 0.00024324420899474495,
      "loss": 2.1894,
      "step": 810
    },
    {
      "epoch": 0.5467577929654942,
      "grad_norm": 0.7108172178268433,
      "learning_rate": 0.00024258518919253755,
      "loss": 2.0376,
      "step": 820
    },
    {
      "epoch": 0.5534255709284881,
      "grad_norm": 0.8470494747161865,
      "learning_rate": 0.00024192616939033021,
      "loss": 2.0793,
      "step": 830
    },
    {
      "epoch": 0.5600933488914819,
      "grad_norm": 0.8367266058921814,
      "learning_rate": 0.00024126714958812282,
      "loss": 2.1079,
      "step": 840
    },
    {
      "epoch": 0.5667611268544758,
      "grad_norm": 0.7980153560638428,
      "learning_rate": 0.00024060812978591542,
      "loss": 2.0639,
      "step": 850
    },
    {
      "epoch": 0.5734289048174696,
      "grad_norm": 1.0444438457489014,
      "learning_rate": 0.00023994910998370806,
      "loss": 2.0828,
      "step": 860
    },
    {
      "epoch": 0.5800966827804634,
      "grad_norm": 0.7842515707015991,
      "learning_rate": 0.00023929009018150066,
      "loss": 2.1257,
      "step": 870
    },
    {
      "epoch": 0.5867644607434572,
      "grad_norm": 0.967511773109436,
      "learning_rate": 0.00023863107037929327,
      "loss": 1.9781,
      "step": 880
    },
    {
      "epoch": 0.5934322387064511,
      "grad_norm": 0.7593426704406738,
      "learning_rate": 0.00023797205057708587,
      "loss": 2.0817,
      "step": 890
    },
    {
      "epoch": 0.6001000166694449,
      "grad_norm": 0.7498713731765747,
      "learning_rate": 0.0002373130307748785,
      "loss": 2.0693,
      "step": 900
    },
    {
      "epoch": 0.6067677946324388,
      "grad_norm": 1.1693081855773926,
      "learning_rate": 0.00023665401097267114,
      "loss": 2.0196,
      "step": 910
    },
    {
      "epoch": 0.6134355725954326,
      "grad_norm": 1.2308986186981201,
      "learning_rate": 0.00023599499117046374,
      "loss": 2.1019,
      "step": 920
    },
    {
      "epoch": 0.6201033505584264,
      "grad_norm": 0.7967254519462585,
      "learning_rate": 0.00023533597136825637,
      "loss": 2.1351,
      "step": 930
    },
    {
      "epoch": 0.6267711285214203,
      "grad_norm": 1.1106336116790771,
      "learning_rate": 0.00023467695156604898,
      "loss": 2.1152,
      "step": 940
    },
    {
      "epoch": 0.633438906484414,
      "grad_norm": 1.06902277469635,
      "learning_rate": 0.00023401793176384159,
      "loss": 2.0459,
      "step": 950
    },
    {
      "epoch": 0.6401066844474079,
      "grad_norm": 0.8861345052719116,
      "learning_rate": 0.00023335891196163422,
      "loss": 2.0769,
      "step": 960
    },
    {
      "epoch": 0.6467744624104017,
      "grad_norm": 1.1046111583709717,
      "learning_rate": 0.00023269989215942682,
      "loss": 2.0666,
      "step": 970
    },
    {
      "epoch": 0.6534422403733956,
      "grad_norm": 0.8869921565055847,
      "learning_rate": 0.00023204087235721946,
      "loss": 2.0259,
      "step": 980
    },
    {
      "epoch": 0.6601100183363894,
      "grad_norm": 0.7745063304901123,
      "learning_rate": 0.0002313818525550121,
      "loss": 2.0097,
      "step": 990
    },
    {
      "epoch": 0.6667777962993833,
      "grad_norm": 0.9775578379631042,
      "learning_rate": 0.0002307228327528047,
      "loss": 2.1221,
      "step": 1000
    },
    {
      "epoch": 0.6734455742623771,
      "grad_norm": 0.9518446922302246,
      "learning_rate": 0.0002300638129505973,
      "loss": 1.9635,
      "step": 1010
    },
    {
      "epoch": 0.680113352225371,
      "grad_norm": 1.1988531351089478,
      "learning_rate": 0.00022940479314838993,
      "loss": 2.0246,
      "step": 1020
    },
    {
      "epoch": 0.6867811301883647,
      "grad_norm": 0.9843546152114868,
      "learning_rate": 0.00022874577334618254,
      "loss": 2.0913,
      "step": 1030
    },
    {
      "epoch": 0.6934489081513585,
      "grad_norm": 0.7809053659439087,
      "learning_rate": 0.00022808675354397514,
      "loss": 2.0117,
      "step": 1040
    },
    {
      "epoch": 0.7001166861143524,
      "grad_norm": 1.0530263185501099,
      "learning_rate": 0.00022742773374176775,
      "loss": 1.9781,
      "step": 1050
    },
    {
      "epoch": 0.7067844640773462,
      "grad_norm": 0.9835890531539917,
      "learning_rate": 0.0002267687139395604,
      "loss": 1.9464,
      "step": 1060
    },
    {
      "epoch": 0.7134522420403401,
      "grad_norm": 1.1429179906845093,
      "learning_rate": 0.000226109694137353,
      "loss": 2.0613,
      "step": 1070
    },
    {
      "epoch": 0.7201200200033339,
      "grad_norm": 1.044668436050415,
      "learning_rate": 0.00022545067433514562,
      "loss": 2.0133,
      "step": 1080
    },
    {
      "epoch": 0.7267877979663278,
      "grad_norm": 0.6817634105682373,
      "learning_rate": 0.00022479165453293825,
      "loss": 2.0448,
      "step": 1090
    },
    {
      "epoch": 0.7334555759293215,
      "grad_norm": 1.4181087017059326,
      "learning_rate": 0.00022413263473073085,
      "loss": 2.0054,
      "step": 1100
    },
    {
      "epoch": 0.7401233538923154,
      "grad_norm": 1.0962764024734497,
      "learning_rate": 0.00022347361492852346,
      "loss": 1.9666,
      "step": 1110
    },
    {
      "epoch": 0.7467911318553092,
      "grad_norm": 1.1726270914077759,
      "learning_rate": 0.0002228145951263161,
      "loss": 2.0922,
      "step": 1120
    },
    {
      "epoch": 0.7534589098183031,
      "grad_norm": 1.304975152015686,
      "learning_rate": 0.0002221555753241087,
      "loss": 2.0395,
      "step": 1130
    },
    {
      "epoch": 0.7601266877812969,
      "grad_norm": 0.982677698135376,
      "learning_rate": 0.00022149655552190133,
      "loss": 1.9886,
      "step": 1140
    },
    {
      "epoch": 0.7667944657442907,
      "grad_norm": 1.2257511615753174,
      "learning_rate": 0.00022083753571969396,
      "loss": 1.9617,
      "step": 1150
    },
    {
      "epoch": 0.7734622437072846,
      "grad_norm": 0.8649735450744629,
      "learning_rate": 0.00022017851591748657,
      "loss": 2.0047,
      "step": 1160
    },
    {
      "epoch": 0.7801300216702783,
      "grad_norm": 1.1930341720581055,
      "learning_rate": 0.00021951949611527917,
      "loss": 1.9718,
      "step": 1170
    },
    {
      "epoch": 0.7867977996332722,
      "grad_norm": 0.765844464302063,
      "learning_rate": 0.0002188604763130718,
      "loss": 2.0071,
      "step": 1180
    },
    {
      "epoch": 0.793465577596266,
      "grad_norm": 1.6615229845046997,
      "learning_rate": 0.0002182014565108644,
      "loss": 2.03,
      "step": 1190
    },
    {
      "epoch": 0.8001333555592599,
      "grad_norm": 1.2507449388504028,
      "learning_rate": 0.00021754243670865701,
      "loss": 1.9623,
      "step": 1200
    },
    {
      "epoch": 0.8068011335222537,
      "grad_norm": 1.2583237886428833,
      "learning_rate": 0.00021688341690644962,
      "loss": 2.047,
      "step": 1210
    },
    {
      "epoch": 0.8134689114852476,
      "grad_norm": 1.0434309244155884,
      "learning_rate": 0.00021622439710424228,
      "loss": 1.9164,
      "step": 1220
    },
    {
      "epoch": 0.8201366894482414,
      "grad_norm": 1.266594648361206,
      "learning_rate": 0.00021556537730203488,
      "loss": 2.0402,
      "step": 1230
    },
    {
      "epoch": 0.8268044674112353,
      "grad_norm": 1.1014741659164429,
      "learning_rate": 0.00021490635749982752,
      "loss": 1.9555,
      "step": 1240
    },
    {
      "epoch": 0.833472245374229,
      "grad_norm": 1.080191969871521,
      "learning_rate": 0.00021424733769762012,
      "loss": 2.0096,
      "step": 1250
    },
    {
      "epoch": 0.8401400233372228,
      "grad_norm": 1.2465169429779053,
      "learning_rate": 0.00021358831789541273,
      "loss": 2.0068,
      "step": 1260
    },
    {
      "epoch": 0.8468078013002167,
      "grad_norm": 1.4036778211593628,
      "learning_rate": 0.00021292929809320533,
      "loss": 1.9336,
      "step": 1270
    },
    {
      "epoch": 0.8534755792632105,
      "grad_norm": 1.0246062278747559,
      "learning_rate": 0.00021227027829099797,
      "loss": 2.0052,
      "step": 1280
    },
    {
      "epoch": 0.8601433572262044,
      "grad_norm": 0.9973902106285095,
      "learning_rate": 0.0002116112584887906,
      "loss": 2.02,
      "step": 1290
    },
    {
      "epoch": 0.8668111351891982,
      "grad_norm": 1.4059927463531494,
      "learning_rate": 0.0002109522386865832,
      "loss": 1.9696,
      "step": 1300
    },
    {
      "epoch": 0.8734789131521921,
      "grad_norm": 1.00184166431427,
      "learning_rate": 0.00021029321888437584,
      "loss": 1.9626,
      "step": 1310
    },
    {
      "epoch": 0.8801466911151858,
      "grad_norm": 1.4653805494308472,
      "learning_rate": 0.00020963419908216844,
      "loss": 1.926,
      "step": 1320
    },
    {
      "epoch": 0.8868144690781797,
      "grad_norm": 1.332573652267456,
      "learning_rate": 0.00020897517927996105,
      "loss": 1.9484,
      "step": 1330
    },
    {
      "epoch": 0.8934822470411735,
      "grad_norm": 0.7556552290916443,
      "learning_rate": 0.00020831615947775368,
      "loss": 1.9471,
      "step": 1340
    },
    {
      "epoch": 0.9001500250041674,
      "grad_norm": 1.5547417402267456,
      "learning_rate": 0.00020765713967554628,
      "loss": 1.9164,
      "step": 1350
    },
    {
      "epoch": 0.9068178029671612,
      "grad_norm": 0.9302621483802795,
      "learning_rate": 0.0002069981198733389,
      "loss": 1.9454,
      "step": 1360
    },
    {
      "epoch": 0.913485580930155,
      "grad_norm": 1.117952823638916,
      "learning_rate": 0.00020633910007113155,
      "loss": 1.8656,
      "step": 1370
    },
    {
      "epoch": 0.9201533588931489,
      "grad_norm": 1.1385095119476318,
      "learning_rate": 0.00020568008026892415,
      "loss": 1.9678,
      "step": 1380
    },
    {
      "epoch": 0.9268211368561426,
      "grad_norm": 1.6552482843399048,
      "learning_rate": 0.00020502106046671676,
      "loss": 2.0567,
      "step": 1390
    },
    {
      "epoch": 0.9334889148191365,
      "grad_norm": 1.8844568729400635,
      "learning_rate": 0.0002043620406645094,
      "loss": 1.982,
      "step": 1400
    },
    {
      "epoch": 0.9401566927821303,
      "grad_norm": 1.0919981002807617,
      "learning_rate": 0.000203703020862302,
      "loss": 2.0023,
      "step": 1410
    },
    {
      "epoch": 0.9468244707451242,
      "grad_norm": 1.0874992609024048,
      "learning_rate": 0.0002030440010600946,
      "loss": 1.9669,
      "step": 1420
    },
    {
      "epoch": 0.953492248708118,
      "grad_norm": 0.8102202415466309,
      "learning_rate": 0.0002023849812578872,
      "loss": 2.0717,
      "step": 1430
    },
    {
      "epoch": 0.9601600266711119,
      "grad_norm": 1.6479789018630981,
      "learning_rate": 0.00020172596145567984,
      "loss": 1.9651,
      "step": 1440
    },
    {
      "epoch": 0.9668278046341057,
      "grad_norm": 1.7701425552368164,
      "learning_rate": 0.00020106694165347247,
      "loss": 1.9003,
      "step": 1450
    },
    {
      "epoch": 0.9734955825970996,
      "grad_norm": 1.2602574825286865,
      "learning_rate": 0.0002004079218512651,
      "loss": 1.883,
      "step": 1460
    },
    {
      "epoch": 0.9801633605600933,
      "grad_norm": 1.802210807800293,
      "learning_rate": 0.0001997489020490577,
      "loss": 1.9253,
      "step": 1470
    },
    {
      "epoch": 0.9868311385230871,
      "grad_norm": 1.5025283098220825,
      "learning_rate": 0.00019908988224685031,
      "loss": 1.9502,
      "step": 1480
    },
    {
      "epoch": 0.993498916486081,
      "grad_norm": 1.0482900142669678,
      "learning_rate": 0.00019843086244464292,
      "loss": 1.8832,
      "step": 1490
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.2793948650360107,
      "learning_rate": 0.00019777184264243555,
      "loss": 2.0061,
      "step": 1500
    },
    {
      "epoch": 1.0,
      "eval_loss": 1.9379124641418457,
      "eval_runtime": 939.4857,
      "eval_samples_per_second": 6.385,
      "eval_steps_per_second": 3.193,
      "step": 1500
    },
    {
      "epoch": 1.0066677779629938,
      "grad_norm": 1.4428117275238037,
      "learning_rate": 0.00019711282284022816,
      "loss": 1.8706,
      "step": 1510
    },
    {
      "epoch": 1.0133355559259876,
      "grad_norm": 1.0652389526367188,
      "learning_rate": 0.00019645380303802076,
      "loss": 1.9055,
      "step": 1520
    },
    {
      "epoch": 1.0200033338889816,
      "grad_norm": 1.0768554210662842,
      "learning_rate": 0.00019579478323581342,
      "loss": 1.9299,
      "step": 1530
    },
    {
      "epoch": 1.0266711118519753,
      "grad_norm": 1.016814947128296,
      "learning_rate": 0.00019513576343360603,
      "loss": 1.9669,
      "step": 1540
    },
    {
      "epoch": 1.0333388898149691,
      "grad_norm": 0.7950933575630188,
      "learning_rate": 0.00019447674363139863,
      "loss": 1.9704,
      "step": 1550
    },
    {
      "epoch": 1.040006667777963,
      "grad_norm": 1.1403374671936035,
      "learning_rate": 0.00019381772382919126,
      "loss": 1.9207,
      "step": 1560
    },
    {
      "epoch": 1.046674445740957,
      "grad_norm": 0.8510197401046753,
      "learning_rate": 0.00019315870402698387,
      "loss": 1.9278,
      "step": 1570
    },
    {
      "epoch": 1.0533422237039507,
      "grad_norm": 1.1432561874389648,
      "learning_rate": 0.00019249968422477647,
      "loss": 1.8256,
      "step": 1580
    },
    {
      "epoch": 1.0600100016669445,
      "grad_norm": 1.3679019212722778,
      "learning_rate": 0.00019184066442256908,
      "loss": 1.8424,
      "step": 1590
    },
    {
      "epoch": 1.0666777796299383,
      "grad_norm": 1.079134464263916,
      "learning_rate": 0.00019118164462036174,
      "loss": 1.9517,
      "step": 1600
    },
    {
      "epoch": 1.073345557592932,
      "grad_norm": 1.2146658897399902,
      "learning_rate": 0.00019052262481815434,
      "loss": 1.8722,
      "step": 1610
    },
    {
      "epoch": 1.080013335555926,
      "grad_norm": 1.0902169942855835,
      "learning_rate": 0.00018986360501594698,
      "loss": 1.9528,
      "step": 1620
    },
    {
      "epoch": 1.0866811135189198,
      "grad_norm": 0.9509077668190002,
      "learning_rate": 0.00018920458521373958,
      "loss": 1.9329,
      "step": 1630
    },
    {
      "epoch": 1.0933488914819136,
      "grad_norm": 0.8689899444580078,
      "learning_rate": 0.0001885455654115322,
      "loss": 1.8555,
      "step": 1640
    },
    {
      "epoch": 1.1000166694449074,
      "grad_norm": 1.4769848585128784,
      "learning_rate": 0.0001878865456093248,
      "loss": 1.9083,
      "step": 1650
    },
    {
      "epoch": 1.1066844474079014,
      "grad_norm": 2.073134422302246,
      "learning_rate": 0.00018722752580711743,
      "loss": 1.849,
      "step": 1660
    },
    {
      "epoch": 1.1133522253708952,
      "grad_norm": 1.0926932096481323,
      "learning_rate": 0.00018656850600491003,
      "loss": 1.8258,
      "step": 1670
    },
    {
      "epoch": 1.120020003333889,
      "grad_norm": 1.407251000404358,
      "learning_rate": 0.00018590948620270266,
      "loss": 1.8794,
      "step": 1680
    },
    {
      "epoch": 1.1266877812968827,
      "grad_norm": 1.161523699760437,
      "learning_rate": 0.0001852504664004953,
      "loss": 1.8967,
      "step": 1690
    },
    {
      "epoch": 1.1333555592598765,
      "grad_norm": 1.2355499267578125,
      "learning_rate": 0.0001845914465982879,
      "loss": 1.787,
      "step": 1700
    },
    {
      "epoch": 1.1400233372228705,
      "grad_norm": 1.1333811283111572,
      "learning_rate": 0.0001839324267960805,
      "loss": 1.8613,
      "step": 1710
    },
    {
      "epoch": 1.1466911151858643,
      "grad_norm": 1.1031666994094849,
      "learning_rate": 0.00018327340699387314,
      "loss": 1.8564,
      "step": 1720
    },
    {
      "epoch": 1.153358893148858,
      "grad_norm": 1.0109891891479492,
      "learning_rate": 0.00018261438719166574,
      "loss": 1.7995,
      "step": 1730
    },
    {
      "epoch": 1.160026671111852,
      "grad_norm": 1.7525643110275269,
      "learning_rate": 0.00018195536738945835,
      "loss": 1.9156,
      "step": 1740
    },
    {
      "epoch": 1.1666944490748459,
      "grad_norm": 0.8447930812835693,
      "learning_rate": 0.00018129634758725095,
      "loss": 1.7961,
      "step": 1750
    },
    {
      "epoch": 1.1733622270378397,
      "grad_norm": 0.8669554591178894,
      "learning_rate": 0.0001806373277850436,
      "loss": 1.8285,
      "step": 1760
    },
    {
      "epoch": 1.1800300050008334,
      "grad_norm": 1.5373905897140503,
      "learning_rate": 0.00017997830798283622,
      "loss": 1.8798,
      "step": 1770
    },
    {
      "epoch": 1.1866977829638272,
      "grad_norm": 1.5286751985549927,
      "learning_rate": 0.00017931928818062885,
      "loss": 1.8005,
      "step": 1780
    },
    {
      "epoch": 1.1933655609268212,
      "grad_norm": 1.1070424318313599,
      "learning_rate": 0.00017866026837842146,
      "loss": 1.8227,
      "step": 1790
    },
    {
      "epoch": 1.200033338889815,
      "grad_norm": 1.1983602046966553,
      "learning_rate": 0.00017800124857621406,
      "loss": 1.8838,
      "step": 1800
    },
    {
      "epoch": 1.2067011168528088,
      "grad_norm": 1.0944172143936157,
      "learning_rate": 0.00017734222877400667,
      "loss": 1.8993,
      "step": 1810
    },
    {
      "epoch": 1.2133688948158026,
      "grad_norm": 1.2234362363815308,
      "learning_rate": 0.0001766832089717993,
      "loss": 1.8006,
      "step": 1820
    },
    {
      "epoch": 1.2200366727787966,
      "grad_norm": 1.0084478855133057,
      "learning_rate": 0.0001760241891695919,
      "loss": 1.8753,
      "step": 1830
    },
    {
      "epoch": 1.2267044507417904,
      "grad_norm": 1.5088831186294556,
      "learning_rate": 0.00017536516936738456,
      "loss": 1.871,
      "step": 1840
    },
    {
      "epoch": 1.2333722287047841,
      "grad_norm": 1.0063034296035767,
      "learning_rate": 0.00017470614956517717,
      "loss": 1.849,
      "step": 1850
    },
    {
      "epoch": 1.240040006667778,
      "grad_norm": 1.6043024063110352,
      "learning_rate": 0.00017404712976296977,
      "loss": 1.8518,
      "step": 1860
    },
    {
      "epoch": 1.2467077846307717,
      "grad_norm": 1.3505477905273438,
      "learning_rate": 0.00017338810996076238,
      "loss": 1.8134,
      "step": 1870
    },
    {
      "epoch": 1.2533755625937657,
      "grad_norm": 1.318923830986023,
      "learning_rate": 0.000172729090158555,
      "loss": 1.8948,
      "step": 1880
    },
    {
      "epoch": 1.2600433405567595,
      "grad_norm": 1.8644366264343262,
      "learning_rate": 0.00017207007035634762,
      "loss": 1.8613,
      "step": 1890
    },
    {
      "epoch": 1.2667111185197533,
      "grad_norm": 1.239943504333496,
      "learning_rate": 0.00017141105055414022,
      "loss": 1.9377,
      "step": 1900
    },
    {
      "epoch": 1.273378896482747,
      "grad_norm": 1.2595385313034058,
      "learning_rate": 0.00017075203075193288,
      "loss": 1.8849,
      "step": 1910
    },
    {
      "epoch": 1.280046674445741,
      "grad_norm": 1.3397010564804077,
      "learning_rate": 0.0001700930109497255,
      "loss": 1.9199,
      "step": 1920
    },
    {
      "epoch": 1.2867144524087348,
      "grad_norm": 2.0248067378997803,
      "learning_rate": 0.0001694339911475181,
      "loss": 1.8884,
      "step": 1930
    },
    {
      "epoch": 1.2933822303717286,
      "grad_norm": 1.247423529624939,
      "learning_rate": 0.00016877497134531072,
      "loss": 1.7658,
      "step": 1940
    },
    {
      "epoch": 1.3000500083347224,
      "grad_norm": 1.6854606866836548,
      "learning_rate": 0.00016811595154310333,
      "loss": 1.797,
      "step": 1950
    },
    {
      "epoch": 1.3067177862977162,
      "grad_norm": 1.6768170595169067,
      "learning_rate": 0.00016745693174089594,
      "loss": 1.8478,
      "step": 1960
    },
    {
      "epoch": 1.3133855642607102,
      "grad_norm": 1.5255736112594604,
      "learning_rate": 0.00016679791193868854,
      "loss": 1.7916,
      "step": 1970
    },
    {
      "epoch": 1.320053342223704,
      "grad_norm": 0.9717207551002502,
      "learning_rate": 0.00016613889213648117,
      "loss": 1.8896,
      "step": 1980
    },
    {
      "epoch": 1.3267211201866977,
      "grad_norm": 1.089207410812378,
      "learning_rate": 0.0001654798723342738,
      "loss": 1.7365,
      "step": 1990
    },
    {
      "epoch": 1.3333888981496917,
      "grad_norm": 1.3619074821472168,
      "learning_rate": 0.00016482085253206644,
      "loss": 1.8848,
      "step": 2000
    },
    {
      "epoch": 1.3400566761126855,
      "grad_norm": 1.4494996070861816,
      "learning_rate": 0.00016416183272985904,
      "loss": 1.8123,
      "step": 2010
    },
    {
      "epoch": 1.3467244540756793,
      "grad_norm": 1.4791895151138306,
      "learning_rate": 0.00016350281292765165,
      "loss": 1.7772,
      "step": 2020
    },
    {
      "epoch": 1.353392232038673,
      "grad_norm": 0.8859991431236267,
      "learning_rate": 0.00016284379312544425,
      "loss": 1.7686,
      "step": 2030
    },
    {
      "epoch": 1.3600600100016669,
      "grad_norm": 0.932937502861023,
      "learning_rate": 0.00016218477332323689,
      "loss": 1.839,
      "step": 2040
    },
    {
      "epoch": 1.3667277879646607,
      "grad_norm": 1.2600455284118652,
      "learning_rate": 0.0001615257535210295,
      "loss": 1.6849,
      "step": 2050
    },
    {
      "epoch": 1.3733955659276547,
      "grad_norm": 1.7814668416976929,
      "learning_rate": 0.0001608667337188221,
      "loss": 1.7956,
      "step": 2060
    },
    {
      "epoch": 1.3800633438906484,
      "grad_norm": 1.4670236110687256,
      "learning_rate": 0.00016020771391661476,
      "loss": 1.8888,
      "step": 2070
    },
    {
      "epoch": 1.3867311218536422,
      "grad_norm": 1.405139684677124,
      "learning_rate": 0.00015954869411440736,
      "loss": 1.8619,
      "step": 2080
    },
    {
      "epoch": 1.3933988998166362,
      "grad_norm": 1.6550320386886597,
      "learning_rate": 0.00015888967431219997,
      "loss": 1.7302,
      "step": 2090
    },
    {
      "epoch": 1.40006667777963,
      "grad_norm": 0.9055110812187195,
      "learning_rate": 0.0001582306545099926,
      "loss": 1.9254,
      "step": 2100
    },
    {
      "epoch": 1.4067344557426238,
      "grad_norm": 0.8661531805992126,
      "learning_rate": 0.0001575716347077852,
      "loss": 1.889,
      "step": 2110
    },
    {
      "epoch": 1.4134022337056176,
      "grad_norm": 1.904550313949585,
      "learning_rate": 0.0001569126149055778,
      "loss": 1.7642,
      "step": 2120
    },
    {
      "epoch": 1.4200700116686114,
      "grad_norm": 0.9233589768409729,
      "learning_rate": 0.00015625359510337044,
      "loss": 1.9271,
      "step": 2130
    },
    {
      "epoch": 1.4267377896316051,
      "grad_norm": 1.5490931272506714,
      "learning_rate": 0.00015559457530116305,
      "loss": 1.7775,
      "step": 2140
    },
    {
      "epoch": 1.4334055675945991,
      "grad_norm": 1.2671159505844116,
      "learning_rate": 0.00015493555549895568,
      "loss": 1.8705,
      "step": 2150
    },
    {
      "epoch": 1.440073345557593,
      "grad_norm": 1.4605755805969238,
      "learning_rate": 0.0001542765356967483,
      "loss": 1.8049,
      "step": 2160
    },
    {
      "epoch": 1.4467411235205867,
      "grad_norm": 1.52382230758667,
      "learning_rate": 0.00015361751589454092,
      "loss": 1.9022,
      "step": 2170
    },
    {
      "epoch": 1.4534089014835807,
      "grad_norm": 1.216550588607788,
      "learning_rate": 0.00015295849609233352,
      "loss": 1.7868,
      "step": 2180
    },
    {
      "epoch": 1.4600766794465745,
      "grad_norm": 1.1730976104736328,
      "learning_rate": 0.00015229947629012613,
      "loss": 1.7867,
      "step": 2190
    },
    {
      "epoch": 1.4667444574095683,
      "grad_norm": 1.4559123516082764,
      "learning_rate": 0.00015164045648791876,
      "loss": 1.9348,
      "step": 2200
    },
    {
      "epoch": 1.473412235372562,
      "grad_norm": 1.152955174446106,
      "learning_rate": 0.00015098143668571136,
      "loss": 1.8674,
      "step": 2210
    },
    {
      "epoch": 1.4800800133355558,
      "grad_norm": 1.7423136234283447,
      "learning_rate": 0.00015032241688350402,
      "loss": 1.7762,
      "step": 2220
    },
    {
      "epoch": 1.4867477912985498,
      "grad_norm": 1.3131155967712402,
      "learning_rate": 0.00014966339708129663,
      "loss": 1.8515,
      "step": 2230
    },
    {
      "epoch": 1.4934155692615436,
      "grad_norm": 1.2423641681671143,
      "learning_rate": 0.00014900437727908923,
      "loss": 1.8039,
      "step": 2240
    },
    {
      "epoch": 1.5000833472245374,
      "grad_norm": 1.6876933574676514,
      "learning_rate": 0.00014834535747688184,
      "loss": 1.8761,
      "step": 2250
    },
    {
      "epoch": 1.5067511251875314,
      "grad_norm": 1.0892423391342163,
      "learning_rate": 0.00014768633767467447,
      "loss": 1.7819,
      "step": 2260
    },
    {
      "epoch": 1.5134189031505252,
      "grad_norm": 1.350125789642334,
      "learning_rate": 0.00014702731787246708,
      "loss": 1.9214,
      "step": 2270
    },
    {
      "epoch": 1.520086681113519,
      "grad_norm": 1.2532914876937866,
      "learning_rate": 0.0001463682980702597,
      "loss": 1.8158,
      "step": 2280
    },
    {
      "epoch": 1.5267544590765127,
      "grad_norm": 1.9107317924499512,
      "learning_rate": 0.00014570927826805231,
      "loss": 1.8361,
      "step": 2290
    },
    {
      "epoch": 1.5334222370395065,
      "grad_norm": 1.207907795906067,
      "learning_rate": 0.00014505025846584492,
      "loss": 1.8472,
      "step": 2300
    },
    {
      "epoch": 1.5400900150025003,
      "grad_norm": 1.382454752922058,
      "learning_rate": 0.00014439123866363755,
      "loss": 1.9204,
      "step": 2310
    },
    {
      "epoch": 1.546757792965494,
      "grad_norm": 0.9634603261947632,
      "learning_rate": 0.00014373221886143018,
      "loss": 1.9015,
      "step": 2320
    },
    {
      "epoch": 1.553425570928488,
      "grad_norm": 1.2038483619689941,
      "learning_rate": 0.0001430731990592228,
      "loss": 1.9073,
      "step": 2330
    },
    {
      "epoch": 1.5600933488914819,
      "grad_norm": 1.5559492111206055,
      "learning_rate": 0.0001424141792570154,
      "loss": 1.848,
      "step": 2340
    },
    {
      "epoch": 1.5667611268544759,
      "grad_norm": 1.4605637788772583,
      "learning_rate": 0.000141755159454808,
      "loss": 1.7131,
      "step": 2350
    },
    {
      "epoch": 1.5734289048174697,
      "grad_norm": 2.1371850967407227,
      "learning_rate": 0.00014109613965260063,
      "loss": 1.7871,
      "step": 2360
    },
    {
      "epoch": 1.5800966827804634,
      "grad_norm": 1.246233582496643,
      "learning_rate": 0.00014043711985039327,
      "loss": 1.741,
      "step": 2370
    },
    {
      "epoch": 1.5867644607434572,
      "grad_norm": 1.3100839853286743,
      "learning_rate": 0.00013977810004818587,
      "loss": 1.7489,
      "step": 2380
    },
    {
      "epoch": 1.593432238706451,
      "grad_norm": 1.72970449924469,
      "learning_rate": 0.00013911908024597848,
      "loss": 1.8364,
      "step": 2390
    },
    {
      "epoch": 1.6001000166694448,
      "grad_norm": 1.3909027576446533,
      "learning_rate": 0.0001384600604437711,
      "loss": 1.8119,
      "step": 2400
    },
    {
      "epoch": 1.6067677946324388,
      "grad_norm": 1.1368650197982788,
      "learning_rate": 0.0001378010406415637,
      "loss": 1.7422,
      "step": 2410
    },
    {
      "epoch": 1.6134355725954326,
      "grad_norm": 1.789451003074646,
      "learning_rate": 0.00013714202083935635,
      "loss": 1.7262,
      "step": 2420
    },
    {
      "epoch": 1.6201033505584264,
      "grad_norm": 1.392600178718567,
      "learning_rate": 0.00013648300103714895,
      "loss": 1.8472,
      "step": 2430
    },
    {
      "epoch": 1.6267711285214204,
      "grad_norm": 1.160955548286438,
      "learning_rate": 0.00013582398123494158,
      "loss": 1.8998,
      "step": 2440
    },
    {
      "epoch": 1.6334389064844141,
      "grad_norm": 0.8701428174972534,
      "learning_rate": 0.0001351649614327342,
      "loss": 1.8954,
      "step": 2450
    },
    {
      "epoch": 1.640106684447408,
      "grad_norm": 1.4545879364013672,
      "learning_rate": 0.0001345059416305268,
      "loss": 1.8081,
      "step": 2460
    },
    {
      "epoch": 1.6467744624104017,
      "grad_norm": 1.7903599739074707,
      "learning_rate": 0.00013384692182831943,
      "loss": 1.8465,
      "step": 2470
    },
    {
      "epoch": 1.6534422403733955,
      "grad_norm": 1.0511579513549805,
      "learning_rate": 0.00013318790202611206,
      "loss": 1.8433,
      "step": 2480
    },
    {
      "epoch": 1.6601100183363893,
      "grad_norm": 1.0865164995193481,
      "learning_rate": 0.00013252888222390466,
      "loss": 1.6914,
      "step": 2490
    },
    {
      "epoch": 1.6667777962993833,
      "grad_norm": 1.036141276359558,
      "learning_rate": 0.00013186986242169727,
      "loss": 1.8552,
      "step": 2500
    },
    {
      "epoch": 1.673445574262377,
      "grad_norm": 0.9170354008674622,
      "learning_rate": 0.0001312108426194899,
      "loss": 1.869,
      "step": 2510
    },
    {
      "epoch": 1.680113352225371,
      "grad_norm": 1.682826042175293,
      "learning_rate": 0.0001305518228172825,
      "loss": 1.8265,
      "step": 2520
    },
    {
      "epoch": 1.6867811301883648,
      "grad_norm": 1.8803993463516235,
      "learning_rate": 0.00012989280301507514,
      "loss": 1.7482,
      "step": 2530
    },
    {
      "epoch": 1.6934489081513586,
      "grad_norm": 1.5184487104415894,
      "learning_rate": 0.00012923378321286774,
      "loss": 1.7901,
      "step": 2540
    },
    {
      "epoch": 1.7001166861143524,
      "grad_norm": 1.1425484418869019,
      "learning_rate": 0.00012857476341066038,
      "loss": 1.9264,
      "step": 2550
    },
    {
      "epoch": 1.7067844640773462,
      "grad_norm": 1.3184067010879517,
      "learning_rate": 0.00012791574360845298,
      "loss": 1.726,
      "step": 2560
    },
    {
      "epoch": 1.71345224204034,
      "grad_norm": 1.5733729600906372,
      "learning_rate": 0.0001272567238062456,
      "loss": 1.8018,
      "step": 2570
    },
    {
      "epoch": 1.7201200200033337,
      "grad_norm": 1.4041916131973267,
      "learning_rate": 0.00012659770400403822,
      "loss": 1.8159,
      "step": 2580
    },
    {
      "epoch": 1.7267877979663278,
      "grad_norm": 1.3677680492401123,
      "learning_rate": 0.00012593868420183085,
      "loss": 1.8802,
      "step": 2590
    },
    {
      "epoch": 1.7334555759293215,
      "grad_norm": 1.1052473783493042,
      "learning_rate": 0.00012527966439962346,
      "loss": 1.7911,
      "step": 2600
    },
    {
      "epoch": 1.7401233538923155,
      "grad_norm": 1.4599281549453735,
      "learning_rate": 0.00012462064459741606,
      "loss": 1.7642,
      "step": 2610
    },
    {
      "epoch": 1.7467911318553093,
      "grad_norm": 1.506308674812317,
      "learning_rate": 0.0001239616247952087,
      "loss": 1.8489,
      "step": 2620
    },
    {
      "epoch": 1.753458909818303,
      "grad_norm": 0.906085193157196,
      "learning_rate": 0.0001233026049930013,
      "loss": 1.7805,
      "step": 2630
    },
    {
      "epoch": 1.7601266877812969,
      "grad_norm": 1.155036211013794,
      "learning_rate": 0.00012264358519079393,
      "loss": 1.7516,
      "step": 2640
    },
    {
      "epoch": 1.7667944657442907,
      "grad_norm": 1.2073681354522705,
      "learning_rate": 0.00012198456538858654,
      "loss": 1.6891,
      "step": 2650
    },
    {
      "epoch": 1.7734622437072844,
      "grad_norm": 1.2478623390197754,
      "learning_rate": 0.00012132554558637916,
      "loss": 1.8079,
      "step": 2660
    },
    {
      "epoch": 1.7801300216702782,
      "grad_norm": 1.2548816204071045,
      "learning_rate": 0.00012066652578417178,
      "loss": 1.8617,
      "step": 2670
    },
    {
      "epoch": 1.7867977996332722,
      "grad_norm": 1.2617751359939575,
      "learning_rate": 0.0001200075059819644,
      "loss": 1.6604,
      "step": 2680
    },
    {
      "epoch": 1.793465577596266,
      "grad_norm": 1.374006986618042,
      "learning_rate": 0.000119348486179757,
      "loss": 1.7388,
      "step": 2690
    },
    {
      "epoch": 1.80013335555926,
      "grad_norm": 1.1135972738265991,
      "learning_rate": 0.00011868946637754962,
      "loss": 1.8165,
      "step": 2700
    },
    {
      "epoch": 1.8068011335222538,
      "grad_norm": 0.9319431185722351,
      "learning_rate": 0.00011803044657534225,
      "loss": 1.8601,
      "step": 2710
    },
    {
      "epoch": 1.8134689114852476,
      "grad_norm": 1.6831120252609253,
      "learning_rate": 0.00011737142677313486,
      "loss": 1.7924,
      "step": 2720
    },
    {
      "epoch": 1.8201366894482414,
      "grad_norm": 1.5857077836990356,
      "learning_rate": 0.00011671240697092747,
      "loss": 1.693,
      "step": 2730
    },
    {
      "epoch": 1.8268044674112351,
      "grad_norm": 1.501206874847412,
      "learning_rate": 0.0001160533871687201,
      "loss": 1.7677,
      "step": 2740
    },
    {
      "epoch": 1.833472245374229,
      "grad_norm": 1.3942477703094482,
      "learning_rate": 0.00011539436736651271,
      "loss": 1.7978,
      "step": 2750
    },
    {
      "epoch": 1.8401400233372227,
      "grad_norm": 1.4257580041885376,
      "learning_rate": 0.00011473534756430533,
      "loss": 1.7988,
      "step": 2760
    },
    {
      "epoch": 1.8468078013002167,
      "grad_norm": 1.5322928428649902,
      "learning_rate": 0.00011407632776209795,
      "loss": 1.8148,
      "step": 2770
    },
    {
      "epoch": 1.8534755792632105,
      "grad_norm": 1.5241197347640991,
      "learning_rate": 0.00011341730795989055,
      "loss": 1.7589,
      "step": 2780
    },
    {
      "epoch": 1.8601433572262045,
      "grad_norm": 1.262542724609375,
      "learning_rate": 0.00011275828815768319,
      "loss": 1.6215,
      "step": 2790
    },
    {
      "epoch": 1.8668111351891983,
      "grad_norm": 1.7077248096466064,
      "learning_rate": 0.00011209926835547579,
      "loss": 1.8094,
      "step": 2800
    },
    {
      "epoch": 1.873478913152192,
      "grad_norm": 2.0164616107940674,
      "learning_rate": 0.00011144024855326841,
      "loss": 1.8561,
      "step": 2810
    },
    {
      "epoch": 1.8801466911151858,
      "grad_norm": 1.3068207502365112,
      "learning_rate": 0.00011078122875106103,
      "loss": 1.8252,
      "step": 2820
    },
    {
      "epoch": 1.8868144690781796,
      "grad_norm": 2.212519407272339,
      "learning_rate": 0.00011012220894885365,
      "loss": 1.7878,
      "step": 2830
    },
    {
      "epoch": 1.8934822470411734,
      "grad_norm": 1.6116046905517578,
      "learning_rate": 0.00010946318914664627,
      "loss": 1.7704,
      "step": 2840
    },
    {
      "epoch": 1.9001500250041674,
      "grad_norm": 1.5135327577590942,
      "learning_rate": 0.00010880416934443889,
      "loss": 1.7078,
      "step": 2850
    },
    {
      "epoch": 1.9068178029671612,
      "grad_norm": 0.9711741209030151,
      "learning_rate": 0.00010814514954223149,
      "loss": 1.7994,
      "step": 2860
    },
    {
      "epoch": 1.913485580930155,
      "grad_norm": 0.7416455745697021,
      "learning_rate": 0.00010748612974002412,
      "loss": 1.751,
      "step": 2870
    },
    {
      "epoch": 1.920153358893149,
      "grad_norm": 1.2386586666107178,
      "learning_rate": 0.00010682710993781673,
      "loss": 1.7375,
      "step": 2880
    },
    {
      "epoch": 1.9268211368561428,
      "grad_norm": 1.3330355882644653,
      "learning_rate": 0.00010616809013560935,
      "loss": 1.7413,
      "step": 2890
    },
    {
      "epoch": 1.9334889148191365,
      "grad_norm": 1.0760843753814697,
      "learning_rate": 0.00010550907033340198,
      "loss": 1.7356,
      "step": 2900
    },
    {
      "epoch": 1.9401566927821303,
      "grad_norm": 1.1764254570007324,
      "learning_rate": 0.00010485005053119459,
      "loss": 1.8364,
      "step": 2910
    },
    {
      "epoch": 1.946824470745124,
      "grad_norm": 1.5269962549209595,
      "learning_rate": 0.0001041910307289872,
      "loss": 1.6508,
      "step": 2920
    },
    {
      "epoch": 1.9534922487081179,
      "grad_norm": 1.1112996339797974,
      "learning_rate": 0.00010353201092677982,
      "loss": 1.8719,
      "step": 2930
    },
    {
      "epoch": 1.9601600266711119,
      "grad_norm": 1.5215951204299927,
      "learning_rate": 0.00010287299112457244,
      "loss": 1.7453,
      "step": 2940
    },
    {
      "epoch": 1.9668278046341057,
      "grad_norm": 1.300141453742981,
      "learning_rate": 0.00010221397132236506,
      "loss": 1.8048,
      "step": 2950
    },
    {
      "epoch": 1.9734955825970997,
      "grad_norm": 1.7877404689788818,
      "learning_rate": 0.00010155495152015768,
      "loss": 1.7286,
      "step": 2960
    },
    {
      "epoch": 1.9801633605600935,
      "grad_norm": 1.644911527633667,
      "learning_rate": 0.00010089593171795029,
      "loss": 1.7811,
      "step": 2970
    },
    {
      "epoch": 1.9868311385230872,
      "grad_norm": 1.842471957206726,
      "learning_rate": 0.00010023691191574292,
      "loss": 1.69,
      "step": 2980
    },
    {
      "epoch": 1.993498916486081,
      "grad_norm": 1.0811711549758911,
      "learning_rate": 9.957789211353552e-05,
      "loss": 1.696,
      "step": 2990
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.0677368640899658,
      "learning_rate": 9.891887231132814e-05,
      "loss": 1.7308,
      "step": 3000
    },
    {
      "epoch": 2.0,
      "eval_loss": 1.7704298496246338,
      "eval_runtime": 938.7058,
      "eval_samples_per_second": 6.391,
      "eval_steps_per_second": 3.196,
      "step": 3000
    },
    {
      "epoch": 2.006667777962994,
      "grad_norm": 1.4702560901641846,
      "learning_rate": 9.825985250912076e-05,
      "loss": 1.6981,
      "step": 3010
    },
    {
      "epoch": 2.0133355559259876,
      "grad_norm": 1.5662680864334106,
      "learning_rate": 9.760083270691338e-05,
      "loss": 1.7446,
      "step": 3020
    },
    {
      "epoch": 2.0200033338889813,
      "grad_norm": 1.587164044380188,
      "learning_rate": 9.6941812904706e-05,
      "loss": 1.7681,
      "step": 3030
    },
    {
      "epoch": 2.026671111851975,
      "grad_norm": 1.1929829120635986,
      "learning_rate": 9.628279310249862e-05,
      "loss": 1.7691,
      "step": 3040
    },
    {
      "epoch": 2.0333388898149694,
      "grad_norm": 1.6363693475723267,
      "learning_rate": 9.562377330029122e-05,
      "loss": 1.7244,
      "step": 3050
    },
    {
      "epoch": 2.040006667777963,
      "grad_norm": 1.2187706232070923,
      "learning_rate": 9.496475349808385e-05,
      "loss": 1.7425,
      "step": 3060
    },
    {
      "epoch": 2.046674445740957,
      "grad_norm": 1.469229817390442,
      "learning_rate": 9.430573369587647e-05,
      "loss": 1.6937,
      "step": 3070
    },
    {
      "epoch": 2.0533422237039507,
      "grad_norm": 1.5641911029815674,
      "learning_rate": 9.364671389366908e-05,
      "loss": 1.7552,
      "step": 3080
    },
    {
      "epoch": 2.0600100016669445,
      "grad_norm": 1.2695951461791992,
      "learning_rate": 9.29876940914617e-05,
      "loss": 1.7514,
      "step": 3090
    },
    {
      "epoch": 2.0666777796299383,
      "grad_norm": 1.1341513395309448,
      "learning_rate": 9.232867428925432e-05,
      "loss": 1.7945,
      "step": 3100
    },
    {
      "epoch": 2.073345557592932,
      "grad_norm": 1.468531608581543,
      "learning_rate": 9.166965448704693e-05,
      "loss": 1.8278,
      "step": 3110
    },
    {
      "epoch": 2.080013335555926,
      "grad_norm": 1.319385051727295,
      "learning_rate": 9.101063468483955e-05,
      "loss": 1.7339,
      "step": 3120
    },
    {
      "epoch": 2.0866811135189196,
      "grad_norm": 1.7652921676635742,
      "learning_rate": 9.035161488263216e-05,
      "loss": 1.6019,
      "step": 3130
    },
    {
      "epoch": 2.093348891481914,
      "grad_norm": 1.4756451845169067,
      "learning_rate": 8.969259508042479e-05,
      "loss": 1.7061,
      "step": 3140
    },
    {
      "epoch": 2.1000166694449076,
      "grad_norm": 1.2928041219711304,
      "learning_rate": 8.903357527821741e-05,
      "loss": 1.7639,
      "step": 3150
    },
    {
      "epoch": 2.1066844474079014,
      "grad_norm": 2.143730640411377,
      "learning_rate": 8.837455547601002e-05,
      "loss": 1.6623,
      "step": 3160
    },
    {
      "epoch": 2.113352225370895,
      "grad_norm": 0.9908286333084106,
      "learning_rate": 8.771553567380263e-05,
      "loss": 1.7958,
      "step": 3170
    },
    {
      "epoch": 2.120020003333889,
      "grad_norm": 1.3162853717803955,
      "learning_rate": 8.705651587159525e-05,
      "loss": 1.6641,
      "step": 3180
    },
    {
      "epoch": 2.1266877812968827,
      "grad_norm": 1.5430599451065063,
      "learning_rate": 8.639749606938787e-05,
      "loss": 1.7193,
      "step": 3190
    },
    {
      "epoch": 2.1333555592598765,
      "grad_norm": 1.308687448501587,
      "learning_rate": 8.573847626718049e-05,
      "loss": 1.7066,
      "step": 3200
    },
    {
      "epoch": 2.1400233372228703,
      "grad_norm": 1.0817822217941284,
      "learning_rate": 8.507945646497311e-05,
      "loss": 1.7239,
      "step": 3210
    },
    {
      "epoch": 2.146691115185864,
      "grad_norm": 1.162359595298767,
      "learning_rate": 8.442043666276573e-05,
      "loss": 1.6992,
      "step": 3220
    },
    {
      "epoch": 2.1533588931488583,
      "grad_norm": 1.4332376718521118,
      "learning_rate": 8.376141686055835e-05,
      "loss": 1.7167,
      "step": 3230
    },
    {
      "epoch": 2.160026671111852,
      "grad_norm": 1.349433422088623,
      "learning_rate": 8.310239705835095e-05,
      "loss": 1.7628,
      "step": 3240
    },
    {
      "epoch": 2.166694449074846,
      "grad_norm": 1.6072967052459717,
      "learning_rate": 8.244337725614358e-05,
      "loss": 1.668,
      "step": 3250
    },
    {
      "epoch": 2.1733622270378397,
      "grad_norm": 1.5488550662994385,
      "learning_rate": 8.17843574539362e-05,
      "loss": 1.807,
      "step": 3260
    },
    {
      "epoch": 2.1800300050008334,
      "grad_norm": 1.0805110931396484,
      "learning_rate": 8.112533765172881e-05,
      "loss": 1.7094,
      "step": 3270
    },
    {
      "epoch": 2.186697782963827,
      "grad_norm": 1.199832797050476,
      "learning_rate": 8.046631784952143e-05,
      "loss": 1.7525,
      "step": 3280
    },
    {
      "epoch": 2.193365560926821,
      "grad_norm": 1.5833231210708618,
      "learning_rate": 7.980729804731405e-05,
      "loss": 1.7312,
      "step": 3290
    },
    {
      "epoch": 2.200033338889815,
      "grad_norm": 1.39691960811615,
      "learning_rate": 7.914827824510666e-05,
      "loss": 1.791,
      "step": 3300
    },
    {
      "epoch": 2.206701116852809,
      "grad_norm": 1.2379343509674072,
      "learning_rate": 7.848925844289928e-05,
      "loss": 1.8018,
      "step": 3310
    },
    {
      "epoch": 2.213368894815803,
      "grad_norm": 1.1444276571273804,
      "learning_rate": 7.783023864069189e-05,
      "loss": 1.7899,
      "step": 3320
    },
    {
      "epoch": 2.2200366727787966,
      "grad_norm": 1.1866735219955444,
      "learning_rate": 7.717121883848452e-05,
      "loss": 1.7084,
      "step": 3330
    },
    {
      "epoch": 2.2267044507417904,
      "grad_norm": 1.519243836402893,
      "learning_rate": 7.651219903627714e-05,
      "loss": 1.6418,
      "step": 3340
    },
    {
      "epoch": 2.233372228704784,
      "grad_norm": 1.2364171743392944,
      "learning_rate": 7.585317923406975e-05,
      "loss": 1.721,
      "step": 3350
    },
    {
      "epoch": 2.240040006667778,
      "grad_norm": 1.2584422826766968,
      "learning_rate": 7.519415943186236e-05,
      "loss": 1.7773,
      "step": 3360
    },
    {
      "epoch": 2.2467077846307717,
      "grad_norm": 1.459875226020813,
      "learning_rate": 7.453513962965498e-05,
      "loss": 1.7136,
      "step": 3370
    },
    {
      "epoch": 2.2533755625937655,
      "grad_norm": 1.190969467163086,
      "learning_rate": 7.38761198274476e-05,
      "loss": 1.7535,
      "step": 3380
    },
    {
      "epoch": 2.2600433405567593,
      "grad_norm": 1.4517287015914917,
      "learning_rate": 7.321710002524022e-05,
      "loss": 1.7211,
      "step": 3390
    },
    {
      "epoch": 2.266711118519753,
      "grad_norm": 1.8810197114944458,
      "learning_rate": 7.255808022303284e-05,
      "loss": 1.7273,
      "step": 3400
    },
    {
      "epoch": 2.2733788964827473,
      "grad_norm": 0.9800001978874207,
      "learning_rate": 7.189906042082544e-05,
      "loss": 1.8299,
      "step": 3410
    },
    {
      "epoch": 2.280046674445741,
      "grad_norm": 1.095727562904358,
      "learning_rate": 7.124004061861808e-05,
      "loss": 1.8017,
      "step": 3420
    },
    {
      "epoch": 2.286714452408735,
      "grad_norm": 1.5921908617019653,
      "learning_rate": 7.058102081641068e-05,
      "loss": 1.9156,
      "step": 3430
    },
    {
      "epoch": 2.2933822303717286,
      "grad_norm": 1.0455877780914307,
      "learning_rate": 6.99220010142033e-05,
      "loss": 1.8008,
      "step": 3440
    },
    {
      "epoch": 2.3000500083347224,
      "grad_norm": 2.0083072185516357,
      "learning_rate": 6.926298121199592e-05,
      "loss": 1.7392,
      "step": 3450
    },
    {
      "epoch": 2.306717786297716,
      "grad_norm": 1.3839833736419678,
      "learning_rate": 6.860396140978854e-05,
      "loss": 1.7332,
      "step": 3460
    },
    {
      "epoch": 2.31338556426071,
      "grad_norm": 1.3841931819915771,
      "learning_rate": 6.794494160758116e-05,
      "loss": 1.8415,
      "step": 3470
    },
    {
      "epoch": 2.320053342223704,
      "grad_norm": 1.5112402439117432,
      "learning_rate": 6.728592180537378e-05,
      "loss": 1.6967,
      "step": 3480
    },
    {
      "epoch": 2.326721120186698,
      "grad_norm": 1.3871252536773682,
      "learning_rate": 6.662690200316638e-05,
      "loss": 1.8075,
      "step": 3490
    },
    {
      "epoch": 2.3333888981496917,
      "grad_norm": 1.5253719091415405,
      "learning_rate": 6.596788220095901e-05,
      "loss": 1.7964,
      "step": 3500
    },
    {
      "epoch": 2.3400566761126855,
      "grad_norm": 1.3486716747283936,
      "learning_rate": 6.530886239875163e-05,
      "loss": 1.7392,
      "step": 3510
    },
    {
      "epoch": 2.3467244540756793,
      "grad_norm": 1.3338960409164429,
      "learning_rate": 6.464984259654424e-05,
      "loss": 1.6741,
      "step": 3520
    },
    {
      "epoch": 2.353392232038673,
      "grad_norm": 1.2539258003234863,
      "learning_rate": 6.399082279433687e-05,
      "loss": 1.8117,
      "step": 3530
    },
    {
      "epoch": 2.360060010001667,
      "grad_norm": 1.3879313468933105,
      "learning_rate": 6.333180299212948e-05,
      "loss": 1.7174,
      "step": 3540
    },
    {
      "epoch": 2.3667277879646607,
      "grad_norm": 1.2318371534347534,
      "learning_rate": 6.26727831899221e-05,
      "loss": 1.7628,
      "step": 3550
    },
    {
      "epoch": 2.3733955659276544,
      "grad_norm": 1.2180101871490479,
      "learning_rate": 6.201376338771471e-05,
      "loss": 1.7505,
      "step": 3560
    },
    {
      "epoch": 2.380063343890648,
      "grad_norm": 1.1772112846374512,
      "learning_rate": 6.135474358550733e-05,
      "loss": 1.6401,
      "step": 3570
    },
    {
      "epoch": 2.3867311218536424,
      "grad_norm": 1.057759404182434,
      "learning_rate": 6.0695723783299944e-05,
      "loss": 1.7614,
      "step": 3580
    },
    {
      "epoch": 2.3933988998166362,
      "grad_norm": 1.3802123069763184,
      "learning_rate": 6.003670398109257e-05,
      "loss": 1.6864,
      "step": 3590
    },
    {
      "epoch": 2.40006667777963,
      "grad_norm": 1.5884557962417603,
      "learning_rate": 5.937768417888518e-05,
      "loss": 1.5482,
      "step": 3600
    },
    {
      "epoch": 2.406734455742624,
      "grad_norm": 1.3493880033493042,
      "learning_rate": 5.87186643766778e-05,
      "loss": 1.7149,
      "step": 3610
    },
    {
      "epoch": 2.4134022337056176,
      "grad_norm": 1.3920247554779053,
      "learning_rate": 5.805964457447041e-05,
      "loss": 1.8033,
      "step": 3620
    },
    {
      "epoch": 2.4200700116686114,
      "grad_norm": 1.6033788919448853,
      "learning_rate": 5.740062477226304e-05,
      "loss": 1.7906,
      "step": 3630
    },
    {
      "epoch": 2.426737789631605,
      "grad_norm": 1.3270719051361084,
      "learning_rate": 5.674160497005565e-05,
      "loss": 1.7696,
      "step": 3640
    },
    {
      "epoch": 2.433405567594599,
      "grad_norm": 1.331860899925232,
      "learning_rate": 5.608258516784827e-05,
      "loss": 1.6649,
      "step": 3650
    },
    {
      "epoch": 2.440073345557593,
      "grad_norm": 0.9855476021766663,
      "learning_rate": 5.542356536564088e-05,
      "loss": 1.7642,
      "step": 3660
    },
    {
      "epoch": 2.446741123520587,
      "grad_norm": 1.350673794746399,
      "learning_rate": 5.4764545563433506e-05,
      "loss": 1.8004,
      "step": 3670
    },
    {
      "epoch": 2.4534089014835807,
      "grad_norm": 1.1088407039642334,
      "learning_rate": 5.410552576122612e-05,
      "loss": 1.7371,
      "step": 3680
    },
    {
      "epoch": 2.4600766794465745,
      "grad_norm": 1.5236295461654663,
      "learning_rate": 5.344650595901874e-05,
      "loss": 1.72,
      "step": 3690
    },
    {
      "epoch": 2.4667444574095683,
      "grad_norm": 1.1948684453964233,
      "learning_rate": 5.278748615681135e-05,
      "loss": 1.6874,
      "step": 3700
    },
    {
      "epoch": 2.473412235372562,
      "grad_norm": 1.1309276819229126,
      "learning_rate": 5.2128466354603975e-05,
      "loss": 1.8138,
      "step": 3710
    },
    {
      "epoch": 2.480080013335556,
      "grad_norm": 1.3508813381195068,
      "learning_rate": 5.146944655239659e-05,
      "loss": 1.7339,
      "step": 3720
    },
    {
      "epoch": 2.4867477912985496,
      "grad_norm": 1.4406965970993042,
      "learning_rate": 5.0810426750189206e-05,
      "loss": 1.8075,
      "step": 3730
    },
    {
      "epoch": 2.4934155692615434,
      "grad_norm": 1.5296725034713745,
      "learning_rate": 5.015140694798182e-05,
      "loss": 1.7377,
      "step": 3740
    },
    {
      "epoch": 2.500083347224537,
      "grad_norm": 1.7305198907852173,
      "learning_rate": 4.949238714577444e-05,
      "loss": 1.6349,
      "step": 3750
    },
    {
      "epoch": 2.5067511251875314,
      "grad_norm": 1.10476553440094,
      "learning_rate": 4.8833367343567055e-05,
      "loss": 1.707,
      "step": 3760
    },
    {
      "epoch": 2.513418903150525,
      "grad_norm": 1.399092435836792,
      "learning_rate": 4.8174347541359674e-05,
      "loss": 1.7167,
      "step": 3770
    },
    {
      "epoch": 2.520086681113519,
      "grad_norm": 1.2648770809173584,
      "learning_rate": 4.751532773915229e-05,
      "loss": 1.7311,
      "step": 3780
    },
    {
      "epoch": 2.5267544590765127,
      "grad_norm": 1.482833743095398,
      "learning_rate": 4.685630793694491e-05,
      "loss": 1.7817,
      "step": 3790
    },
    {
      "epoch": 2.5334222370395065,
      "grad_norm": 1.0671570301055908,
      "learning_rate": 4.6197288134737524e-05,
      "loss": 1.6834,
      "step": 3800
    },
    {
      "epoch": 2.5400900150025003,
      "grad_norm": 0.8318336009979248,
      "learning_rate": 4.553826833253014e-05,
      "loss": 1.7622,
      "step": 3810
    },
    {
      "epoch": 2.546757792965494,
      "grad_norm": 1.3416470289230347,
      "learning_rate": 4.487924853032276e-05,
      "loss": 1.7205,
      "step": 3820
    },
    {
      "epoch": 2.5534255709284883,
      "grad_norm": 1.1320196390151978,
      "learning_rate": 4.422022872811538e-05,
      "loss": 1.7183,
      "step": 3830
    },
    {
      "epoch": 2.560093348891482,
      "grad_norm": 1.6095948219299316,
      "learning_rate": 4.3561208925908e-05,
      "loss": 1.792,
      "step": 3840
    },
    {
      "epoch": 2.566761126854476,
      "grad_norm": 1.370827555656433,
      "learning_rate": 4.290218912370061e-05,
      "loss": 1.7184,
      "step": 3850
    },
    {
      "epoch": 2.5734289048174697,
      "grad_norm": 1.5216604471206665,
      "learning_rate": 4.2243169321493236e-05,
      "loss": 1.59,
      "step": 3860
    },
    {
      "epoch": 2.5800966827804634,
      "grad_norm": 1.6831499338150024,
      "learning_rate": 4.158414951928585e-05,
      "loss": 1.7887,
      "step": 3870
    },
    {
      "epoch": 2.5867644607434572,
      "grad_norm": 1.137139916419983,
      "learning_rate": 4.092512971707847e-05,
      "loss": 1.694,
      "step": 3880
    },
    {
      "epoch": 2.593432238706451,
      "grad_norm": 1.561358094215393,
      "learning_rate": 4.026610991487108e-05,
      "loss": 1.7002,
      "step": 3890
    },
    {
      "epoch": 2.600100016669445,
      "grad_norm": 1.4728471040725708,
      "learning_rate": 3.9607090112663705e-05,
      "loss": 1.6771,
      "step": 3900
    },
    {
      "epoch": 2.6067677946324386,
      "grad_norm": 1.1627354621887207,
      "learning_rate": 3.894807031045632e-05,
      "loss": 1.7222,
      "step": 3910
    },
    {
      "epoch": 2.6134355725954324,
      "grad_norm": 1.3042993545532227,
      "learning_rate": 3.8289050508248936e-05,
      "loss": 1.7335,
      "step": 3920
    },
    {
      "epoch": 2.620103350558426,
      "grad_norm": 0.9659577012062073,
      "learning_rate": 3.763003070604155e-05,
      "loss": 1.776,
      "step": 3930
    },
    {
      "epoch": 2.6267711285214204,
      "grad_norm": 1.4159433841705322,
      "learning_rate": 3.6971010903834167e-05,
      "loss": 1.778,
      "step": 3940
    },
    {
      "epoch": 2.633438906484414,
      "grad_norm": 1.3220936059951782,
      "learning_rate": 3.6311991101626785e-05,
      "loss": 1.677,
      "step": 3950
    },
    {
      "epoch": 2.640106684447408,
      "grad_norm": 1.126364827156067,
      "learning_rate": 3.5652971299419404e-05,
      "loss": 1.7115,
      "step": 3960
    },
    {
      "epoch": 2.6467744624104017,
      "grad_norm": 1.3240220546722412,
      "learning_rate": 3.499395149721202e-05,
      "loss": 1.6299,
      "step": 3970
    },
    {
      "epoch": 2.6534422403733955,
      "grad_norm": 1.2891520261764526,
      "learning_rate": 3.4334931695004635e-05,
      "loss": 1.6653,
      "step": 3980
    },
    {
      "epoch": 2.6601100183363893,
      "grad_norm": 1.2019180059432983,
      "learning_rate": 3.367591189279726e-05,
      "loss": 1.61,
      "step": 3990
    },
    {
      "epoch": 2.6667777962993835,
      "grad_norm": 1.3045960664749146,
      "learning_rate": 3.301689209058987e-05,
      "loss": 1.677,
      "step": 4000
    },
    {
      "epoch": 2.6734455742623773,
      "grad_norm": 1.3691377639770508,
      "learning_rate": 3.235787228838249e-05,
      "loss": 1.6871,
      "step": 4010
    },
    {
      "epoch": 2.680113352225371,
      "grad_norm": 1.2910363674163818,
      "learning_rate": 3.169885248617511e-05,
      "loss": 1.6624,
      "step": 4020
    },
    {
      "epoch": 2.686781130188365,
      "grad_norm": 1.1968133449554443,
      "learning_rate": 3.103983268396773e-05,
      "loss": 1.7702,
      "step": 4030
    },
    {
      "epoch": 2.6934489081513586,
      "grad_norm": 1.5577061176300049,
      "learning_rate": 3.0380812881760344e-05,
      "loss": 1.7629,
      "step": 4040
    },
    {
      "epoch": 2.7001166861143524,
      "grad_norm": 1.2419660091400146,
      "learning_rate": 2.972179307955296e-05,
      "loss": 1.8342,
      "step": 4050
    },
    {
      "epoch": 2.706784464077346,
      "grad_norm": 1.3800675868988037,
      "learning_rate": 2.906277327734558e-05,
      "loss": 1.7146,
      "step": 4060
    },
    {
      "epoch": 2.71345224204034,
      "grad_norm": 1.5581070184707642,
      "learning_rate": 2.8403753475138194e-05,
      "loss": 1.7909,
      "step": 4070
    },
    {
      "epoch": 2.7201200200033337,
      "grad_norm": 1.4503706693649292,
      "learning_rate": 2.7744733672930813e-05,
      "loss": 1.7426,
      "step": 4080
    },
    {
      "epoch": 2.7267877979663275,
      "grad_norm": 1.1574171781539917,
      "learning_rate": 2.7085713870723428e-05,
      "loss": 1.6866,
      "step": 4090
    },
    {
      "epoch": 2.7334555759293213,
      "grad_norm": 1.2824703454971313,
      "learning_rate": 2.6426694068516047e-05,
      "loss": 1.6706,
      "step": 4100
    },
    {
      "epoch": 2.7401233538923155,
      "grad_norm": 1.314833641052246,
      "learning_rate": 2.5767674266308666e-05,
      "loss": 1.7528,
      "step": 4110
    },
    {
      "epoch": 2.7467911318553093,
      "grad_norm": 1.21599543094635,
      "learning_rate": 2.510865446410128e-05,
      "loss": 1.7153,
      "step": 4120
    },
    {
      "epoch": 2.753458909818303,
      "grad_norm": 1.2252146005630493,
      "learning_rate": 2.44496346618939e-05,
      "loss": 1.7258,
      "step": 4130
    },
    {
      "epoch": 2.760126687781297,
      "grad_norm": 0.9300530552864075,
      "learning_rate": 2.3790614859686515e-05,
      "loss": 1.7102,
      "step": 4140
    },
    {
      "epoch": 2.7667944657442907,
      "grad_norm": 1.7498046159744263,
      "learning_rate": 2.3131595057479134e-05,
      "loss": 1.6759,
      "step": 4150
    },
    {
      "epoch": 2.7734622437072844,
      "grad_norm": 1.939148187637329,
      "learning_rate": 2.247257525527175e-05,
      "loss": 1.7364,
      "step": 4160
    },
    {
      "epoch": 2.7801300216702782,
      "grad_norm": 1.697322964668274,
      "learning_rate": 2.181355545306437e-05,
      "loss": 1.7762,
      "step": 4170
    },
    {
      "epoch": 2.7867977996332725,
      "grad_norm": 1.502873420715332,
      "learning_rate": 2.1154535650856984e-05,
      "loss": 1.766,
      "step": 4180
    },
    {
      "epoch": 2.7934655775962662,
      "grad_norm": 1.3132370710372925,
      "learning_rate": 2.0495515848649603e-05,
      "loss": 1.7065,
      "step": 4190
    },
    {
      "epoch": 2.80013335555926,
      "grad_norm": 1.4975224733352661,
      "learning_rate": 1.9836496046442218e-05,
      "loss": 1.6151,
      "step": 4200
    },
    {
      "epoch": 2.806801133522254,
      "grad_norm": 1.5022845268249512,
      "learning_rate": 1.9177476244234837e-05,
      "loss": 1.8004,
      "step": 4210
    },
    {
      "epoch": 2.8134689114852476,
      "grad_norm": 1.5636537075042725,
      "learning_rate": 1.8518456442027452e-05,
      "loss": 1.6693,
      "step": 4220
    },
    {
      "epoch": 2.8201366894482414,
      "grad_norm": 1.5164765119552612,
      "learning_rate": 1.785943663982007e-05,
      "loss": 1.7672,
      "step": 4230
    },
    {
      "epoch": 2.826804467411235,
      "grad_norm": 1.8107458353042603,
      "learning_rate": 1.720041683761269e-05,
      "loss": 1.6654,
      "step": 4240
    },
    {
      "epoch": 2.833472245374229,
      "grad_norm": 1.05283522605896,
      "learning_rate": 1.6541397035405305e-05,
      "loss": 1.638,
      "step": 4250
    },
    {
      "epoch": 2.8401400233372227,
      "grad_norm": 1.4070842266082764,
      "learning_rate": 1.5882377233197924e-05,
      "loss": 1.6087,
      "step": 4260
    },
    {
      "epoch": 2.8468078013002165,
      "grad_norm": 1.339499831199646,
      "learning_rate": 1.5223357430990541e-05,
      "loss": 1.7224,
      "step": 4270
    },
    {
      "epoch": 2.8534755792632103,
      "grad_norm": 1.4595078229904175,
      "learning_rate": 1.4564337628783158e-05,
      "loss": 1.6115,
      "step": 4280
    },
    {
      "epoch": 2.8601433572262045,
      "grad_norm": 1.2317192554473877,
      "learning_rate": 1.3905317826575775e-05,
      "loss": 1.5919,
      "step": 4290
    },
    {
      "epoch": 2.8668111351891983,
      "grad_norm": 1.7904547452926636,
      "learning_rate": 1.3246298024368393e-05,
      "loss": 1.7134,
      "step": 4300
    },
    {
      "epoch": 2.873478913152192,
      "grad_norm": 1.293461799621582,
      "learning_rate": 1.258727822216101e-05,
      "loss": 1.8295,
      "step": 4310
    },
    {
      "epoch": 2.880146691115186,
      "grad_norm": 1.2034872770309448,
      "learning_rate": 1.1928258419953627e-05,
      "loss": 1.702,
      "step": 4320
    },
    {
      "epoch": 2.8868144690781796,
      "grad_norm": 1.2416203022003174,
      "learning_rate": 1.1269238617746244e-05,
      "loss": 1.67,
      "step": 4330
    },
    {
      "epoch": 2.8934822470411734,
      "grad_norm": 1.3858879804611206,
      "learning_rate": 1.0610218815538861e-05,
      "loss": 1.7134,
      "step": 4340
    },
    {
      "epoch": 2.9001500250041676,
      "grad_norm": 1.3642828464508057,
      "learning_rate": 9.951199013331478e-06,
      "loss": 1.5822,
      "step": 4350
    },
    {
      "epoch": 2.9068178029671614,
      "grad_norm": 1.3281519412994385,
      "learning_rate": 9.292179211124095e-06,
      "loss": 1.7035,
      "step": 4360
    },
    {
      "epoch": 2.913485580930155,
      "grad_norm": 1.4051448106765747,
      "learning_rate": 8.633159408916714e-06,
      "loss": 1.7403,
      "step": 4370
    },
    {
      "epoch": 2.920153358893149,
      "grad_norm": 1.5235862731933594,
      "learning_rate": 7.974139606709331e-06,
      "loss": 1.7277,
      "step": 4380
    },
    {
      "epoch": 2.9268211368561428,
      "grad_norm": 0.8591926097869873,
      "learning_rate": 7.315119804501948e-06,
      "loss": 1.738,
      "step": 4390
    },
    {
      "epoch": 2.9334889148191365,
      "grad_norm": 1.1874583959579468,
      "learning_rate": 6.656100002294565e-06,
      "loss": 1.6992,
      "step": 4400
    },
    {
      "epoch": 2.9401566927821303,
      "grad_norm": 0.9932998418807983,
      "learning_rate": 5.9970802000871825e-06,
      "loss": 1.7817,
      "step": 4410
    },
    {
      "epoch": 2.946824470745124,
      "grad_norm": 1.162232756614685,
      "learning_rate": 5.3380603978797996e-06,
      "loss": 1.7155,
      "step": 4420
    },
    {
      "epoch": 2.953492248708118,
      "grad_norm": 1.80790376663208,
      "learning_rate": 4.6790405956724175e-06,
      "loss": 1.694,
      "step": 4430
    },
    {
      "epoch": 2.9601600266711117,
      "grad_norm": 1.4972960948944092,
      "learning_rate": 4.020020793465035e-06,
      "loss": 1.6788,
      "step": 4440
    },
    {
      "epoch": 2.9668278046341054,
      "grad_norm": 1.189021348953247,
      "learning_rate": 3.361000991257652e-06,
      "loss": 1.834,
      "step": 4450
    },
    {
      "epoch": 2.9734955825970997,
      "grad_norm": 1.2303272485733032,
      "learning_rate": 2.7019811890502693e-06,
      "loss": 1.7673,
      "step": 4460
    },
    {
      "epoch": 2.9801633605600935,
      "grad_norm": 1.0029988288879395,
      "learning_rate": 2.0429613868428864e-06,
      "loss": 1.7185,
      "step": 4470
    },
    {
      "epoch": 2.9868311385230872,
      "grad_norm": 1.0456085205078125,
      "learning_rate": 1.3839415846355037e-06,
      "loss": 1.718,
      "step": 4480
    },
    {
      "epoch": 2.993498916486081,
      "grad_norm": 1.3000534772872925,
      "learning_rate": 7.24921782428121e-07,
      "loss": 1.6734,
      "step": 4490
    },
    {
      "epoch": 3.0,
      "grad_norm": 1.38356614112854,
      "learning_rate": 6.590198022073828e-08,
      "loss": 1.7723,
      "step": 4500
    },
    {
      "epoch": 3.0,
      "eval_loss": 1.7190537452697754,
      "eval_runtime": 938.5204,
      "eval_samples_per_second": 6.392,
      "eval_steps_per_second": 3.197,
      "step": 4500
    }
  ],
  "logging_steps": 10,
  "max_steps": 4500,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "EarlyStoppingCallback": {
      "args": {
        "early_stopping_patience": 2,
        "early_stopping_threshold": 0.0
      },
      "attributes": {
        "early_stopping_patience_counter": 0
      }
    },
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 3.1277219787413914e+18,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
