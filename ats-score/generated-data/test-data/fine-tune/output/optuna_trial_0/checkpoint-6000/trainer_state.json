{
  "best_global_step": 6000,
  "best_metric": 1.5967330932617188,
  "best_model_checkpoint": "json_outputs_all_data/fine-tune/optuna_output/optuna_trial_0/checkpoint-6000",
  "epoch": 4.0,
  "eval_steps": 500,
  "global_step": 6000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.006667777962993832,
      "grad_norm": 0.32810989022254944,
      "learning_rate": 0.0004183632092588223,
      "loss": 2.6306,
      "step": 10
    },
    {
      "epoch": 0.013335555925987664,
      "grad_norm": 0.38575780391693115,
      "learning_rate": 0.00041766488976414885,
      "loss": 2.4492,
      "step": 20
    },
    {
      "epoch": 0.020003333888981498,
      "grad_norm": 0.29914987087249756,
      "learning_rate": 0.0004169665702694755,
      "loss": 2.3976,
      "step": 30
    },
    {
      "epoch": 0.026671111851975328,
      "grad_norm": 0.30259764194488525,
      "learning_rate": 0.0004162682507748021,
      "loss": 2.4204,
      "step": 40
    },
    {
      "epoch": 0.03333888981496916,
      "grad_norm": 0.28018254041671753,
      "learning_rate": 0.0004155699312801287,
      "loss": 2.3761,
      "step": 50
    },
    {
      "epoch": 0.040006667777962995,
      "grad_norm": 0.31873196363449097,
      "learning_rate": 0.00041487161178545534,
      "loss": 2.3674,
      "step": 60
    },
    {
      "epoch": 0.046674445740956826,
      "grad_norm": 0.2887691259384155,
      "learning_rate": 0.00041417329229078196,
      "loss": 2.3386,
      "step": 70
    },
    {
      "epoch": 0.053342223703950656,
      "grad_norm": 0.3059695363044739,
      "learning_rate": 0.0004134749727961086,
      "loss": 2.3952,
      "step": 80
    },
    {
      "epoch": 0.06001000166694449,
      "grad_norm": 0.3198026120662689,
      "learning_rate": 0.0004127766533014352,
      "loss": 2.3648,
      "step": 90
    },
    {
      "epoch": 0.06667777962993832,
      "grad_norm": 0.350125253200531,
      "learning_rate": 0.0004120783338067618,
      "loss": 2.3483,
      "step": 100
    },
    {
      "epoch": 0.07334555759293215,
      "grad_norm": 0.3463718593120575,
      "learning_rate": 0.00041138001431208844,
      "loss": 2.3559,
      "step": 110
    },
    {
      "epoch": 0.08001333555592599,
      "grad_norm": 0.3567451536655426,
      "learning_rate": 0.00041068169481741506,
      "loss": 2.3661,
      "step": 120
    },
    {
      "epoch": 0.08668111351891981,
      "grad_norm": 0.34567245841026306,
      "learning_rate": 0.0004099833753227417,
      "loss": 2.3107,
      "step": 130
    },
    {
      "epoch": 0.09334889148191365,
      "grad_norm": 0.32749462127685547,
      "learning_rate": 0.0004092850558280683,
      "loss": 2.3387,
      "step": 140
    },
    {
      "epoch": 0.10001666944490749,
      "grad_norm": 0.345060259103775,
      "learning_rate": 0.00040858673633339487,
      "loss": 2.3268,
      "step": 150
    },
    {
      "epoch": 0.10668444740790131,
      "grad_norm": 0.37619733810424805,
      "learning_rate": 0.00040788841683872155,
      "loss": 2.2983,
      "step": 160
    },
    {
      "epoch": 0.11335222537089515,
      "grad_norm": 0.36205825209617615,
      "learning_rate": 0.00040719009734404817,
      "loss": 2.2552,
      "step": 170
    },
    {
      "epoch": 0.12002000333388899,
      "grad_norm": 0.3842182457447052,
      "learning_rate": 0.00040649177784937474,
      "loss": 2.2931,
      "step": 180
    },
    {
      "epoch": 0.12668778129688282,
      "grad_norm": 0.354936808347702,
      "learning_rate": 0.0004057934583547014,
      "loss": 2.2833,
      "step": 190
    },
    {
      "epoch": 0.13335555925987663,
      "grad_norm": 0.37913838028907776,
      "learning_rate": 0.00040509513886002803,
      "loss": 2.2526,
      "step": 200
    },
    {
      "epoch": 0.14002333722287047,
      "grad_norm": 0.3786057233810425,
      "learning_rate": 0.0004043968193653546,
      "loss": 2.282,
      "step": 210
    },
    {
      "epoch": 0.1466911151858643,
      "grad_norm": 0.4281308948993683,
      "learning_rate": 0.0004036984998706813,
      "loss": 2.3074,
      "step": 220
    },
    {
      "epoch": 0.15335889314885814,
      "grad_norm": 0.41049009561538696,
      "learning_rate": 0.00040300018037600784,
      "loss": 2.2431,
      "step": 230
    },
    {
      "epoch": 0.16002667111185198,
      "grad_norm": 0.3956005871295929,
      "learning_rate": 0.00040230186088133446,
      "loss": 2.2352,
      "step": 240
    },
    {
      "epoch": 0.16669444907484582,
      "grad_norm": 0.41442716121673584,
      "learning_rate": 0.00040160354138666114,
      "loss": 2.2372,
      "step": 250
    },
    {
      "epoch": 0.17336222703783963,
      "grad_norm": 0.49533042311668396,
      "learning_rate": 0.0004009052218919877,
      "loss": 2.231,
      "step": 260
    },
    {
      "epoch": 0.18003000500083347,
      "grad_norm": 0.39491748809814453,
      "learning_rate": 0.0004002069023973144,
      "loss": 2.3096,
      "step": 270
    },
    {
      "epoch": 0.1866977829638273,
      "grad_norm": 0.4078337550163269,
      "learning_rate": 0.00039950858290264095,
      "loss": 2.3441,
      "step": 280
    },
    {
      "epoch": 0.19336556092682114,
      "grad_norm": 0.4038297235965729,
      "learning_rate": 0.00039881026340796757,
      "loss": 2.2545,
      "step": 290
    },
    {
      "epoch": 0.20003333888981498,
      "grad_norm": 0.448860764503479,
      "learning_rate": 0.00039811194391329424,
      "loss": 2.2519,
      "step": 300
    },
    {
      "epoch": 0.2067011168528088,
      "grad_norm": 0.47106054425239563,
      "learning_rate": 0.0003974136244186208,
      "loss": 2.2188,
      "step": 310
    },
    {
      "epoch": 0.21336889481580262,
      "grad_norm": 0.4645620882511139,
      "learning_rate": 0.00039671530492394743,
      "loss": 2.2463,
      "step": 320
    },
    {
      "epoch": 0.22003667277879646,
      "grad_norm": 0.45466455817222595,
      "learning_rate": 0.0003960169854292741,
      "loss": 2.1975,
      "step": 330
    },
    {
      "epoch": 0.2267044507417903,
      "grad_norm": 0.4653702676296234,
      "learning_rate": 0.0003953186659346007,
      "loss": 2.1957,
      "step": 340
    },
    {
      "epoch": 0.23337222870478413,
      "grad_norm": 0.5884807109832764,
      "learning_rate": 0.0003946203464399273,
      "loss": 2.1986,
      "step": 350
    },
    {
      "epoch": 0.24004000666777797,
      "grad_norm": 0.4762730598449707,
      "learning_rate": 0.0003939220269452539,
      "loss": 2.2525,
      "step": 360
    },
    {
      "epoch": 0.24670778463077178,
      "grad_norm": 0.4369903802871704,
      "learning_rate": 0.00039322370745058054,
      "loss": 2.2638,
      "step": 370
    },
    {
      "epoch": 0.25337556259376565,
      "grad_norm": 0.4950580596923828,
      "learning_rate": 0.00039252538795590716,
      "loss": 2.2303,
      "step": 380
    },
    {
      "epoch": 0.2600433405567595,
      "grad_norm": 0.5207235813140869,
      "learning_rate": 0.0003918270684612338,
      "loss": 2.1824,
      "step": 390
    },
    {
      "epoch": 0.26671111851975327,
      "grad_norm": 0.7006320357322693,
      "learning_rate": 0.0003911287489665604,
      "loss": 2.2091,
      "step": 400
    },
    {
      "epoch": 0.2733788964827471,
      "grad_norm": 0.6825000047683716,
      "learning_rate": 0.000390430429471887,
      "loss": 2.1604,
      "step": 410
    },
    {
      "epoch": 0.28004667444574094,
      "grad_norm": 0.6855624914169312,
      "learning_rate": 0.00038973210997721364,
      "loss": 2.1482,
      "step": 420
    },
    {
      "epoch": 0.2867144524087348,
      "grad_norm": 0.7572834491729736,
      "learning_rate": 0.00038903379048254027,
      "loss": 2.1613,
      "step": 430
    },
    {
      "epoch": 0.2933822303717286,
      "grad_norm": 0.5885182619094849,
      "learning_rate": 0.0003883354709878669,
      "loss": 2.1799,
      "step": 440
    },
    {
      "epoch": 0.30005000833472245,
      "grad_norm": 0.6374354362487793,
      "learning_rate": 0.0003876371514931935,
      "loss": 2.2201,
      "step": 450
    },
    {
      "epoch": 0.3067177862977163,
      "grad_norm": 0.6098117232322693,
      "learning_rate": 0.00038693883199852013,
      "loss": 2.2253,
      "step": 460
    },
    {
      "epoch": 0.3133855642607101,
      "grad_norm": 0.5834445953369141,
      "learning_rate": 0.0003862405125038467,
      "loss": 2.2315,
      "step": 470
    },
    {
      "epoch": 0.32005334222370396,
      "grad_norm": 0.5384225845336914,
      "learning_rate": 0.00038554219300917337,
      "loss": 2.1372,
      "step": 480
    },
    {
      "epoch": 0.3267211201866978,
      "grad_norm": 0.6369578242301941,
      "learning_rate": 0.0003848438735145,
      "loss": 2.1645,
      "step": 490
    },
    {
      "epoch": 0.33338889814969164,
      "grad_norm": 0.6250157356262207,
      "learning_rate": 0.00038414555401982656,
      "loss": 2.186,
      "step": 500
    },
    {
      "epoch": 0.3400566761126855,
      "grad_norm": 0.586298942565918,
      "learning_rate": 0.00038344723452515323,
      "loss": 2.1783,
      "step": 510
    },
    {
      "epoch": 0.34672445407567926,
      "grad_norm": 0.7099979519844055,
      "learning_rate": 0.0003827489150304798,
      "loss": 2.1215,
      "step": 520
    },
    {
      "epoch": 0.3533922320386731,
      "grad_norm": 0.6554271578788757,
      "learning_rate": 0.0003820505955358065,
      "loss": 2.1365,
      "step": 530
    },
    {
      "epoch": 0.36006001000166693,
      "grad_norm": 0.7016919255256653,
      "learning_rate": 0.0003813522760411331,
      "loss": 2.0764,
      "step": 540
    },
    {
      "epoch": 0.36672778796466077,
      "grad_norm": 0.5235919952392578,
      "learning_rate": 0.00038065395654645966,
      "loss": 2.1106,
      "step": 550
    },
    {
      "epoch": 0.3733955659276546,
      "grad_norm": 0.561679482460022,
      "learning_rate": 0.00037995563705178634,
      "loss": 2.0048,
      "step": 560
    },
    {
      "epoch": 0.38006334389064844,
      "grad_norm": 0.7099279165267944,
      "learning_rate": 0.00037925731755711296,
      "loss": 2.1273,
      "step": 570
    },
    {
      "epoch": 0.3867311218536423,
      "grad_norm": 0.49306848645210266,
      "learning_rate": 0.00037855899806243953,
      "loss": 2.1486,
      "step": 580
    },
    {
      "epoch": 0.3933988998166361,
      "grad_norm": 0.47975417971611023,
      "learning_rate": 0.0003778606785677662,
      "loss": 2.1402,
      "step": 590
    },
    {
      "epoch": 0.40006667777962995,
      "grad_norm": 0.5790997743606567,
      "learning_rate": 0.00037716235907309277,
      "loss": 2.0494,
      "step": 600
    },
    {
      "epoch": 0.4067344557426238,
      "grad_norm": 0.7290424108505249,
      "learning_rate": 0.0003764640395784194,
      "loss": 2.1285,
      "step": 610
    },
    {
      "epoch": 0.4134022337056176,
      "grad_norm": 0.7098791003227234,
      "learning_rate": 0.00037576572008374607,
      "loss": 2.0679,
      "step": 620
    },
    {
      "epoch": 0.4200700116686114,
      "grad_norm": 0.929844856262207,
      "learning_rate": 0.00037506740058907263,
      "loss": 2.1115,
      "step": 630
    },
    {
      "epoch": 0.42673778963160525,
      "grad_norm": 0.7417595386505127,
      "learning_rate": 0.00037436908109439926,
      "loss": 2.0869,
      "step": 640
    },
    {
      "epoch": 0.4334055675945991,
      "grad_norm": 0.7283481955528259,
      "learning_rate": 0.00037367076159972593,
      "loss": 2.1021,
      "step": 650
    },
    {
      "epoch": 0.4400733455575929,
      "grad_norm": 0.5873082876205444,
      "learning_rate": 0.0003729724421050525,
      "loss": 2.1104,
      "step": 660
    },
    {
      "epoch": 0.44674112352058676,
      "grad_norm": 0.99212247133255,
      "learning_rate": 0.0003722741226103791,
      "loss": 2.0312,
      "step": 670
    },
    {
      "epoch": 0.4534089014835806,
      "grad_norm": 0.5249514579772949,
      "learning_rate": 0.00037157580311570574,
      "loss": 2.103,
      "step": 680
    },
    {
      "epoch": 0.46007667944657443,
      "grad_norm": 1.0053930282592773,
      "learning_rate": 0.00037087748362103236,
      "loss": 2.0315,
      "step": 690
    },
    {
      "epoch": 0.46674445740956827,
      "grad_norm": 0.846792995929718,
      "learning_rate": 0.000370179164126359,
      "loss": 2.0345,
      "step": 700
    },
    {
      "epoch": 0.4734122353725621,
      "grad_norm": 0.9712527990341187,
      "learning_rate": 0.0003694808446316856,
      "loss": 2.0739,
      "step": 710
    },
    {
      "epoch": 0.48008001333555594,
      "grad_norm": 0.6780537962913513,
      "learning_rate": 0.0003687825251370122,
      "loss": 1.989,
      "step": 720
    },
    {
      "epoch": 0.4867477912985498,
      "grad_norm": 0.6858667731285095,
      "learning_rate": 0.00036808420564233885,
      "loss": 2.1061,
      "step": 730
    },
    {
      "epoch": 0.49341556926154356,
      "grad_norm": 0.9634599685668945,
      "learning_rate": 0.00036738588614766547,
      "loss": 2.0732,
      "step": 740
    },
    {
      "epoch": 0.5000833472245374,
      "grad_norm": 0.7135239243507385,
      "learning_rate": 0.0003666875666529921,
      "loss": 2.0307,
      "step": 750
    },
    {
      "epoch": 0.5067511251875313,
      "grad_norm": 0.7177687287330627,
      "learning_rate": 0.0003659892471583187,
      "loss": 2.078,
      "step": 760
    },
    {
      "epoch": 0.5134189031505251,
      "grad_norm": 0.7278617024421692,
      "learning_rate": 0.00036529092766364533,
      "loss": 1.9843,
      "step": 770
    },
    {
      "epoch": 0.520086681113519,
      "grad_norm": 0.725906491279602,
      "learning_rate": 0.00036459260816897195,
      "loss": 2.0316,
      "step": 780
    },
    {
      "epoch": 0.5267544590765127,
      "grad_norm": 0.6153966784477234,
      "learning_rate": 0.00036389428867429857,
      "loss": 2.0589,
      "step": 790
    },
    {
      "epoch": 0.5334222370395065,
      "grad_norm": 1.0646175146102905,
      "learning_rate": 0.0003631959691796252,
      "loss": 2.0434,
      "step": 800
    },
    {
      "epoch": 0.5400900150025004,
      "grad_norm": 0.6041815280914307,
      "learning_rate": 0.0003624976496849518,
      "loss": 2.1391,
      "step": 810
    },
    {
      "epoch": 0.5467577929654942,
      "grad_norm": 0.6841614842414856,
      "learning_rate": 0.00036179933019027844,
      "loss": 1.9572,
      "step": 820
    },
    {
      "epoch": 0.5534255709284881,
      "grad_norm": 0.7420514822006226,
      "learning_rate": 0.00036110101069560506,
      "loss": 2.0266,
      "step": 830
    },
    {
      "epoch": 0.5600933488914819,
      "grad_norm": 0.7249805927276611,
      "learning_rate": 0.0003604026912009316,
      "loss": 2.042,
      "step": 840
    },
    {
      "epoch": 0.5667611268544758,
      "grad_norm": 0.6854544878005981,
      "learning_rate": 0.0003597043717062583,
      "loss": 1.9993,
      "step": 850
    },
    {
      "epoch": 0.5734289048174696,
      "grad_norm": 1.0282933712005615,
      "learning_rate": 0.0003590060522115849,
      "loss": 2.0001,
      "step": 860
    },
    {
      "epoch": 0.5800966827804634,
      "grad_norm": 0.733170211315155,
      "learning_rate": 0.0003583077327169115,
      "loss": 2.0618,
      "step": 870
    },
    {
      "epoch": 0.5867644607434572,
      "grad_norm": 0.9659358859062195,
      "learning_rate": 0.00035760941322223816,
      "loss": 1.8838,
      "step": 880
    },
    {
      "epoch": 0.5934322387064511,
      "grad_norm": 0.597634494304657,
      "learning_rate": 0.0003569110937275648,
      "loss": 2.0124,
      "step": 890
    },
    {
      "epoch": 0.6001000166694449,
      "grad_norm": 0.5791945457458496,
      "learning_rate": 0.00035621277423289135,
      "loss": 2.0031,
      "step": 900
    },
    {
      "epoch": 0.6067677946324388,
      "grad_norm": 0.8383742570877075,
      "learning_rate": 0.000355514454738218,
      "loss": 1.9639,
      "step": 910
    },
    {
      "epoch": 0.6134355725954326,
      "grad_norm": 0.9846789836883545,
      "learning_rate": 0.0003548161352435446,
      "loss": 2.0501,
      "step": 920
    },
    {
      "epoch": 0.6201033505584264,
      "grad_norm": 0.8260608911514282,
      "learning_rate": 0.0003541178157488712,
      "loss": 2.0733,
      "step": 930
    },
    {
      "epoch": 0.6267711285214203,
      "grad_norm": 1.1090114116668701,
      "learning_rate": 0.0003534194962541979,
      "loss": 2.0317,
      "step": 940
    },
    {
      "epoch": 0.633438906484414,
      "grad_norm": 0.9676381349563599,
      "learning_rate": 0.00035272117675952446,
      "loss": 1.9867,
      "step": 950
    },
    {
      "epoch": 0.6401066844474079,
      "grad_norm": 1.152280569076538,
      "learning_rate": 0.0003520228572648511,
      "loss": 2.0158,
      "step": 960
    },
    {
      "epoch": 0.6467744624104017,
      "grad_norm": 1.0991276502609253,
      "learning_rate": 0.0003513245377701777,
      "loss": 2.0044,
      "step": 970
    },
    {
      "epoch": 0.6534422403733956,
      "grad_norm": 0.9301159381866455,
      "learning_rate": 0.0003506262182755043,
      "loss": 1.951,
      "step": 980
    },
    {
      "epoch": 0.6601100183363894,
      "grad_norm": 0.7493791580200195,
      "learning_rate": 0.00034992789878083094,
      "loss": 1.9415,
      "step": 990
    },
    {
      "epoch": 0.6667777962993833,
      "grad_norm": 1.0477572679519653,
      "learning_rate": 0.00034922957928615756,
      "loss": 2.06,
      "step": 1000
    },
    {
      "epoch": 0.6734455742623771,
      "grad_norm": 0.8507853746414185,
      "learning_rate": 0.0003485312597914842,
      "loss": 1.8792,
      "step": 1010
    },
    {
      "epoch": 0.680113352225371,
      "grad_norm": 1.3043146133422852,
      "learning_rate": 0.00034783294029681086,
      "loss": 1.9598,
      "step": 1020
    },
    {
      "epoch": 0.6867811301883647,
      "grad_norm": 1.0141544342041016,
      "learning_rate": 0.0003471346208021374,
      "loss": 2.0245,
      "step": 1030
    },
    {
      "epoch": 0.6934489081513585,
      "grad_norm": 0.7228897213935852,
      "learning_rate": 0.00034643630130746405,
      "loss": 1.9408,
      "step": 1040
    },
    {
      "epoch": 0.7001166861143524,
      "grad_norm": 0.9446371793746948,
      "learning_rate": 0.00034573798181279067,
      "loss": 1.9033,
      "step": 1050
    },
    {
      "epoch": 0.7067844640773462,
      "grad_norm": 0.9239996671676636,
      "learning_rate": 0.0003450396623181173,
      "loss": 1.8684,
      "step": 1060
    },
    {
      "epoch": 0.7134522420403401,
      "grad_norm": 1.2270503044128418,
      "learning_rate": 0.0003443413428234439,
      "loss": 1.9978,
      "step": 1070
    },
    {
      "epoch": 0.7201200200033339,
      "grad_norm": 1.0665180683135986,
      "learning_rate": 0.00034364302332877053,
      "loss": 1.9428,
      "step": 1080
    },
    {
      "epoch": 0.7267877979663278,
      "grad_norm": 0.6201373934745789,
      "learning_rate": 0.00034294470383409715,
      "loss": 1.978,
      "step": 1090
    },
    {
      "epoch": 0.7334555759293215,
      "grad_norm": 1.0970972776412964,
      "learning_rate": 0.0003422463843394238,
      "loss": 1.9298,
      "step": 1100
    },
    {
      "epoch": 0.7401233538923154,
      "grad_norm": 1.014961838722229,
      "learning_rate": 0.0003415480648447504,
      "loss": 1.9082,
      "step": 1110
    },
    {
      "epoch": 0.7467911318553092,
      "grad_norm": 0.9398582577705383,
      "learning_rate": 0.000340849745350077,
      "loss": 2.0216,
      "step": 1120
    },
    {
      "epoch": 0.7534589098183031,
      "grad_norm": 1.1034479141235352,
      "learning_rate": 0.00034015142585540364,
      "loss": 1.9543,
      "step": 1130
    },
    {
      "epoch": 0.7601266877812969,
      "grad_norm": 1.2437934875488281,
      "learning_rate": 0.00033945310636073026,
      "loss": 1.9145,
      "step": 1140
    },
    {
      "epoch": 0.7667944657442907,
      "grad_norm": 1.1765769720077515,
      "learning_rate": 0.0003387547868660569,
      "loss": 1.8926,
      "step": 1150
    },
    {
      "epoch": 0.7734622437072846,
      "grad_norm": 0.7985234260559082,
      "learning_rate": 0.00033805646737138345,
      "loss": 1.9441,
      "step": 1160
    },
    {
      "epoch": 0.7801300216702783,
      "grad_norm": 1.3481873273849487,
      "learning_rate": 0.0003373581478767101,
      "loss": 1.901,
      "step": 1170
    },
    {
      "epoch": 0.7867977996332722,
      "grad_norm": 0.6365499496459961,
      "learning_rate": 0.00033665982838203674,
      "loss": 1.9547,
      "step": 1180
    },
    {
      "epoch": 0.793465577596266,
      "grad_norm": 1.1702231168746948,
      "learning_rate": 0.0003359615088873633,
      "loss": 1.9738,
      "step": 1190
    },
    {
      "epoch": 0.8001333555592599,
      "grad_norm": 1.2417173385620117,
      "learning_rate": 0.00033526318939269,
      "loss": 1.8931,
      "step": 1200
    },
    {
      "epoch": 0.8068011335222537,
      "grad_norm": 1.2457925081253052,
      "learning_rate": 0.00033456486989801655,
      "loss": 1.9942,
      "step": 1210
    },
    {
      "epoch": 0.8134689114852476,
      "grad_norm": 1.3167526721954346,
      "learning_rate": 0.0003338665504033432,
      "loss": 1.8352,
      "step": 1220
    },
    {
      "epoch": 0.8201366894482414,
      "grad_norm": 1.0945627689361572,
      "learning_rate": 0.00033316823090866985,
      "loss": 1.9859,
      "step": 1230
    },
    {
      "epoch": 0.8268044674112353,
      "grad_norm": 1.1300173997879028,
      "learning_rate": 0.0003324699114139964,
      "loss": 1.8916,
      "step": 1240
    },
    {
      "epoch": 0.833472245374229,
      "grad_norm": 1.1486326456069946,
      "learning_rate": 0.00033177159191932304,
      "loss": 1.9557,
      "step": 1250
    },
    {
      "epoch": 0.8401400233372228,
      "grad_norm": 1.1384365558624268,
      "learning_rate": 0.0003310732724246497,
      "loss": 1.9511,
      "step": 1260
    },
    {
      "epoch": 0.8468078013002167,
      "grad_norm": 1.8290923833847046,
      "learning_rate": 0.0003303749529299763,
      "loss": 1.8638,
      "step": 1270
    },
    {
      "epoch": 0.8534755792632105,
      "grad_norm": 1.0078034400939941,
      "learning_rate": 0.00032967663343530295,
      "loss": 1.9374,
      "step": 1280
    },
    {
      "epoch": 0.8601433572262044,
      "grad_norm": 1.154338002204895,
      "learning_rate": 0.0003289783139406295,
      "loss": 1.9562,
      "step": 1290
    },
    {
      "epoch": 0.8668111351891982,
      "grad_norm": 1.0904078483581543,
      "learning_rate": 0.00032827999444595614,
      "loss": 1.8974,
      "step": 1300
    },
    {
      "epoch": 0.8734789131521921,
      "grad_norm": 0.9671677947044373,
      "learning_rate": 0.0003275816749512828,
      "loss": 1.9051,
      "step": 1310
    },
    {
      "epoch": 0.8801466911151858,
      "grad_norm": 1.373342752456665,
      "learning_rate": 0.0003268833554566094,
      "loss": 1.8679,
      "step": 1320
    },
    {
      "epoch": 0.8868144690781797,
      "grad_norm": 1.2968370914459229,
      "learning_rate": 0.000326185035961936,
      "loss": 1.8869,
      "step": 1330
    },
    {
      "epoch": 0.8934822470411735,
      "grad_norm": 0.733215868473053,
      "learning_rate": 0.0003254867164672627,
      "loss": 1.8922,
      "step": 1340
    },
    {
      "epoch": 0.9001500250041674,
      "grad_norm": 1.1812515258789062,
      "learning_rate": 0.00032478839697258925,
      "loss": 1.8522,
      "step": 1350
    },
    {
      "epoch": 0.9068178029671612,
      "grad_norm": 0.834976077079773,
      "learning_rate": 0.00032409007747791587,
      "loss": 1.8909,
      "step": 1360
    },
    {
      "epoch": 0.913485580930155,
      "grad_norm": 0.9185742735862732,
      "learning_rate": 0.0003233917579832425,
      "loss": 1.7928,
      "step": 1370
    },
    {
      "epoch": 0.9201533588931489,
      "grad_norm": 0.9329525232315063,
      "learning_rate": 0.0003226934384885691,
      "loss": 1.9026,
      "step": 1380
    },
    {
      "epoch": 0.9268211368561426,
      "grad_norm": 1.3482803106307983,
      "learning_rate": 0.00032199511899389573,
      "loss": 2.0067,
      "step": 1390
    },
    {
      "epoch": 0.9334889148191365,
      "grad_norm": 1.4951205253601074,
      "learning_rate": 0.00032129679949922235,
      "loss": 1.9328,
      "step": 1400
    },
    {
      "epoch": 0.9401566927821303,
      "grad_norm": 0.9972417950630188,
      "learning_rate": 0.000320598480004549,
      "loss": 1.9506,
      "step": 1410
    },
    {
      "epoch": 0.9468244707451242,
      "grad_norm": 0.8867614269256592,
      "learning_rate": 0.0003199001605098756,
      "loss": 1.9057,
      "step": 1420
    },
    {
      "epoch": 0.953492248708118,
      "grad_norm": 0.7247448563575745,
      "learning_rate": 0.0003192018410152022,
      "loss": 2.0216,
      "step": 1430
    },
    {
      "epoch": 0.9601600266711119,
      "grad_norm": 1.4887337684631348,
      "learning_rate": 0.00031850352152052884,
      "loss": 1.9022,
      "step": 1440
    },
    {
      "epoch": 0.9668278046341057,
      "grad_norm": 1.6253668069839478,
      "learning_rate": 0.0003178052020258554,
      "loss": 1.8429,
      "step": 1450
    },
    {
      "epoch": 0.9734955825970996,
      "grad_norm": 1.0575793981552124,
      "learning_rate": 0.0003171068825311821,
      "loss": 1.8126,
      "step": 1460
    },
    {
      "epoch": 0.9801633605600933,
      "grad_norm": 1.4045302867889404,
      "learning_rate": 0.0003164085630365087,
      "loss": 1.8633,
      "step": 1470
    },
    {
      "epoch": 0.9868311385230871,
      "grad_norm": 1.3716095685958862,
      "learning_rate": 0.00031571024354183527,
      "loss": 1.8873,
      "step": 1480
    },
    {
      "epoch": 0.993498916486081,
      "grad_norm": 1.3699584007263184,
      "learning_rate": 0.00031501192404716194,
      "loss": 1.8284,
      "step": 1490
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.390191912651062,
      "learning_rate": 0.00031431360455248857,
      "loss": 1.9377,
      "step": 1500
    },
    {
      "epoch": 1.0,
      "eval_loss": 1.8632733821868896,
      "eval_runtime": 939.3034,
      "eval_samples_per_second": 6.387,
      "eval_steps_per_second": 3.194,
      "step": 1500
    },
    {
      "epoch": 1.0066677779629938,
      "grad_norm": 1.2317359447479248,
      "learning_rate": 0.0003136152850578152,
      "loss": 1.7891,
      "step": 1510
    },
    {
      "epoch": 1.0133355559259876,
      "grad_norm": 0.963683009147644,
      "learning_rate": 0.0003129169655631418,
      "loss": 1.8503,
      "step": 1520
    },
    {
      "epoch": 1.0200033338889816,
      "grad_norm": 0.9739201068878174,
      "learning_rate": 0.0003122186460684684,
      "loss": 1.8782,
      "step": 1530
    },
    {
      "epoch": 1.0266711118519753,
      "grad_norm": 0.9015824794769287,
      "learning_rate": 0.00031152032657379505,
      "loss": 1.91,
      "step": 1540
    },
    {
      "epoch": 1.0333388898149691,
      "grad_norm": 0.7999708652496338,
      "learning_rate": 0.00031082200707912167,
      "loss": 1.9084,
      "step": 1550
    },
    {
      "epoch": 1.040006667777963,
      "grad_norm": 1.0488921403884888,
      "learning_rate": 0.00031012368758444824,
      "loss": 1.8497,
      "step": 1560
    },
    {
      "epoch": 1.046674445740957,
      "grad_norm": 0.9521612524986267,
      "learning_rate": 0.0003094253680897749,
      "loss": 1.8745,
      "step": 1570
    },
    {
      "epoch": 1.0533422237039507,
      "grad_norm": 1.223623514175415,
      "learning_rate": 0.00030872704859510154,
      "loss": 1.7608,
      "step": 1580
    },
    {
      "epoch": 1.0600100016669445,
      "grad_norm": 1.1485872268676758,
      "learning_rate": 0.0003080287291004281,
      "loss": 1.7725,
      "step": 1590
    },
    {
      "epoch": 1.0666777796299383,
      "grad_norm": 0.8962045311927795,
      "learning_rate": 0.0003073304096057548,
      "loss": 1.8907,
      "step": 1600
    },
    {
      "epoch": 1.073345557592932,
      "grad_norm": 1.0390825271606445,
      "learning_rate": 0.00030663209011108134,
      "loss": 1.7978,
      "step": 1610
    },
    {
      "epoch": 1.080013335555926,
      "grad_norm": 0.9159239530563354,
      "learning_rate": 0.00030593377061640797,
      "loss": 1.8928,
      "step": 1620
    },
    {
      "epoch": 1.0866811135189198,
      "grad_norm": 0.8450107574462891,
      "learning_rate": 0.00030523545112173464,
      "loss": 1.8705,
      "step": 1630
    },
    {
      "epoch": 1.0933488914819136,
      "grad_norm": 0.8255006074905396,
      "learning_rate": 0.0003045371316270612,
      "loss": 1.7946,
      "step": 1640
    },
    {
      "epoch": 1.1000166694449074,
      "grad_norm": 1.1333292722702026,
      "learning_rate": 0.00030383881213238783,
      "loss": 1.8546,
      "step": 1650
    },
    {
      "epoch": 1.1066844474079014,
      "grad_norm": 1.0817304849624634,
      "learning_rate": 0.00030314049263771445,
      "loss": 1.7851,
      "step": 1660
    },
    {
      "epoch": 1.1133522253708952,
      "grad_norm": 0.8844102621078491,
      "learning_rate": 0.00030244217314304107,
      "loss": 1.7606,
      "step": 1670
    },
    {
      "epoch": 1.120020003333889,
      "grad_norm": 1.3763810396194458,
      "learning_rate": 0.0003017438536483677,
      "loss": 1.8232,
      "step": 1680
    },
    {
      "epoch": 1.1266877812968827,
      "grad_norm": 1.0344361066818237,
      "learning_rate": 0.0003010455341536943,
      "loss": 1.8318,
      "step": 1690
    },
    {
      "epoch": 1.1333555592598765,
      "grad_norm": 1.2732315063476562,
      "learning_rate": 0.00030034721465902093,
      "loss": 1.7212,
      "step": 1700
    },
    {
      "epoch": 1.1400233372228705,
      "grad_norm": 1.0406163930892944,
      "learning_rate": 0.00029964889516434756,
      "loss": 1.7976,
      "step": 1710
    },
    {
      "epoch": 1.1466911151858643,
      "grad_norm": 1.0068973302841187,
      "learning_rate": 0.0002989505756696742,
      "loss": 1.8007,
      "step": 1720
    },
    {
      "epoch": 1.153358893148858,
      "grad_norm": 0.8806529641151428,
      "learning_rate": 0.0002982522561750008,
      "loss": 1.73,
      "step": 1730
    },
    {
      "epoch": 1.160026671111852,
      "grad_norm": 1.4136061668395996,
      "learning_rate": 0.0002975539366803274,
      "loss": 1.8588,
      "step": 1740
    },
    {
      "epoch": 1.1666944490748459,
      "grad_norm": 0.7997166514396667,
      "learning_rate": 0.00029685561718565404,
      "loss": 1.722,
      "step": 1750
    },
    {
      "epoch": 1.1733622270378397,
      "grad_norm": 0.7783832550048828,
      "learning_rate": 0.00029615729769098066,
      "loss": 1.7642,
      "step": 1760
    },
    {
      "epoch": 1.1800300050008334,
      "grad_norm": 1.1661971807479858,
      "learning_rate": 0.0002954589781963073,
      "loss": 1.8151,
      "step": 1770
    },
    {
      "epoch": 1.1866977829638272,
      "grad_norm": 1.2078300714492798,
      "learning_rate": 0.0002947606587016339,
      "loss": 1.7364,
      "step": 1780
    },
    {
      "epoch": 1.1933655609268212,
      "grad_norm": 1.100316047668457,
      "learning_rate": 0.0002940623392069605,
      "loss": 1.7519,
      "step": 1790
    },
    {
      "epoch": 1.200033338889815,
      "grad_norm": 0.8577283620834351,
      "learning_rate": 0.00029336401971228715,
      "loss": 1.8264,
      "step": 1800
    },
    {
      "epoch": 1.2067011168528088,
      "grad_norm": 1.0121631622314453,
      "learning_rate": 0.00029266570021761377,
      "loss": 1.8412,
      "step": 1810
    },
    {
      "epoch": 1.2133688948158026,
      "grad_norm": 0.9671733975410461,
      "learning_rate": 0.0002919673807229404,
      "loss": 1.7257,
      "step": 1820
    },
    {
      "epoch": 1.2200366727787966,
      "grad_norm": 1.213698148727417,
      "learning_rate": 0.000291269061228267,
      "loss": 1.8186,
      "step": 1830
    },
    {
      "epoch": 1.2267044507417904,
      "grad_norm": 1.1592589616775513,
      "learning_rate": 0.00029057074173359363,
      "loss": 1.8049,
      "step": 1840
    },
    {
      "epoch": 1.2333722287047841,
      "grad_norm": 1.1857556104660034,
      "learning_rate": 0.0002898724222389202,
      "loss": 1.7904,
      "step": 1850
    },
    {
      "epoch": 1.240040006667778,
      "grad_norm": 1.7068692445755005,
      "learning_rate": 0.0002891741027442469,
      "loss": 1.793,
      "step": 1860
    },
    {
      "epoch": 1.2467077846307717,
      "grad_norm": 1.1577688455581665,
      "learning_rate": 0.0002884757832495735,
      "loss": 1.7591,
      "step": 1870
    },
    {
      "epoch": 1.2533755625937657,
      "grad_norm": 0.9523351788520813,
      "learning_rate": 0.00028777746375490006,
      "loss": 1.8333,
      "step": 1880
    },
    {
      "epoch": 1.2600433405567595,
      "grad_norm": 1.1670341491699219,
      "learning_rate": 0.00028707914426022674,
      "loss": 1.8034,
      "step": 1890
    },
    {
      "epoch": 1.2667111185197533,
      "grad_norm": 1.5947494506835938,
      "learning_rate": 0.0002863808247655533,
      "loss": 1.8888,
      "step": 1900
    },
    {
      "epoch": 1.273378896482747,
      "grad_norm": 1.1111334562301636,
      "learning_rate": 0.0002856825052708799,
      "loss": 1.8324,
      "step": 1910
    },
    {
      "epoch": 1.280046674445741,
      "grad_norm": 1.1668633222579956,
      "learning_rate": 0.0002849841857762066,
      "loss": 1.862,
      "step": 1920
    },
    {
      "epoch": 1.2867144524087348,
      "grad_norm": 1.5702987909317017,
      "learning_rate": 0.00028428586628153317,
      "loss": 1.8429,
      "step": 1930
    },
    {
      "epoch": 1.2933822303717286,
      "grad_norm": 1.5913034677505493,
      "learning_rate": 0.0002835875467868598,
      "loss": 1.6967,
      "step": 1940
    },
    {
      "epoch": 1.3000500083347224,
      "grad_norm": 1.4417365789413452,
      "learning_rate": 0.00028288922729218646,
      "loss": 1.7382,
      "step": 1950
    },
    {
      "epoch": 1.3067177862977162,
      "grad_norm": 1.803626537322998,
      "learning_rate": 0.00028219090779751303,
      "loss": 1.7956,
      "step": 1960
    },
    {
      "epoch": 1.3133855642607102,
      "grad_norm": 1.324723482131958,
      "learning_rate": 0.00028149258830283965,
      "loss": 1.732,
      "step": 1970
    },
    {
      "epoch": 1.320053342223704,
      "grad_norm": 0.9815978407859802,
      "learning_rate": 0.0002807942688081663,
      "loss": 1.8361,
      "step": 1980
    },
    {
      "epoch": 1.3267211201866977,
      "grad_norm": 0.9780428409576416,
      "learning_rate": 0.0002800959493134929,
      "loss": 1.6782,
      "step": 1990
    },
    {
      "epoch": 1.3333888981496917,
      "grad_norm": 1.1575024127960205,
      "learning_rate": 0.0002793976298188195,
      "loss": 1.8133,
      "step": 2000
    },
    {
      "epoch": 1.3400566761126855,
      "grad_norm": 1.185102105140686,
      "learning_rate": 0.00027869931032414614,
      "loss": 1.764,
      "step": 2010
    },
    {
      "epoch": 1.3467244540756793,
      "grad_norm": 1.7054206132888794,
      "learning_rate": 0.00027800099082947276,
      "loss": 1.701,
      "step": 2020
    },
    {
      "epoch": 1.353392232038673,
      "grad_norm": 0.853549599647522,
      "learning_rate": 0.00027730267133479943,
      "loss": 1.7217,
      "step": 2030
    },
    {
      "epoch": 1.3600600100016669,
      "grad_norm": 0.8546625375747681,
      "learning_rate": 0.000276604351840126,
      "loss": 1.7903,
      "step": 2040
    },
    {
      "epoch": 1.3667277879646607,
      "grad_norm": 1.5282241106033325,
      "learning_rate": 0.0002759060323454526,
      "loss": 1.6181,
      "step": 2050
    },
    {
      "epoch": 1.3733955659276547,
      "grad_norm": 1.597848653793335,
      "learning_rate": 0.00027520771285077924,
      "loss": 1.7384,
      "step": 2060
    },
    {
      "epoch": 1.3800633438906484,
      "grad_norm": 1.2276251316070557,
      "learning_rate": 0.00027450939335610586,
      "loss": 1.8242,
      "step": 2070
    },
    {
      "epoch": 1.3867311218536422,
      "grad_norm": 1.1409274339675903,
      "learning_rate": 0.0002738110738614325,
      "loss": 1.8037,
      "step": 2080
    },
    {
      "epoch": 1.3933988998166362,
      "grad_norm": 1.497134804725647,
      "learning_rate": 0.0002731127543667591,
      "loss": 1.6694,
      "step": 2090
    },
    {
      "epoch": 1.40006667777963,
      "grad_norm": 0.8393813967704773,
      "learning_rate": 0.0002724144348720857,
      "loss": 1.8776,
      "step": 2100
    },
    {
      "epoch": 1.4067344557426238,
      "grad_norm": 0.8237645030021667,
      "learning_rate": 0.00027171611537741235,
      "loss": 1.8189,
      "step": 2110
    },
    {
      "epoch": 1.4134022337056176,
      "grad_norm": 1.4887582063674927,
      "learning_rate": 0.00027101779588273897,
      "loss": 1.7002,
      "step": 2120
    },
    {
      "epoch": 1.4200700116686114,
      "grad_norm": 0.8113765120506287,
      "learning_rate": 0.0002703194763880656,
      "loss": 1.8684,
      "step": 2130
    },
    {
      "epoch": 1.4267377896316051,
      "grad_norm": 1.0926467180252075,
      "learning_rate": 0.00026962115689339216,
      "loss": 1.721,
      "step": 2140
    },
    {
      "epoch": 1.4334055675945991,
      "grad_norm": 1.121466040611267,
      "learning_rate": 0.00026892283739871883,
      "loss": 1.8102,
      "step": 2150
    },
    {
      "epoch": 1.440073345557593,
      "grad_norm": 1.5265092849731445,
      "learning_rate": 0.00026822451790404545,
      "loss": 1.7531,
      "step": 2160
    },
    {
      "epoch": 1.4467411235205867,
      "grad_norm": 1.0273231267929077,
      "learning_rate": 0.000267526198409372,
      "loss": 1.8552,
      "step": 2170
    },
    {
      "epoch": 1.4534089014835807,
      "grad_norm": 1.2362560033798218,
      "learning_rate": 0.0002668278789146987,
      "loss": 1.7084,
      "step": 2180
    },
    {
      "epoch": 1.4600766794465745,
      "grad_norm": 1.0361589193344116,
      "learning_rate": 0.0002661295594200253,
      "loss": 1.7324,
      "step": 2190
    },
    {
      "epoch": 1.4667444574095683,
      "grad_norm": 1.1536082029342651,
      "learning_rate": 0.0002654312399253519,
      "loss": 1.8834,
      "step": 2200
    },
    {
      "epoch": 1.473412235372562,
      "grad_norm": 1.0963934659957886,
      "learning_rate": 0.00026473292043067856,
      "loss": 1.8059,
      "step": 2210
    },
    {
      "epoch": 1.4800800133355558,
      "grad_norm": 0.9665204882621765,
      "learning_rate": 0.0002640346009360051,
      "loss": 1.7155,
      "step": 2220
    },
    {
      "epoch": 1.4867477912985498,
      "grad_norm": 0.9917728304862976,
      "learning_rate": 0.00026333628144133175,
      "loss": 1.7964,
      "step": 2230
    },
    {
      "epoch": 1.4934155692615436,
      "grad_norm": 0.9817929863929749,
      "learning_rate": 0.0002626379619466584,
      "loss": 1.7492,
      "step": 2240
    },
    {
      "epoch": 1.5000833472245374,
      "grad_norm": 1.284232497215271,
      "learning_rate": 0.000261939642451985,
      "loss": 1.8181,
      "step": 2250
    },
    {
      "epoch": 1.5067511251875314,
      "grad_norm": 0.8879544734954834,
      "learning_rate": 0.00026124132295731167,
      "loss": 1.7229,
      "step": 2260
    },
    {
      "epoch": 1.5134189031505252,
      "grad_norm": 1.2130361795425415,
      "learning_rate": 0.0002605430034626383,
      "loss": 1.8593,
      "step": 2270
    },
    {
      "epoch": 1.520086681113519,
      "grad_norm": 1.1147172451019287,
      "learning_rate": 0.00025984468396796485,
      "loss": 1.7475,
      "step": 2280
    },
    {
      "epoch": 1.5267544590765127,
      "grad_norm": 1.4741525650024414,
      "learning_rate": 0.00025914636447329153,
      "loss": 1.7775,
      "step": 2290
    },
    {
      "epoch": 1.5334222370395065,
      "grad_norm": 1.4066741466522217,
      "learning_rate": 0.0002584480449786181,
      "loss": 1.7888,
      "step": 2300
    },
    {
      "epoch": 1.5400900150025003,
      "grad_norm": 1.2990604639053345,
      "learning_rate": 0.0002577497254839447,
      "loss": 1.8764,
      "step": 2310
    },
    {
      "epoch": 1.546757792965494,
      "grad_norm": 1.2001926898956299,
      "learning_rate": 0.0002570514059892714,
      "loss": 1.8591,
      "step": 2320
    },
    {
      "epoch": 1.553425570928488,
      "grad_norm": 1.5526615381240845,
      "learning_rate": 0.00025635308649459796,
      "loss": 1.8505,
      "step": 2330
    },
    {
      "epoch": 1.5600933488914819,
      "grad_norm": 1.6894357204437256,
      "learning_rate": 0.0002556547669999246,
      "loss": 1.805,
      "step": 2340
    },
    {
      "epoch": 1.5667611268544759,
      "grad_norm": 0.9041248559951782,
      "learning_rate": 0.0002549564475052512,
      "loss": 1.6367,
      "step": 2350
    },
    {
      "epoch": 1.5734289048174697,
      "grad_norm": 1.530871033668518,
      "learning_rate": 0.0002542581280105778,
      "loss": 1.7235,
      "step": 2360
    },
    {
      "epoch": 1.5800966827804634,
      "grad_norm": 1.078953742980957,
      "learning_rate": 0.00025355980851590444,
      "loss": 1.6706,
      "step": 2370
    },
    {
      "epoch": 1.5867644607434572,
      "grad_norm": 1.2942759990692139,
      "learning_rate": 0.00025286148902123107,
      "loss": 1.681,
      "step": 2380
    },
    {
      "epoch": 1.593432238706451,
      "grad_norm": 1.842761754989624,
      "learning_rate": 0.0002521631695265577,
      "loss": 1.7818,
      "step": 2390
    },
    {
      "epoch": 1.6001000166694448,
      "grad_norm": 1.155759334564209,
      "learning_rate": 0.0002514648500318843,
      "loss": 1.7558,
      "step": 2400
    },
    {
      "epoch": 1.6067677946324388,
      "grad_norm": 1.07778799533844,
      "learning_rate": 0.00025076653053721093,
      "loss": 1.6828,
      "step": 2410
    },
    {
      "epoch": 1.6134355725954326,
      "grad_norm": 1.5695902109146118,
      "learning_rate": 0.00025006821104253755,
      "loss": 1.6692,
      "step": 2420
    },
    {
      "epoch": 1.6201033505584264,
      "grad_norm": 1.1393076181411743,
      "learning_rate": 0.00024936989154786417,
      "loss": 1.7969,
      "step": 2430
    },
    {
      "epoch": 1.6267711285214204,
      "grad_norm": 1.119102120399475,
      "learning_rate": 0.0002486715720531908,
      "loss": 1.8367,
      "step": 2440
    },
    {
      "epoch": 1.6334389064844141,
      "grad_norm": 0.8071368336677551,
      "learning_rate": 0.0002479732525585174,
      "loss": 1.8503,
      "step": 2450
    },
    {
      "epoch": 1.640106684447408,
      "grad_norm": 0.967618465423584,
      "learning_rate": 0.000247274933063844,
      "loss": 1.7468,
      "step": 2460
    },
    {
      "epoch": 1.6467744624104017,
      "grad_norm": 1.4984201192855835,
      "learning_rate": 0.00024657661356917066,
      "loss": 1.7876,
      "step": 2470
    },
    {
      "epoch": 1.6534422403733955,
      "grad_norm": 0.9926750063896179,
      "learning_rate": 0.0002458782940744973,
      "loss": 1.7936,
      "step": 2480
    },
    {
      "epoch": 1.6601100183363893,
      "grad_norm": 1.165162205696106,
      "learning_rate": 0.00024517997457982384,
      "loss": 1.6286,
      "step": 2490
    },
    {
      "epoch": 1.6667777962993833,
      "grad_norm": 1.057963490486145,
      "learning_rate": 0.0002444816550851505,
      "loss": 1.8016,
      "step": 2500
    },
    {
      "epoch": 1.673445574262377,
      "grad_norm": 0.8064178824424744,
      "learning_rate": 0.0002437833355904771,
      "loss": 1.8118,
      "step": 2510
    },
    {
      "epoch": 1.680113352225371,
      "grad_norm": 1.4246456623077393,
      "learning_rate": 0.00024308501609580376,
      "loss": 1.7739,
      "step": 2520
    },
    {
      "epoch": 1.6867811301883648,
      "grad_norm": 1.719232201576233,
      "learning_rate": 0.00024238669660113036,
      "loss": 1.6912,
      "step": 2530
    },
    {
      "epoch": 1.6934489081513586,
      "grad_norm": 1.2786849737167358,
      "learning_rate": 0.00024168837710645698,
      "loss": 1.726,
      "step": 2540
    },
    {
      "epoch": 1.7001166861143524,
      "grad_norm": 0.9904757738113403,
      "learning_rate": 0.00024099005761178362,
      "loss": 1.8811,
      "step": 2550
    },
    {
      "epoch": 1.7067844640773462,
      "grad_norm": 1.1054967641830444,
      "learning_rate": 0.00024029173811711022,
      "loss": 1.6597,
      "step": 2560
    },
    {
      "epoch": 1.71345224204034,
      "grad_norm": 1.7783452272415161,
      "learning_rate": 0.00023959341862243684,
      "loss": 1.7587,
      "step": 2570
    },
    {
      "epoch": 1.7201200200033337,
      "grad_norm": 1.8216346502304077,
      "learning_rate": 0.0002388950991277635,
      "loss": 1.7637,
      "step": 2580
    },
    {
      "epoch": 1.7267877979663278,
      "grad_norm": 1.3371554613113403,
      "learning_rate": 0.00023819677963309008,
      "loss": 1.8329,
      "step": 2590
    },
    {
      "epoch": 1.7334555759293215,
      "grad_norm": 0.9315134882926941,
      "learning_rate": 0.00023749846013841668,
      "loss": 1.7269,
      "step": 2600
    },
    {
      "epoch": 1.7401233538923155,
      "grad_norm": 1.3130046129226685,
      "learning_rate": 0.00023680014064374332,
      "loss": 1.7028,
      "step": 2610
    },
    {
      "epoch": 1.7467911318553093,
      "grad_norm": 1.1999356746673584,
      "learning_rate": 0.00023610182114906995,
      "loss": 1.7883,
      "step": 2620
    },
    {
      "epoch": 1.753458909818303,
      "grad_norm": 0.9058637022972107,
      "learning_rate": 0.00023540350165439654,
      "loss": 1.7224,
      "step": 2630
    },
    {
      "epoch": 1.7601266877812969,
      "grad_norm": 1.0595777034759521,
      "learning_rate": 0.0002347051821597232,
      "loss": 1.6987,
      "step": 2640
    },
    {
      "epoch": 1.7667944657442907,
      "grad_norm": 1.0986770391464233,
      "learning_rate": 0.00023400686266504978,
      "loss": 1.6339,
      "step": 2650
    },
    {
      "epoch": 1.7734622437072844,
      "grad_norm": 1.5075137615203857,
      "learning_rate": 0.0002333085431703764,
      "loss": 1.7595,
      "step": 2660
    },
    {
      "epoch": 1.7801300216702782,
      "grad_norm": 1.2085899114608765,
      "learning_rate": 0.00023261022367570305,
      "loss": 1.825,
      "step": 2670
    },
    {
      "epoch": 1.7867977996332722,
      "grad_norm": 0.9856380820274353,
      "learning_rate": 0.00023191190418102965,
      "loss": 1.5953,
      "step": 2680
    },
    {
      "epoch": 1.793465577596266,
      "grad_norm": 1.2886357307434082,
      "learning_rate": 0.00023121358468635627,
      "loss": 1.6896,
      "step": 2690
    },
    {
      "epoch": 1.80013335555926,
      "grad_norm": 1.0151034593582153,
      "learning_rate": 0.00023051526519168291,
      "loss": 1.7633,
      "step": 2700
    },
    {
      "epoch": 1.8068011335222538,
      "grad_norm": 1.042529821395874,
      "learning_rate": 0.0002298169456970095,
      "loss": 1.8086,
      "step": 2710
    },
    {
      "epoch": 1.8134689114852476,
      "grad_norm": 1.383440613746643,
      "learning_rate": 0.0002291186262023361,
      "loss": 1.7443,
      "step": 2720
    },
    {
      "epoch": 1.8201366894482414,
      "grad_norm": 1.338571310043335,
      "learning_rate": 0.00022842030670766275,
      "loss": 1.6353,
      "step": 2730
    },
    {
      "epoch": 1.8268044674112351,
      "grad_norm": 1.1648510694503784,
      "learning_rate": 0.00022772198721298937,
      "loss": 1.7133,
      "step": 2740
    },
    {
      "epoch": 1.833472245374229,
      "grad_norm": 1.3428423404693604,
      "learning_rate": 0.00022702366771831597,
      "loss": 1.7311,
      "step": 2750
    },
    {
      "epoch": 1.8401400233372227,
      "grad_norm": 1.2339067459106445,
      "learning_rate": 0.00022632534822364261,
      "loss": 1.75,
      "step": 2760
    },
    {
      "epoch": 1.8468078013002167,
      "grad_norm": 1.3049066066741943,
      "learning_rate": 0.0002256270287289692,
      "loss": 1.7674,
      "step": 2770
    },
    {
      "epoch": 1.8534755792632105,
      "grad_norm": 1.528030276298523,
      "learning_rate": 0.00022492870923429588,
      "loss": 1.6954,
      "step": 2780
    },
    {
      "epoch": 1.8601433572262045,
      "grad_norm": 1.3703449964523315,
      "learning_rate": 0.00022423038973962248,
      "loss": 1.5596,
      "step": 2790
    },
    {
      "epoch": 1.8668111351891983,
      "grad_norm": 1.287546157836914,
      "learning_rate": 0.00022353207024494907,
      "loss": 1.7506,
      "step": 2800
    },
    {
      "epoch": 1.873478913152192,
      "grad_norm": 1.4202672243118286,
      "learning_rate": 0.00022283375075027572,
      "loss": 1.7967,
      "step": 2810
    },
    {
      "epoch": 1.8801466911151858,
      "grad_norm": 1.384665846824646,
      "learning_rate": 0.00022213543125560234,
      "loss": 1.7619,
      "step": 2820
    },
    {
      "epoch": 1.8868144690781796,
      "grad_norm": 1.6028579473495483,
      "learning_rate": 0.00022143711176092894,
      "loss": 1.7323,
      "step": 2830
    },
    {
      "epoch": 1.8934822470411734,
      "grad_norm": 1.090141773223877,
      "learning_rate": 0.00022073879226625558,
      "loss": 1.7121,
      "step": 2840
    },
    {
      "epoch": 1.9001500250041674,
      "grad_norm": 1.3659073114395142,
      "learning_rate": 0.00022004047277158218,
      "loss": 1.6457,
      "step": 2850
    },
    {
      "epoch": 1.9068178029671612,
      "grad_norm": 0.8244592547416687,
      "learning_rate": 0.0002193421532769088,
      "loss": 1.745,
      "step": 2860
    },
    {
      "epoch": 1.913485580930155,
      "grad_norm": 0.6803396344184875,
      "learning_rate": 0.00021864383378223545,
      "loss": 1.7025,
      "step": 2870
    },
    {
      "epoch": 1.920153358893149,
      "grad_norm": 1.0845510959625244,
      "learning_rate": 0.00021794551428756204,
      "loss": 1.6824,
      "step": 2880
    },
    {
      "epoch": 1.9268211368561428,
      "grad_norm": 1.1597222089767456,
      "learning_rate": 0.00021724719479288864,
      "loss": 1.6776,
      "step": 2890
    },
    {
      "epoch": 1.9334889148191365,
      "grad_norm": 1.1405868530273438,
      "learning_rate": 0.0002165488752982153,
      "loss": 1.6731,
      "step": 2900
    },
    {
      "epoch": 1.9401566927821303,
      "grad_norm": 1.0881319046020508,
      "learning_rate": 0.0002158505558035419,
      "loss": 1.7933,
      "step": 2910
    },
    {
      "epoch": 1.946824470745124,
      "grad_norm": 1.1311556100845337,
      "learning_rate": 0.0002151522363088685,
      "loss": 1.5947,
      "step": 2920
    },
    {
      "epoch": 1.9534922487081179,
      "grad_norm": 1.0388108491897583,
      "learning_rate": 0.00021445391681419515,
      "loss": 1.8078,
      "step": 2930
    },
    {
      "epoch": 1.9601600266711119,
      "grad_norm": 1.2680864334106445,
      "learning_rate": 0.00021375559731952177,
      "loss": 1.6914,
      "step": 2940
    },
    {
      "epoch": 1.9668278046341057,
      "grad_norm": 1.1293370723724365,
      "learning_rate": 0.00021305727782484836,
      "loss": 1.752,
      "step": 2950
    },
    {
      "epoch": 1.9734955825970997,
      "grad_norm": 1.505660891532898,
      "learning_rate": 0.000212358958330175,
      "loss": 1.6526,
      "step": 2960
    },
    {
      "epoch": 1.9801633605600935,
      "grad_norm": 1.5148062705993652,
      "learning_rate": 0.0002116606388355016,
      "loss": 1.7246,
      "step": 2970
    },
    {
      "epoch": 1.9868311385230872,
      "grad_norm": 1.4657163619995117,
      "learning_rate": 0.00021096231934082823,
      "loss": 1.6232,
      "step": 2980
    },
    {
      "epoch": 1.993498916486081,
      "grad_norm": 1.0604902505874634,
      "learning_rate": 0.00021026399984615487,
      "loss": 1.632,
      "step": 2990
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.0102331638336182,
      "learning_rate": 0.00020956568035148147,
      "loss": 1.6697,
      "step": 3000
    },
    {
      "epoch": 2.0,
      "eval_loss": 1.7007488012313843,
      "eval_runtime": 939.8142,
      "eval_samples_per_second": 6.383,
      "eval_steps_per_second": 3.192,
      "step": 3000
    },
    {
      "epoch": 2.006667777962994,
      "grad_norm": 1.3297151327133179,
      "learning_rate": 0.0002088673608568081,
      "loss": 1.6387,
      "step": 3010
    },
    {
      "epoch": 2.0133355559259876,
      "grad_norm": 1.2343201637268066,
      "learning_rate": 0.00020816904136213474,
      "loss": 1.6969,
      "step": 3020
    },
    {
      "epoch": 2.0200033338889813,
      "grad_norm": 1.4568469524383545,
      "learning_rate": 0.00020747072186746133,
      "loss": 1.7129,
      "step": 3030
    },
    {
      "epoch": 2.026671111851975,
      "grad_norm": 1.1363019943237305,
      "learning_rate": 0.00020677240237278795,
      "loss": 1.715,
      "step": 3040
    },
    {
      "epoch": 2.0333388898149694,
      "grad_norm": 1.4174209833145142,
      "learning_rate": 0.00020607408287811457,
      "loss": 1.6557,
      "step": 3050
    },
    {
      "epoch": 2.040006667777963,
      "grad_norm": 1.219214916229248,
      "learning_rate": 0.0002053757633834412,
      "loss": 1.7011,
      "step": 3060
    },
    {
      "epoch": 2.046674445740957,
      "grad_norm": 1.323732614517212,
      "learning_rate": 0.00020467744388876782,
      "loss": 1.634,
      "step": 3070
    },
    {
      "epoch": 2.0533422237039507,
      "grad_norm": 1.4304213523864746,
      "learning_rate": 0.00020397912439409444,
      "loss": 1.6919,
      "step": 3080
    },
    {
      "epoch": 2.0600100016669445,
      "grad_norm": 1.139235019683838,
      "learning_rate": 0.00020328080489942106,
      "loss": 1.6928,
      "step": 3090
    },
    {
      "epoch": 2.0666777796299383,
      "grad_norm": 1.0941790342330933,
      "learning_rate": 0.00020258248540474768,
      "loss": 1.7432,
      "step": 3100
    },
    {
      "epoch": 2.073345557592932,
      "grad_norm": 1.3150668144226074,
      "learning_rate": 0.0002018841659100743,
      "loss": 1.7773,
      "step": 3110
    },
    {
      "epoch": 2.080013335555926,
      "grad_norm": 1.2220231294631958,
      "learning_rate": 0.00020118584641540092,
      "loss": 1.6777,
      "step": 3120
    },
    {
      "epoch": 2.0866811135189196,
      "grad_norm": 1.7570991516113281,
      "learning_rate": 0.00020048752692072752,
      "loss": 1.5372,
      "step": 3130
    },
    {
      "epoch": 2.093348891481914,
      "grad_norm": 1.3431382179260254,
      "learning_rate": 0.00019978920742605416,
      "loss": 1.6377,
      "step": 3140
    },
    {
      "epoch": 2.1000166694449076,
      "grad_norm": 1.2109650373458862,
      "learning_rate": 0.00019909088793138079,
      "loss": 1.7131,
      "step": 3150
    },
    {
      "epoch": 2.1066844474079014,
      "grad_norm": 1.7938767671585083,
      "learning_rate": 0.00019839256843670738,
      "loss": 1.6065,
      "step": 3160
    },
    {
      "epoch": 2.113352225370895,
      "grad_norm": 0.9397298097610474,
      "learning_rate": 0.000197694248942034,
      "loss": 1.7482,
      "step": 3170
    },
    {
      "epoch": 2.120020003333889,
      "grad_norm": 1.0393702983856201,
      "learning_rate": 0.00019699592944736062,
      "loss": 1.5957,
      "step": 3180
    },
    {
      "epoch": 2.1266877812968827,
      "grad_norm": 1.2494057416915894,
      "learning_rate": 0.00019629760995268727,
      "loss": 1.6652,
      "step": 3190
    },
    {
      "epoch": 2.1333555592598765,
      "grad_norm": 1.065126895904541,
      "learning_rate": 0.00019559929045801386,
      "loss": 1.6544,
      "step": 3200
    },
    {
      "epoch": 2.1400233372228703,
      "grad_norm": 1.0262706279754639,
      "learning_rate": 0.00019490097096334049,
      "loss": 1.6624,
      "step": 3210
    },
    {
      "epoch": 2.146691115185864,
      "grad_norm": 1.1368356943130493,
      "learning_rate": 0.0001942026514686671,
      "loss": 1.6336,
      "step": 3220
    },
    {
      "epoch": 2.1533588931488583,
      "grad_norm": 1.2641069889068604,
      "learning_rate": 0.00019350433197399373,
      "loss": 1.6515,
      "step": 3230
    },
    {
      "epoch": 2.160026671111852,
      "grad_norm": 1.1290338039398193,
      "learning_rate": 0.00019280601247932035,
      "loss": 1.6954,
      "step": 3240
    },
    {
      "epoch": 2.166694449074846,
      "grad_norm": 1.5346599817276,
      "learning_rate": 0.00019210769298464697,
      "loss": 1.5983,
      "step": 3250
    },
    {
      "epoch": 2.1733622270378397,
      "grad_norm": 1.442688226699829,
      "learning_rate": 0.0001914093734899736,
      "loss": 1.7431,
      "step": 3260
    },
    {
      "epoch": 2.1800300050008334,
      "grad_norm": 1.3346377611160278,
      "learning_rate": 0.0001907110539953002,
      "loss": 1.655,
      "step": 3270
    },
    {
      "epoch": 2.186697782963827,
      "grad_norm": 1.2161176204681396,
      "learning_rate": 0.00019001273450062683,
      "loss": 1.681,
      "step": 3280
    },
    {
      "epoch": 2.193365560926821,
      "grad_norm": 1.706708550453186,
      "learning_rate": 0.00018931441500595343,
      "loss": 1.672,
      "step": 3290
    },
    {
      "epoch": 2.200033338889815,
      "grad_norm": 1.205567717552185,
      "learning_rate": 0.00018861609551128005,
      "loss": 1.7417,
      "step": 3300
    },
    {
      "epoch": 2.206701116852809,
      "grad_norm": 1.1480603218078613,
      "learning_rate": 0.0001879177760166067,
      "loss": 1.7448,
      "step": 3310
    },
    {
      "epoch": 2.213368894815803,
      "grad_norm": 1.188904047012329,
      "learning_rate": 0.00018721945652193332,
      "loss": 1.7408,
      "step": 3320
    },
    {
      "epoch": 2.2200366727787966,
      "grad_norm": 1.0684118270874023,
      "learning_rate": 0.0001865211370272599,
      "loss": 1.65,
      "step": 3330
    },
    {
      "epoch": 2.2267044507417904,
      "grad_norm": 1.238302230834961,
      "learning_rate": 0.00018582281753258653,
      "loss": 1.5757,
      "step": 3340
    },
    {
      "epoch": 2.233372228704784,
      "grad_norm": 1.177586555480957,
      "learning_rate": 0.00018512449803791318,
      "loss": 1.666,
      "step": 3350
    },
    {
      "epoch": 2.240040006667778,
      "grad_norm": 1.1122899055480957,
      "learning_rate": 0.00018442617854323978,
      "loss": 1.7077,
      "step": 3360
    },
    {
      "epoch": 2.2467077846307717,
      "grad_norm": 1.433377742767334,
      "learning_rate": 0.0001837278590485664,
      "loss": 1.6483,
      "step": 3370
    },
    {
      "epoch": 2.2533755625937655,
      "grad_norm": 1.0157521963119507,
      "learning_rate": 0.00018302953955389302,
      "loss": 1.6962,
      "step": 3380
    },
    {
      "epoch": 2.2600433405567593,
      "grad_norm": 1.1747736930847168,
      "learning_rate": 0.00018233122005921964,
      "loss": 1.6609,
      "step": 3390
    },
    {
      "epoch": 2.266711118519753,
      "grad_norm": 1.6815886497497559,
      "learning_rate": 0.00018163290056454626,
      "loss": 1.6603,
      "step": 3400
    },
    {
      "epoch": 2.2733788964827473,
      "grad_norm": 1.1840122938156128,
      "learning_rate": 0.00018093458106987288,
      "loss": 1.7675,
      "step": 3410
    },
    {
      "epoch": 2.280046674445741,
      "grad_norm": 0.9585826396942139,
      "learning_rate": 0.00018023626157519948,
      "loss": 1.7346,
      "step": 3420
    },
    {
      "epoch": 2.286714452408735,
      "grad_norm": 1.3215676546096802,
      "learning_rate": 0.00017953794208052612,
      "loss": 1.8652,
      "step": 3430
    },
    {
      "epoch": 2.2933822303717286,
      "grad_norm": 0.944248378276825,
      "learning_rate": 0.00017883962258585274,
      "loss": 1.7455,
      "step": 3440
    },
    {
      "epoch": 2.3000500083347224,
      "grad_norm": 1.8878300189971924,
      "learning_rate": 0.00017814130309117937,
      "loss": 1.674,
      "step": 3450
    },
    {
      "epoch": 2.306717786297716,
      "grad_norm": 1.3039242029190063,
      "learning_rate": 0.00017744298359650596,
      "loss": 1.6847,
      "step": 3460
    },
    {
      "epoch": 2.31338556426071,
      "grad_norm": 1.2629740238189697,
      "learning_rate": 0.0001767446641018326,
      "loss": 1.7917,
      "step": 3470
    },
    {
      "epoch": 2.320053342223704,
      "grad_norm": 1.5796524286270142,
      "learning_rate": 0.00017604634460715923,
      "loss": 1.6424,
      "step": 3480
    },
    {
      "epoch": 2.326721120186698,
      "grad_norm": 1.141499400138855,
      "learning_rate": 0.00017534802511248582,
      "loss": 1.7379,
      "step": 3490
    },
    {
      "epoch": 2.3333888981496917,
      "grad_norm": 1.2699376344680786,
      "learning_rate": 0.00017464970561781244,
      "loss": 1.7468,
      "step": 3500
    },
    {
      "epoch": 2.3400566761126855,
      "grad_norm": 1.0979840755462646,
      "learning_rate": 0.0001739513861231391,
      "loss": 1.6741,
      "step": 3510
    },
    {
      "epoch": 2.3467244540756793,
      "grad_norm": 1.1723097562789917,
      "learning_rate": 0.0001732530666284657,
      "loss": 1.6181,
      "step": 3520
    },
    {
      "epoch": 2.353392232038673,
      "grad_norm": 1.2110482454299927,
      "learning_rate": 0.0001725547471337923,
      "loss": 1.7586,
      "step": 3530
    },
    {
      "epoch": 2.360060010001667,
      "grad_norm": 1.2009575366973877,
      "learning_rate": 0.00017185642763911893,
      "loss": 1.6486,
      "step": 3540
    },
    {
      "epoch": 2.3667277879646607,
      "grad_norm": 1.144400954246521,
      "learning_rate": 0.00017115810814444555,
      "loss": 1.7081,
      "step": 3550
    },
    {
      "epoch": 2.3733955659276544,
      "grad_norm": 1.1816400289535522,
      "learning_rate": 0.00017045978864977217,
      "loss": 1.692,
      "step": 3560
    },
    {
      "epoch": 2.380063343890648,
      "grad_norm": 0.9937824606895447,
      "learning_rate": 0.0001697614691550988,
      "loss": 1.5751,
      "step": 3570
    },
    {
      "epoch": 2.3867311218536424,
      "grad_norm": 0.9351544976234436,
      "learning_rate": 0.00016906314966042541,
      "loss": 1.7063,
      "step": 3580
    },
    {
      "epoch": 2.3933988998166362,
      "grad_norm": 1.4302102327346802,
      "learning_rate": 0.00016836483016575204,
      "loss": 1.6291,
      "step": 3590
    },
    {
      "epoch": 2.40006667777963,
      "grad_norm": 1.8012986183166504,
      "learning_rate": 0.00016766651067107866,
      "loss": 1.4875,
      "step": 3600
    },
    {
      "epoch": 2.406734455742624,
      "grad_norm": 1.399792194366455,
      "learning_rate": 0.00016696819117640528,
      "loss": 1.6469,
      "step": 3610
    },
    {
      "epoch": 2.4134022337056176,
      "grad_norm": 1.334816813468933,
      "learning_rate": 0.00016626987168173187,
      "loss": 1.7563,
      "step": 3620
    },
    {
      "epoch": 2.4200700116686114,
      "grad_norm": 1.6743810176849365,
      "learning_rate": 0.00016557155218705852,
      "loss": 1.729,
      "step": 3630
    },
    {
      "epoch": 2.426737789631605,
      "grad_norm": 1.5423390865325928,
      "learning_rate": 0.00016487323269238514,
      "loss": 1.7048,
      "step": 3640
    },
    {
      "epoch": 2.433405567594599,
      "grad_norm": 1.3547199964523315,
      "learning_rate": 0.00016417491319771174,
      "loss": 1.6124,
      "step": 3650
    },
    {
      "epoch": 2.440073345557593,
      "grad_norm": 0.847960889339447,
      "learning_rate": 0.00016347659370303836,
      "loss": 1.7004,
      "step": 3660
    },
    {
      "epoch": 2.446741123520587,
      "grad_norm": 1.298964500427246,
      "learning_rate": 0.000162778274208365,
      "loss": 1.7372,
      "step": 3670
    },
    {
      "epoch": 2.4534089014835807,
      "grad_norm": 1.0651676654815674,
      "learning_rate": 0.0001620799547136916,
      "loss": 1.6567,
      "step": 3680
    },
    {
      "epoch": 2.4600766794465745,
      "grad_norm": 1.3454993963241577,
      "learning_rate": 0.00016138163521901822,
      "loss": 1.6709,
      "step": 3690
    },
    {
      "epoch": 2.4667444574095683,
      "grad_norm": 1.1024965047836304,
      "learning_rate": 0.00016068331572434484,
      "loss": 1.623,
      "step": 3700
    },
    {
      "epoch": 2.473412235372562,
      "grad_norm": 1.1325932741165161,
      "learning_rate": 0.0001599849962296715,
      "loss": 1.7553,
      "step": 3710
    },
    {
      "epoch": 2.480080013335556,
      "grad_norm": 1.1731058359146118,
      "learning_rate": 0.00015928667673499808,
      "loss": 1.6741,
      "step": 3720
    },
    {
      "epoch": 2.4867477912985496,
      "grad_norm": 1.505311131477356,
      "learning_rate": 0.0001585883572403247,
      "loss": 1.7525,
      "step": 3730
    },
    {
      "epoch": 2.4934155692615434,
      "grad_norm": 1.5113911628723145,
      "learning_rate": 0.00015789003774565133,
      "loss": 1.6764,
      "step": 3740
    },
    {
      "epoch": 2.500083347224537,
      "grad_norm": 1.460142970085144,
      "learning_rate": 0.00015719171825097795,
      "loss": 1.5704,
      "step": 3750
    },
    {
      "epoch": 2.5067511251875314,
      "grad_norm": 1.0805463790893555,
      "learning_rate": 0.00015649339875630457,
      "loss": 1.6565,
      "step": 3760
    },
    {
      "epoch": 2.513418903150525,
      "grad_norm": 1.2856181859970093,
      "learning_rate": 0.0001557950792616312,
      "loss": 1.6519,
      "step": 3770
    },
    {
      "epoch": 2.520086681113519,
      "grad_norm": 1.3038402795791626,
      "learning_rate": 0.00015509675976695778,
      "loss": 1.6579,
      "step": 3780
    },
    {
      "epoch": 2.5267544590765127,
      "grad_norm": 1.2909029722213745,
      "learning_rate": 0.00015439844027228443,
      "loss": 1.7068,
      "step": 3790
    },
    {
      "epoch": 2.5334222370395065,
      "grad_norm": 0.993620753288269,
      "learning_rate": 0.00015370012077761105,
      "loss": 1.6271,
      "step": 3800
    },
    {
      "epoch": 2.5400900150025003,
      "grad_norm": 0.730636715888977,
      "learning_rate": 0.00015300180128293765,
      "loss": 1.7097,
      "step": 3810
    },
    {
      "epoch": 2.546757792965494,
      "grad_norm": 1.459794044494629,
      "learning_rate": 0.00015230348178826427,
      "loss": 1.6666,
      "step": 3820
    },
    {
      "epoch": 2.5534255709284883,
      "grad_norm": 1.2322192192077637,
      "learning_rate": 0.00015160516229359092,
      "loss": 1.6559,
      "step": 3830
    },
    {
      "epoch": 2.560093348891482,
      "grad_norm": 1.5525215864181519,
      "learning_rate": 0.00015090684279891754,
      "loss": 1.7349,
      "step": 3840
    },
    {
      "epoch": 2.566761126854476,
      "grad_norm": 1.2958927154541016,
      "learning_rate": 0.00015020852330424413,
      "loss": 1.6476,
      "step": 3850
    },
    {
      "epoch": 2.5734289048174697,
      "grad_norm": 1.3774731159210205,
      "learning_rate": 0.00014951020380957075,
      "loss": 1.5123,
      "step": 3860
    },
    {
      "epoch": 2.5800966827804634,
      "grad_norm": 1.5452765226364136,
      "learning_rate": 0.00014881188431489737,
      "loss": 1.725,
      "step": 3870
    },
    {
      "epoch": 2.5867644607434572,
      "grad_norm": 0.9853900671005249,
      "learning_rate": 0.000148113564820224,
      "loss": 1.6267,
      "step": 3880
    },
    {
      "epoch": 2.593432238706451,
      "grad_norm": 1.6294629573822021,
      "learning_rate": 0.00014741524532555062,
      "loss": 1.6357,
      "step": 3890
    },
    {
      "epoch": 2.600100016669445,
      "grad_norm": 1.4887661933898926,
      "learning_rate": 0.00014671692583087724,
      "loss": 1.6193,
      "step": 3900
    },
    {
      "epoch": 2.6067677946324386,
      "grad_norm": 1.1393083333969116,
      "learning_rate": 0.00014601860633620386,
      "loss": 1.6563,
      "step": 3910
    },
    {
      "epoch": 2.6134355725954324,
      "grad_norm": 1.3166720867156982,
      "learning_rate": 0.00014532028684153048,
      "loss": 1.6813,
      "step": 3920
    },
    {
      "epoch": 2.620103350558426,
      "grad_norm": 0.9456837773323059,
      "learning_rate": 0.0001446219673468571,
      "loss": 1.7164,
      "step": 3930
    },
    {
      "epoch": 2.6267711285214204,
      "grad_norm": 1.5839332342147827,
      "learning_rate": 0.00014392364785218372,
      "loss": 1.7149,
      "step": 3940
    },
    {
      "epoch": 2.633438906484414,
      "grad_norm": 1.2727700471878052,
      "learning_rate": 0.00014322532835751034,
      "loss": 1.6069,
      "step": 3950
    },
    {
      "epoch": 2.640106684447408,
      "grad_norm": 1.219808578491211,
      "learning_rate": 0.00014252700886283696,
      "loss": 1.654,
      "step": 3960
    },
    {
      "epoch": 2.6467744624104017,
      "grad_norm": 1.3610934019088745,
      "learning_rate": 0.00014182868936816358,
      "loss": 1.5608,
      "step": 3970
    },
    {
      "epoch": 2.6534422403733955,
      "grad_norm": 1.0711458921432495,
      "learning_rate": 0.00014113036987349018,
      "loss": 1.5894,
      "step": 3980
    },
    {
      "epoch": 2.6601100183363893,
      "grad_norm": 1.1778455972671509,
      "learning_rate": 0.0001404320503788168,
      "loss": 1.5439,
      "step": 3990
    },
    {
      "epoch": 2.6667777962993835,
      "grad_norm": 1.2069976329803467,
      "learning_rate": 0.00013973373088414345,
      "loss": 1.612,
      "step": 4000
    },
    {
      "epoch": 2.6734455742623773,
      "grad_norm": 1.4214868545532227,
      "learning_rate": 0.00013903541138947004,
      "loss": 1.6111,
      "step": 4010
    },
    {
      "epoch": 2.680113352225371,
      "grad_norm": 1.36837637424469,
      "learning_rate": 0.00013833709189479666,
      "loss": 1.5958,
      "step": 4020
    },
    {
      "epoch": 2.686781130188365,
      "grad_norm": 1.2655106782913208,
      "learning_rate": 0.00013763877240012328,
      "loss": 1.703,
      "step": 4030
    },
    {
      "epoch": 2.6934489081513586,
      "grad_norm": 1.5152897834777832,
      "learning_rate": 0.0001369404529054499,
      "loss": 1.6892,
      "step": 4040
    },
    {
      "epoch": 2.7001166861143524,
      "grad_norm": 1.1513261795043945,
      "learning_rate": 0.00013624213341077653,
      "loss": 1.7818,
      "step": 4050
    },
    {
      "epoch": 2.706784464077346,
      "grad_norm": 1.2400448322296143,
      "learning_rate": 0.00013554381391610315,
      "loss": 1.6484,
      "step": 4060
    },
    {
      "epoch": 2.71345224204034,
      "grad_norm": 1.5581018924713135,
      "learning_rate": 0.00013484549442142977,
      "loss": 1.7218,
      "step": 4070
    },
    {
      "epoch": 2.7201200200033337,
      "grad_norm": 1.4523746967315674,
      "learning_rate": 0.0001341471749267564,
      "loss": 1.6776,
      "step": 4080
    },
    {
      "epoch": 2.7267877979663275,
      "grad_norm": 1.0468788146972656,
      "learning_rate": 0.000133448855432083,
      "loss": 1.628,
      "step": 4090
    },
    {
      "epoch": 2.7334555759293213,
      "grad_norm": 1.141424298286438,
      "learning_rate": 0.00013275053593740963,
      "loss": 1.5996,
      "step": 4100
    },
    {
      "epoch": 2.7401233538923155,
      "grad_norm": 1.1459434032440186,
      "learning_rate": 0.00013205221644273623,
      "loss": 1.6865,
      "step": 4110
    },
    {
      "epoch": 2.7467911318553093,
      "grad_norm": 1.0984364748001099,
      "learning_rate": 0.00013135389694806288,
      "loss": 1.6571,
      "step": 4120
    },
    {
      "epoch": 2.753458909818303,
      "grad_norm": 1.1491873264312744,
      "learning_rate": 0.0001306555774533895,
      "loss": 1.6643,
      "step": 4130
    },
    {
      "epoch": 2.760126687781297,
      "grad_norm": 0.876213788986206,
      "learning_rate": 0.0001299572579587161,
      "loss": 1.6431,
      "step": 4140
    },
    {
      "epoch": 2.7667944657442907,
      "grad_norm": 1.733447790145874,
      "learning_rate": 0.0001292589384640427,
      "loss": 1.6011,
      "step": 4150
    },
    {
      "epoch": 2.7734622437072844,
      "grad_norm": 1.918981671333313,
      "learning_rate": 0.00012856061896936936,
      "loss": 1.6733,
      "step": 4160
    },
    {
      "epoch": 2.7801300216702782,
      "grad_norm": 1.5309267044067383,
      "learning_rate": 0.00012786229947469595,
      "loss": 1.7057,
      "step": 4170
    },
    {
      "epoch": 2.7867977996332725,
      "grad_norm": 2.0740673542022705,
      "learning_rate": 0.00012716397998002257,
      "loss": 1.7068,
      "step": 4180
    },
    {
      "epoch": 2.7934655775962662,
      "grad_norm": 1.2520596981048584,
      "learning_rate": 0.0001264656604853492,
      "loss": 1.6481,
      "step": 4190
    },
    {
      "epoch": 2.80013335555926,
      "grad_norm": 1.5145176649093628,
      "learning_rate": 0.00012576734099067584,
      "loss": 1.55,
      "step": 4200
    },
    {
      "epoch": 2.806801133522254,
      "grad_norm": 1.3956878185272217,
      "learning_rate": 0.00012506902149600244,
      "loss": 1.7375,
      "step": 4210
    },
    {
      "epoch": 2.8134689114852476,
      "grad_norm": 1.5371562242507935,
      "learning_rate": 0.00012437070200132906,
      "loss": 1.585,
      "step": 4220
    },
    {
      "epoch": 2.8201366894482414,
      "grad_norm": 1.380319356918335,
      "learning_rate": 0.00012367238250665568,
      "loss": 1.7046,
      "step": 4230
    },
    {
      "epoch": 2.826804467411235,
      "grad_norm": 1.6727598905563354,
      "learning_rate": 0.0001229740630119823,
      "loss": 1.5943,
      "step": 4240
    },
    {
      "epoch": 2.833472245374229,
      "grad_norm": 1.2202638387680054,
      "learning_rate": 0.00012227574351730892,
      "loss": 1.5734,
      "step": 4250
    },
    {
      "epoch": 2.8401400233372227,
      "grad_norm": 1.3727518320083618,
      "learning_rate": 0.00012157742402263554,
      "loss": 1.5514,
      "step": 4260
    },
    {
      "epoch": 2.8468078013002165,
      "grad_norm": 1.3707953691482544,
      "learning_rate": 0.00012087910452796215,
      "loss": 1.6451,
      "step": 4270
    },
    {
      "epoch": 2.8534755792632103,
      "grad_norm": 1.3881410360336304,
      "learning_rate": 0.00012018078503328877,
      "loss": 1.5344,
      "step": 4280
    },
    {
      "epoch": 2.8601433572262045,
      "grad_norm": 1.387985110282898,
      "learning_rate": 0.00011948246553861541,
      "loss": 1.5212,
      "step": 4290
    },
    {
      "epoch": 2.8668111351891983,
      "grad_norm": 1.6418778896331787,
      "learning_rate": 0.000118784146043942,
      "loss": 1.6505,
      "step": 4300
    },
    {
      "epoch": 2.873478913152192,
      "grad_norm": 1.1014801263809204,
      "learning_rate": 0.00011808582654926864,
      "loss": 1.7637,
      "step": 4310
    },
    {
      "epoch": 2.880146691115186,
      "grad_norm": 1.2324674129486084,
      "learning_rate": 0.00011738750705459526,
      "loss": 1.6379,
      "step": 4320
    },
    {
      "epoch": 2.8868144690781796,
      "grad_norm": 1.2298847436904907,
      "learning_rate": 0.00011668918755992189,
      "loss": 1.595,
      "step": 4330
    },
    {
      "epoch": 2.8934822470411734,
      "grad_norm": 1.3763394355773926,
      "learning_rate": 0.00011599086806524849,
      "loss": 1.646,
      "step": 4340
    },
    {
      "epoch": 2.9001500250041676,
      "grad_norm": 1.2749974727630615,
      "learning_rate": 0.00011529254857057512,
      "loss": 1.4991,
      "step": 4350
    },
    {
      "epoch": 2.9068178029671614,
      "grad_norm": 1.2292399406433105,
      "learning_rate": 0.00011459422907590174,
      "loss": 1.6291,
      "step": 4360
    },
    {
      "epoch": 2.913485580930155,
      "grad_norm": 1.3518553972244263,
      "learning_rate": 0.00011389590958122835,
      "loss": 1.6727,
      "step": 4370
    },
    {
      "epoch": 2.920153358893149,
      "grad_norm": 1.4244844913482666,
      "learning_rate": 0.00011319759008655497,
      "loss": 1.6633,
      "step": 4380
    },
    {
      "epoch": 2.9268211368561428,
      "grad_norm": 1.1361534595489502,
      "learning_rate": 0.0001124992705918816,
      "loss": 1.6638,
      "step": 4390
    },
    {
      "epoch": 2.9334889148191365,
      "grad_norm": 1.2675498723983765,
      "learning_rate": 0.0001118009510972082,
      "loss": 1.6515,
      "step": 4400
    },
    {
      "epoch": 2.9401566927821303,
      "grad_norm": 1.0719245672225952,
      "learning_rate": 0.00011110263160253483,
      "loss": 1.7081,
      "step": 4410
    },
    {
      "epoch": 2.946824470745124,
      "grad_norm": 1.2727861404418945,
      "learning_rate": 0.00011040431210786146,
      "loss": 1.64,
      "step": 4420
    },
    {
      "epoch": 2.953492248708118,
      "grad_norm": 1.5805304050445557,
      "learning_rate": 0.00010970599261318806,
      "loss": 1.6158,
      "step": 4430
    },
    {
      "epoch": 2.9601600266711117,
      "grad_norm": 1.4730167388916016,
      "learning_rate": 0.00010900767311851468,
      "loss": 1.606,
      "step": 4440
    },
    {
      "epoch": 2.9668278046341054,
      "grad_norm": 1.2185311317443848,
      "learning_rate": 0.00010830935362384132,
      "loss": 1.761,
      "step": 4450
    },
    {
      "epoch": 2.9734955825970997,
      "grad_norm": 1.254386305809021,
      "learning_rate": 0.00010761103412916794,
      "loss": 1.7067,
      "step": 4460
    },
    {
      "epoch": 2.9801633605600935,
      "grad_norm": 0.9591907262802124,
      "learning_rate": 0.00010691271463449455,
      "loss": 1.6529,
      "step": 4470
    },
    {
      "epoch": 2.9868311385230872,
      "grad_norm": 1.0665969848632812,
      "learning_rate": 0.00010621439513982117,
      "loss": 1.6491,
      "step": 4480
    },
    {
      "epoch": 2.993498916486081,
      "grad_norm": 1.330950379371643,
      "learning_rate": 0.0001055160756451478,
      "loss": 1.5932,
      "step": 4490
    },
    {
      "epoch": 3.0,
      "grad_norm": 1.4376790523529053,
      "learning_rate": 0.0001048177561504744,
      "loss": 1.6831,
      "step": 4500
    },
    {
      "epoch": 3.0,
      "eval_loss": 1.6279106140136719,
      "eval_runtime": 940.012,
      "eval_samples_per_second": 6.382,
      "eval_steps_per_second": 3.191,
      "step": 4500
    },
    {
      "epoch": 3.006667777962994,
      "grad_norm": 1.2760462760925293,
      "learning_rate": 0.00010411943665580103,
      "loss": 1.61,
      "step": 4510
    },
    {
      "epoch": 3.0133355559259876,
      "grad_norm": 1.1161415576934814,
      "learning_rate": 0.00010342111716112764,
      "loss": 1.6274,
      "step": 4520
    },
    {
      "epoch": 3.0200033338889813,
      "grad_norm": 1.2501312494277954,
      "learning_rate": 0.00010272279766645427,
      "loss": 1.6307,
      "step": 4530
    },
    {
      "epoch": 3.026671111851975,
      "grad_norm": 1.1764134168624878,
      "learning_rate": 0.00010202447817178088,
      "loss": 1.5529,
      "step": 4540
    },
    {
      "epoch": 3.0333388898149694,
      "grad_norm": 1.4157228469848633,
      "learning_rate": 0.00010132615867710752,
      "loss": 1.6456,
      "step": 4550
    },
    {
      "epoch": 3.040006667777963,
      "grad_norm": 1.421010971069336,
      "learning_rate": 0.00010062783918243412,
      "loss": 1.5494,
      "step": 4560
    },
    {
      "epoch": 3.046674445740957,
      "grad_norm": 1.1861226558685303,
      "learning_rate": 9.992951968776075e-05,
      "loss": 1.6673,
      "step": 4570
    },
    {
      "epoch": 3.0533422237039507,
      "grad_norm": 1.2233017683029175,
      "learning_rate": 9.923120019308737e-05,
      "loss": 1.5794,
      "step": 4580
    },
    {
      "epoch": 3.0600100016669445,
      "grad_norm": 1.1744778156280518,
      "learning_rate": 9.853288069841399e-05,
      "loss": 1.5964,
      "step": 4590
    },
    {
      "epoch": 3.0666777796299383,
      "grad_norm": 1.2686493396759033,
      "learning_rate": 9.783456120374061e-05,
      "loss": 1.6652,
      "step": 4600
    },
    {
      "epoch": 3.073345557592932,
      "grad_norm": 1.5355122089385986,
      "learning_rate": 9.713624170906723e-05,
      "loss": 1.5725,
      "step": 4610
    },
    {
      "epoch": 3.080013335555926,
      "grad_norm": 1.1227504014968872,
      "learning_rate": 9.643792221439384e-05,
      "loss": 1.6241,
      "step": 4620
    },
    {
      "epoch": 3.0866811135189196,
      "grad_norm": 1.0874139070510864,
      "learning_rate": 9.573960271972047e-05,
      "loss": 1.6817,
      "step": 4630
    },
    {
      "epoch": 3.093348891481914,
      "grad_norm": 1.35758638381958,
      "learning_rate": 9.504128322504708e-05,
      "loss": 1.6707,
      "step": 4640
    },
    {
      "epoch": 3.1000166694449076,
      "grad_norm": 1.3994132280349731,
      "learning_rate": 9.43429637303737e-05,
      "loss": 1.5896,
      "step": 4650
    },
    {
      "epoch": 3.1066844474079014,
      "grad_norm": 1.4224036931991577,
      "learning_rate": 9.364464423570032e-05,
      "loss": 1.6208,
      "step": 4660
    },
    {
      "epoch": 3.113352225370895,
      "grad_norm": 1.3138179779052734,
      "learning_rate": 9.294632474102694e-05,
      "loss": 1.5092,
      "step": 4670
    },
    {
      "epoch": 3.120020003333889,
      "grad_norm": 1.1830341815948486,
      "learning_rate": 9.224800524635356e-05,
      "loss": 1.5762,
      "step": 4680
    },
    {
      "epoch": 3.1266877812968827,
      "grad_norm": 1.7985234260559082,
      "learning_rate": 9.154968575168019e-05,
      "loss": 1.5921,
      "step": 4690
    },
    {
      "epoch": 3.1333555592598765,
      "grad_norm": 1.0222660303115845,
      "learning_rate": 9.08513662570068e-05,
      "loss": 1.7298,
      "step": 4700
    },
    {
      "epoch": 3.1400233372228703,
      "grad_norm": 1.3403475284576416,
      "learning_rate": 9.015304676233341e-05,
      "loss": 1.4935,
      "step": 4710
    },
    {
      "epoch": 3.146691115185864,
      "grad_norm": 1.1333158016204834,
      "learning_rate": 8.945472726766004e-05,
      "loss": 1.5912,
      "step": 4720
    },
    {
      "epoch": 3.1533588931488583,
      "grad_norm": 1.2337583303451538,
      "learning_rate": 8.875640777298666e-05,
      "loss": 1.552,
      "step": 4730
    },
    {
      "epoch": 3.160026671111852,
      "grad_norm": 1.1047343015670776,
      "learning_rate": 8.805808827831328e-05,
      "loss": 1.6641,
      "step": 4740
    },
    {
      "epoch": 3.166694449074846,
      "grad_norm": 1.276893138885498,
      "learning_rate": 8.73597687836399e-05,
      "loss": 1.5298,
      "step": 4750
    },
    {
      "epoch": 3.1733622270378397,
      "grad_norm": 1.0243818759918213,
      "learning_rate": 8.666144928896652e-05,
      "loss": 1.7903,
      "step": 4760
    },
    {
      "epoch": 3.1800300050008334,
      "grad_norm": 1.245377540588379,
      "learning_rate": 8.596312979429313e-05,
      "loss": 1.5979,
      "step": 4770
    },
    {
      "epoch": 3.186697782963827,
      "grad_norm": 1.341579556465149,
      "learning_rate": 8.526481029961975e-05,
      "loss": 1.7048,
      "step": 4780
    },
    {
      "epoch": 3.193365560926821,
      "grad_norm": 1.2914243936538696,
      "learning_rate": 8.456649080494637e-05,
      "loss": 1.6215,
      "step": 4790
    },
    {
      "epoch": 3.200033338889815,
      "grad_norm": 1.3439724445343018,
      "learning_rate": 8.386817131027299e-05,
      "loss": 1.7106,
      "step": 4800
    },
    {
      "epoch": 3.206701116852809,
      "grad_norm": 1.5189955234527588,
      "learning_rate": 8.316985181559961e-05,
      "loss": 1.5989,
      "step": 4810
    },
    {
      "epoch": 3.213368894815803,
      "grad_norm": 1.6806589365005493,
      "learning_rate": 8.247153232092623e-05,
      "loss": 1.6623,
      "step": 4820
    },
    {
      "epoch": 3.2200366727787966,
      "grad_norm": 1.2669243812561035,
      "learning_rate": 8.177321282625284e-05,
      "loss": 1.6562,
      "step": 4830
    },
    {
      "epoch": 3.2267044507417904,
      "grad_norm": 1.4564707279205322,
      "learning_rate": 8.107489333157948e-05,
      "loss": 1.6486,
      "step": 4840
    },
    {
      "epoch": 3.233372228704784,
      "grad_norm": 1.2933263778686523,
      "learning_rate": 8.037657383690608e-05,
      "loss": 1.5278,
      "step": 4850
    },
    {
      "epoch": 3.240040006667778,
      "grad_norm": 1.1241540908813477,
      "learning_rate": 7.967825434223272e-05,
      "loss": 1.7177,
      "step": 4860
    },
    {
      "epoch": 3.2467077846307717,
      "grad_norm": 1.3784735202789307,
      "learning_rate": 7.897993484755933e-05,
      "loss": 1.5585,
      "step": 4870
    },
    {
      "epoch": 3.2533755625937655,
      "grad_norm": 1.1017990112304688,
      "learning_rate": 7.828161535288595e-05,
      "loss": 1.7542,
      "step": 4880
    },
    {
      "epoch": 3.2600433405567593,
      "grad_norm": 1.1747511625289917,
      "learning_rate": 7.758329585821257e-05,
      "loss": 1.6152,
      "step": 4890
    },
    {
      "epoch": 3.266711118519753,
      "grad_norm": 0.9505265951156616,
      "learning_rate": 7.688497636353919e-05,
      "loss": 1.701,
      "step": 4900
    },
    {
      "epoch": 3.2733788964827473,
      "grad_norm": 1.5479224920272827,
      "learning_rate": 7.618665686886581e-05,
      "loss": 1.6379,
      "step": 4910
    },
    {
      "epoch": 3.280046674445741,
      "grad_norm": 1.3257882595062256,
      "learning_rate": 7.548833737419243e-05,
      "loss": 1.6017,
      "step": 4920
    },
    {
      "epoch": 3.286714452408735,
      "grad_norm": 1.4240752458572388,
      "learning_rate": 7.479001787951904e-05,
      "loss": 1.6409,
      "step": 4930
    },
    {
      "epoch": 3.2933822303717286,
      "grad_norm": 1.1239608526229858,
      "learning_rate": 7.409169838484567e-05,
      "loss": 1.7829,
      "step": 4940
    },
    {
      "epoch": 3.3000500083347224,
      "grad_norm": 1.7532039880752563,
      "learning_rate": 7.339337889017228e-05,
      "loss": 1.5889,
      "step": 4950
    },
    {
      "epoch": 3.306717786297716,
      "grad_norm": 1.1594310998916626,
      "learning_rate": 7.26950593954989e-05,
      "loss": 1.7159,
      "step": 4960
    },
    {
      "epoch": 3.31338556426071,
      "grad_norm": 0.8659295439720154,
      "learning_rate": 7.199673990082552e-05,
      "loss": 1.6331,
      "step": 4970
    },
    {
      "epoch": 3.320053342223704,
      "grad_norm": 1.1283482313156128,
      "learning_rate": 7.129842040615215e-05,
      "loss": 1.6349,
      "step": 4980
    },
    {
      "epoch": 3.326721120186698,
      "grad_norm": 1.7541128396987915,
      "learning_rate": 7.060010091147877e-05,
      "loss": 1.5759,
      "step": 4990
    },
    {
      "epoch": 3.3333888981496917,
      "grad_norm": 1.2798552513122559,
      "learning_rate": 6.990178141680539e-05,
      "loss": 1.6401,
      "step": 5000
    },
    {
      "epoch": 3.3400566761126855,
      "grad_norm": 1.512205958366394,
      "learning_rate": 6.9203461922132e-05,
      "loss": 1.7276,
      "step": 5010
    },
    {
      "epoch": 3.3467244540756793,
      "grad_norm": 1.4200315475463867,
      "learning_rate": 6.850514242745863e-05,
      "loss": 1.5121,
      "step": 5020
    },
    {
      "epoch": 3.353392232038673,
      "grad_norm": 1.3863775730133057,
      "learning_rate": 6.780682293278524e-05,
      "loss": 1.613,
      "step": 5030
    },
    {
      "epoch": 3.360060010001667,
      "grad_norm": 1.4010672569274902,
      "learning_rate": 6.710850343811187e-05,
      "loss": 1.581,
      "step": 5040
    },
    {
      "epoch": 3.3667277879646607,
      "grad_norm": 1.0304791927337646,
      "learning_rate": 6.641018394343848e-05,
      "loss": 1.515,
      "step": 5050
    },
    {
      "epoch": 3.3733955659276544,
      "grad_norm": 1.329466700553894,
      "learning_rate": 6.57118644487651e-05,
      "loss": 1.5683,
      "step": 5060
    },
    {
      "epoch": 3.380063343890648,
      "grad_norm": 1.317671775817871,
      "learning_rate": 6.501354495409172e-05,
      "loss": 1.7511,
      "step": 5070
    },
    {
      "epoch": 3.3867311218536424,
      "grad_norm": 0.9320998191833496,
      "learning_rate": 6.431522545941834e-05,
      "loss": 1.5828,
      "step": 5080
    },
    {
      "epoch": 3.3933988998166362,
      "grad_norm": 1.1758917570114136,
      "learning_rate": 6.361690596474495e-05,
      "loss": 1.6225,
      "step": 5090
    },
    {
      "epoch": 3.40006667777963,
      "grad_norm": 1.2021933794021606,
      "learning_rate": 6.291858647007159e-05,
      "loss": 1.7002,
      "step": 5100
    },
    {
      "epoch": 3.406734455742624,
      "grad_norm": 1.1581764221191406,
      "learning_rate": 6.22202669753982e-05,
      "loss": 1.4628,
      "step": 5110
    },
    {
      "epoch": 3.4134022337056176,
      "grad_norm": 1.3866987228393555,
      "learning_rate": 6.152194748072483e-05,
      "loss": 1.5927,
      "step": 5120
    },
    {
      "epoch": 3.4200700116686114,
      "grad_norm": 1.1836278438568115,
      "learning_rate": 6.0823627986051436e-05,
      "loss": 1.5088,
      "step": 5130
    },
    {
      "epoch": 3.426737789631605,
      "grad_norm": 1.2877531051635742,
      "learning_rate": 6.012530849137806e-05,
      "loss": 1.5203,
      "step": 5140
    },
    {
      "epoch": 3.433405567594599,
      "grad_norm": 1.1849242448806763,
      "learning_rate": 5.942698899670468e-05,
      "loss": 1.6932,
      "step": 5150
    },
    {
      "epoch": 3.440073345557593,
      "grad_norm": 1.178922176361084,
      "learning_rate": 5.872866950203129e-05,
      "loss": 1.6132,
      "step": 5160
    },
    {
      "epoch": 3.446741123520587,
      "grad_norm": 1.0906449556350708,
      "learning_rate": 5.803035000735792e-05,
      "loss": 1.6999,
      "step": 5170
    },
    {
      "epoch": 3.4534089014835807,
      "grad_norm": 1.5782288312911987,
      "learning_rate": 5.7332030512684535e-05,
      "loss": 1.5792,
      "step": 5180
    },
    {
      "epoch": 3.4600766794465745,
      "grad_norm": 0.9944142699241638,
      "learning_rate": 5.663371101801115e-05,
      "loss": 1.5523,
      "step": 5190
    },
    {
      "epoch": 3.4667444574095683,
      "grad_norm": 1.2811683416366577,
      "learning_rate": 5.593539152333778e-05,
      "loss": 1.6545,
      "step": 5200
    },
    {
      "epoch": 3.473412235372562,
      "grad_norm": 1.1203216314315796,
      "learning_rate": 5.523707202866439e-05,
      "loss": 1.6187,
      "step": 5210
    },
    {
      "epoch": 3.480080013335556,
      "grad_norm": 1.464054822921753,
      "learning_rate": 5.453875253399102e-05,
      "loss": 1.5391,
      "step": 5220
    },
    {
      "epoch": 3.4867477912985496,
      "grad_norm": 1.248974084854126,
      "learning_rate": 5.3840433039317634e-05,
      "loss": 1.5442,
      "step": 5230
    },
    {
      "epoch": 3.4934155692615434,
      "grad_norm": 1.264073133468628,
      "learning_rate": 5.314211354464425e-05,
      "loss": 1.7353,
      "step": 5240
    },
    {
      "epoch": 3.500083347224537,
      "grad_norm": 1.6947014331817627,
      "learning_rate": 5.2443794049970876e-05,
      "loss": 1.5216,
      "step": 5250
    },
    {
      "epoch": 3.5067511251875314,
      "grad_norm": 1.1550168991088867,
      "learning_rate": 5.174547455529749e-05,
      "loss": 1.7067,
      "step": 5260
    },
    {
      "epoch": 3.513418903150525,
      "grad_norm": 1.3818095922470093,
      "learning_rate": 5.104715506062411e-05,
      "loss": 1.6944,
      "step": 5270
    },
    {
      "epoch": 3.520086681113519,
      "grad_norm": 1.4608975648880005,
      "learning_rate": 5.034883556595073e-05,
      "loss": 1.5739,
      "step": 5280
    },
    {
      "epoch": 3.5267544590765127,
      "grad_norm": 0.9608869552612305,
      "learning_rate": 4.965051607127735e-05,
      "loss": 1.6045,
      "step": 5290
    },
    {
      "epoch": 3.5334222370395065,
      "grad_norm": 1.1775420904159546,
      "learning_rate": 4.895219657660397e-05,
      "loss": 1.5373,
      "step": 5300
    },
    {
      "epoch": 3.5400900150025003,
      "grad_norm": 1.060001015663147,
      "learning_rate": 4.825387708193059e-05,
      "loss": 1.5779,
      "step": 5310
    },
    {
      "epoch": 3.546757792965494,
      "grad_norm": 1.1662462949752808,
      "learning_rate": 4.755555758725721e-05,
      "loss": 1.7932,
      "step": 5320
    },
    {
      "epoch": 3.5534255709284883,
      "grad_norm": 1.3721976280212402,
      "learning_rate": 4.685723809258383e-05,
      "loss": 1.5902,
      "step": 5330
    },
    {
      "epoch": 3.560093348891482,
      "grad_norm": 1.420168399810791,
      "learning_rate": 4.6158918597910446e-05,
      "loss": 1.6144,
      "step": 5340
    },
    {
      "epoch": 3.566761126854476,
      "grad_norm": 1.4912312030792236,
      "learning_rate": 4.546059910323707e-05,
      "loss": 1.5525,
      "step": 5350
    },
    {
      "epoch": 3.5734289048174697,
      "grad_norm": 1.1504955291748047,
      "learning_rate": 4.476227960856369e-05,
      "loss": 1.5999,
      "step": 5360
    },
    {
      "epoch": 3.5800966827804634,
      "grad_norm": 1.6872450113296509,
      "learning_rate": 4.406396011389031e-05,
      "loss": 1.5459,
      "step": 5370
    },
    {
      "epoch": 3.5867644607434572,
      "grad_norm": 1.3579516410827637,
      "learning_rate": 4.3365640619216924e-05,
      "loss": 1.7072,
      "step": 5380
    },
    {
      "epoch": 3.593432238706451,
      "grad_norm": 1.2214889526367188,
      "learning_rate": 4.2667321124543545e-05,
      "loss": 1.675,
      "step": 5390
    },
    {
      "epoch": 3.600100016669445,
      "grad_norm": 1.3758604526519775,
      "learning_rate": 4.1969001629870166e-05,
      "loss": 1.6108,
      "step": 5400
    },
    {
      "epoch": 3.6067677946324386,
      "grad_norm": 1.1389421224594116,
      "learning_rate": 4.127068213519679e-05,
      "loss": 1.6018,
      "step": 5410
    },
    {
      "epoch": 3.6134355725954324,
      "grad_norm": 1.177811622619629,
      "learning_rate": 4.05723626405234e-05,
      "loss": 1.6938,
      "step": 5420
    },
    {
      "epoch": 3.620103350558426,
      "grad_norm": 1.1036336421966553,
      "learning_rate": 3.987404314585002e-05,
      "loss": 1.6167,
      "step": 5430
    },
    {
      "epoch": 3.6267711285214204,
      "grad_norm": 1.332392930984497,
      "learning_rate": 3.9175723651176644e-05,
      "loss": 1.5526,
      "step": 5440
    },
    {
      "epoch": 3.633438906484414,
      "grad_norm": 1.354536533355713,
      "learning_rate": 3.8477404156503265e-05,
      "loss": 1.6158,
      "step": 5450
    },
    {
      "epoch": 3.640106684447408,
      "grad_norm": 1.2798337936401367,
      "learning_rate": 3.7779084661829886e-05,
      "loss": 1.5284,
      "step": 5460
    },
    {
      "epoch": 3.6467744624104017,
      "grad_norm": 1.1967713832855225,
      "learning_rate": 3.70807651671565e-05,
      "loss": 1.5819,
      "step": 5470
    },
    {
      "epoch": 3.6534422403733955,
      "grad_norm": 1.3136628866195679,
      "learning_rate": 3.638244567248312e-05,
      "loss": 1.5522,
      "step": 5480
    },
    {
      "epoch": 3.6601100183363893,
      "grad_norm": 1.369765043258667,
      "learning_rate": 3.568412617780974e-05,
      "loss": 1.6133,
      "step": 5490
    },
    {
      "epoch": 3.6667777962993835,
      "grad_norm": 1.1213115453720093,
      "learning_rate": 3.4985806683136364e-05,
      "loss": 1.7186,
      "step": 5500
    },
    {
      "epoch": 3.6734455742623773,
      "grad_norm": 1.275213360786438,
      "learning_rate": 3.428748718846298e-05,
      "loss": 1.584,
      "step": 5510
    },
    {
      "epoch": 3.680113352225371,
      "grad_norm": 1.448107123374939,
      "learning_rate": 3.35891676937896e-05,
      "loss": 1.666,
      "step": 5520
    },
    {
      "epoch": 3.686781130188365,
      "grad_norm": 1.263742446899414,
      "learning_rate": 3.289084819911622e-05,
      "loss": 1.5709,
      "step": 5530
    },
    {
      "epoch": 3.6934489081513586,
      "grad_norm": 1.3154242038726807,
      "learning_rate": 3.219252870444284e-05,
      "loss": 1.6606,
      "step": 5540
    },
    {
      "epoch": 3.7001166861143524,
      "grad_norm": 1.2815344333648682,
      "learning_rate": 3.149420920976946e-05,
      "loss": 1.6306,
      "step": 5550
    },
    {
      "epoch": 3.706784464077346,
      "grad_norm": 1.134682297706604,
      "learning_rate": 3.079588971509608e-05,
      "loss": 1.5179,
      "step": 5560
    },
    {
      "epoch": 3.71345224204034,
      "grad_norm": 1.2577637434005737,
      "learning_rate": 3.00975702204227e-05,
      "loss": 1.5291,
      "step": 5570
    },
    {
      "epoch": 3.7201200200033337,
      "grad_norm": 1.3238800764083862,
      "learning_rate": 2.939925072574932e-05,
      "loss": 1.6136,
      "step": 5580
    },
    {
      "epoch": 3.7267877979663275,
      "grad_norm": 1.0202581882476807,
      "learning_rate": 2.870093123107594e-05,
      "loss": 1.6234,
      "step": 5590
    },
    {
      "epoch": 3.7334555759293213,
      "grad_norm": 1.4447731971740723,
      "learning_rate": 2.8002611736402555e-05,
      "loss": 1.6515,
      "step": 5600
    },
    {
      "epoch": 3.7401233538923155,
      "grad_norm": 1.260446310043335,
      "learning_rate": 2.7304292241729177e-05,
      "loss": 1.4987,
      "step": 5610
    },
    {
      "epoch": 3.7467911318553093,
      "grad_norm": 1.1185128688812256,
      "learning_rate": 2.6605972747055798e-05,
      "loss": 1.6339,
      "step": 5620
    },
    {
      "epoch": 3.753458909818303,
      "grad_norm": 1.3052735328674316,
      "learning_rate": 2.5907653252382415e-05,
      "loss": 1.5424,
      "step": 5630
    },
    {
      "epoch": 3.760126687781297,
      "grad_norm": 1.0942552089691162,
      "learning_rate": 2.5209333757709037e-05,
      "loss": 1.5877,
      "step": 5640
    },
    {
      "epoch": 3.7667944657442907,
      "grad_norm": 1.1350711584091187,
      "learning_rate": 2.4511014263035654e-05,
      "loss": 1.6481,
      "step": 5650
    },
    {
      "epoch": 3.7734622437072844,
      "grad_norm": 1.243507742881775,
      "learning_rate": 2.3812694768362276e-05,
      "loss": 1.6044,
      "step": 5660
    },
    {
      "epoch": 3.7801300216702782,
      "grad_norm": 1.231286883354187,
      "learning_rate": 2.3114375273688893e-05,
      "loss": 1.6193,
      "step": 5670
    },
    {
      "epoch": 3.7867977996332725,
      "grad_norm": 1.378434181213379,
      "learning_rate": 2.241605577901551e-05,
      "loss": 1.5717,
      "step": 5680
    },
    {
      "epoch": 3.7934655775962662,
      "grad_norm": 1.342939019203186,
      "learning_rate": 2.1717736284342132e-05,
      "loss": 1.6168,
      "step": 5690
    },
    {
      "epoch": 3.80013335555926,
      "grad_norm": 1.1872845888137817,
      "learning_rate": 2.101941678966875e-05,
      "loss": 1.5992,
      "step": 5700
    },
    {
      "epoch": 3.806801133522254,
      "grad_norm": 1.4975266456604004,
      "learning_rate": 2.032109729499537e-05,
      "loss": 1.5581,
      "step": 5710
    },
    {
      "epoch": 3.8134689114852476,
      "grad_norm": 1.4722304344177246,
      "learning_rate": 1.962277780032199e-05,
      "loss": 1.5438,
      "step": 5720
    },
    {
      "epoch": 3.8201366894482414,
      "grad_norm": 1.4947190284729004,
      "learning_rate": 1.892445830564861e-05,
      "loss": 1.5829,
      "step": 5730
    },
    {
      "epoch": 3.826804467411235,
      "grad_norm": 1.5354857444763184,
      "learning_rate": 1.8226138810975228e-05,
      "loss": 1.4944,
      "step": 5740
    },
    {
      "epoch": 3.833472245374229,
      "grad_norm": 1.2542518377304077,
      "learning_rate": 1.752781931630185e-05,
      "loss": 1.6528,
      "step": 5750
    },
    {
      "epoch": 3.8401400233372227,
      "grad_norm": 0.9981706738471985,
      "learning_rate": 1.682949982162847e-05,
      "loss": 1.6096,
      "step": 5760
    },
    {
      "epoch": 3.8468078013002165,
      "grad_norm": 1.2807468175888062,
      "learning_rate": 1.6131180326955088e-05,
      "loss": 1.5865,
      "step": 5770
    },
    {
      "epoch": 3.8534755792632103,
      "grad_norm": 1.3375751972198486,
      "learning_rate": 1.543286083228171e-05,
      "loss": 1.5416,
      "step": 5780
    },
    {
      "epoch": 3.8601433572262045,
      "grad_norm": 1.07326340675354,
      "learning_rate": 1.4734541337608327e-05,
      "loss": 1.6538,
      "step": 5790
    },
    {
      "epoch": 3.8668111351891983,
      "grad_norm": 1.256959319114685,
      "learning_rate": 1.4036221842934948e-05,
      "loss": 1.527,
      "step": 5800
    },
    {
      "epoch": 3.873478913152192,
      "grad_norm": 1.4591299295425415,
      "learning_rate": 1.3337902348261566e-05,
      "loss": 1.6205,
      "step": 5810
    },
    {
      "epoch": 3.880146691115186,
      "grad_norm": 1.351456642150879,
      "learning_rate": 1.2639582853588187e-05,
      "loss": 1.5881,
      "step": 5820
    },
    {
      "epoch": 3.8868144690781796,
      "grad_norm": 1.4599758386611938,
      "learning_rate": 1.1941263358914806e-05,
      "loss": 1.4414,
      "step": 5830
    },
    {
      "epoch": 3.8934822470411734,
      "grad_norm": 1.3409169912338257,
      "learning_rate": 1.1242943864241426e-05,
      "loss": 1.5402,
      "step": 5840
    },
    {
      "epoch": 3.9001500250041676,
      "grad_norm": 0.993253231048584,
      "learning_rate": 1.0544624369568045e-05,
      "loss": 1.6219,
      "step": 5850
    },
    {
      "epoch": 3.9068178029671614,
      "grad_norm": 1.1587653160095215,
      "learning_rate": 9.846304874894665e-06,
      "loss": 1.6836,
      "step": 5860
    },
    {
      "epoch": 3.913485580930155,
      "grad_norm": 1.0327348709106445,
      "learning_rate": 9.147985380221284e-06,
      "loss": 1.6941,
      "step": 5870
    },
    {
      "epoch": 3.920153358893149,
      "grad_norm": 1.1567375659942627,
      "learning_rate": 8.449665885547904e-06,
      "loss": 1.7179,
      "step": 5880
    },
    {
      "epoch": 3.9268211368561428,
      "grad_norm": 1.3889228105545044,
      "learning_rate": 7.751346390874523e-06,
      "loss": 1.617,
      "step": 5890
    },
    {
      "epoch": 3.9334889148191365,
      "grad_norm": 1.0031780004501343,
      "learning_rate": 7.053026896201143e-06,
      "loss": 1.6738,
      "step": 5900
    },
    {
      "epoch": 3.9401566927821303,
      "grad_norm": 1.3332340717315674,
      "learning_rate": 6.354707401527762e-06,
      "loss": 1.5152,
      "step": 5910
    },
    {
      "epoch": 3.946824470745124,
      "grad_norm": 1.5270272493362427,
      "learning_rate": 5.6563879068543815e-06,
      "loss": 1.5164,
      "step": 5920
    },
    {
      "epoch": 3.953492248708118,
      "grad_norm": 1.1688321828842163,
      "learning_rate": 4.958068412181001e-06,
      "loss": 1.5459,
      "step": 5930
    },
    {
      "epoch": 3.9601600266711117,
      "grad_norm": 1.1430675983428955,
      "learning_rate": 4.2597489175076204e-06,
      "loss": 1.6125,
      "step": 5940
    },
    {
      "epoch": 3.9668278046341054,
      "grad_norm": 1.2285622358322144,
      "learning_rate": 3.5614294228342407e-06,
      "loss": 1.6241,
      "step": 5950
    },
    {
      "epoch": 3.9734955825970997,
      "grad_norm": 1.3936017751693726,
      "learning_rate": 2.86310992816086e-06,
      "loss": 1.6219,
      "step": 5960
    },
    {
      "epoch": 3.9801633605600935,
      "grad_norm": 1.051208257675171,
      "learning_rate": 2.1647904334874792e-06,
      "loss": 1.718,
      "step": 5970
    },
    {
      "epoch": 3.9868311385230872,
      "grad_norm": 1.2258496284484863,
      "learning_rate": 1.466470938814099e-06,
      "loss": 1.6567,
      "step": 5980
    },
    {
      "epoch": 3.993498916486081,
      "grad_norm": 1.2973692417144775,
      "learning_rate": 7.681514441407185e-07,
      "loss": 1.5388,
      "step": 5990
    },
    {
      "epoch": 4.0,
      "grad_norm": 1.602875828742981,
      "learning_rate": 6.983194946733804e-08,
      "loss": 1.6719,
      "step": 6000
    },
    {
      "epoch": 4.0,
      "eval_loss": 1.5967330932617188,
      "eval_runtime": 938.2384,
      "eval_samples_per_second": 6.394,
      "eval_steps_per_second": 3.197,
      "step": 6000
    }
  ],
  "logging_steps": 10,
  "max_steps": 6000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 4,
  "save_steps": 500,
  "stateful_callbacks": {
    "EarlyStoppingCallback": {
      "args": {
        "early_stopping_patience": 2,
        "early_stopping_threshold": 0.0
      },
      "attributes": {
        "early_stopping_patience_counter": 0
      }
    },
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 4.1702959716551885e+18,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
