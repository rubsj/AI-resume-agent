{
  "best_global_step": 4500,
  "best_metric": 1.6245359182357788,
  "best_model_checkpoint": "json_outputs_all_data/fine-tune/optuna_output/optuna_trial_6/checkpoint-4500",
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 4500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.006667777962993832,
      "grad_norm": 0.4669632613658905,
      "learning_rate": 0.0003722425651500947,
      "loss": 2.6111,
      "step": 10
    },
    {
      "epoch": 0.013335555925987664,
      "grad_norm": 0.47788840532302856,
      "learning_rate": 0.0003714137017229068,
      "loss": 2.4334,
      "step": 20
    },
    {
      "epoch": 0.020003333888981498,
      "grad_norm": 0.4143941104412079,
      "learning_rate": 0.00037058483829571884,
      "loss": 2.39,
      "step": 30
    },
    {
      "epoch": 0.026671111851975328,
      "grad_norm": 0.3881409764289856,
      "learning_rate": 0.00036975597486853094,
      "loss": 2.4118,
      "step": 40
    },
    {
      "epoch": 0.03333888981496916,
      "grad_norm": 0.40175002813339233,
      "learning_rate": 0.00036892711144134304,
      "loss": 2.3702,
      "step": 50
    },
    {
      "epoch": 0.040006667777962995,
      "grad_norm": 0.4822803735733032,
      "learning_rate": 0.00036809824801415514,
      "loss": 2.3603,
      "step": 60
    },
    {
      "epoch": 0.046674445740956826,
      "grad_norm": 0.4184829890727997,
      "learning_rate": 0.0003672693845869672,
      "loss": 2.3296,
      "step": 70
    },
    {
      "epoch": 0.053342223703950656,
      "grad_norm": 0.4202280342578888,
      "learning_rate": 0.0003664405211597793,
      "loss": 2.3875,
      "step": 80
    },
    {
      "epoch": 0.06001000166694449,
      "grad_norm": 0.4657385051250458,
      "learning_rate": 0.0003656116577325913,
      "loss": 2.3563,
      "step": 90
    },
    {
      "epoch": 0.06667777962993832,
      "grad_norm": 0.4917095899581909,
      "learning_rate": 0.0003647827943054034,
      "loss": 2.3382,
      "step": 100
    },
    {
      "epoch": 0.07334555759293215,
      "grad_norm": 0.4877881109714508,
      "learning_rate": 0.00036395393087821547,
      "loss": 2.3462,
      "step": 110
    },
    {
      "epoch": 0.08001333555592599,
      "grad_norm": 0.4978720247745514,
      "learning_rate": 0.00036312506745102757,
      "loss": 2.3561,
      "step": 120
    },
    {
      "epoch": 0.08668111351891981,
      "grad_norm": 0.4861852526664734,
      "learning_rate": 0.00036229620402383967,
      "loss": 2.3001,
      "step": 130
    },
    {
      "epoch": 0.09334889148191365,
      "grad_norm": 0.4648638665676117,
      "learning_rate": 0.00036146734059665177,
      "loss": 2.3287,
      "step": 140
    },
    {
      "epoch": 0.10001666944490749,
      "grad_norm": 0.5047901272773743,
      "learning_rate": 0.0003606384771694638,
      "loss": 2.3165,
      "step": 150
    },
    {
      "epoch": 0.10668444740790131,
      "grad_norm": 0.5331128239631653,
      "learning_rate": 0.0003598096137422759,
      "loss": 2.286,
      "step": 160
    },
    {
      "epoch": 0.11335222537089515,
      "grad_norm": 0.5069987773895264,
      "learning_rate": 0.00035898075031508796,
      "loss": 2.2412,
      "step": 170
    },
    {
      "epoch": 0.12002000333388899,
      "grad_norm": 0.5279510617256165,
      "learning_rate": 0.00035815188688790005,
      "loss": 2.2797,
      "step": 180
    },
    {
      "epoch": 0.12668778129688282,
      "grad_norm": 0.5062550902366638,
      "learning_rate": 0.00035732302346071215,
      "loss": 2.2689,
      "step": 190
    },
    {
      "epoch": 0.13335555925987663,
      "grad_norm": 0.5429608821868896,
      "learning_rate": 0.0003564941600335242,
      "loss": 2.2391,
      "step": 200
    },
    {
      "epoch": 0.14002333722287047,
      "grad_norm": 0.5258311033248901,
      "learning_rate": 0.0003556652966063363,
      "loss": 2.2659,
      "step": 210
    },
    {
      "epoch": 0.1466911151858643,
      "grad_norm": 0.5986135005950928,
      "learning_rate": 0.0003548364331791484,
      "loss": 2.2924,
      "step": 220
    },
    {
      "epoch": 0.15335889314885814,
      "grad_norm": 0.5727757811546326,
      "learning_rate": 0.0003540075697519605,
      "loss": 2.2283,
      "step": 230
    },
    {
      "epoch": 0.16002667111185198,
      "grad_norm": 0.5322254300117493,
      "learning_rate": 0.00035317870632477254,
      "loss": 2.2166,
      "step": 240
    },
    {
      "epoch": 0.16669444907484582,
      "grad_norm": 0.5599490404129028,
      "learning_rate": 0.00035234984289758464,
      "loss": 2.2171,
      "step": 250
    },
    {
      "epoch": 0.17336222703783963,
      "grad_norm": 0.6476614475250244,
      "learning_rate": 0.0003515209794703967,
      "loss": 2.2089,
      "step": 260
    },
    {
      "epoch": 0.18003000500083347,
      "grad_norm": 0.5696126222610474,
      "learning_rate": 0.0003506921160432088,
      "loss": 2.2897,
      "step": 270
    },
    {
      "epoch": 0.1866977829638273,
      "grad_norm": 0.5660556554794312,
      "learning_rate": 0.00034986325261602083,
      "loss": 2.325,
      "step": 280
    },
    {
      "epoch": 0.19336556092682114,
      "grad_norm": 0.6174798607826233,
      "learning_rate": 0.000349034389188833,
      "loss": 2.2364,
      "step": 290
    },
    {
      "epoch": 0.20003333888981498,
      "grad_norm": 0.6206704378128052,
      "learning_rate": 0.000348205525761645,
      "loss": 2.2314,
      "step": 300
    },
    {
      "epoch": 0.2067011168528088,
      "grad_norm": 0.624812126159668,
      "learning_rate": 0.0003473766623344571,
      "loss": 2.1998,
      "step": 310
    },
    {
      "epoch": 0.21336889481580262,
      "grad_norm": 0.6259896755218506,
      "learning_rate": 0.00034654779890726917,
      "loss": 2.2236,
      "step": 320
    },
    {
      "epoch": 0.22003667277879646,
      "grad_norm": 0.6584479808807373,
      "learning_rate": 0.00034571893548008127,
      "loss": 2.1761,
      "step": 330
    },
    {
      "epoch": 0.2267044507417903,
      "grad_norm": 0.6831868886947632,
      "learning_rate": 0.0003448900720528933,
      "loss": 2.1754,
      "step": 340
    },
    {
      "epoch": 0.23337222870478413,
      "grad_norm": 0.7405436038970947,
      "learning_rate": 0.0003440612086257054,
      "loss": 2.1741,
      "step": 350
    },
    {
      "epoch": 0.24004000666777797,
      "grad_norm": 0.656455397605896,
      "learning_rate": 0.0003432323451985175,
      "loss": 2.2331,
      "step": 360
    },
    {
      "epoch": 0.24670778463077178,
      "grad_norm": 0.6557227969169617,
      "learning_rate": 0.0003424034817713296,
      "loss": 2.2368,
      "step": 370
    },
    {
      "epoch": 0.25337556259376565,
      "grad_norm": 0.7327377200126648,
      "learning_rate": 0.00034157461834414166,
      "loss": 2.2099,
      "step": 380
    },
    {
      "epoch": 0.2600433405567595,
      "grad_norm": 0.7436521649360657,
      "learning_rate": 0.00034074575491695376,
      "loss": 2.1554,
      "step": 390
    },
    {
      "epoch": 0.26671111851975327,
      "grad_norm": 1.1584140062332153,
      "learning_rate": 0.00033991689148976585,
      "loss": 2.1861,
      "step": 400
    },
    {
      "epoch": 0.2733788964827471,
      "grad_norm": 1.0181223154067993,
      "learning_rate": 0.0003390880280625779,
      "loss": 2.1374,
      "step": 410
    },
    {
      "epoch": 0.28004667444574094,
      "grad_norm": 0.974349319934845,
      "learning_rate": 0.00033825916463539,
      "loss": 2.1292,
      "step": 420
    },
    {
      "epoch": 0.2867144524087348,
      "grad_norm": 0.949829638004303,
      "learning_rate": 0.00033743030120820204,
      "loss": 2.1342,
      "step": 430
    },
    {
      "epoch": 0.2933822303717286,
      "grad_norm": 1.0135692358016968,
      "learning_rate": 0.0003366014377810142,
      "loss": 2.1531,
      "step": 440
    },
    {
      "epoch": 0.30005000833472245,
      "grad_norm": 0.8496396541595459,
      "learning_rate": 0.00033577257435382624,
      "loss": 2.1985,
      "step": 450
    },
    {
      "epoch": 0.3067177862977163,
      "grad_norm": 0.7402092814445496,
      "learning_rate": 0.00033494371092663834,
      "loss": 2.2101,
      "step": 460
    },
    {
      "epoch": 0.3133855642607101,
      "grad_norm": 0.7758647799491882,
      "learning_rate": 0.0003341148474994504,
      "loss": 2.2089,
      "step": 470
    },
    {
      "epoch": 0.32005334222370396,
      "grad_norm": 0.7496413588523865,
      "learning_rate": 0.0003332859840722625,
      "loss": 2.1117,
      "step": 480
    },
    {
      "epoch": 0.3267211201866978,
      "grad_norm": 1.2912827730178833,
      "learning_rate": 0.00033245712064507453,
      "loss": 2.1381,
      "step": 490
    },
    {
      "epoch": 0.33338889814969164,
      "grad_norm": 0.8616319298744202,
      "learning_rate": 0.00033162825721788663,
      "loss": 2.1587,
      "step": 500
    },
    {
      "epoch": 0.3400566761126855,
      "grad_norm": 0.739643931388855,
      "learning_rate": 0.0003307993937906987,
      "loss": 2.1544,
      "step": 510
    },
    {
      "epoch": 0.34672445407567926,
      "grad_norm": 0.9263166189193726,
      "learning_rate": 0.0003299705303635108,
      "loss": 2.087,
      "step": 520
    },
    {
      "epoch": 0.3533922320386731,
      "grad_norm": 0.8754060864448547,
      "learning_rate": 0.00032914166693632287,
      "loss": 2.1059,
      "step": 530
    },
    {
      "epoch": 0.36006001000166693,
      "grad_norm": 0.9422492384910583,
      "learning_rate": 0.00032831280350913497,
      "loss": 2.0567,
      "step": 540
    },
    {
      "epoch": 0.36672778796466077,
      "grad_norm": 0.8335532546043396,
      "learning_rate": 0.000327483940081947,
      "loss": 2.0824,
      "step": 550
    },
    {
      "epoch": 0.3733955659276546,
      "grad_norm": 0.7959364652633667,
      "learning_rate": 0.0003266550766547591,
      "loss": 1.9722,
      "step": 560
    },
    {
      "epoch": 0.38006334389064844,
      "grad_norm": 0.9496449828147888,
      "learning_rate": 0.0003258262132275712,
      "loss": 2.1045,
      "step": 570
    },
    {
      "epoch": 0.3867311218536423,
      "grad_norm": 0.6415063738822937,
      "learning_rate": 0.00032499734980038326,
      "loss": 2.1243,
      "step": 580
    },
    {
      "epoch": 0.3933988998166361,
      "grad_norm": 0.7093745470046997,
      "learning_rate": 0.00032416848637319536,
      "loss": 2.1128,
      "step": 590
    },
    {
      "epoch": 0.40006667777962995,
      "grad_norm": 0.7698641419410706,
      "learning_rate": 0.00032333962294600746,
      "loss": 2.0178,
      "step": 600
    },
    {
      "epoch": 0.4067344557426238,
      "grad_norm": 0.9130144119262695,
      "learning_rate": 0.00032251075951881956,
      "loss": 2.1026,
      "step": 610
    },
    {
      "epoch": 0.4134022337056176,
      "grad_norm": 0.9151550531387329,
      "learning_rate": 0.0003216818960916316,
      "loss": 2.0347,
      "step": 620
    },
    {
      "epoch": 0.4200700116686114,
      "grad_norm": 1.0923283100128174,
      "learning_rate": 0.0003208530326644437,
      "loss": 2.0827,
      "step": 630
    },
    {
      "epoch": 0.42673778963160525,
      "grad_norm": 1.133553147315979,
      "learning_rate": 0.00032002416923725574,
      "loss": 2.0444,
      "step": 640
    },
    {
      "epoch": 0.4334055675945991,
      "grad_norm": 1.1928887367248535,
      "learning_rate": 0.00031919530581006784,
      "loss": 2.0678,
      "step": 650
    },
    {
      "epoch": 0.4400733455575929,
      "grad_norm": 0.8382468819618225,
      "learning_rate": 0.0003183664423828799,
      "loss": 2.0908,
      "step": 660
    },
    {
      "epoch": 0.44674112352058676,
      "grad_norm": 1.2922494411468506,
      "learning_rate": 0.00031753757895569204,
      "loss": 1.9997,
      "step": 670
    },
    {
      "epoch": 0.4534089014835806,
      "grad_norm": 0.7766062021255493,
      "learning_rate": 0.0003167087155285041,
      "loss": 2.0777,
      "step": 680
    },
    {
      "epoch": 0.46007667944657443,
      "grad_norm": 1.295320987701416,
      "learning_rate": 0.0003158798521013162,
      "loss": 2.0018,
      "step": 690
    },
    {
      "epoch": 0.46674445740956827,
      "grad_norm": 1.303778052330017,
      "learning_rate": 0.00031505098867412823,
      "loss": 2.0001,
      "step": 700
    },
    {
      "epoch": 0.4734122353725621,
      "grad_norm": 1.3663392066955566,
      "learning_rate": 0.00031422212524694033,
      "loss": 2.0411,
      "step": 710
    },
    {
      "epoch": 0.48008001333555594,
      "grad_norm": 0.9104655385017395,
      "learning_rate": 0.0003133932618197524,
      "loss": 1.9572,
      "step": 720
    },
    {
      "epoch": 0.4867477912985498,
      "grad_norm": 1.1339695453643799,
      "learning_rate": 0.0003125643983925645,
      "loss": 2.081,
      "step": 730
    },
    {
      "epoch": 0.49341556926154356,
      "grad_norm": 1.464197039604187,
      "learning_rate": 0.00031173553496537657,
      "loss": 2.0514,
      "step": 740
    },
    {
      "epoch": 0.5000833472245374,
      "grad_norm": 1.013063669204712,
      "learning_rate": 0.00031090667153818867,
      "loss": 2.0005,
      "step": 750
    },
    {
      "epoch": 0.5067511251875313,
      "grad_norm": 0.973540186882019,
      "learning_rate": 0.0003100778081110007,
      "loss": 2.0463,
      "step": 760
    },
    {
      "epoch": 0.5134189031505251,
      "grad_norm": 1.0563175678253174,
      "learning_rate": 0.0003092489446838128,
      "loss": 1.9633,
      "step": 770
    },
    {
      "epoch": 0.520086681113519,
      "grad_norm": 1.023362636566162,
      "learning_rate": 0.0003084200812566249,
      "loss": 2.0048,
      "step": 780
    },
    {
      "epoch": 0.5267544590765127,
      "grad_norm": 0.8521928787231445,
      "learning_rate": 0.00030759121782943696,
      "loss": 2.0315,
      "step": 790
    },
    {
      "epoch": 0.5334222370395065,
      "grad_norm": 1.5038124322891235,
      "learning_rate": 0.00030676235440224906,
      "loss": 2.0102,
      "step": 800
    },
    {
      "epoch": 0.5400900150025004,
      "grad_norm": 0.780642032623291,
      "learning_rate": 0.0003059334909750611,
      "loss": 2.1158,
      "step": 810
    },
    {
      "epoch": 0.5467577929654942,
      "grad_norm": 1.006760835647583,
      "learning_rate": 0.0003051046275478732,
      "loss": 1.9372,
      "step": 820
    },
    {
      "epoch": 0.5534255709284881,
      "grad_norm": 1.1172791719436646,
      "learning_rate": 0.0003042757641206853,
      "loss": 1.9918,
      "step": 830
    },
    {
      "epoch": 0.5600933488914819,
      "grad_norm": 1.6688951253890991,
      "learning_rate": 0.0003034469006934974,
      "loss": 2.0151,
      "step": 840
    },
    {
      "epoch": 0.5667611268544758,
      "grad_norm": 1.046115517616272,
      "learning_rate": 0.00030261803726630945,
      "loss": 1.9744,
      "step": 850
    },
    {
      "epoch": 0.5734289048174696,
      "grad_norm": 1.5769919157028198,
      "learning_rate": 0.00030178917383912154,
      "loss": 1.9824,
      "step": 860
    },
    {
      "epoch": 0.5800966827804634,
      "grad_norm": 1.0145776271820068,
      "learning_rate": 0.0003009603104119336,
      "loss": 2.0423,
      "step": 870
    },
    {
      "epoch": 0.5867644607434572,
      "grad_norm": 1.3737579584121704,
      "learning_rate": 0.0003001314469847457,
      "loss": 1.8486,
      "step": 880
    },
    {
      "epoch": 0.5934322387064511,
      "grad_norm": 1.3021759986877441,
      "learning_rate": 0.00029930258355755773,
      "loss": 1.9876,
      "step": 890
    },
    {
      "epoch": 0.6001000166694449,
      "grad_norm": 0.8400405049324036,
      "learning_rate": 0.00029847372013036983,
      "loss": 1.9786,
      "step": 900
    },
    {
      "epoch": 0.6067677946324388,
      "grad_norm": 1.2210954427719116,
      "learning_rate": 0.00029764485670318193,
      "loss": 1.9461,
      "step": 910
    },
    {
      "epoch": 0.6134355725954326,
      "grad_norm": 1.2802451848983765,
      "learning_rate": 0.00029681599327599403,
      "loss": 2.0265,
      "step": 920
    },
    {
      "epoch": 0.6201033505584264,
      "grad_norm": 1.0444350242614746,
      "learning_rate": 0.0002959871298488061,
      "loss": 2.0502,
      "step": 930
    },
    {
      "epoch": 0.6267711285214203,
      "grad_norm": 1.3295387029647827,
      "learning_rate": 0.0002951582664216182,
      "loss": 2.0115,
      "step": 940
    },
    {
      "epoch": 0.633438906484414,
      "grad_norm": 1.3628690242767334,
      "learning_rate": 0.0002943294029944303,
      "loss": 1.9759,
      "step": 950
    },
    {
      "epoch": 0.6401066844474079,
      "grad_norm": 1.9723105430603027,
      "learning_rate": 0.0002935005395672423,
      "loss": 2.0002,
      "step": 960
    },
    {
      "epoch": 0.6467744624104017,
      "grad_norm": 1.1189887523651123,
      "learning_rate": 0.0002926716761400544,
      "loss": 1.9762,
      "step": 970
    },
    {
      "epoch": 0.6534422403733956,
      "grad_norm": 1.2111217975616455,
      "learning_rate": 0.0002918428127128665,
      "loss": 1.9229,
      "step": 980
    },
    {
      "epoch": 0.6601100183363894,
      "grad_norm": 1.0519137382507324,
      "learning_rate": 0.0002910139492856786,
      "loss": 1.9178,
      "step": 990
    },
    {
      "epoch": 0.6667777962993833,
      "grad_norm": 1.2050846815109253,
      "learning_rate": 0.00029018508585849066,
      "loss": 2.0308,
      "step": 1000
    },
    {
      "epoch": 0.6734455742623771,
      "grad_norm": 1.1845167875289917,
      "learning_rate": 0.00028935622243130276,
      "loss": 1.8434,
      "step": 1010
    },
    {
      "epoch": 0.680113352225371,
      "grad_norm": 1.3739588260650635,
      "learning_rate": 0.0002885273590041148,
      "loss": 1.9361,
      "step": 1020
    },
    {
      "epoch": 0.6867811301883647,
      "grad_norm": 1.523878574371338,
      "learning_rate": 0.0002876984955769269,
      "loss": 1.9909,
      "step": 1030
    },
    {
      "epoch": 0.6934489081513585,
      "grad_norm": 0.9964140057563782,
      "learning_rate": 0.00028686963214973895,
      "loss": 1.9098,
      "step": 1040
    },
    {
      "epoch": 0.7001166861143524,
      "grad_norm": 1.3218742609024048,
      "learning_rate": 0.00028604076872255105,
      "loss": 1.8697,
      "step": 1050
    },
    {
      "epoch": 0.7067844640773462,
      "grad_norm": 1.55715811252594,
      "learning_rate": 0.00028521190529536315,
      "loss": 1.838,
      "step": 1060
    },
    {
      "epoch": 0.7134522420403401,
      "grad_norm": 1.2858188152313232,
      "learning_rate": 0.00028438304186817525,
      "loss": 1.9726,
      "step": 1070
    },
    {
      "epoch": 0.7201200200033339,
      "grad_norm": 1.5585863590240479,
      "learning_rate": 0.0002835541784409873,
      "loss": 1.9104,
      "step": 1080
    },
    {
      "epoch": 0.7267877979663278,
      "grad_norm": 0.8557196259498596,
      "learning_rate": 0.0002827253150137994,
      "loss": 1.9495,
      "step": 1090
    },
    {
      "epoch": 0.7334555759293215,
      "grad_norm": 1.6553868055343628,
      "learning_rate": 0.00028189645158661143,
      "loss": 1.9079,
      "step": 1100
    },
    {
      "epoch": 0.7401233538923154,
      "grad_norm": 1.3753130435943604,
      "learning_rate": 0.00028106758815942353,
      "loss": 1.887,
      "step": 1110
    },
    {
      "epoch": 0.7467911318553092,
      "grad_norm": 1.5824077129364014,
      "learning_rate": 0.00028023872473223563,
      "loss": 1.9991,
      "step": 1120
    },
    {
      "epoch": 0.7534589098183031,
      "grad_norm": 1.590487003326416,
      "learning_rate": 0.0002794098613050477,
      "loss": 1.931,
      "step": 1130
    },
    {
      "epoch": 0.7601266877812969,
      "grad_norm": 1.3336905241012573,
      "learning_rate": 0.0002785809978778598,
      "loss": 1.8947,
      "step": 1140
    },
    {
      "epoch": 0.7667944657442907,
      "grad_norm": 1.4625515937805176,
      "learning_rate": 0.0002777521344506719,
      "loss": 1.8684,
      "step": 1150
    },
    {
      "epoch": 0.7734622437072846,
      "grad_norm": 1.1147676706314087,
      "learning_rate": 0.000276923271023484,
      "loss": 1.9276,
      "step": 1160
    },
    {
      "epoch": 0.7801300216702783,
      "grad_norm": 1.4938182830810547,
      "learning_rate": 0.000276094407596296,
      "loss": 1.8713,
      "step": 1170
    },
    {
      "epoch": 0.7867977996332722,
      "grad_norm": 0.8823300004005432,
      "learning_rate": 0.0002752655441691081,
      "loss": 1.9396,
      "step": 1180
    },
    {
      "epoch": 0.793465577596266,
      "grad_norm": 1.805251121520996,
      "learning_rate": 0.00027443668074192016,
      "loss": 1.9481,
      "step": 1190
    },
    {
      "epoch": 0.8001333555592599,
      "grad_norm": 1.8441582918167114,
      "learning_rate": 0.00027360781731473226,
      "loss": 1.882,
      "step": 1200
    },
    {
      "epoch": 0.8068011335222537,
      "grad_norm": 1.545931339263916,
      "learning_rate": 0.0002727789538875443,
      "loss": 1.9738,
      "step": 1210
    },
    {
      "epoch": 0.8134689114852476,
      "grad_norm": 1.1092675924301147,
      "learning_rate": 0.00027195009046035646,
      "loss": 1.8078,
      "step": 1220
    },
    {
      "epoch": 0.8201366894482414,
      "grad_norm": 1.6223193407058716,
      "learning_rate": 0.0002711212270331685,
      "loss": 1.964,
      "step": 1230
    },
    {
      "epoch": 0.8268044674112353,
      "grad_norm": 1.4971380233764648,
      "learning_rate": 0.0002702923636059806,
      "loss": 1.8729,
      "step": 1240
    },
    {
      "epoch": 0.833472245374229,
      "grad_norm": 1.565187692642212,
      "learning_rate": 0.00026946350017879265,
      "loss": 1.9388,
      "step": 1250
    },
    {
      "epoch": 0.8401400233372228,
      "grad_norm": 1.7848262786865234,
      "learning_rate": 0.00026863463675160475,
      "loss": 1.9233,
      "step": 1260
    },
    {
      "epoch": 0.8468078013002167,
      "grad_norm": 2.656618595123291,
      "learning_rate": 0.0002678057733244168,
      "loss": 1.8347,
      "step": 1270
    },
    {
      "epoch": 0.8534755792632105,
      "grad_norm": 1.2738276720046997,
      "learning_rate": 0.0002669769098972289,
      "loss": 1.9203,
      "step": 1280
    },
    {
      "epoch": 0.8601433572262044,
      "grad_norm": 1.5125216245651245,
      "learning_rate": 0.000266148046470041,
      "loss": 1.9394,
      "step": 1290
    },
    {
      "epoch": 0.8668111351891982,
      "grad_norm": 1.4103766679763794,
      "learning_rate": 0.0002653191830428531,
      "loss": 1.8827,
      "step": 1300
    },
    {
      "epoch": 0.8734789131521921,
      "grad_norm": 1.5216245651245117,
      "learning_rate": 0.00026449031961566513,
      "loss": 1.8838,
      "step": 1310
    },
    {
      "epoch": 0.8801466911151858,
      "grad_norm": 1.6392872333526611,
      "learning_rate": 0.00026366145618847723,
      "loss": 1.8554,
      "step": 1320
    },
    {
      "epoch": 0.8868144690781797,
      "grad_norm": 1.7247987985610962,
      "learning_rate": 0.00026283259276128933,
      "loss": 1.8701,
      "step": 1330
    },
    {
      "epoch": 0.8934822470411735,
      "grad_norm": 1.0955449342727661,
      "learning_rate": 0.0002620037293341014,
      "loss": 1.8741,
      "step": 1340
    },
    {
      "epoch": 0.9001500250041674,
      "grad_norm": 1.4653024673461914,
      "learning_rate": 0.0002611748659069135,
      "loss": 1.8384,
      "step": 1350
    },
    {
      "epoch": 0.9068178029671612,
      "grad_norm": 1.2770999670028687,
      "learning_rate": 0.0002603460024797255,
      "loss": 1.8695,
      "step": 1360
    },
    {
      "epoch": 0.913485580930155,
      "grad_norm": 1.3996180295944214,
      "learning_rate": 0.0002595171390525376,
      "loss": 1.7766,
      "step": 1370
    },
    {
      "epoch": 0.9201533588931489,
      "grad_norm": 1.405342936515808,
      "learning_rate": 0.0002586882756253497,
      "loss": 1.8897,
      "step": 1380
    },
    {
      "epoch": 0.9268211368561426,
      "grad_norm": 1.927616000175476,
      "learning_rate": 0.0002578594121981618,
      "loss": 1.9936,
      "step": 1390
    },
    {
      "epoch": 0.9334889148191365,
      "grad_norm": 1.6960381269454956,
      "learning_rate": 0.00025703054877097386,
      "loss": 1.903,
      "step": 1400
    },
    {
      "epoch": 0.9401566927821303,
      "grad_norm": 1.7388180494308472,
      "learning_rate": 0.00025620168534378596,
      "loss": 1.936,
      "step": 1410
    },
    {
      "epoch": 0.9468244707451242,
      "grad_norm": 1.1916636228561401,
      "learning_rate": 0.000255372821916598,
      "loss": 1.8868,
      "step": 1420
    },
    {
      "epoch": 0.953492248708118,
      "grad_norm": 1.0541261434555054,
      "learning_rate": 0.0002545439584894101,
      "loss": 2.001,
      "step": 1430
    },
    {
      "epoch": 0.9601600266711119,
      "grad_norm": 1.6852326393127441,
      "learning_rate": 0.00025371509506222215,
      "loss": 1.8864,
      "step": 1440
    },
    {
      "epoch": 0.9668278046341057,
      "grad_norm": 2.2026491165161133,
      "learning_rate": 0.0002528862316350343,
      "loss": 1.8271,
      "step": 1450
    },
    {
      "epoch": 0.9734955825970996,
      "grad_norm": 1.5162855386734009,
      "learning_rate": 0.00025205736820784635,
      "loss": 1.7975,
      "step": 1460
    },
    {
      "epoch": 0.9801633605600933,
      "grad_norm": 2.842106819152832,
      "learning_rate": 0.00025122850478065845,
      "loss": 1.8471,
      "step": 1470
    },
    {
      "epoch": 0.9868311385230871,
      "grad_norm": 1.6971137523651123,
      "learning_rate": 0.0002503996413534705,
      "loss": 1.8695,
      "step": 1480
    },
    {
      "epoch": 0.993498916486081,
      "grad_norm": 1.4114668369293213,
      "learning_rate": 0.0002495707779262826,
      "loss": 1.8131,
      "step": 1490
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.6898571252822876,
      "learning_rate": 0.00024874191449909464,
      "loss": 1.9262,
      "step": 1500
    },
    {
      "epoch": 1.0,
      "eval_loss": 1.8454452753067017,
      "eval_runtime": 937.8314,
      "eval_samples_per_second": 6.397,
      "eval_steps_per_second": 3.199,
      "step": 1500
    },
    {
      "epoch": 1.0066677779629938,
      "grad_norm": 1.6582889556884766,
      "learning_rate": 0.00024791305107190674,
      "loss": 1.7786,
      "step": 1510
    },
    {
      "epoch": 1.0133355559259876,
      "grad_norm": 1.4919606447219849,
      "learning_rate": 0.00024708418764471884,
      "loss": 1.8303,
      "step": 1520
    },
    {
      "epoch": 1.0200033338889816,
      "grad_norm": 1.305558443069458,
      "learning_rate": 0.00024625532421753093,
      "loss": 1.8649,
      "step": 1530
    },
    {
      "epoch": 1.0266711118519753,
      "grad_norm": 1.2413841485977173,
      "learning_rate": 0.000245426460790343,
      "loss": 1.8959,
      "step": 1540
    },
    {
      "epoch": 1.0333388898149691,
      "grad_norm": 1.1993811130523682,
      "learning_rate": 0.0002445975973631551,
      "loss": 1.8978,
      "step": 1550
    },
    {
      "epoch": 1.040006667777963,
      "grad_norm": 1.3511881828308105,
      "learning_rate": 0.00024376873393596715,
      "loss": 1.8336,
      "step": 1560
    },
    {
      "epoch": 1.046674445740957,
      "grad_norm": 1.0581620931625366,
      "learning_rate": 0.00024293987050877922,
      "loss": 1.8633,
      "step": 1570
    },
    {
      "epoch": 1.0533422237039507,
      "grad_norm": 1.542506217956543,
      "learning_rate": 0.0002421110070815913,
      "loss": 1.7546,
      "step": 1580
    },
    {
      "epoch": 1.0600100016669445,
      "grad_norm": 1.517530083656311,
      "learning_rate": 0.0002412821436544034,
      "loss": 1.7604,
      "step": 1590
    },
    {
      "epoch": 1.0666777796299383,
      "grad_norm": 1.4286270141601562,
      "learning_rate": 0.0002404532802272155,
      "loss": 1.8748,
      "step": 1600
    },
    {
      "epoch": 1.073345557592932,
      "grad_norm": 1.6277472972869873,
      "learning_rate": 0.00023962441680002756,
      "loss": 1.7793,
      "step": 1610
    },
    {
      "epoch": 1.080013335555926,
      "grad_norm": 1.245134711265564,
      "learning_rate": 0.00023879555337283964,
      "loss": 1.8814,
      "step": 1620
    },
    {
      "epoch": 1.0866811135189198,
      "grad_norm": 1.0206029415130615,
      "learning_rate": 0.00023796668994565174,
      "loss": 1.8602,
      "step": 1630
    },
    {
      "epoch": 1.0933488914819136,
      "grad_norm": 1.186353325843811,
      "learning_rate": 0.0002371378265184638,
      "loss": 1.7888,
      "step": 1640
    },
    {
      "epoch": 1.1000166694449074,
      "grad_norm": 1.7597112655639648,
      "learning_rate": 0.00023630896309127588,
      "loss": 1.8371,
      "step": 1650
    },
    {
      "epoch": 1.1066844474079014,
      "grad_norm": 1.4314680099487305,
      "learning_rate": 0.00023548009966408795,
      "loss": 1.7713,
      "step": 1660
    },
    {
      "epoch": 1.1133522253708952,
      "grad_norm": 1.5388933420181274,
      "learning_rate": 0.00023465123623690002,
      "loss": 1.7521,
      "step": 1670
    },
    {
      "epoch": 1.120020003333889,
      "grad_norm": 2.30867075920105,
      "learning_rate": 0.00023382237280971215,
      "loss": 1.8097,
      "step": 1680
    },
    {
      "epoch": 1.1266877812968827,
      "grad_norm": 1.5541037321090698,
      "learning_rate": 0.00023299350938252422,
      "loss": 1.8144,
      "step": 1690
    },
    {
      "epoch": 1.1333555592598765,
      "grad_norm": 1.7900447845458984,
      "learning_rate": 0.0002321646459553363,
      "loss": 1.7103,
      "step": 1700
    },
    {
      "epoch": 1.1400233372228705,
      "grad_norm": 1.950342059135437,
      "learning_rate": 0.00023133578252814837,
      "loss": 1.7735,
      "step": 1710
    },
    {
      "epoch": 1.1466911151858643,
      "grad_norm": 1.3190224170684814,
      "learning_rate": 0.00023050691910096044,
      "loss": 1.7878,
      "step": 1720
    },
    {
      "epoch": 1.153358893148858,
      "grad_norm": 1.1796369552612305,
      "learning_rate": 0.0002296780556737725,
      "loss": 1.7122,
      "step": 1730
    },
    {
      "epoch": 1.160026671111852,
      "grad_norm": 1.926487922668457,
      "learning_rate": 0.00022884919224658458,
      "loss": 1.8523,
      "step": 1740
    },
    {
      "epoch": 1.1666944490748459,
      "grad_norm": 1.120153784751892,
      "learning_rate": 0.00022802032881939665,
      "loss": 1.7083,
      "step": 1750
    },
    {
      "epoch": 1.1733622270378397,
      "grad_norm": 1.17902672290802,
      "learning_rate": 0.00022719146539220878,
      "loss": 1.7533,
      "step": 1760
    },
    {
      "epoch": 1.1800300050008334,
      "grad_norm": 1.4300169944763184,
      "learning_rate": 0.00022636260196502085,
      "loss": 1.8024,
      "step": 1770
    },
    {
      "epoch": 1.1866977829638272,
      "grad_norm": 1.7532416582107544,
      "learning_rate": 0.00022553373853783292,
      "loss": 1.7293,
      "step": 1780
    },
    {
      "epoch": 1.1933655609268212,
      "grad_norm": 1.4687714576721191,
      "learning_rate": 0.000224704875110645,
      "loss": 1.7416,
      "step": 1790
    },
    {
      "epoch": 1.200033338889815,
      "grad_norm": 1.1998281478881836,
      "learning_rate": 0.0002238760116834571,
      "loss": 1.8082,
      "step": 1800
    },
    {
      "epoch": 1.2067011168528088,
      "grad_norm": 1.338095784187317,
      "learning_rate": 0.00022304714825626917,
      "loss": 1.8269,
      "step": 1810
    },
    {
      "epoch": 1.2133688948158026,
      "grad_norm": 1.3841261863708496,
      "learning_rate": 0.00022221828482908124,
      "loss": 1.7127,
      "step": 1820
    },
    {
      "epoch": 1.2200366727787966,
      "grad_norm": 1.3284716606140137,
      "learning_rate": 0.0002213894214018933,
      "loss": 1.7992,
      "step": 1830
    },
    {
      "epoch": 1.2267044507417904,
      "grad_norm": 1.636233925819397,
      "learning_rate": 0.00022056055797470544,
      "loss": 1.7922,
      "step": 1840
    },
    {
      "epoch": 1.2333722287047841,
      "grad_norm": 1.6385228633880615,
      "learning_rate": 0.0002197316945475175,
      "loss": 1.784,
      "step": 1850
    },
    {
      "epoch": 1.240040006667778,
      "grad_norm": 2.1042988300323486,
      "learning_rate": 0.00021890283112032958,
      "loss": 1.7788,
      "step": 1860
    },
    {
      "epoch": 1.2467077846307717,
      "grad_norm": 1.5670130252838135,
      "learning_rate": 0.00021807396769314165,
      "loss": 1.748,
      "step": 1870
    },
    {
      "epoch": 1.2533755625937657,
      "grad_norm": 1.4555819034576416,
      "learning_rate": 0.00021724510426595372,
      "loss": 1.8201,
      "step": 1880
    },
    {
      "epoch": 1.2600433405567595,
      "grad_norm": 1.6908644437789917,
      "learning_rate": 0.0002164162408387658,
      "loss": 1.7892,
      "step": 1890
    },
    {
      "epoch": 1.2667111185197533,
      "grad_norm": 1.4725371599197388,
      "learning_rate": 0.00021558737741157787,
      "loss": 1.8731,
      "step": 1900
    },
    {
      "epoch": 1.273378896482747,
      "grad_norm": 1.4662179946899414,
      "learning_rate": 0.00021475851398439,
      "loss": 1.8171,
      "step": 1910
    },
    {
      "epoch": 1.280046674445741,
      "grad_norm": 1.5002516508102417,
      "learning_rate": 0.00021392965055720207,
      "loss": 1.8449,
      "step": 1920
    },
    {
      "epoch": 1.2867144524087348,
      "grad_norm": 1.9041340351104736,
      "learning_rate": 0.00021310078713001414,
      "loss": 1.8357,
      "step": 1930
    },
    {
      "epoch": 1.2933822303717286,
      "grad_norm": 1.8317232131958008,
      "learning_rate": 0.0002122719237028262,
      "loss": 1.6839,
      "step": 1940
    },
    {
      "epoch": 1.3000500083347224,
      "grad_norm": 1.682023286819458,
      "learning_rate": 0.00021144306027563828,
      "loss": 1.7243,
      "step": 1950
    },
    {
      "epoch": 1.3067177862977162,
      "grad_norm": 2.4257054328918457,
      "learning_rate": 0.00021061419684845035,
      "loss": 1.7765,
      "step": 1960
    },
    {
      "epoch": 1.3133855642607102,
      "grad_norm": 1.8135712146759033,
      "learning_rate": 0.00020978533342126245,
      "loss": 1.7141,
      "step": 1970
    },
    {
      "epoch": 1.320053342223704,
      "grad_norm": 1.3840585947036743,
      "learning_rate": 0.00020895646999407453,
      "loss": 1.8302,
      "step": 1980
    },
    {
      "epoch": 1.3267211201866977,
      "grad_norm": 1.6915236711502075,
      "learning_rate": 0.00020812760656688662,
      "loss": 1.6663,
      "step": 1990
    },
    {
      "epoch": 1.3333888981496917,
      "grad_norm": 2.2412734031677246,
      "learning_rate": 0.0002072987431396987,
      "loss": 1.7908,
      "step": 2000
    },
    {
      "epoch": 1.3400566761126855,
      "grad_norm": 1.7453763484954834,
      "learning_rate": 0.0002064698797125108,
      "loss": 1.7524,
      "step": 2010
    },
    {
      "epoch": 1.3467244540756793,
      "grad_norm": 1.6860737800598145,
      "learning_rate": 0.00020564101628532287,
      "loss": 1.6868,
      "step": 2020
    },
    {
      "epoch": 1.353392232038673,
      "grad_norm": 1.444348692893982,
      "learning_rate": 0.00020481215285813494,
      "loss": 1.7098,
      "step": 2030
    },
    {
      "epoch": 1.3600600100016669,
      "grad_norm": 1.2683374881744385,
      "learning_rate": 0.000203983289430947,
      "loss": 1.7746,
      "step": 2040
    },
    {
      "epoch": 1.3667277879646607,
      "grad_norm": 2.1438663005828857,
      "learning_rate": 0.00020315442600375908,
      "loss": 1.602,
      "step": 2050
    },
    {
      "epoch": 1.3733955659276547,
      "grad_norm": 3.2396738529205322,
      "learning_rate": 0.00020232556257657116,
      "loss": 1.7237,
      "step": 2060
    },
    {
      "epoch": 1.3800633438906484,
      "grad_norm": 1.8407009840011597,
      "learning_rate": 0.00020149669914938328,
      "loss": 1.8121,
      "step": 2070
    },
    {
      "epoch": 1.3867311218536422,
      "grad_norm": 1.7609434127807617,
      "learning_rate": 0.00020066783572219535,
      "loss": 1.7936,
      "step": 2080
    },
    {
      "epoch": 1.3933988998166362,
      "grad_norm": 1.9245222806930542,
      "learning_rate": 0.00019983897229500743,
      "loss": 1.6637,
      "step": 2090
    },
    {
      "epoch": 1.40006667777963,
      "grad_norm": 1.359649419784546,
      "learning_rate": 0.0001990101088678195,
      "loss": 1.8594,
      "step": 2100
    },
    {
      "epoch": 1.4067344557426238,
      "grad_norm": 1.1194932460784912,
      "learning_rate": 0.00019818124544063157,
      "loss": 1.8076,
      "step": 2110
    },
    {
      "epoch": 1.4134022337056176,
      "grad_norm": 1.9828214645385742,
      "learning_rate": 0.00019735238201344364,
      "loss": 1.6956,
      "step": 2120
    },
    {
      "epoch": 1.4200700116686114,
      "grad_norm": 1.4913049936294556,
      "learning_rate": 0.0001965235185862557,
      "loss": 1.8592,
      "step": 2130
    },
    {
      "epoch": 1.4267377896316051,
      "grad_norm": 1.7652688026428223,
      "learning_rate": 0.0001956946551590678,
      "loss": 1.7073,
      "step": 2140
    },
    {
      "epoch": 1.4334055675945991,
      "grad_norm": 1.4731124639511108,
      "learning_rate": 0.0001948657917318799,
      "loss": 1.7985,
      "step": 2150
    },
    {
      "epoch": 1.440073345557593,
      "grad_norm": 1.8941649198532104,
      "learning_rate": 0.00019403692830469198,
      "loss": 1.7438,
      "step": 2160
    },
    {
      "epoch": 1.4467411235205867,
      "grad_norm": 1.4624967575073242,
      "learning_rate": 0.00019320806487750406,
      "loss": 1.8416,
      "step": 2170
    },
    {
      "epoch": 1.4534089014835807,
      "grad_norm": 1.7987061738967896,
      "learning_rate": 0.00019237920145031615,
      "loss": 1.6978,
      "step": 2180
    },
    {
      "epoch": 1.4600766794465745,
      "grad_norm": 1.4372336864471436,
      "learning_rate": 0.00019155033802312823,
      "loss": 1.7128,
      "step": 2190
    },
    {
      "epoch": 1.4667444574095683,
      "grad_norm": 1.6650216579437256,
      "learning_rate": 0.0001907214745959403,
      "loss": 1.8768,
      "step": 2200
    },
    {
      "epoch": 1.473412235372562,
      "grad_norm": 1.6020293235778809,
      "learning_rate": 0.00018989261116875237,
      "loss": 1.7959,
      "step": 2210
    },
    {
      "epoch": 1.4800800133355558,
      "grad_norm": 1.318086862564087,
      "learning_rate": 0.00018906374774156447,
      "loss": 1.6988,
      "step": 2220
    },
    {
      "epoch": 1.4867477912985498,
      "grad_norm": 1.2476558685302734,
      "learning_rate": 0.00018823488431437657,
      "loss": 1.7799,
      "step": 2230
    },
    {
      "epoch": 1.4934155692615436,
      "grad_norm": 1.455418586730957,
      "learning_rate": 0.00018740602088718864,
      "loss": 1.7283,
      "step": 2240
    },
    {
      "epoch": 1.5000833472245374,
      "grad_norm": 1.911312222480774,
      "learning_rate": 0.0001865771574600007,
      "loss": 1.8111,
      "step": 2250
    },
    {
      "epoch": 1.5067511251875314,
      "grad_norm": 1.2741503715515137,
      "learning_rate": 0.00018574829403281278,
      "loss": 1.7147,
      "step": 2260
    },
    {
      "epoch": 1.5134189031505252,
      "grad_norm": 1.718123197555542,
      "learning_rate": 0.00018491943060562486,
      "loss": 1.8465,
      "step": 2270
    },
    {
      "epoch": 1.520086681113519,
      "grad_norm": 1.7067338228225708,
      "learning_rate": 0.00018409056717843696,
      "loss": 1.7281,
      "step": 2280
    },
    {
      "epoch": 1.5267544590765127,
      "grad_norm": 2.111421823501587,
      "learning_rate": 0.00018326170375124903,
      "loss": 1.7662,
      "step": 2290
    },
    {
      "epoch": 1.5334222370395065,
      "grad_norm": 1.370949625968933,
      "learning_rate": 0.0001824328403240611,
      "loss": 1.7733,
      "step": 2300
    },
    {
      "epoch": 1.5400900150025003,
      "grad_norm": 1.6876643896102905,
      "learning_rate": 0.00018160397689687317,
      "loss": 1.8682,
      "step": 2310
    },
    {
      "epoch": 1.546757792965494,
      "grad_norm": 2.0126240253448486,
      "learning_rate": 0.00018077511346968527,
      "loss": 1.8476,
      "step": 2320
    },
    {
      "epoch": 1.553425570928488,
      "grad_norm": 1.596774935722351,
      "learning_rate": 0.00017994625004249734,
      "loss": 1.8398,
      "step": 2330
    },
    {
      "epoch": 1.5600933488914819,
      "grad_norm": 1.9437516927719116,
      "learning_rate": 0.00017911738661530941,
      "loss": 1.7914,
      "step": 2340
    },
    {
      "epoch": 1.5667611268544759,
      "grad_norm": 1.417719841003418,
      "learning_rate": 0.0001782885231881215,
      "loss": 1.6335,
      "step": 2350
    },
    {
      "epoch": 1.5734289048174697,
      "grad_norm": 2.080493927001953,
      "learning_rate": 0.00017745965976093359,
      "loss": 1.7141,
      "step": 2360
    },
    {
      "epoch": 1.5800966827804634,
      "grad_norm": 1.5120389461517334,
      "learning_rate": 0.00017663079633374568,
      "loss": 1.6595,
      "step": 2370
    },
    {
      "epoch": 1.5867644607434572,
      "grad_norm": 2.140458345413208,
      "learning_rate": 0.00017580193290655776,
      "loss": 1.6699,
      "step": 2380
    },
    {
      "epoch": 1.593432238706451,
      "grad_norm": 2.2279281616210938,
      "learning_rate": 0.00017497306947936983,
      "loss": 1.7654,
      "step": 2390
    },
    {
      "epoch": 1.6001000166694448,
      "grad_norm": 1.59160578250885,
      "learning_rate": 0.00017414420605218193,
      "loss": 1.7392,
      "step": 2400
    },
    {
      "epoch": 1.6067677946324388,
      "grad_norm": 1.4890575408935547,
      "learning_rate": 0.000173315342624994,
      "loss": 1.6728,
      "step": 2410
    },
    {
      "epoch": 1.6134355725954326,
      "grad_norm": 2.124849319458008,
      "learning_rate": 0.00017248647919780607,
      "loss": 1.6551,
      "step": 2420
    },
    {
      "epoch": 1.6201033505584264,
      "grad_norm": 1.6501619815826416,
      "learning_rate": 0.00017165761577061814,
      "loss": 1.7812,
      "step": 2430
    },
    {
      "epoch": 1.6267711285214204,
      "grad_norm": 1.4267693758010864,
      "learning_rate": 0.00017082875234343024,
      "loss": 1.822,
      "step": 2440
    },
    {
      "epoch": 1.6334389064844141,
      "grad_norm": 1.058796763420105,
      "learning_rate": 0.00016999988891624231,
      "loss": 1.8359,
      "step": 2450
    },
    {
      "epoch": 1.640106684447408,
      "grad_norm": 1.4749044179916382,
      "learning_rate": 0.00016917102548905439,
      "loss": 1.7293,
      "step": 2460
    },
    {
      "epoch": 1.6467744624104017,
      "grad_norm": 1.8419150114059448,
      "learning_rate": 0.00016834216206186646,
      "loss": 1.7776,
      "step": 2470
    },
    {
      "epoch": 1.6534422403733955,
      "grad_norm": 1.3862733840942383,
      "learning_rate": 0.00016751329863467856,
      "loss": 1.7767,
      "step": 2480
    },
    {
      "epoch": 1.6601100183363893,
      "grad_norm": 1.3254282474517822,
      "learning_rate": 0.00016668443520749063,
      "loss": 1.6245,
      "step": 2490
    },
    {
      "epoch": 1.6667777962993833,
      "grad_norm": 1.475258231163025,
      "learning_rate": 0.0001658555717803027,
      "loss": 1.7983,
      "step": 2500
    },
    {
      "epoch": 1.673445574262377,
      "grad_norm": 1.1313788890838623,
      "learning_rate": 0.00016502670835311477,
      "loss": 1.7986,
      "step": 2510
    },
    {
      "epoch": 1.680113352225371,
      "grad_norm": 1.767122507095337,
      "learning_rate": 0.00016419784492592687,
      "loss": 1.7617,
      "step": 2520
    },
    {
      "epoch": 1.6867811301883648,
      "grad_norm": 2.2575511932373047,
      "learning_rate": 0.00016336898149873894,
      "loss": 1.6746,
      "step": 2530
    },
    {
      "epoch": 1.6934489081513586,
      "grad_norm": 1.7580084800720215,
      "learning_rate": 0.00016254011807155104,
      "loss": 1.714,
      "step": 2540
    },
    {
      "epoch": 1.7001166861143524,
      "grad_norm": 1.5405222177505493,
      "learning_rate": 0.00016171125464436312,
      "loss": 1.8637,
      "step": 2550
    },
    {
      "epoch": 1.7067844640773462,
      "grad_norm": 1.6639235019683838,
      "learning_rate": 0.0001608823912171752,
      "loss": 1.648,
      "step": 2560
    },
    {
      "epoch": 1.71345224204034,
      "grad_norm": 2.0010018348693848,
      "learning_rate": 0.00016005352778998729,
      "loss": 1.742,
      "step": 2570
    },
    {
      "epoch": 1.7201200200033337,
      "grad_norm": 1.868546485900879,
      "learning_rate": 0.00015922466436279936,
      "loss": 1.7509,
      "step": 2580
    },
    {
      "epoch": 1.7267877979663278,
      "grad_norm": 1.7130547761917114,
      "learning_rate": 0.00015839580093561146,
      "loss": 1.8179,
      "step": 2590
    },
    {
      "epoch": 1.7334555759293215,
      "grad_norm": 1.3593939542770386,
      "learning_rate": 0.00015756693750842353,
      "loss": 1.714,
      "step": 2600
    },
    {
      "epoch": 1.7401233538923155,
      "grad_norm": 1.4928778409957886,
      "learning_rate": 0.0001567380740812356,
      "loss": 1.6959,
      "step": 2610
    },
    {
      "epoch": 1.7467911318553093,
      "grad_norm": 1.4001771211624146,
      "learning_rate": 0.00015590921065404767,
      "loss": 1.7728,
      "step": 2620
    },
    {
      "epoch": 1.753458909818303,
      "grad_norm": 1.2961760759353638,
      "learning_rate": 0.00015508034722685977,
      "loss": 1.7065,
      "step": 2630
    },
    {
      "epoch": 1.7601266877812969,
      "grad_norm": 1.211674451828003,
      "learning_rate": 0.00015425148379967184,
      "loss": 1.6857,
      "step": 2640
    },
    {
      "epoch": 1.7667944657442907,
      "grad_norm": 1.454798698425293,
      "learning_rate": 0.00015342262037248392,
      "loss": 1.6274,
      "step": 2650
    },
    {
      "epoch": 1.7734622437072844,
      "grad_norm": 1.8372368812561035,
      "learning_rate": 0.000152593756945296,
      "loss": 1.7399,
      "step": 2660
    },
    {
      "epoch": 1.7801300216702782,
      "grad_norm": 1.505135416984558,
      "learning_rate": 0.0001517648935181081,
      "loss": 1.814,
      "step": 2670
    },
    {
      "epoch": 1.7867977996332722,
      "grad_norm": 1.396628975868225,
      "learning_rate": 0.00015093603009092016,
      "loss": 1.579,
      "step": 2680
    },
    {
      "epoch": 1.793465577596266,
      "grad_norm": 1.6918107271194458,
      "learning_rate": 0.00015010716666373223,
      "loss": 1.6764,
      "step": 2690
    },
    {
      "epoch": 1.80013335555926,
      "grad_norm": 1.4411777257919312,
      "learning_rate": 0.0001492783032365443,
      "loss": 1.7511,
      "step": 2700
    },
    {
      "epoch": 1.8068011335222538,
      "grad_norm": 1.2631851434707642,
      "learning_rate": 0.0001484494398093564,
      "loss": 1.7922,
      "step": 2710
    },
    {
      "epoch": 1.8134689114852476,
      "grad_norm": 1.8298088312149048,
      "learning_rate": 0.00014762057638216847,
      "loss": 1.7257,
      "step": 2720
    },
    {
      "epoch": 1.8201366894482414,
      "grad_norm": 1.896410584449768,
      "learning_rate": 0.00014679171295498055,
      "loss": 1.614,
      "step": 2730
    },
    {
      "epoch": 1.8268044674112351,
      "grad_norm": 1.783185362815857,
      "learning_rate": 0.00014596284952779265,
      "loss": 1.698,
      "step": 2740
    },
    {
      "epoch": 1.833472245374229,
      "grad_norm": 1.9152042865753174,
      "learning_rate": 0.00014513398610060472,
      "loss": 1.724,
      "step": 2750
    },
    {
      "epoch": 1.8401400233372227,
      "grad_norm": 1.7705386877059937,
      "learning_rate": 0.00014430512267341682,
      "loss": 1.7386,
      "step": 2760
    },
    {
      "epoch": 1.8468078013002167,
      "grad_norm": 2.3812458515167236,
      "learning_rate": 0.0001434762592462289,
      "loss": 1.7498,
      "step": 2770
    },
    {
      "epoch": 1.8534755792632105,
      "grad_norm": 1.7934987545013428,
      "learning_rate": 0.00014264739581904096,
      "loss": 1.6819,
      "step": 2780
    },
    {
      "epoch": 1.8601433572262045,
      "grad_norm": 1.5469870567321777,
      "learning_rate": 0.00014181853239185306,
      "loss": 1.5407,
      "step": 2790
    },
    {
      "epoch": 1.8668111351891983,
      "grad_norm": 1.6807156801223755,
      "learning_rate": 0.00014098966896466513,
      "loss": 1.7386,
      "step": 2800
    },
    {
      "epoch": 1.873478913152192,
      "grad_norm": 1.8914916515350342,
      "learning_rate": 0.0001401608055374772,
      "loss": 1.7873,
      "step": 2810
    },
    {
      "epoch": 1.8801466911151858,
      "grad_norm": 1.735231876373291,
      "learning_rate": 0.00013933194211028927,
      "loss": 1.7484,
      "step": 2820
    },
    {
      "epoch": 1.8868144690781796,
      "grad_norm": 2.4967401027679443,
      "learning_rate": 0.00013850307868310137,
      "loss": 1.7245,
      "step": 2830
    },
    {
      "epoch": 1.8934822470411734,
      "grad_norm": 1.6253166198730469,
      "learning_rate": 0.00013767421525591345,
      "loss": 1.7016,
      "step": 2840
    },
    {
      "epoch": 1.9001500250041674,
      "grad_norm": 1.8054885864257812,
      "learning_rate": 0.00013684535182872552,
      "loss": 1.64,
      "step": 2850
    },
    {
      "epoch": 1.9068178029671612,
      "grad_norm": 1.0901669263839722,
      "learning_rate": 0.0001360164884015376,
      "loss": 1.7375,
      "step": 2860
    },
    {
      "epoch": 1.913485580930155,
      "grad_norm": 0.958641529083252,
      "learning_rate": 0.0001351876249743497,
      "loss": 1.6972,
      "step": 2870
    },
    {
      "epoch": 1.920153358893149,
      "grad_norm": 1.2745234966278076,
      "learning_rate": 0.00013435876154716176,
      "loss": 1.6702,
      "step": 2880
    },
    {
      "epoch": 1.9268211368561428,
      "grad_norm": 1.5217499732971191,
      "learning_rate": 0.00013352989811997383,
      "loss": 1.6545,
      "step": 2890
    },
    {
      "epoch": 1.9334889148191365,
      "grad_norm": 1.5242360830307007,
      "learning_rate": 0.00013270103469278593,
      "loss": 1.6588,
      "step": 2900
    },
    {
      "epoch": 1.9401566927821303,
      "grad_norm": 1.6212961673736572,
      "learning_rate": 0.000131872171265598,
      "loss": 1.7796,
      "step": 2910
    },
    {
      "epoch": 1.946824470745124,
      "grad_norm": 1.4335081577301025,
      "learning_rate": 0.00013104330783841008,
      "loss": 1.5822,
      "step": 2920
    },
    {
      "epoch": 1.9534922487081179,
      "grad_norm": 1.3740262985229492,
      "learning_rate": 0.00013021444441122217,
      "loss": 1.7972,
      "step": 2930
    },
    {
      "epoch": 1.9601600266711119,
      "grad_norm": 1.7649590969085693,
      "learning_rate": 0.00012938558098403425,
      "loss": 1.6783,
      "step": 2940
    },
    {
      "epoch": 1.9668278046341057,
      "grad_norm": 1.7586607933044434,
      "learning_rate": 0.00012855671755684635,
      "loss": 1.7406,
      "step": 2950
    },
    {
      "epoch": 1.9734955825970997,
      "grad_norm": 2.1108438968658447,
      "learning_rate": 0.00012772785412965842,
      "loss": 1.6399,
      "step": 2960
    },
    {
      "epoch": 1.9801633605600935,
      "grad_norm": 1.7787750959396362,
      "learning_rate": 0.0001268989907024705,
      "loss": 1.7078,
      "step": 2970
    },
    {
      "epoch": 1.9868311385230872,
      "grad_norm": 1.960111141204834,
      "learning_rate": 0.0001260701272752826,
      "loss": 1.6057,
      "step": 2980
    },
    {
      "epoch": 1.993498916486081,
      "grad_norm": 1.3264213800430298,
      "learning_rate": 0.00012524126384809466,
      "loss": 1.6214,
      "step": 2990
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.218306541442871,
      "learning_rate": 0.00012441240042090673,
      "loss": 1.6574,
      "step": 3000
    },
    {
      "epoch": 2.0,
      "eval_loss": 1.686467170715332,
      "eval_runtime": 938.3687,
      "eval_samples_per_second": 6.393,
      "eval_steps_per_second": 3.197,
      "step": 3000
    },
    {
      "epoch": 2.006667777962994,
      "grad_norm": 1.8600192070007324,
      "learning_rate": 0.0001235835369937188,
      "loss": 1.6262,
      "step": 3010
    },
    {
      "epoch": 2.0133355559259876,
      "grad_norm": 1.9687329530715942,
      "learning_rate": 0.0001227546735665309,
      "loss": 1.6844,
      "step": 3020
    },
    {
      "epoch": 2.0200033338889813,
      "grad_norm": 1.8408067226409912,
      "learning_rate": 0.00012192581013934298,
      "loss": 1.6989,
      "step": 3030
    },
    {
      "epoch": 2.026671111851975,
      "grad_norm": 1.5751515626907349,
      "learning_rate": 0.00012109694671215505,
      "loss": 1.7019,
      "step": 3040
    },
    {
      "epoch": 2.0333388898149694,
      "grad_norm": 2.197195529937744,
      "learning_rate": 0.00012026808328496713,
      "loss": 1.6465,
      "step": 3050
    },
    {
      "epoch": 2.040006667777963,
      "grad_norm": 1.5194990634918213,
      "learning_rate": 0.00011943921985777922,
      "loss": 1.6878,
      "step": 3060
    },
    {
      "epoch": 2.046674445740957,
      "grad_norm": 1.615337610244751,
      "learning_rate": 0.00011861035643059129,
      "loss": 1.6213,
      "step": 3070
    },
    {
      "epoch": 2.0533422237039507,
      "grad_norm": 1.8855642080307007,
      "learning_rate": 0.00011778149300340338,
      "loss": 1.6817,
      "step": 3080
    },
    {
      "epoch": 2.0600100016669445,
      "grad_norm": 1.5879249572753906,
      "learning_rate": 0.00011695262957621545,
      "loss": 1.6737,
      "step": 3090
    },
    {
      "epoch": 2.0666777796299383,
      "grad_norm": 1.4706196784973145,
      "learning_rate": 0.00011612376614902755,
      "loss": 1.7273,
      "step": 3100
    },
    {
      "epoch": 2.073345557592932,
      "grad_norm": 1.78494393825531,
      "learning_rate": 0.00011529490272183962,
      "loss": 1.7683,
      "step": 3110
    },
    {
      "epoch": 2.080013335555926,
      "grad_norm": 1.5377782583236694,
      "learning_rate": 0.00011446603929465169,
      "loss": 1.6618,
      "step": 3120
    },
    {
      "epoch": 2.0866811135189196,
      "grad_norm": 2.2589352130889893,
      "learning_rate": 0.00011363717586746376,
      "loss": 1.5254,
      "step": 3130
    },
    {
      "epoch": 2.093348891481914,
      "grad_norm": 1.8371202945709229,
      "learning_rate": 0.00011280831244027586,
      "loss": 1.6279,
      "step": 3140
    },
    {
      "epoch": 2.1000166694449076,
      "grad_norm": 1.500949501991272,
      "learning_rate": 0.00011197944901308793,
      "loss": 1.6955,
      "step": 3150
    },
    {
      "epoch": 2.1066844474079014,
      "grad_norm": 2.062915802001953,
      "learning_rate": 0.00011115058558590002,
      "loss": 1.5987,
      "step": 3160
    },
    {
      "epoch": 2.113352225370895,
      "grad_norm": 1.2485051155090332,
      "learning_rate": 0.00011032172215871209,
      "loss": 1.7351,
      "step": 3170
    },
    {
      "epoch": 2.120020003333889,
      "grad_norm": 1.4621682167053223,
      "learning_rate": 0.00010949285873152419,
      "loss": 1.581,
      "step": 3180
    },
    {
      "epoch": 2.1266877812968827,
      "grad_norm": 1.6947036981582642,
      "learning_rate": 0.00010866399530433626,
      "loss": 1.654,
      "step": 3190
    },
    {
      "epoch": 2.1333555592598765,
      "grad_norm": 1.6066913604736328,
      "learning_rate": 0.00010783513187714833,
      "loss": 1.6363,
      "step": 3200
    },
    {
      "epoch": 2.1400233372228703,
      "grad_norm": 1.331783652305603,
      "learning_rate": 0.00010700626844996043,
      "loss": 1.6453,
      "step": 3210
    },
    {
      "epoch": 2.146691115185864,
      "grad_norm": 1.4532644748687744,
      "learning_rate": 0.0001061774050227725,
      "loss": 1.6193,
      "step": 3220
    },
    {
      "epoch": 2.1533588931488583,
      "grad_norm": 1.7763983011245728,
      "learning_rate": 0.00010534854159558458,
      "loss": 1.6374,
      "step": 3230
    },
    {
      "epoch": 2.160026671111852,
      "grad_norm": 1.5553075075149536,
      "learning_rate": 0.00010451967816839665,
      "loss": 1.6812,
      "step": 3240
    },
    {
      "epoch": 2.166694449074846,
      "grad_norm": 2.098101854324341,
      "learning_rate": 0.00010369081474120875,
      "loss": 1.5946,
      "step": 3250
    },
    {
      "epoch": 2.1733622270378397,
      "grad_norm": 1.9640496969223022,
      "learning_rate": 0.00010286195131402082,
      "loss": 1.7315,
      "step": 3260
    },
    {
      "epoch": 2.1800300050008334,
      "grad_norm": 1.6720885038375854,
      "learning_rate": 0.0001020330878868329,
      "loss": 1.644,
      "step": 3270
    },
    {
      "epoch": 2.186697782963827,
      "grad_norm": 1.5469087362289429,
      "learning_rate": 0.00010120422445964498,
      "loss": 1.6647,
      "step": 3280
    },
    {
      "epoch": 2.193365560926821,
      "grad_norm": 1.85563063621521,
      "learning_rate": 0.00010037536103245708,
      "loss": 1.6559,
      "step": 3290
    },
    {
      "epoch": 2.200033338889815,
      "grad_norm": 1.6290597915649414,
      "learning_rate": 9.954649760526915e-05,
      "loss": 1.7288,
      "step": 3300
    },
    {
      "epoch": 2.206701116852809,
      "grad_norm": 1.6647603511810303,
      "learning_rate": 9.871763417808122e-05,
      "loss": 1.7326,
      "step": 3310
    },
    {
      "epoch": 2.213368894815803,
      "grad_norm": 1.5884613990783691,
      "learning_rate": 9.788877075089329e-05,
      "loss": 1.728,
      "step": 3320
    },
    {
      "epoch": 2.2200366727787966,
      "grad_norm": 1.5142993927001953,
      "learning_rate": 9.705990732370539e-05,
      "loss": 1.6326,
      "step": 3330
    },
    {
      "epoch": 2.2267044507417904,
      "grad_norm": 1.6083989143371582,
      "learning_rate": 9.623104389651746e-05,
      "loss": 1.5592,
      "step": 3340
    },
    {
      "epoch": 2.233372228704784,
      "grad_norm": 1.5638220310211182,
      "learning_rate": 9.540218046932955e-05,
      "loss": 1.6582,
      "step": 3350
    },
    {
      "epoch": 2.240040006667778,
      "grad_norm": 1.9444184303283691,
      "learning_rate": 9.457331704214162e-05,
      "loss": 1.7001,
      "step": 3360
    },
    {
      "epoch": 2.2467077846307717,
      "grad_norm": 1.7585461139678955,
      "learning_rate": 9.374445361495372e-05,
      "loss": 1.6323,
      "step": 3370
    },
    {
      "epoch": 2.2533755625937655,
      "grad_norm": 1.3900837898254395,
      "learning_rate": 9.291559018776579e-05,
      "loss": 1.6808,
      "step": 3380
    },
    {
      "epoch": 2.2600433405567593,
      "grad_norm": 1.6019630432128906,
      "learning_rate": 9.208672676057786e-05,
      "loss": 1.6438,
      "step": 3390
    },
    {
      "epoch": 2.266711118519753,
      "grad_norm": 2.130291700363159,
      "learning_rate": 9.125786333338995e-05,
      "loss": 1.6494,
      "step": 3400
    },
    {
      "epoch": 2.2733788964827473,
      "grad_norm": 1.34523606300354,
      "learning_rate": 9.042899990620202e-05,
      "loss": 1.7551,
      "step": 3410
    },
    {
      "epoch": 2.280046674445741,
      "grad_norm": 1.3951373100280762,
      "learning_rate": 8.960013647901411e-05,
      "loss": 1.7335,
      "step": 3420
    },
    {
      "epoch": 2.286714452408735,
      "grad_norm": 1.6779038906097412,
      "learning_rate": 8.877127305182618e-05,
      "loss": 1.8483,
      "step": 3430
    },
    {
      "epoch": 2.2933822303717286,
      "grad_norm": 1.2956392765045166,
      "learning_rate": 8.794240962463827e-05,
      "loss": 1.7269,
      "step": 3440
    },
    {
      "epoch": 2.3000500083347224,
      "grad_norm": 2.098106861114502,
      "learning_rate": 8.711354619745035e-05,
      "loss": 1.6597,
      "step": 3450
    },
    {
      "epoch": 2.306717786297716,
      "grad_norm": 1.6918394565582275,
      "learning_rate": 8.628468277026244e-05,
      "loss": 1.6737,
      "step": 3460
    },
    {
      "epoch": 2.31338556426071,
      "grad_norm": 1.5728648900985718,
      "learning_rate": 8.545581934307451e-05,
      "loss": 1.7728,
      "step": 3470
    },
    {
      "epoch": 2.320053342223704,
      "grad_norm": 1.744490146636963,
      "learning_rate": 8.46269559158866e-05,
      "loss": 1.6349,
      "step": 3480
    },
    {
      "epoch": 2.326721120186698,
      "grad_norm": 1.8114808797836304,
      "learning_rate": 8.379809248869867e-05,
      "loss": 1.7249,
      "step": 3490
    },
    {
      "epoch": 2.3333888981496917,
      "grad_norm": 1.7267438173294067,
      "learning_rate": 8.296922906151075e-05,
      "loss": 1.7304,
      "step": 3500
    },
    {
      "epoch": 2.3400566761126855,
      "grad_norm": 1.6970280408859253,
      "learning_rate": 8.214036563432284e-05,
      "loss": 1.6652,
      "step": 3510
    },
    {
      "epoch": 2.3467244540756793,
      "grad_norm": 1.582240343093872,
      "learning_rate": 8.131150220713491e-05,
      "loss": 1.6031,
      "step": 3520
    },
    {
      "epoch": 2.353392232038673,
      "grad_norm": 1.6153340339660645,
      "learning_rate": 8.0482638779947e-05,
      "loss": 1.7519,
      "step": 3530
    },
    {
      "epoch": 2.360060010001667,
      "grad_norm": 1.7688790559768677,
      "learning_rate": 7.965377535275908e-05,
      "loss": 1.6274,
      "step": 3540
    },
    {
      "epoch": 2.3667277879646607,
      "grad_norm": 1.6171410083770752,
      "learning_rate": 7.882491192557117e-05,
      "loss": 1.6952,
      "step": 3550
    },
    {
      "epoch": 2.3733955659276544,
      "grad_norm": 1.48570716381073,
      "learning_rate": 7.799604849838324e-05,
      "loss": 1.6753,
      "step": 3560
    },
    {
      "epoch": 2.380063343890648,
      "grad_norm": 1.2745575904846191,
      "learning_rate": 7.716718507119532e-05,
      "loss": 1.564,
      "step": 3570
    },
    {
      "epoch": 2.3867311218536424,
      "grad_norm": 1.3517810106277466,
      "learning_rate": 7.63383216440074e-05,
      "loss": 1.6967,
      "step": 3580
    },
    {
      "epoch": 2.3933988998166362,
      "grad_norm": 1.968427062034607,
      "learning_rate": 7.550945821681948e-05,
      "loss": 1.6111,
      "step": 3590
    },
    {
      "epoch": 2.40006667777963,
      "grad_norm": 2.121835231781006,
      "learning_rate": 7.468059478963155e-05,
      "loss": 1.4692,
      "step": 3600
    },
    {
      "epoch": 2.406734455742624,
      "grad_norm": 1.7007360458374023,
      "learning_rate": 7.385173136244364e-05,
      "loss": 1.635,
      "step": 3610
    },
    {
      "epoch": 2.4134022337056176,
      "grad_norm": 1.626580834388733,
      "learning_rate": 7.302286793525571e-05,
      "loss": 1.7354,
      "step": 3620
    },
    {
      "epoch": 2.4200700116686114,
      "grad_norm": 1.7782689332962036,
      "learning_rate": 7.21940045080678e-05,
      "loss": 1.716,
      "step": 3630
    },
    {
      "epoch": 2.426737789631605,
      "grad_norm": 1.9062994718551636,
      "learning_rate": 7.136514108087988e-05,
      "loss": 1.6893,
      "step": 3640
    },
    {
      "epoch": 2.433405567594599,
      "grad_norm": 1.6541712284088135,
      "learning_rate": 7.053627765369197e-05,
      "loss": 1.5996,
      "step": 3650
    },
    {
      "epoch": 2.440073345557593,
      "grad_norm": 1.154437780380249,
      "learning_rate": 6.970741422650404e-05,
      "loss": 1.6877,
      "step": 3660
    },
    {
      "epoch": 2.446741123520587,
      "grad_norm": 1.7270870208740234,
      "learning_rate": 6.887855079931612e-05,
      "loss": 1.7244,
      "step": 3670
    },
    {
      "epoch": 2.4534089014835807,
      "grad_norm": 1.4614360332489014,
      "learning_rate": 6.80496873721282e-05,
      "loss": 1.6375,
      "step": 3680
    },
    {
      "epoch": 2.4600766794465745,
      "grad_norm": 1.770699143409729,
      "learning_rate": 6.722082394494028e-05,
      "loss": 1.6537,
      "step": 3690
    },
    {
      "epoch": 2.4667444574095683,
      "grad_norm": 1.51199471950531,
      "learning_rate": 6.639196051775235e-05,
      "loss": 1.6109,
      "step": 3700
    },
    {
      "epoch": 2.473412235372562,
      "grad_norm": 1.4700217247009277,
      "learning_rate": 6.556309709056444e-05,
      "loss": 1.7407,
      "step": 3710
    },
    {
      "epoch": 2.480080013335556,
      "grad_norm": 1.6319646835327148,
      "learning_rate": 6.473423366337652e-05,
      "loss": 1.6609,
      "step": 3720
    },
    {
      "epoch": 2.4867477912985496,
      "grad_norm": 1.9966275691986084,
      "learning_rate": 6.390537023618861e-05,
      "loss": 1.7454,
      "step": 3730
    },
    {
      "epoch": 2.4934155692615434,
      "grad_norm": 2.007519006729126,
      "learning_rate": 6.307650680900068e-05,
      "loss": 1.6653,
      "step": 3740
    },
    {
      "epoch": 2.500083347224537,
      "grad_norm": 1.8261387348175049,
      "learning_rate": 6.224764338181277e-05,
      "loss": 1.568,
      "step": 3750
    },
    {
      "epoch": 2.5067511251875314,
      "grad_norm": 1.4336518049240112,
      "learning_rate": 6.141877995462484e-05,
      "loss": 1.6409,
      "step": 3760
    },
    {
      "epoch": 2.513418903150525,
      "grad_norm": 1.7409323453903198,
      "learning_rate": 6.0589916527436924e-05,
      "loss": 1.6403,
      "step": 3770
    },
    {
      "epoch": 2.520086681113519,
      "grad_norm": 1.7687116861343384,
      "learning_rate": 5.9761053100248996e-05,
      "loss": 1.6491,
      "step": 3780
    },
    {
      "epoch": 2.5267544590765127,
      "grad_norm": 1.7237823009490967,
      "learning_rate": 5.893218967306108e-05,
      "loss": 1.689,
      "step": 3790
    },
    {
      "epoch": 2.5334222370395065,
      "grad_norm": 1.5259853601455688,
      "learning_rate": 5.810332624587316e-05,
      "loss": 1.6151,
      "step": 3800
    },
    {
      "epoch": 2.5400900150025003,
      "grad_norm": 1.0477957725524902,
      "learning_rate": 5.7274462818685246e-05,
      "loss": 1.6915,
      "step": 3810
    },
    {
      "epoch": 2.546757792965494,
      "grad_norm": 1.7430217266082764,
      "learning_rate": 5.644559939149732e-05,
      "loss": 1.6466,
      "step": 3820
    },
    {
      "epoch": 2.5534255709284883,
      "grad_norm": 1.382637858390808,
      "learning_rate": 5.5616735964309404e-05,
      "loss": 1.6433,
      "step": 3830
    },
    {
      "epoch": 2.560093348891482,
      "grad_norm": 1.815636396408081,
      "learning_rate": 5.478787253712149e-05,
      "loss": 1.7187,
      "step": 3840
    },
    {
      "epoch": 2.566761126854476,
      "grad_norm": 1.5709766149520874,
      "learning_rate": 5.395900910993357e-05,
      "loss": 1.6399,
      "step": 3850
    },
    {
      "epoch": 2.5734289048174697,
      "grad_norm": 1.7791800498962402,
      "learning_rate": 5.313014568274565e-05,
      "loss": 1.4922,
      "step": 3860
    },
    {
      "epoch": 2.5800966827804634,
      "grad_norm": 1.9874327182769775,
      "learning_rate": 5.2301282255557725e-05,
      "loss": 1.7131,
      "step": 3870
    },
    {
      "epoch": 2.5867644607434572,
      "grad_norm": 1.3725963830947876,
      "learning_rate": 5.147241882836981e-05,
      "loss": 1.6175,
      "step": 3880
    },
    {
      "epoch": 2.593432238706451,
      "grad_norm": 2.169583320617676,
      "learning_rate": 5.064355540118189e-05,
      "loss": 1.6279,
      "step": 3890
    },
    {
      "epoch": 2.600100016669445,
      "grad_norm": 1.786558747291565,
      "learning_rate": 4.9814691973993975e-05,
      "loss": 1.6046,
      "step": 3900
    },
    {
      "epoch": 2.6067677946324386,
      "grad_norm": 1.399614691734314,
      "learning_rate": 4.898582854680605e-05,
      "loss": 1.6423,
      "step": 3910
    },
    {
      "epoch": 2.6134355725954324,
      "grad_norm": 1.5881714820861816,
      "learning_rate": 4.815696511961813e-05,
      "loss": 1.6698,
      "step": 3920
    },
    {
      "epoch": 2.620103350558426,
      "grad_norm": 1.3564951419830322,
      "learning_rate": 4.7328101692430205e-05,
      "loss": 1.7071,
      "step": 3930
    },
    {
      "epoch": 2.6267711285214204,
      "grad_norm": 1.97831392288208,
      "learning_rate": 4.649923826524229e-05,
      "loss": 1.7036,
      "step": 3940
    },
    {
      "epoch": 2.633438906484414,
      "grad_norm": 1.6673505306243896,
      "learning_rate": 4.567037483805437e-05,
      "loss": 1.5953,
      "step": 3950
    },
    {
      "epoch": 2.640106684447408,
      "grad_norm": 1.5095298290252686,
      "learning_rate": 4.484151141086645e-05,
      "loss": 1.6402,
      "step": 3960
    },
    {
      "epoch": 2.6467744624104017,
      "grad_norm": 1.7326332330703735,
      "learning_rate": 4.4012647983678526e-05,
      "loss": 1.5535,
      "step": 3970
    },
    {
      "epoch": 2.6534422403733955,
      "grad_norm": 1.3684258460998535,
      "learning_rate": 4.318378455649061e-05,
      "loss": 1.5791,
      "step": 3980
    },
    {
      "epoch": 2.6601100183363893,
      "grad_norm": 1.4156242609024048,
      "learning_rate": 4.23549211293027e-05,
      "loss": 1.5333,
      "step": 3990
    },
    {
      "epoch": 2.6667777962993835,
      "grad_norm": 1.7145822048187256,
      "learning_rate": 4.1526057702114776e-05,
      "loss": 1.6006,
      "step": 4000
    },
    {
      "epoch": 2.6734455742623773,
      "grad_norm": 1.7402327060699463,
      "learning_rate": 4.0697194274926855e-05,
      "loss": 1.6009,
      "step": 4010
    },
    {
      "epoch": 2.680113352225371,
      "grad_norm": 1.5584510564804077,
      "learning_rate": 3.9868330847738933e-05,
      "loss": 1.5815,
      "step": 4020
    },
    {
      "epoch": 2.686781130188365,
      "grad_norm": 1.5765410661697388,
      "learning_rate": 3.903946742055101e-05,
      "loss": 1.6903,
      "step": 4030
    },
    {
      "epoch": 2.6934489081513586,
      "grad_norm": 2.044725179672241,
      "learning_rate": 3.82106039933631e-05,
      "loss": 1.6741,
      "step": 4040
    },
    {
      "epoch": 2.7001166861143524,
      "grad_norm": 1.5742886066436768,
      "learning_rate": 3.7381740566175176e-05,
      "loss": 1.7714,
      "step": 4050
    },
    {
      "epoch": 2.706784464077346,
      "grad_norm": 1.5585441589355469,
      "learning_rate": 3.6552877138987255e-05,
      "loss": 1.6352,
      "step": 4060
    },
    {
      "epoch": 2.71345224204034,
      "grad_norm": 2.0219695568084717,
      "learning_rate": 3.5724013711799334e-05,
      "loss": 1.712,
      "step": 4070
    },
    {
      "epoch": 2.7201200200033337,
      "grad_norm": 1.808477759361267,
      "learning_rate": 3.489515028461142e-05,
      "loss": 1.6687,
      "step": 4080
    },
    {
      "epoch": 2.7267877979663275,
      "grad_norm": 1.437406301498413,
      "learning_rate": 3.40662868574235e-05,
      "loss": 1.615,
      "step": 4090
    },
    {
      "epoch": 2.7334555759293213,
      "grad_norm": 1.6064040660858154,
      "learning_rate": 3.323742343023558e-05,
      "loss": 1.5865,
      "step": 4100
    },
    {
      "epoch": 2.7401233538923155,
      "grad_norm": 1.5249910354614258,
      "learning_rate": 3.2408560003047656e-05,
      "loss": 1.6811,
      "step": 4110
    },
    {
      "epoch": 2.7467911318553093,
      "grad_norm": 1.4435906410217285,
      "learning_rate": 3.1579696575859734e-05,
      "loss": 1.6473,
      "step": 4120
    },
    {
      "epoch": 2.753458909818303,
      "grad_norm": 1.5789270401000977,
      "learning_rate": 3.075083314867182e-05,
      "loss": 1.6549,
      "step": 4130
    },
    {
      "epoch": 2.760126687781297,
      "grad_norm": 1.2751028537750244,
      "learning_rate": 2.99219697214839e-05,
      "loss": 1.6386,
      "step": 4140
    },
    {
      "epoch": 2.7667944657442907,
      "grad_norm": 1.9698646068572998,
      "learning_rate": 2.9093106294295977e-05,
      "loss": 1.6022,
      "step": 4150
    },
    {
      "epoch": 2.7734622437072844,
      "grad_norm": 1.9643694162368774,
      "learning_rate": 2.826424286710806e-05,
      "loss": 1.667,
      "step": 4160
    },
    {
      "epoch": 2.7801300216702782,
      "grad_norm": 1.93950355052948,
      "learning_rate": 2.7435379439920138e-05,
      "loss": 1.6992,
      "step": 4170
    },
    {
      "epoch": 2.7867977996332725,
      "grad_norm": 1.7954519987106323,
      "learning_rate": 2.660651601273222e-05,
      "loss": 1.6976,
      "step": 4180
    },
    {
      "epoch": 2.7934655775962662,
      "grad_norm": 1.5647519826889038,
      "learning_rate": 2.57776525855443e-05,
      "loss": 1.6385,
      "step": 4190
    },
    {
      "epoch": 2.80013335555926,
      "grad_norm": 1.9165802001953125,
      "learning_rate": 2.4948789158356378e-05,
      "loss": 1.5398,
      "step": 4200
    },
    {
      "epoch": 2.806801133522254,
      "grad_norm": 1.7329872846603394,
      "learning_rate": 2.411992573116846e-05,
      "loss": 1.7263,
      "step": 4210
    },
    {
      "epoch": 2.8134689114852476,
      "grad_norm": 1.8545253276824951,
      "learning_rate": 2.329106230398054e-05,
      "loss": 1.5748,
      "step": 4220
    },
    {
      "epoch": 2.8201366894482414,
      "grad_norm": 1.710039734840393,
      "learning_rate": 2.2462198876792624e-05,
      "loss": 1.6905,
      "step": 4230
    },
    {
      "epoch": 2.826804467411235,
      "grad_norm": 1.9711954593658447,
      "learning_rate": 2.1633335449604703e-05,
      "loss": 1.5915,
      "step": 4240
    },
    {
      "epoch": 2.833472245374229,
      "grad_norm": 1.303393006324768,
      "learning_rate": 2.080447202241678e-05,
      "loss": 1.567,
      "step": 4250
    },
    {
      "epoch": 2.8401400233372227,
      "grad_norm": 1.6968531608581543,
      "learning_rate": 1.9975608595228864e-05,
      "loss": 1.54,
      "step": 4260
    },
    {
      "epoch": 2.8468078013002165,
      "grad_norm": 1.7101588249206543,
      "learning_rate": 1.9146745168040943e-05,
      "loss": 1.6344,
      "step": 4270
    },
    {
      "epoch": 2.8534755792632103,
      "grad_norm": 1.6962614059448242,
      "learning_rate": 1.8317881740853025e-05,
      "loss": 1.5271,
      "step": 4280
    },
    {
      "epoch": 2.8601433572262045,
      "grad_norm": 1.6657943725585938,
      "learning_rate": 1.7489018313665103e-05,
      "loss": 1.5116,
      "step": 4290
    },
    {
      "epoch": 2.8668111351891983,
      "grad_norm": 2.0708539485931396,
      "learning_rate": 1.6660154886477186e-05,
      "loss": 1.6441,
      "step": 4300
    },
    {
      "epoch": 2.873478913152192,
      "grad_norm": 1.439133882522583,
      "learning_rate": 1.5831291459289264e-05,
      "loss": 1.7472,
      "step": 4310
    },
    {
      "epoch": 2.880146691115186,
      "grad_norm": 1.5044673681259155,
      "learning_rate": 1.5002428032101345e-05,
      "loss": 1.6272,
      "step": 4320
    },
    {
      "epoch": 2.8868144690781796,
      "grad_norm": 1.5122851133346558,
      "learning_rate": 1.4173564604913425e-05,
      "loss": 1.5827,
      "step": 4330
    },
    {
      "epoch": 2.8934822470411734,
      "grad_norm": 1.7731226682662964,
      "learning_rate": 1.3344701177725506e-05,
      "loss": 1.6429,
      "step": 4340
    },
    {
      "epoch": 2.9001500250041676,
      "grad_norm": 1.7283427715301514,
      "learning_rate": 1.2515837750537586e-05,
      "loss": 1.4951,
      "step": 4350
    },
    {
      "epoch": 2.9068178029671614,
      "grad_norm": 1.544653058052063,
      "learning_rate": 1.1686974323349666e-05,
      "loss": 1.6238,
      "step": 4360
    },
    {
      "epoch": 2.913485580930155,
      "grad_norm": 1.694036602973938,
      "learning_rate": 1.0858110896161747e-05,
      "loss": 1.6625,
      "step": 4370
    },
    {
      "epoch": 2.920153358893149,
      "grad_norm": 1.6725683212280273,
      "learning_rate": 1.0029247468973827e-05,
      "loss": 1.6531,
      "step": 4380
    },
    {
      "epoch": 2.9268211368561428,
      "grad_norm": 1.3855291604995728,
      "learning_rate": 9.200384041785908e-06,
      "loss": 1.6556,
      "step": 4390
    },
    {
      "epoch": 2.9334889148191365,
      "grad_norm": 1.4338148832321167,
      "learning_rate": 8.371520614597988e-06,
      "loss": 1.6468,
      "step": 4400
    },
    {
      "epoch": 2.9401566927821303,
      "grad_norm": 1.3045260906219482,
      "learning_rate": 7.542657187410068e-06,
      "loss": 1.6996,
      "step": 4410
    },
    {
      "epoch": 2.946824470745124,
      "grad_norm": 1.5582703351974487,
      "learning_rate": 6.713793760222148e-06,
      "loss": 1.6311,
      "step": 4420
    },
    {
      "epoch": 2.953492248708118,
      "grad_norm": 1.904028058052063,
      "learning_rate": 5.8849303330342294e-06,
      "loss": 1.6092,
      "step": 4430
    },
    {
      "epoch": 2.9601600266711117,
      "grad_norm": 1.6826974153518677,
      "learning_rate": 5.056066905846309e-06,
      "loss": 1.5973,
      "step": 4440
    },
    {
      "epoch": 2.9668278046341054,
      "grad_norm": 1.333640217781067,
      "learning_rate": 4.22720347865839e-06,
      "loss": 1.7555,
      "step": 4450
    },
    {
      "epoch": 2.9734955825970997,
      "grad_norm": 1.4057055711746216,
      "learning_rate": 3.3983400514704707e-06,
      "loss": 1.6967,
      "step": 4460
    },
    {
      "epoch": 2.9801633605600935,
      "grad_norm": 1.3053659200668335,
      "learning_rate": 2.5694766242825507e-06,
      "loss": 1.6455,
      "step": 4470
    },
    {
      "epoch": 2.9868311385230872,
      "grad_norm": 1.3343294858932495,
      "learning_rate": 1.7406131970946314e-06,
      "loss": 1.6441,
      "step": 4480
    },
    {
      "epoch": 2.993498916486081,
      "grad_norm": 1.5438286066055298,
      "learning_rate": 9.117497699067115e-07,
      "loss": 1.59,
      "step": 4490
    },
    {
      "epoch": 3.0,
      "grad_norm": 1.8933216333389282,
      "learning_rate": 8.288634271879196e-08,
      "loss": 1.6793,
      "step": 4500
    },
    {
      "epoch": 3.0,
      "eval_loss": 1.6245359182357788,
      "eval_runtime": 938.7083,
      "eval_samples_per_second": 6.391,
      "eval_steps_per_second": 3.196,
      "step": 4500
    }
  ],
  "logging_steps": 10,
  "max_steps": 4500,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "EarlyStoppingCallback": {
      "args": {
        "early_stopping_patience": 2,
        "early_stopping_threshold": 0.0
      },
      "attributes": {
        "early_stopping_patience_counter": 0
      }
    },
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 3.1277219787413914e+18,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
