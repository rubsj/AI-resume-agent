{
  "best_global_step": 3000,
  "best_metric": 1.8174083232879639,
  "best_model_checkpoint": "json_outputs_all_data/fine-tune/optuna_output/optuna_trial_3/checkpoint-3000",
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 3000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.006667777962993832,
      "grad_norm": 0.5343880653381348,
      "learning_rate": 0.0002240318791844374,
      "loss": 2.6377,
      "step": 10
    },
    {
      "epoch": 0.013335555925987664,
      "grad_norm": 0.47111305594444275,
      "learning_rate": 0.00022328285919385085,
      "loss": 2.4674,
      "step": 20
    },
    {
      "epoch": 0.020003333888981498,
      "grad_norm": 0.4522990882396698,
      "learning_rate": 0.00022253383920326427,
      "loss": 2.41,
      "step": 30
    },
    {
      "epoch": 0.026671111851975328,
      "grad_norm": 0.4450888931751251,
      "learning_rate": 0.00022178481921267775,
      "loss": 2.43,
      "step": 40
    },
    {
      "epoch": 0.03333888981496916,
      "grad_norm": 0.4455593526363373,
      "learning_rate": 0.0002210357992220912,
      "loss": 2.3852,
      "step": 50
    },
    {
      "epoch": 0.040006667777962995,
      "grad_norm": 0.522596538066864,
      "learning_rate": 0.00022028677923150464,
      "loss": 2.3746,
      "step": 60
    },
    {
      "epoch": 0.046674445740956826,
      "grad_norm": 0.45216435194015503,
      "learning_rate": 0.00021953775924091808,
      "loss": 2.3474,
      "step": 70
    },
    {
      "epoch": 0.053342223703950656,
      "grad_norm": 0.48421528935432434,
      "learning_rate": 0.00021878873925033156,
      "loss": 2.4043,
      "step": 80
    },
    {
      "epoch": 0.06001000166694449,
      "grad_norm": 0.5063048601150513,
      "learning_rate": 0.000218039719259745,
      "loss": 2.3733,
      "step": 90
    },
    {
      "epoch": 0.06667777962993832,
      "grad_norm": 0.5588052272796631,
      "learning_rate": 0.00021729069926915842,
      "loss": 2.3571,
      "step": 100
    },
    {
      "epoch": 0.07334555759293215,
      "grad_norm": 0.5498393774032593,
      "learning_rate": 0.0002165416792785719,
      "loss": 2.3646,
      "step": 110
    },
    {
      "epoch": 0.08001333555592599,
      "grad_norm": 0.5699737668037415,
      "learning_rate": 0.00021579265928798534,
      "loss": 2.3771,
      "step": 120
    },
    {
      "epoch": 0.08668111351891981,
      "grad_norm": 0.5399962663650513,
      "learning_rate": 0.00021504363929739879,
      "loss": 2.3204,
      "step": 130
    },
    {
      "epoch": 0.09334889148191365,
      "grad_norm": 0.5192946791648865,
      "learning_rate": 0.00021429461930681223,
      "loss": 2.3504,
      "step": 140
    },
    {
      "epoch": 0.10001666944490749,
      "grad_norm": 0.5589209794998169,
      "learning_rate": 0.00021354559931622568,
      "loss": 2.3383,
      "step": 150
    },
    {
      "epoch": 0.10668444740790131,
      "grad_norm": 0.5954470038414001,
      "learning_rate": 0.00021279657932563912,
      "loss": 2.3114,
      "step": 160
    },
    {
      "epoch": 0.11335222537089515,
      "grad_norm": 0.5821408033370972,
      "learning_rate": 0.00021204755933505257,
      "loss": 2.2721,
      "step": 170
    },
    {
      "epoch": 0.12002000333388899,
      "grad_norm": 0.6052495837211609,
      "learning_rate": 0.00021129853934446604,
      "loss": 2.3068,
      "step": 180
    },
    {
      "epoch": 0.12668778129688282,
      "grad_norm": 0.581914484500885,
      "learning_rate": 0.0002105495193538795,
      "loss": 2.2984,
      "step": 190
    },
    {
      "epoch": 0.13335555925987663,
      "grad_norm": 0.6144691109657288,
      "learning_rate": 0.00020980049936329293,
      "loss": 2.2699,
      "step": 200
    },
    {
      "epoch": 0.14002333722287047,
      "grad_norm": 0.5778465270996094,
      "learning_rate": 0.00020905147937270638,
      "loss": 2.3013,
      "step": 210
    },
    {
      "epoch": 0.1466911151858643,
      "grad_norm": 0.6489197611808777,
      "learning_rate": 0.00020830245938211983,
      "loss": 2.3258,
      "step": 220
    },
    {
      "epoch": 0.15335889314885814,
      "grad_norm": 0.6796532273292542,
      "learning_rate": 0.00020755343939153327,
      "loss": 2.26,
      "step": 230
    },
    {
      "epoch": 0.16002667111185198,
      "grad_norm": 0.6207619309425354,
      "learning_rate": 0.00020680441940094672,
      "loss": 2.2576,
      "step": 240
    },
    {
      "epoch": 0.16669444907484582,
      "grad_norm": 0.6356180906295776,
      "learning_rate": 0.0002060553994103602,
      "loss": 2.2599,
      "step": 250
    },
    {
      "epoch": 0.17336222703783963,
      "grad_norm": 0.7599827647209167,
      "learning_rate": 0.0002053063794197736,
      "loss": 2.2562,
      "step": 260
    },
    {
      "epoch": 0.18003000500083347,
      "grad_norm": 0.5975410342216492,
      "learning_rate": 0.00020455735942918708,
      "loss": 2.3318,
      "step": 270
    },
    {
      "epoch": 0.1866977829638273,
      "grad_norm": 0.6835879683494568,
      "learning_rate": 0.00020380833943860053,
      "loss": 2.3664,
      "step": 280
    },
    {
      "epoch": 0.19336556092682114,
      "grad_norm": 0.6386905908584595,
      "learning_rate": 0.00020305931944801395,
      "loss": 2.276,
      "step": 290
    },
    {
      "epoch": 0.20003333888981498,
      "grad_norm": 0.6536569595336914,
      "learning_rate": 0.00020231029945742742,
      "loss": 2.2784,
      "step": 300
    },
    {
      "epoch": 0.2067011168528088,
      "grad_norm": 0.6959589719772339,
      "learning_rate": 0.00020156127946684087,
      "loss": 2.2476,
      "step": 310
    },
    {
      "epoch": 0.21336889481580262,
      "grad_norm": 0.7049395442008972,
      "learning_rate": 0.00020081225947625434,
      "loss": 2.2748,
      "step": 320
    },
    {
      "epoch": 0.22003667277879646,
      "grad_norm": 0.7400964498519897,
      "learning_rate": 0.00020006323948566776,
      "loss": 2.2272,
      "step": 330
    },
    {
      "epoch": 0.2267044507417903,
      "grad_norm": 0.6927144527435303,
      "learning_rate": 0.0001993142194950812,
      "loss": 2.2303,
      "step": 340
    },
    {
      "epoch": 0.23337222870478413,
      "grad_norm": 0.8775679469108582,
      "learning_rate": 0.00019856519950449468,
      "loss": 2.2351,
      "step": 350
    },
    {
      "epoch": 0.24004000666777797,
      "grad_norm": 0.6666982173919678,
      "learning_rate": 0.0001978161795139081,
      "loss": 2.282,
      "step": 360
    },
    {
      "epoch": 0.24670778463077178,
      "grad_norm": 0.6776239275932312,
      "learning_rate": 0.00019706715952332157,
      "loss": 2.2961,
      "step": 370
    },
    {
      "epoch": 0.25337556259376565,
      "grad_norm": 0.7849373817443848,
      "learning_rate": 0.00019631813953273502,
      "loss": 2.2627,
      "step": 380
    },
    {
      "epoch": 0.2600433405567595,
      "grad_norm": 0.7828976511955261,
      "learning_rate": 0.00019556911954214846,
      "loss": 2.2227,
      "step": 390
    },
    {
      "epoch": 0.26671111851975327,
      "grad_norm": 0.8390271663665771,
      "learning_rate": 0.0001948200995515619,
      "loss": 2.245,
      "step": 400
    },
    {
      "epoch": 0.2733788964827471,
      "grad_norm": 0.9565172791481018,
      "learning_rate": 0.00019407107956097535,
      "loss": 2.2035,
      "step": 410
    },
    {
      "epoch": 0.28004667444574094,
      "grad_norm": 1.0010102987289429,
      "learning_rate": 0.0001933220595703888,
      "loss": 2.1901,
      "step": 420
    },
    {
      "epoch": 0.2867144524087348,
      "grad_norm": 0.7662108540534973,
      "learning_rate": 0.00019257303957980225,
      "loss": 2.1939,
      "step": 430
    },
    {
      "epoch": 0.2933822303717286,
      "grad_norm": 0.8510870337486267,
      "learning_rate": 0.00019182401958921572,
      "loss": 2.2179,
      "step": 440
    },
    {
      "epoch": 0.30005000833472245,
      "grad_norm": 0.6579949259757996,
      "learning_rate": 0.00019107499959862916,
      "loss": 2.2515,
      "step": 450
    },
    {
      "epoch": 0.3067177862977163,
      "grad_norm": 0.7202030420303345,
      "learning_rate": 0.00019032597960804258,
      "loss": 2.2585,
      "step": 460
    },
    {
      "epoch": 0.3133855642607101,
      "grad_norm": 0.8238518834114075,
      "learning_rate": 0.00018957695961745606,
      "loss": 2.265,
      "step": 470
    },
    {
      "epoch": 0.32005334222370396,
      "grad_norm": 0.7689182758331299,
      "learning_rate": 0.0001888279396268695,
      "loss": 2.1848,
      "step": 480
    },
    {
      "epoch": 0.3267211201866978,
      "grad_norm": 1.0245846509933472,
      "learning_rate": 0.00018807891963628295,
      "loss": 2.2098,
      "step": 490
    },
    {
      "epoch": 0.33338889814969164,
      "grad_norm": 0.8874949812889099,
      "learning_rate": 0.0001873298996456964,
      "loss": 2.2365,
      "step": 500
    },
    {
      "epoch": 0.3400566761126855,
      "grad_norm": 0.8664621114730835,
      "learning_rate": 0.00018658087965510987,
      "loss": 2.2201,
      "step": 510
    },
    {
      "epoch": 0.34672445407567926,
      "grad_norm": 1.273528814315796,
      "learning_rate": 0.0001858318596645233,
      "loss": 2.1705,
      "step": 520
    },
    {
      "epoch": 0.3533922320386731,
      "grad_norm": 0.8947978615760803,
      "learning_rate": 0.00018508283967393673,
      "loss": 2.1926,
      "step": 530
    },
    {
      "epoch": 0.36006001000166693,
      "grad_norm": 1.0123461484909058,
      "learning_rate": 0.0001843338196833502,
      "loss": 2.1358,
      "step": 540
    },
    {
      "epoch": 0.36672778796466077,
      "grad_norm": 0.8803287744522095,
      "learning_rate": 0.00018358479969276362,
      "loss": 2.1573,
      "step": 550
    },
    {
      "epoch": 0.3733955659276546,
      "grad_norm": 0.8231608867645264,
      "learning_rate": 0.0001828357797021771,
      "loss": 2.0624,
      "step": 560
    },
    {
      "epoch": 0.38006334389064844,
      "grad_norm": 0.9674989581108093,
      "learning_rate": 0.00018208675971159054,
      "loss": 2.1743,
      "step": 570
    },
    {
      "epoch": 0.3867311218536423,
      "grad_norm": 0.7516270875930786,
      "learning_rate": 0.000181337739721004,
      "loss": 2.1921,
      "step": 580
    },
    {
      "epoch": 0.3933988998166361,
      "grad_norm": 0.8026522397994995,
      "learning_rate": 0.00018058871973041744,
      "loss": 2.1835,
      "step": 590
    },
    {
      "epoch": 0.40006667777962995,
      "grad_norm": 0.8261969685554504,
      "learning_rate": 0.00017983969973983088,
      "loss": 2.1185,
      "step": 600
    },
    {
      "epoch": 0.4067344557426238,
      "grad_norm": 0.9116111397743225,
      "learning_rate": 0.00017909067974924435,
      "loss": 2.1666,
      "step": 610
    },
    {
      "epoch": 0.4134022337056176,
      "grad_norm": 0.9311140179634094,
      "learning_rate": 0.00017834165975865777,
      "loss": 2.1252,
      "step": 620
    },
    {
      "epoch": 0.4200700116686114,
      "grad_norm": 1.0866367816925049,
      "learning_rate": 0.00017759263976807125,
      "loss": 2.1618,
      "step": 630
    },
    {
      "epoch": 0.42673778963160525,
      "grad_norm": 1.0169074535369873,
      "learning_rate": 0.0001768436197774847,
      "loss": 2.1422,
      "step": 640
    },
    {
      "epoch": 0.4334055675945991,
      "grad_norm": 1.0417265892028809,
      "learning_rate": 0.0001760945997868981,
      "loss": 2.1679,
      "step": 650
    },
    {
      "epoch": 0.4400733455575929,
      "grad_norm": 0.8853954076766968,
      "learning_rate": 0.00017534557979631158,
      "loss": 2.158,
      "step": 660
    },
    {
      "epoch": 0.44674112352058676,
      "grad_norm": 1.4123926162719727,
      "learning_rate": 0.00017459655980572503,
      "loss": 2.0847,
      "step": 670
    },
    {
      "epoch": 0.4534089014835806,
      "grad_norm": 0.9332658052444458,
      "learning_rate": 0.00017384753981513848,
      "loss": 2.1489,
      "step": 680
    },
    {
      "epoch": 0.46007667944657443,
      "grad_norm": 1.160896897315979,
      "learning_rate": 0.00017309851982455192,
      "loss": 2.0847,
      "step": 690
    },
    {
      "epoch": 0.46674445740956827,
      "grad_norm": 1.2395434379577637,
      "learning_rate": 0.0001723494998339654,
      "loss": 2.0936,
      "step": 700
    },
    {
      "epoch": 0.4734122353725621,
      "grad_norm": 1.254928708076477,
      "learning_rate": 0.00017160047984337884,
      "loss": 2.1408,
      "step": 710
    },
    {
      "epoch": 0.48008001333555594,
      "grad_norm": 1.2258044481277466,
      "learning_rate": 0.00017085145985279226,
      "loss": 2.0526,
      "step": 720
    },
    {
      "epoch": 0.4867477912985498,
      "grad_norm": 1.0431779623031616,
      "learning_rate": 0.00017010243986220573,
      "loss": 2.1492,
      "step": 730
    },
    {
      "epoch": 0.49341556926154356,
      "grad_norm": 1.2121258974075317,
      "learning_rate": 0.00016935341987161918,
      "loss": 2.1298,
      "step": 740
    },
    {
      "epoch": 0.5000833472245374,
      "grad_norm": 1.0071827173233032,
      "learning_rate": 0.00016860439988103262,
      "loss": 2.102,
      "step": 750
    },
    {
      "epoch": 0.5067511251875313,
      "grad_norm": 0.9765022993087769,
      "learning_rate": 0.00016785537989044607,
      "loss": 2.1326,
      "step": 760
    },
    {
      "epoch": 0.5134189031505251,
      "grad_norm": 1.0499193668365479,
      "learning_rate": 0.00016710635989985952,
      "loss": 2.0372,
      "step": 770
    },
    {
      "epoch": 0.520086681113519,
      "grad_norm": 1.041703462600708,
      "learning_rate": 0.00016635733990927296,
      "loss": 2.0919,
      "step": 780
    },
    {
      "epoch": 0.5267544590765127,
      "grad_norm": 0.8728442192077637,
      "learning_rate": 0.0001656083199186864,
      "loss": 2.1096,
      "step": 790
    },
    {
      "epoch": 0.5334222370395065,
      "grad_norm": 1.6778113842010498,
      "learning_rate": 0.00016485929992809988,
      "loss": 2.0971,
      "step": 800
    },
    {
      "epoch": 0.5400900150025004,
      "grad_norm": 1.0779480934143066,
      "learning_rate": 0.0001641102799375133,
      "loss": 2.1855,
      "step": 810
    },
    {
      "epoch": 0.5467577929654942,
      "grad_norm": 0.9510670900344849,
      "learning_rate": 0.00016336125994692677,
      "loss": 2.0346,
      "step": 820
    },
    {
      "epoch": 0.5534255709284881,
      "grad_norm": 1.1109097003936768,
      "learning_rate": 0.00016261223995634022,
      "loss": 2.0765,
      "step": 830
    },
    {
      "epoch": 0.5600933488914819,
      "grad_norm": 1.072069525718689,
      "learning_rate": 0.00016186321996575367,
      "loss": 2.1021,
      "step": 840
    },
    {
      "epoch": 0.5667611268544758,
      "grad_norm": 1.049601674079895,
      "learning_rate": 0.0001611141999751671,
      "loss": 2.0571,
      "step": 850
    },
    {
      "epoch": 0.5734289048174696,
      "grad_norm": 1.5008738040924072,
      "learning_rate": 0.00016036517998458056,
      "loss": 2.0769,
      "step": 860
    },
    {
      "epoch": 0.5800966827804634,
      "grad_norm": 1.1491920948028564,
      "learning_rate": 0.00015961615999399403,
      "loss": 2.1222,
      "step": 870
    },
    {
      "epoch": 0.5867644607434572,
      "grad_norm": 1.5036758184432983,
      "learning_rate": 0.00015886714000340745,
      "loss": 1.9728,
      "step": 880
    },
    {
      "epoch": 0.5934322387064511,
      "grad_norm": 1.1733834743499756,
      "learning_rate": 0.00015811812001282092,
      "loss": 2.0821,
      "step": 890
    },
    {
      "epoch": 0.6001000166694449,
      "grad_norm": 1.0073555707931519,
      "learning_rate": 0.00015736910002223437,
      "loss": 2.0649,
      "step": 900
    },
    {
      "epoch": 0.6067677946324388,
      "grad_norm": 1.7133086919784546,
      "learning_rate": 0.0001566200800316478,
      "loss": 2.0196,
      "step": 910
    },
    {
      "epoch": 0.6134355725954326,
      "grad_norm": 1.3040004968643188,
      "learning_rate": 0.00015587106004106126,
      "loss": 2.0986,
      "step": 920
    },
    {
      "epoch": 0.6201033505584264,
      "grad_norm": 1.1574733257293701,
      "learning_rate": 0.0001551220400504747,
      "loss": 2.1285,
      "step": 930
    },
    {
      "epoch": 0.6267711285214203,
      "grad_norm": 1.377074122428894,
      "learning_rate": 0.00015437302005988818,
      "loss": 2.1143,
      "step": 940
    },
    {
      "epoch": 0.633438906484414,
      "grad_norm": 1.4042465686798096,
      "learning_rate": 0.0001536240000693016,
      "loss": 2.047,
      "step": 950
    },
    {
      "epoch": 0.6401066844474079,
      "grad_norm": 1.7583776712417603,
      "learning_rate": 0.00015287498007871504,
      "loss": 2.0725,
      "step": 960
    },
    {
      "epoch": 0.6467744624104017,
      "grad_norm": 1.3909072875976562,
      "learning_rate": 0.00015212596008812852,
      "loss": 2.0692,
      "step": 970
    },
    {
      "epoch": 0.6534422403733956,
      "grad_norm": 1.273395299911499,
      "learning_rate": 0.00015137694009754194,
      "loss": 2.0222,
      "step": 980
    },
    {
      "epoch": 0.6601100183363894,
      "grad_norm": 1.4802343845367432,
      "learning_rate": 0.0001506279201069554,
      "loss": 2.0093,
      "step": 990
    },
    {
      "epoch": 0.6667777962993833,
      "grad_norm": 1.380411982536316,
      "learning_rate": 0.00014987890011636885,
      "loss": 2.1189,
      "step": 1000
    },
    {
      "epoch": 0.6734455742623771,
      "grad_norm": 1.56609046459198,
      "learning_rate": 0.0001491298801257823,
      "loss": 1.9593,
      "step": 1010
    },
    {
      "epoch": 0.680113352225371,
      "grad_norm": 2.222093343734741,
      "learning_rate": 0.00014838086013519575,
      "loss": 2.0213,
      "step": 1020
    },
    {
      "epoch": 0.6867811301883647,
      "grad_norm": 1.2844229936599731,
      "learning_rate": 0.0001476318401446092,
      "loss": 2.0877,
      "step": 1030
    },
    {
      "epoch": 0.6934489081513585,
      "grad_norm": 1.003288745880127,
      "learning_rate": 0.00014688282015402264,
      "loss": 2.0023,
      "step": 1040
    },
    {
      "epoch": 0.7001166861143524,
      "grad_norm": 1.512398600578308,
      "learning_rate": 0.00014613380016343608,
      "loss": 1.9747,
      "step": 1050
    },
    {
      "epoch": 0.7067844640773462,
      "grad_norm": 1.422386884689331,
      "learning_rate": 0.00014538478017284956,
      "loss": 1.9503,
      "step": 1060
    },
    {
      "epoch": 0.7134522420403401,
      "grad_norm": 1.6395717859268188,
      "learning_rate": 0.000144635760182263,
      "loss": 2.0596,
      "step": 1070
    },
    {
      "epoch": 0.7201200200033339,
      "grad_norm": 1.6180084943771362,
      "learning_rate": 0.00014388674019167645,
      "loss": 2.0062,
      "step": 1080
    },
    {
      "epoch": 0.7267877979663278,
      "grad_norm": 1.0643893480300903,
      "learning_rate": 0.0001431377202010899,
      "loss": 2.0441,
      "step": 1090
    },
    {
      "epoch": 0.7334555759293215,
      "grad_norm": 1.6165719032287598,
      "learning_rate": 0.00014238870021050334,
      "loss": 2.0051,
      "step": 1100
    },
    {
      "epoch": 0.7401233538923154,
      "grad_norm": 1.7456414699554443,
      "learning_rate": 0.0001416396802199168,
      "loss": 1.9733,
      "step": 1110
    },
    {
      "epoch": 0.7467911318553092,
      "grad_norm": 1.5669625997543335,
      "learning_rate": 0.00014089066022933023,
      "loss": 2.0937,
      "step": 1120
    },
    {
      "epoch": 0.7534589098183031,
      "grad_norm": 1.6170710325241089,
      "learning_rate": 0.0001401416402387437,
      "loss": 2.0383,
      "step": 1130
    },
    {
      "epoch": 0.7601266877812969,
      "grad_norm": 1.4779576063156128,
      "learning_rate": 0.00013939262024815713,
      "loss": 1.9852,
      "step": 1140
    },
    {
      "epoch": 0.7667944657442907,
      "grad_norm": 1.7611688375473022,
      "learning_rate": 0.00013864360025757057,
      "loss": 1.9648,
      "step": 1150
    },
    {
      "epoch": 0.7734622437072846,
      "grad_norm": 1.3351749181747437,
      "learning_rate": 0.00013789458026698404,
      "loss": 2.0061,
      "step": 1160
    },
    {
      "epoch": 0.7801300216702783,
      "grad_norm": 1.573513150215149,
      "learning_rate": 0.00013714556027639746,
      "loss": 1.9753,
      "step": 1170
    },
    {
      "epoch": 0.7867977996332722,
      "grad_norm": 1.0282225608825684,
      "learning_rate": 0.00013639654028581094,
      "loss": 2.009,
      "step": 1180
    },
    {
      "epoch": 0.793465577596266,
      "grad_norm": 2.141368865966797,
      "learning_rate": 0.00013564752029522438,
      "loss": 2.0325,
      "step": 1190
    },
    {
      "epoch": 0.8001333555592599,
      "grad_norm": 1.554168462753296,
      "learning_rate": 0.00013489850030463786,
      "loss": 1.9683,
      "step": 1200
    },
    {
      "epoch": 0.8068011335222537,
      "grad_norm": 1.9316633939743042,
      "learning_rate": 0.00013414948031405127,
      "loss": 2.047,
      "step": 1210
    },
    {
      "epoch": 0.8134689114852476,
      "grad_norm": 2.039975643157959,
      "learning_rate": 0.00013340046032346472,
      "loss": 1.9178,
      "step": 1220
    },
    {
      "epoch": 0.8201366894482414,
      "grad_norm": 1.4897420406341553,
      "learning_rate": 0.0001326514403328782,
      "loss": 2.0437,
      "step": 1230
    },
    {
      "epoch": 0.8268044674112353,
      "grad_norm": 1.5122647285461426,
      "learning_rate": 0.0001319024203422916,
      "loss": 1.9642,
      "step": 1240
    },
    {
      "epoch": 0.833472245374229,
      "grad_norm": 1.5228976011276245,
      "learning_rate": 0.00013115340035170509,
      "loss": 2.014,
      "step": 1250
    },
    {
      "epoch": 0.8401400233372228,
      "grad_norm": 2.0042452812194824,
      "learning_rate": 0.00013040438036111853,
      "loss": 2.0121,
      "step": 1260
    },
    {
      "epoch": 0.8468078013002167,
      "grad_norm": 2.7750535011291504,
      "learning_rate": 0.00012965536037053198,
      "loss": 1.9439,
      "step": 1270
    },
    {
      "epoch": 0.8534755792632105,
      "grad_norm": 1.4513986110687256,
      "learning_rate": 0.00012890634037994542,
      "loss": 2.0093,
      "step": 1280
    },
    {
      "epoch": 0.8601433572262044,
      "grad_norm": 1.4496991634368896,
      "learning_rate": 0.00012815732038935887,
      "loss": 2.0237,
      "step": 1290
    },
    {
      "epoch": 0.8668111351891982,
      "grad_norm": 1.5925257205963135,
      "learning_rate": 0.00012740830039877231,
      "loss": 1.9733,
      "step": 1300
    },
    {
      "epoch": 0.8734789131521921,
      "grad_norm": 1.4781728982925415,
      "learning_rate": 0.00012665928040818576,
      "loss": 1.9677,
      "step": 1310
    },
    {
      "epoch": 0.8801466911151858,
      "grad_norm": 2.0847840309143066,
      "learning_rate": 0.00012591026041759923,
      "loss": 1.9308,
      "step": 1320
    },
    {
      "epoch": 0.8868144690781797,
      "grad_norm": 1.9497902393341064,
      "learning_rate": 0.00012516124042701268,
      "loss": 1.9596,
      "step": 1330
    },
    {
      "epoch": 0.8934822470411735,
      "grad_norm": 1.1415832042694092,
      "learning_rate": 0.0001244122204364261,
      "loss": 1.9524,
      "step": 1340
    },
    {
      "epoch": 0.9001500250041674,
      "grad_norm": 2.2979516983032227,
      "learning_rate": 0.00012366320044583957,
      "loss": 1.9328,
      "step": 1350
    },
    {
      "epoch": 0.9068178029671612,
      "grad_norm": 1.448180913925171,
      "learning_rate": 0.00012291418045525302,
      "loss": 1.9552,
      "step": 1360
    },
    {
      "epoch": 0.913485580930155,
      "grad_norm": 1.2262730598449707,
      "learning_rate": 0.00012216516046466646,
      "loss": 1.8666,
      "step": 1370
    },
    {
      "epoch": 0.9201533588931489,
      "grad_norm": 1.3498716354370117,
      "learning_rate": 0.00012141614047407991,
      "loss": 1.9732,
      "step": 1380
    },
    {
      "epoch": 0.9268211368561426,
      "grad_norm": 1.791790246963501,
      "learning_rate": 0.00012066712048349337,
      "loss": 2.0619,
      "step": 1390
    },
    {
      "epoch": 0.9334889148191365,
      "grad_norm": 1.8241612911224365,
      "learning_rate": 0.0001199181004929068,
      "loss": 1.9858,
      "step": 1400
    },
    {
      "epoch": 0.9401566927821303,
      "grad_norm": 1.3552191257476807,
      "learning_rate": 0.00011916908050232026,
      "loss": 2.0095,
      "step": 1410
    },
    {
      "epoch": 0.9468244707451242,
      "grad_norm": 1.4972807168960571,
      "learning_rate": 0.00011842006051173372,
      "loss": 1.9764,
      "step": 1420
    },
    {
      "epoch": 0.953492248708118,
      "grad_norm": 1.1430363655090332,
      "learning_rate": 0.00011767104052114715,
      "loss": 2.0758,
      "step": 1430
    },
    {
      "epoch": 0.9601600266711119,
      "grad_norm": 2.682908535003662,
      "learning_rate": 0.0001169220205305606,
      "loss": 1.9744,
      "step": 1440
    },
    {
      "epoch": 0.9668278046341057,
      "grad_norm": 2.743666887283325,
      "learning_rate": 0.00011617300053997406,
      "loss": 1.9072,
      "step": 1450
    },
    {
      "epoch": 0.9734955825970996,
      "grad_norm": 1.6913983821868896,
      "learning_rate": 0.00011542398054938752,
      "loss": 1.8955,
      "step": 1460
    },
    {
      "epoch": 0.9801633605600933,
      "grad_norm": 1.927062749862671,
      "learning_rate": 0.00011467496055880095,
      "loss": 1.9368,
      "step": 1470
    },
    {
      "epoch": 0.9868311385230871,
      "grad_norm": 2.547619581222534,
      "learning_rate": 0.00011392594056821441,
      "loss": 1.9579,
      "step": 1480
    },
    {
      "epoch": 0.993498916486081,
      "grad_norm": 2.032468795776367,
      "learning_rate": 0.00011317692057762786,
      "loss": 1.895,
      "step": 1490
    },
    {
      "epoch": 1.0,
      "grad_norm": 2.551286220550537,
      "learning_rate": 0.00011242790058704129,
      "loss": 2.0144,
      "step": 1500
    },
    {
      "epoch": 1.0,
      "eval_loss": 1.9409127235412598,
      "eval_runtime": 939.5657,
      "eval_samples_per_second": 6.385,
      "eval_steps_per_second": 3.193,
      "step": 1500
    },
    {
      "epoch": 1.0066677779629938,
      "grad_norm": 1.9876621961593628,
      "learning_rate": 0.00011167888059645475,
      "loss": 1.877,
      "step": 1510
    },
    {
      "epoch": 1.0133355559259876,
      "grad_norm": 1.6464308500289917,
      "learning_rate": 0.0001109298606058682,
      "loss": 1.9146,
      "step": 1520
    },
    {
      "epoch": 1.0200033338889816,
      "grad_norm": 1.6557950973510742,
      "learning_rate": 0.00011018084061528165,
      "loss": 1.9388,
      "step": 1530
    },
    {
      "epoch": 1.0266711118519753,
      "grad_norm": 1.444344401359558,
      "learning_rate": 0.0001094318206246951,
      "loss": 1.976,
      "step": 1540
    },
    {
      "epoch": 1.0333388898149691,
      "grad_norm": 1.1253176927566528,
      "learning_rate": 0.00010868280063410854,
      "loss": 1.9798,
      "step": 1550
    },
    {
      "epoch": 1.040006667777963,
      "grad_norm": 1.592197299003601,
      "learning_rate": 0.00010793378064352199,
      "loss": 1.9338,
      "step": 1560
    },
    {
      "epoch": 1.046674445740957,
      "grad_norm": 1.2594597339630127,
      "learning_rate": 0.00010718476065293544,
      "loss": 1.9344,
      "step": 1570
    },
    {
      "epoch": 1.0533422237039507,
      "grad_norm": 1.9561591148376465,
      "learning_rate": 0.0001064357406623489,
      "loss": 1.8391,
      "step": 1580
    },
    {
      "epoch": 1.0600100016669445,
      "grad_norm": 1.87005615234375,
      "learning_rate": 0.00010568672067176234,
      "loss": 1.8605,
      "step": 1590
    },
    {
      "epoch": 1.0666777796299383,
      "grad_norm": 1.39884352684021,
      "learning_rate": 0.0001049377006811758,
      "loss": 1.9609,
      "step": 1600
    },
    {
      "epoch": 1.073345557592932,
      "grad_norm": 1.778030276298523,
      "learning_rate": 0.00010418868069058925,
      "loss": 1.8807,
      "step": 1610
    },
    {
      "epoch": 1.080013335555926,
      "grad_norm": 1.4710934162139893,
      "learning_rate": 0.00010343966070000268,
      "loss": 1.9621,
      "step": 1620
    },
    {
      "epoch": 1.0866811135189198,
      "grad_norm": 1.5187294483184814,
      "learning_rate": 0.00010269064070941614,
      "loss": 1.9466,
      "step": 1630
    },
    {
      "epoch": 1.0933488914819136,
      "grad_norm": 1.4482239484786987,
      "learning_rate": 0.00010194162071882959,
      "loss": 1.8657,
      "step": 1640
    },
    {
      "epoch": 1.1000166694449074,
      "grad_norm": 1.9973909854888916,
      "learning_rate": 0.00010119260072824303,
      "loss": 1.921,
      "step": 1650
    },
    {
      "epoch": 1.1066844474079014,
      "grad_norm": 1.9516185522079468,
      "learning_rate": 0.00010044358073765649,
      "loss": 1.8634,
      "step": 1660
    },
    {
      "epoch": 1.1133522253708952,
      "grad_norm": 1.7585629224777222,
      "learning_rate": 9.969456074706994e-05,
      "loss": 1.8398,
      "step": 1670
    },
    {
      "epoch": 1.120020003333889,
      "grad_norm": 1.9964467287063599,
      "learning_rate": 9.894554075648338e-05,
      "loss": 1.8938,
      "step": 1680
    },
    {
      "epoch": 1.1266877812968827,
      "grad_norm": 1.4991673231124878,
      "learning_rate": 9.819652076589683e-05,
      "loss": 1.9092,
      "step": 1690
    },
    {
      "epoch": 1.1333555592598765,
      "grad_norm": 2.070495367050171,
      "learning_rate": 9.744750077531027e-05,
      "loss": 1.8043,
      "step": 1700
    },
    {
      "epoch": 1.1400233372228705,
      "grad_norm": 1.5290415287017822,
      "learning_rate": 9.669848078472373e-05,
      "loss": 1.8759,
      "step": 1710
    },
    {
      "epoch": 1.1466911151858643,
      "grad_norm": 1.7562260627746582,
      "learning_rate": 9.594946079413718e-05,
      "loss": 1.8665,
      "step": 1720
    },
    {
      "epoch": 1.153358893148858,
      "grad_norm": 1.6150237321853638,
      "learning_rate": 9.520044080355064e-05,
      "loss": 1.8152,
      "step": 1730
    },
    {
      "epoch": 1.160026671111852,
      "grad_norm": 2.1079277992248535,
      "learning_rate": 9.445142081296407e-05,
      "loss": 1.9248,
      "step": 1740
    },
    {
      "epoch": 1.1666944490748459,
      "grad_norm": 1.0787771940231323,
      "learning_rate": 9.370240082237752e-05,
      "loss": 1.8134,
      "step": 1750
    },
    {
      "epoch": 1.1733622270378397,
      "grad_norm": 1.5522607564926147,
      "learning_rate": 9.295338083179098e-05,
      "loss": 1.8448,
      "step": 1760
    },
    {
      "epoch": 1.1800300050008334,
      "grad_norm": 2.07295560836792,
      "learning_rate": 9.220436084120442e-05,
      "loss": 1.8933,
      "step": 1770
    },
    {
      "epoch": 1.1866977829638272,
      "grad_norm": 2.2503111362457275,
      "learning_rate": 9.145534085061787e-05,
      "loss": 1.8117,
      "step": 1780
    },
    {
      "epoch": 1.1933655609268212,
      "grad_norm": 1.6476895809173584,
      "learning_rate": 9.070632086003133e-05,
      "loss": 1.8371,
      "step": 1790
    },
    {
      "epoch": 1.200033338889815,
      "grad_norm": 1.5207879543304443,
      "learning_rate": 8.995730086944478e-05,
      "loss": 1.8949,
      "step": 1800
    },
    {
      "epoch": 1.2067011168528088,
      "grad_norm": 1.4955098628997803,
      "learning_rate": 8.920828087885822e-05,
      "loss": 1.9107,
      "step": 1810
    },
    {
      "epoch": 1.2133688948158026,
      "grad_norm": 1.5926152467727661,
      "learning_rate": 8.845926088827167e-05,
      "loss": 1.8155,
      "step": 1820
    },
    {
      "epoch": 1.2200366727787966,
      "grad_norm": 1.4697185754776,
      "learning_rate": 8.771024089768511e-05,
      "loss": 1.8896,
      "step": 1830
    },
    {
      "epoch": 1.2267044507417904,
      "grad_norm": 1.669506311416626,
      "learning_rate": 8.696122090709857e-05,
      "loss": 1.8878,
      "step": 1840
    },
    {
      "epoch": 1.2333722287047841,
      "grad_norm": 1.407632827758789,
      "learning_rate": 8.621220091651202e-05,
      "loss": 1.8647,
      "step": 1850
    },
    {
      "epoch": 1.240040006667778,
      "grad_norm": 2.601649522781372,
      "learning_rate": 8.546318092592548e-05,
      "loss": 1.8709,
      "step": 1860
    },
    {
      "epoch": 1.2467077846307717,
      "grad_norm": 1.6193265914916992,
      "learning_rate": 8.471416093533891e-05,
      "loss": 1.8287,
      "step": 1870
    },
    {
      "epoch": 1.2533755625937657,
      "grad_norm": 1.5287505388259888,
      "learning_rate": 8.396514094475236e-05,
      "loss": 1.9056,
      "step": 1880
    },
    {
      "epoch": 1.2600433405567595,
      "grad_norm": 2.025380849838257,
      "learning_rate": 8.321612095416582e-05,
      "loss": 1.8715,
      "step": 1890
    },
    {
      "epoch": 1.2667111185197533,
      "grad_norm": 1.6228477954864502,
      "learning_rate": 8.246710096357926e-05,
      "loss": 1.9519,
      "step": 1900
    },
    {
      "epoch": 1.273378896482747,
      "grad_norm": 1.745816946029663,
      "learning_rate": 8.171808097299272e-05,
      "loss": 1.901,
      "step": 1910
    },
    {
      "epoch": 1.280046674445741,
      "grad_norm": 1.8023282289505005,
      "learning_rate": 8.096906098240617e-05,
      "loss": 1.9317,
      "step": 1920
    },
    {
      "epoch": 1.2867144524087348,
      "grad_norm": 2.0181658267974854,
      "learning_rate": 8.02200409918196e-05,
      "loss": 1.8975,
      "step": 1930
    },
    {
      "epoch": 1.2933822303717286,
      "grad_norm": 1.8164279460906982,
      "learning_rate": 7.947102100123306e-05,
      "loss": 1.7832,
      "step": 1940
    },
    {
      "epoch": 1.3000500083347224,
      "grad_norm": 2.0253634452819824,
      "learning_rate": 7.87220010106465e-05,
      "loss": 1.8125,
      "step": 1950
    },
    {
      "epoch": 1.3067177862977162,
      "grad_norm": 2.111058473587036,
      "learning_rate": 7.797298102005995e-05,
      "loss": 1.856,
      "step": 1960
    },
    {
      "epoch": 1.3133855642607102,
      "grad_norm": 2.08693265914917,
      "learning_rate": 7.722396102947341e-05,
      "loss": 1.8132,
      "step": 1970
    },
    {
      "epoch": 1.320053342223704,
      "grad_norm": 1.386176347732544,
      "learning_rate": 7.647494103888686e-05,
      "loss": 1.9054,
      "step": 1980
    },
    {
      "epoch": 1.3267211201866977,
      "grad_norm": 1.6313347816467285,
      "learning_rate": 7.57259210483003e-05,
      "loss": 1.7529,
      "step": 1990
    },
    {
      "epoch": 1.3333888981496917,
      "grad_norm": 1.7046386003494263,
      "learning_rate": 7.497690105771375e-05,
      "loss": 1.9046,
      "step": 2000
    },
    {
      "epoch": 1.3400566761126855,
      "grad_norm": 1.922353744506836,
      "learning_rate": 7.42278810671272e-05,
      "loss": 1.8234,
      "step": 2010
    },
    {
      "epoch": 1.3467244540756793,
      "grad_norm": 2.1841299533843994,
      "learning_rate": 7.347886107654065e-05,
      "loss": 1.795,
      "step": 2020
    },
    {
      "epoch": 1.353392232038673,
      "grad_norm": 1.288373351097107,
      "learning_rate": 7.27298410859541e-05,
      "loss": 1.7903,
      "step": 2030
    },
    {
      "epoch": 1.3600600100016669,
      "grad_norm": 1.5053412914276123,
      "learning_rate": 7.198082109536756e-05,
      "loss": 1.853,
      "step": 2040
    },
    {
      "epoch": 1.3667277879646607,
      "grad_norm": 2.1440820693969727,
      "learning_rate": 7.1231801104781e-05,
      "loss": 1.7107,
      "step": 2050
    },
    {
      "epoch": 1.3733955659276547,
      "grad_norm": 2.0599429607391357,
      "learning_rate": 7.048278111419444e-05,
      "loss": 1.8181,
      "step": 2060
    },
    {
      "epoch": 1.3800633438906484,
      "grad_norm": 1.6755205392837524,
      "learning_rate": 6.97337611236079e-05,
      "loss": 1.9055,
      "step": 2070
    },
    {
      "epoch": 1.3867311218536422,
      "grad_norm": 2.395555019378662,
      "learning_rate": 6.898474113302134e-05,
      "loss": 1.878,
      "step": 2080
    },
    {
      "epoch": 1.3933988998166362,
      "grad_norm": 2.4337446689605713,
      "learning_rate": 6.823572114243479e-05,
      "loss": 1.7468,
      "step": 2090
    },
    {
      "epoch": 1.40006667777963,
      "grad_norm": 1.3166109323501587,
      "learning_rate": 6.748670115184825e-05,
      "loss": 1.9411,
      "step": 2100
    },
    {
      "epoch": 1.4067344557426238,
      "grad_norm": 1.3217018842697144,
      "learning_rate": 6.67376811612617e-05,
      "loss": 1.907,
      "step": 2110
    },
    {
      "epoch": 1.4134022337056176,
      "grad_norm": 2.3200464248657227,
      "learning_rate": 6.598866117067514e-05,
      "loss": 1.7873,
      "step": 2120
    },
    {
      "epoch": 1.4200700116686114,
      "grad_norm": 1.300701379776001,
      "learning_rate": 6.523964118008859e-05,
      "loss": 1.943,
      "step": 2130
    },
    {
      "epoch": 1.4267377896316051,
      "grad_norm": 1.6755272150039673,
      "learning_rate": 6.449062118950203e-05,
      "loss": 1.7969,
      "step": 2140
    },
    {
      "epoch": 1.4334055675945991,
      "grad_norm": 1.9792630672454834,
      "learning_rate": 6.374160119891549e-05,
      "loss": 1.8888,
      "step": 2150
    },
    {
      "epoch": 1.440073345557593,
      "grad_norm": 2.2408552169799805,
      "learning_rate": 6.299258120832894e-05,
      "loss": 1.8229,
      "step": 2160
    },
    {
      "epoch": 1.4467411235205867,
      "grad_norm": 1.974768877029419,
      "learning_rate": 6.22435612177424e-05,
      "loss": 1.917,
      "step": 2170
    },
    {
      "epoch": 1.4534089014835807,
      "grad_norm": 1.861130952835083,
      "learning_rate": 6.149454122715583e-05,
      "loss": 1.8025,
      "step": 2180
    },
    {
      "epoch": 1.4600766794465745,
      "grad_norm": 1.766618251800537,
      "learning_rate": 6.074552123656928e-05,
      "loss": 1.8076,
      "step": 2190
    },
    {
      "epoch": 1.4667444574095683,
      "grad_norm": 1.8743497133255005,
      "learning_rate": 5.9996501245982735e-05,
      "loss": 1.9516,
      "step": 2200
    },
    {
      "epoch": 1.473412235372562,
      "grad_norm": 1.745882272720337,
      "learning_rate": 5.924748125539618e-05,
      "loss": 1.8827,
      "step": 2210
    },
    {
      "epoch": 1.4800800133355558,
      "grad_norm": 1.456998348236084,
      "learning_rate": 5.849846126480964e-05,
      "loss": 1.7951,
      "step": 2220
    },
    {
      "epoch": 1.4867477912985498,
      "grad_norm": 1.4050835371017456,
      "learning_rate": 5.774944127422308e-05,
      "loss": 1.8647,
      "step": 2230
    },
    {
      "epoch": 1.4934155692615436,
      "grad_norm": 1.6301414966583252,
      "learning_rate": 5.7000421283636526e-05,
      "loss": 1.8255,
      "step": 2240
    },
    {
      "epoch": 1.5000833472245374,
      "grad_norm": 1.729873538017273,
      "learning_rate": 5.6251401293049985e-05,
      "loss": 1.8923,
      "step": 2250
    },
    {
      "epoch": 1.5067511251875314,
      "grad_norm": 1.4367108345031738,
      "learning_rate": 5.5502381302463424e-05,
      "loss": 1.7998,
      "step": 2260
    },
    {
      "epoch": 1.5134189031505252,
      "grad_norm": 1.9231281280517578,
      "learning_rate": 5.475336131187688e-05,
      "loss": 1.9453,
      "step": 2270
    },
    {
      "epoch": 1.520086681113519,
      "grad_norm": 1.653169870376587,
      "learning_rate": 5.400434132129033e-05,
      "loss": 1.8365,
      "step": 2280
    },
    {
      "epoch": 1.5267544590765127,
      "grad_norm": 2.2526321411132812,
      "learning_rate": 5.325532133070377e-05,
      "loss": 1.8521,
      "step": 2290
    },
    {
      "epoch": 1.5334222370395065,
      "grad_norm": 1.4734408855438232,
      "learning_rate": 5.250630134011722e-05,
      "loss": 1.8605,
      "step": 2300
    },
    {
      "epoch": 1.5400900150025003,
      "grad_norm": 1.7644442319869995,
      "learning_rate": 5.1757281349530675e-05,
      "loss": 1.9385,
      "step": 2310
    },
    {
      "epoch": 1.546757792965494,
      "grad_norm": 1.351728916168213,
      "learning_rate": 5.100826135894412e-05,
      "loss": 1.9166,
      "step": 2320
    },
    {
      "epoch": 1.553425570928488,
      "grad_norm": 1.6552897691726685,
      "learning_rate": 5.025924136835757e-05,
      "loss": 1.9265,
      "step": 2330
    },
    {
      "epoch": 1.5600933488914819,
      "grad_norm": 2.3140876293182373,
      "learning_rate": 4.951022137777102e-05,
      "loss": 1.8688,
      "step": 2340
    },
    {
      "epoch": 1.5667611268544759,
      "grad_norm": 1.490618109703064,
      "learning_rate": 4.8761201387184465e-05,
      "loss": 1.7376,
      "step": 2350
    },
    {
      "epoch": 1.5734289048174697,
      "grad_norm": 2.3267579078674316,
      "learning_rate": 4.801218139659792e-05,
      "loss": 1.8136,
      "step": 2360
    },
    {
      "epoch": 1.5800966827804634,
      "grad_norm": 1.768515706062317,
      "learning_rate": 4.726316140601137e-05,
      "loss": 1.7643,
      "step": 2370
    },
    {
      "epoch": 1.5867644607434572,
      "grad_norm": 1.848042368888855,
      "learning_rate": 4.6514141415424817e-05,
      "loss": 1.777,
      "step": 2380
    },
    {
      "epoch": 1.593432238706451,
      "grad_norm": 2.4507362842559814,
      "learning_rate": 4.576512142483826e-05,
      "loss": 1.8597,
      "step": 2390
    },
    {
      "epoch": 1.6001000166694448,
      "grad_norm": 1.8656047582626343,
      "learning_rate": 4.5016101434251715e-05,
      "loss": 1.8355,
      "step": 2400
    },
    {
      "epoch": 1.6067677946324388,
      "grad_norm": 1.546240210533142,
      "learning_rate": 4.426708144366516e-05,
      "loss": 1.7611,
      "step": 2410
    },
    {
      "epoch": 1.6134355725954326,
      "grad_norm": 2.6047909259796143,
      "learning_rate": 4.351806145307861e-05,
      "loss": 1.7508,
      "step": 2420
    },
    {
      "epoch": 1.6201033505584264,
      "grad_norm": 1.6348631381988525,
      "learning_rate": 4.276904146249206e-05,
      "loss": 1.8641,
      "step": 2430
    },
    {
      "epoch": 1.6267711285214204,
      "grad_norm": 1.4074373245239258,
      "learning_rate": 4.202002147190551e-05,
      "loss": 1.9216,
      "step": 2440
    },
    {
      "epoch": 1.6334389064844141,
      "grad_norm": 1.3445425033569336,
      "learning_rate": 4.127100148131896e-05,
      "loss": 1.9152,
      "step": 2450
    },
    {
      "epoch": 1.640106684447408,
      "grad_norm": 1.8514323234558105,
      "learning_rate": 4.052198149073241e-05,
      "loss": 1.8339,
      "step": 2460
    },
    {
      "epoch": 1.6467744624104017,
      "grad_norm": 2.1281745433807373,
      "learning_rate": 3.977296150014586e-05,
      "loss": 1.8692,
      "step": 2470
    },
    {
      "epoch": 1.6534422403733955,
      "grad_norm": 1.3811824321746826,
      "learning_rate": 3.90239415095593e-05,
      "loss": 1.8649,
      "step": 2480
    },
    {
      "epoch": 1.6601100183363893,
      "grad_norm": 1.5096583366394043,
      "learning_rate": 3.8274921518972756e-05,
      "loss": 1.7212,
      "step": 2490
    },
    {
      "epoch": 1.6667777962993833,
      "grad_norm": 1.54547119140625,
      "learning_rate": 3.752590152838621e-05,
      "loss": 1.8832,
      "step": 2500
    },
    {
      "epoch": 1.673445574262377,
      "grad_norm": 1.2251574993133545,
      "learning_rate": 3.677688153779965e-05,
      "loss": 1.8975,
      "step": 2510
    },
    {
      "epoch": 1.680113352225371,
      "grad_norm": 2.032773494720459,
      "learning_rate": 3.60278615472131e-05,
      "loss": 1.8493,
      "step": 2520
    },
    {
      "epoch": 1.6867811301883648,
      "grad_norm": 2.769503355026245,
      "learning_rate": 3.527884155662655e-05,
      "loss": 1.779,
      "step": 2530
    },
    {
      "epoch": 1.6934489081513586,
      "grad_norm": 2.2036538124084473,
      "learning_rate": 3.452982156604e-05,
      "loss": 1.8223,
      "step": 2540
    },
    {
      "epoch": 1.7001166861143524,
      "grad_norm": 1.6264684200286865,
      "learning_rate": 3.378080157545345e-05,
      "loss": 1.9441,
      "step": 2550
    },
    {
      "epoch": 1.7067844640773462,
      "grad_norm": 1.6185513734817505,
      "learning_rate": 3.30317815848669e-05,
      "loss": 1.7587,
      "step": 2560
    },
    {
      "epoch": 1.71345224204034,
      "grad_norm": 2.190612554550171,
      "learning_rate": 3.2282761594280344e-05,
      "loss": 1.8266,
      "step": 2570
    },
    {
      "epoch": 1.7201200200033337,
      "grad_norm": 2.144853353500366,
      "learning_rate": 3.15337416036938e-05,
      "loss": 1.8421,
      "step": 2580
    },
    {
      "epoch": 1.7267877979663278,
      "grad_norm": 1.7128126621246338,
      "learning_rate": 3.078472161310725e-05,
      "loss": 1.9103,
      "step": 2590
    },
    {
      "epoch": 1.7334555759293215,
      "grad_norm": 1.5930031538009644,
      "learning_rate": 3.0035701622520692e-05,
      "loss": 1.8167,
      "step": 2600
    },
    {
      "epoch": 1.7401233538923155,
      "grad_norm": 1.846116304397583,
      "learning_rate": 2.928668163193414e-05,
      "loss": 1.7895,
      "step": 2610
    },
    {
      "epoch": 1.7467911318553093,
      "grad_norm": 1.6104364395141602,
      "learning_rate": 2.8537661641347594e-05,
      "loss": 1.8666,
      "step": 2620
    },
    {
      "epoch": 1.753458909818303,
      "grad_norm": 1.4701653718948364,
      "learning_rate": 2.778864165076104e-05,
      "loss": 1.8039,
      "step": 2630
    },
    {
      "epoch": 1.7601266877812969,
      "grad_norm": 1.3585829734802246,
      "learning_rate": 2.703962166017449e-05,
      "loss": 1.7738,
      "step": 2640
    },
    {
      "epoch": 1.7667944657442907,
      "grad_norm": 1.6099369525909424,
      "learning_rate": 2.6290601669587942e-05,
      "loss": 1.7177,
      "step": 2650
    },
    {
      "epoch": 1.7734622437072844,
      "grad_norm": 1.6355185508728027,
      "learning_rate": 2.5541581679001388e-05,
      "loss": 1.8391,
      "step": 2660
    },
    {
      "epoch": 1.7801300216702782,
      "grad_norm": 1.6755468845367432,
      "learning_rate": 2.4792561688414837e-05,
      "loss": 1.882,
      "step": 2670
    },
    {
      "epoch": 1.7867977996332722,
      "grad_norm": 1.675413727760315,
      "learning_rate": 2.4043541697828287e-05,
      "loss": 1.6974,
      "step": 2680
    },
    {
      "epoch": 1.793465577596266,
      "grad_norm": 1.8662031888961792,
      "learning_rate": 2.3294521707241736e-05,
      "loss": 1.7709,
      "step": 2690
    },
    {
      "epoch": 1.80013335555926,
      "grad_norm": 1.6138955354690552,
      "learning_rate": 2.2545501716655182e-05,
      "loss": 1.8486,
      "step": 2700
    },
    {
      "epoch": 1.8068011335222538,
      "grad_norm": 1.2912325859069824,
      "learning_rate": 2.1796481726068635e-05,
      "loss": 1.886,
      "step": 2710
    },
    {
      "epoch": 1.8134689114852476,
      "grad_norm": 2.2786569595336914,
      "learning_rate": 2.104746173548208e-05,
      "loss": 1.8234,
      "step": 2720
    },
    {
      "epoch": 1.8201366894482414,
      "grad_norm": 2.0333411693573,
      "learning_rate": 2.0298441744895533e-05,
      "loss": 1.7217,
      "step": 2730
    },
    {
      "epoch": 1.8268044674112351,
      "grad_norm": 1.940285325050354,
      "learning_rate": 1.954942175430898e-05,
      "loss": 1.7948,
      "step": 2740
    },
    {
      "epoch": 1.833472245374229,
      "grad_norm": 2.000497579574585,
      "learning_rate": 1.880040176372243e-05,
      "loss": 1.823,
      "step": 2750
    },
    {
      "epoch": 1.8401400233372227,
      "grad_norm": 1.8904346227645874,
      "learning_rate": 1.805138177313588e-05,
      "loss": 1.8283,
      "step": 2760
    },
    {
      "epoch": 1.8468078013002167,
      "grad_norm": 2.0039875507354736,
      "learning_rate": 1.7302361782549327e-05,
      "loss": 1.8511,
      "step": 2770
    },
    {
      "epoch": 1.8534755792632105,
      "grad_norm": 1.890284538269043,
      "learning_rate": 1.6553341791962777e-05,
      "loss": 1.7912,
      "step": 2780
    },
    {
      "epoch": 1.8601433572262045,
      "grad_norm": 1.7154229879379272,
      "learning_rate": 1.5804321801376226e-05,
      "loss": 1.6694,
      "step": 2790
    },
    {
      "epoch": 1.8668111351891983,
      "grad_norm": 1.87376868724823,
      "learning_rate": 1.5055301810789675e-05,
      "loss": 1.8364,
      "step": 2800
    },
    {
      "epoch": 1.873478913152192,
      "grad_norm": 2.0322377681732178,
      "learning_rate": 1.4306281820203123e-05,
      "loss": 1.8825,
      "step": 2810
    },
    {
      "epoch": 1.8801466911151858,
      "grad_norm": 1.629072666168213,
      "learning_rate": 1.3557261829616574e-05,
      "loss": 1.8596,
      "step": 2820
    },
    {
      "epoch": 1.8868144690781796,
      "grad_norm": 2.3127100467681885,
      "learning_rate": 1.2808241839030022e-05,
      "loss": 1.8206,
      "step": 2830
    },
    {
      "epoch": 1.8934822470411734,
      "grad_norm": 1.6443568468093872,
      "learning_rate": 1.2059221848443471e-05,
      "loss": 1.8058,
      "step": 2840
    },
    {
      "epoch": 1.9001500250041674,
      "grad_norm": 2.037720203399658,
      "learning_rate": 1.131020185785692e-05,
      "loss": 1.7424,
      "step": 2850
    },
    {
      "epoch": 1.9068178029671612,
      "grad_norm": 1.1303163766860962,
      "learning_rate": 1.0561181867270368e-05,
      "loss": 1.8285,
      "step": 2860
    },
    {
      "epoch": 1.913485580930155,
      "grad_norm": 1.028782606124878,
      "learning_rate": 9.812161876683817e-06,
      "loss": 1.7856,
      "step": 2870
    },
    {
      "epoch": 1.920153358893149,
      "grad_norm": 1.4687622785568237,
      "learning_rate": 9.063141886097267e-06,
      "loss": 1.7722,
      "step": 2880
    },
    {
      "epoch": 1.9268211368561428,
      "grad_norm": 1.9744274616241455,
      "learning_rate": 8.314121895510716e-06,
      "loss": 1.7793,
      "step": 2890
    },
    {
      "epoch": 1.9334889148191365,
      "grad_norm": 1.5339246988296509,
      "learning_rate": 7.5651019049241645e-06,
      "loss": 1.7741,
      "step": 2900
    },
    {
      "epoch": 1.9401566927821303,
      "grad_norm": 1.7729872465133667,
      "learning_rate": 6.816081914337614e-06,
      "loss": 1.8701,
      "step": 2910
    },
    {
      "epoch": 1.946824470745124,
      "grad_norm": 1.7197774648666382,
      "learning_rate": 6.067061923751063e-06,
      "loss": 1.6859,
      "step": 2920
    },
    {
      "epoch": 1.9534922487081179,
      "grad_norm": 1.4736016988754272,
      "learning_rate": 5.318041933164512e-06,
      "loss": 1.9093,
      "step": 2930
    },
    {
      "epoch": 1.9601600266711119,
      "grad_norm": 2.0894088745117188,
      "learning_rate": 4.56902194257796e-06,
      "loss": 1.7863,
      "step": 2940
    },
    {
      "epoch": 1.9668278046341057,
      "grad_norm": 1.6206830739974976,
      "learning_rate": 3.82000195199141e-06,
      "loss": 1.8377,
      "step": 2950
    },
    {
      "epoch": 1.9734955825970997,
      "grad_norm": 2.010479688644409,
      "learning_rate": 3.0709819614048593e-06,
      "loss": 1.7747,
      "step": 2960
    },
    {
      "epoch": 1.9801633605600935,
      "grad_norm": 1.6726831197738647,
      "learning_rate": 2.321961970818308e-06,
      "loss": 1.811,
      "step": 2970
    },
    {
      "epoch": 1.9868311385230872,
      "grad_norm": 2.1523139476776123,
      "learning_rate": 1.5729419802317571e-06,
      "loss": 1.7287,
      "step": 2980
    },
    {
      "epoch": 1.993498916486081,
      "grad_norm": 1.3485302925109863,
      "learning_rate": 8.239219896452061e-07,
      "loss": 1.7387,
      "step": 2990
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.4598023891448975,
      "learning_rate": 7.49019990586551e-08,
      "loss": 1.7709,
      "step": 3000
    },
    {
      "epoch": 2.0,
      "eval_loss": 1.8174083232879639,
      "eval_runtime": 937.9361,
      "eval_samples_per_second": 6.396,
      "eval_steps_per_second": 3.199,
      "step": 3000
    }
  ],
  "logging_steps": 10,
  "max_steps": 3000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 500,
  "stateful_callbacks": {
    "EarlyStoppingCallback": {
      "args": {
        "early_stopping_patience": 2,
        "early_stopping_threshold": 0.0
      },
      "attributes": {
        "early_stopping_patience_counter": 0
      }
    },
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.0851479858275942e+18,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
