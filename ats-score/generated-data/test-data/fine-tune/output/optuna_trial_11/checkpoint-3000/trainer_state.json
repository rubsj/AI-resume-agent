{
  "best_global_step": 3000,
  "best_metric": 2.0527429580688477,
  "best_model_checkpoint": "json_outputs_all_data/fine-tune/optuna_output/optuna_trial_11/checkpoint-3000",
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 3000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.006667777962993832,
      "grad_norm": 0.22339338064193726,
      "learning_rate": 9.988670098263506e-05,
      "loss": 2.693,
      "step": 10
    },
    {
      "epoch": 0.013335555925987664,
      "grad_norm": 0.21842911839485168,
      "learning_rate": 9.955274344006523e-05,
      "loss": 2.5646,
      "step": 20
    },
    {
      "epoch": 0.020003333888981498,
      "grad_norm": 0.23425008356571198,
      "learning_rate": 9.92187858974954e-05,
      "loss": 2.5074,
      "step": 30
    },
    {
      "epoch": 0.026671111851975328,
      "grad_norm": 0.2226899266242981,
      "learning_rate": 9.888482835492557e-05,
      "loss": 2.5103,
      "step": 40
    },
    {
      "epoch": 0.03333888981496916,
      "grad_norm": 0.2594967484474182,
      "learning_rate": 9.855087081235575e-05,
      "loss": 2.4441,
      "step": 50
    },
    {
      "epoch": 0.040006667777962995,
      "grad_norm": 0.27808982133865356,
      "learning_rate": 9.821691326978592e-05,
      "loss": 2.4213,
      "step": 60
    },
    {
      "epoch": 0.046674445740956826,
      "grad_norm": 0.24453085660934448,
      "learning_rate": 9.78829557272161e-05,
      "loss": 2.3939,
      "step": 70
    },
    {
      "epoch": 0.053342223703950656,
      "grad_norm": 0.24925394356250763,
      "learning_rate": 9.754899818464628e-05,
      "loss": 2.4436,
      "step": 80
    },
    {
      "epoch": 0.06001000166694449,
      "grad_norm": 0.27500391006469727,
      "learning_rate": 9.721504064207645e-05,
      "loss": 2.4162,
      "step": 90
    },
    {
      "epoch": 0.06667777962993832,
      "grad_norm": 0.2992651164531708,
      "learning_rate": 9.688108309950662e-05,
      "loss": 2.3999,
      "step": 100
    },
    {
      "epoch": 0.07334555759293215,
      "grad_norm": 0.29613834619522095,
      "learning_rate": 9.654712555693679e-05,
      "loss": 2.4049,
      "step": 110
    },
    {
      "epoch": 0.08001333555592599,
      "grad_norm": 0.2944088280200958,
      "learning_rate": 9.621316801436697e-05,
      "loss": 2.419,
      "step": 120
    },
    {
      "epoch": 0.08668111351891981,
      "grad_norm": 0.29695215821266174,
      "learning_rate": 9.587921047179713e-05,
      "loss": 2.3625,
      "step": 130
    },
    {
      "epoch": 0.09334889148191365,
      "grad_norm": 0.28258007764816284,
      "learning_rate": 9.554525292922731e-05,
      "loss": 2.3934,
      "step": 140
    },
    {
      "epoch": 0.10001666944490749,
      "grad_norm": 0.3284846544265747,
      "learning_rate": 9.521129538665748e-05,
      "loss": 2.3795,
      "step": 150
    },
    {
      "epoch": 0.10668444740790131,
      "grad_norm": 0.353372722864151,
      "learning_rate": 9.487733784408766e-05,
      "loss": 2.3577,
      "step": 160
    },
    {
      "epoch": 0.11335222537089515,
      "grad_norm": 0.3192945122718811,
      "learning_rate": 9.454338030151784e-05,
      "loss": 2.3249,
      "step": 170
    },
    {
      "epoch": 0.12002000333388899,
      "grad_norm": 0.3343449831008911,
      "learning_rate": 9.420942275894801e-05,
      "loss": 2.3545,
      "step": 180
    },
    {
      "epoch": 0.12668778129688282,
      "grad_norm": 0.34540313482284546,
      "learning_rate": 9.387546521637819e-05,
      "loss": 2.3488,
      "step": 190
    },
    {
      "epoch": 0.13335555925987663,
      "grad_norm": 0.3326379954814911,
      "learning_rate": 9.354150767380835e-05,
      "loss": 2.324,
      "step": 200
    },
    {
      "epoch": 0.14002333722287047,
      "grad_norm": 0.32322877645492554,
      "learning_rate": 9.320755013123853e-05,
      "loss": 2.361,
      "step": 210
    },
    {
      "epoch": 0.1466911151858643,
      "grad_norm": 0.3613000214099884,
      "learning_rate": 9.28735925886687e-05,
      "loss": 2.3827,
      "step": 220
    },
    {
      "epoch": 0.15335889314885814,
      "grad_norm": 0.37711694836616516,
      "learning_rate": 9.253963504609887e-05,
      "loss": 2.3143,
      "step": 230
    },
    {
      "epoch": 0.16002667111185198,
      "grad_norm": 0.373211145401001,
      "learning_rate": 9.220567750352906e-05,
      "loss": 2.32,
      "step": 240
    },
    {
      "epoch": 0.16669444907484582,
      "grad_norm": 0.36388543248176575,
      "learning_rate": 9.187171996095923e-05,
      "loss": 2.3277,
      "step": 250
    },
    {
      "epoch": 0.17336222703783963,
      "grad_norm": 0.4201940894126892,
      "learning_rate": 9.15377624183894e-05,
      "loss": 2.3247,
      "step": 260
    },
    {
      "epoch": 0.18003000500083347,
      "grad_norm": 0.374002605676651,
      "learning_rate": 9.120380487581957e-05,
      "loss": 2.3902,
      "step": 270
    },
    {
      "epoch": 0.1866977829638273,
      "grad_norm": 0.4168948233127594,
      "learning_rate": 9.086984733324975e-05,
      "loss": 2.4324,
      "step": 280
    },
    {
      "epoch": 0.19336556092682114,
      "grad_norm": 0.38186880946159363,
      "learning_rate": 9.053588979067991e-05,
      "loss": 2.3388,
      "step": 290
    },
    {
      "epoch": 0.20003333888981498,
      "grad_norm": 0.3896980285644531,
      "learning_rate": 9.020193224811009e-05,
      "loss": 2.3482,
      "step": 300
    },
    {
      "epoch": 0.2067011168528088,
      "grad_norm": 0.4016822874546051,
      "learning_rate": 8.986797470554026e-05,
      "loss": 2.318,
      "step": 310
    },
    {
      "epoch": 0.21336889481580262,
      "grad_norm": 0.37509578466415405,
      "learning_rate": 8.953401716297044e-05,
      "loss": 2.35,
      "step": 320
    },
    {
      "epoch": 0.22003667277879646,
      "grad_norm": 0.41375210881233215,
      "learning_rate": 8.920005962040062e-05,
      "loss": 2.3028,
      "step": 330
    },
    {
      "epoch": 0.2267044507417903,
      "grad_norm": 0.4151321351528168,
      "learning_rate": 8.886610207783079e-05,
      "loss": 2.3076,
      "step": 340
    },
    {
      "epoch": 0.23337222870478413,
      "grad_norm": 0.437682569026947,
      "learning_rate": 8.853214453526097e-05,
      "loss": 2.3218,
      "step": 350
    },
    {
      "epoch": 0.24004000666777797,
      "grad_norm": 0.4202446937561035,
      "learning_rate": 8.819818699269113e-05,
      "loss": 2.3528,
      "step": 360
    },
    {
      "epoch": 0.24670778463077178,
      "grad_norm": 0.4139777719974518,
      "learning_rate": 8.786422945012131e-05,
      "loss": 2.3771,
      "step": 370
    },
    {
      "epoch": 0.25337556259376565,
      "grad_norm": 0.4803926944732666,
      "learning_rate": 8.753027190755148e-05,
      "loss": 2.3396,
      "step": 380
    },
    {
      "epoch": 0.2600433405567595,
      "grad_norm": 0.45079562067985535,
      "learning_rate": 8.719631436498165e-05,
      "loss": 2.3154,
      "step": 390
    },
    {
      "epoch": 0.26671111851975327,
      "grad_norm": 0.42669999599456787,
      "learning_rate": 8.686235682241182e-05,
      "loss": 2.3268,
      "step": 400
    },
    {
      "epoch": 0.2733788964827471,
      "grad_norm": 0.45842570066452026,
      "learning_rate": 8.6528399279842e-05,
      "loss": 2.2941,
      "step": 410
    },
    {
      "epoch": 0.28004667444574094,
      "grad_norm": 0.4479469060897827,
      "learning_rate": 8.619444173727218e-05,
      "loss": 2.2778,
      "step": 420
    },
    {
      "epoch": 0.2867144524087348,
      "grad_norm": 0.4298601746559143,
      "learning_rate": 8.586048419470235e-05,
      "loss": 2.2848,
      "step": 430
    },
    {
      "epoch": 0.2933822303717286,
      "grad_norm": 0.4612620770931244,
      "learning_rate": 8.552652665213253e-05,
      "loss": 2.3184,
      "step": 440
    },
    {
      "epoch": 0.30005000833472245,
      "grad_norm": 0.40337705612182617,
      "learning_rate": 8.51925691095627e-05,
      "loss": 2.3352,
      "step": 450
    },
    {
      "epoch": 0.3067177862977163,
      "grad_norm": 0.42833635210990906,
      "learning_rate": 8.485861156699287e-05,
      "loss": 2.3391,
      "step": 460
    },
    {
      "epoch": 0.3133855642607101,
      "grad_norm": 0.4507805109024048,
      "learning_rate": 8.452465402442304e-05,
      "loss": 2.3522,
      "step": 470
    },
    {
      "epoch": 0.32005334222370396,
      "grad_norm": 0.46003004908561707,
      "learning_rate": 8.419069648185322e-05,
      "loss": 2.2959,
      "step": 480
    },
    {
      "epoch": 0.3267211201866978,
      "grad_norm": 0.49208158254623413,
      "learning_rate": 8.385673893928338e-05,
      "loss": 2.3135,
      "step": 490
    },
    {
      "epoch": 0.33338889814969164,
      "grad_norm": 0.4925770163536072,
      "learning_rate": 8.352278139671356e-05,
      "loss": 2.3409,
      "step": 500
    },
    {
      "epoch": 0.3400566761126855,
      "grad_norm": 0.5137686133384705,
      "learning_rate": 8.318882385414374e-05,
      "loss": 2.3177,
      "step": 510
    },
    {
      "epoch": 0.34672445407567926,
      "grad_norm": 0.4672934114933014,
      "learning_rate": 8.285486631157391e-05,
      "loss": 2.2827,
      "step": 520
    },
    {
      "epoch": 0.3533922320386731,
      "grad_norm": 0.4863295555114746,
      "learning_rate": 8.252090876900409e-05,
      "loss": 2.3185,
      "step": 530
    },
    {
      "epoch": 0.36006001000166693,
      "grad_norm": 0.48994824290275574,
      "learning_rate": 8.218695122643427e-05,
      "loss": 2.2549,
      "step": 540
    },
    {
      "epoch": 0.36672778796466077,
      "grad_norm": 0.49137061834335327,
      "learning_rate": 8.185299368386443e-05,
      "loss": 2.2648,
      "step": 550
    },
    {
      "epoch": 0.3733955659276546,
      "grad_norm": 0.4857079088687897,
      "learning_rate": 8.15190361412946e-05,
      "loss": 2.1945,
      "step": 560
    },
    {
      "epoch": 0.38006334389064844,
      "grad_norm": 0.5297011137008667,
      "learning_rate": 8.118507859872478e-05,
      "loss": 2.2828,
      "step": 570
    },
    {
      "epoch": 0.3867311218536423,
      "grad_norm": 0.485480397939682,
      "learning_rate": 8.085112105615496e-05,
      "loss": 2.2987,
      "step": 580
    },
    {
      "epoch": 0.3933988998166361,
      "grad_norm": 0.5508373975753784,
      "learning_rate": 8.051716351358512e-05,
      "loss": 2.2939,
      "step": 590
    },
    {
      "epoch": 0.40006667777962995,
      "grad_norm": 0.5013153553009033,
      "learning_rate": 8.01832059710153e-05,
      "loss": 2.2735,
      "step": 600
    },
    {
      "epoch": 0.4067344557426238,
      "grad_norm": 0.5207000970840454,
      "learning_rate": 7.984924842844549e-05,
      "loss": 2.2655,
      "step": 610
    },
    {
      "epoch": 0.4134022337056176,
      "grad_norm": 0.5406561493873596,
      "learning_rate": 7.951529088587565e-05,
      "loss": 2.2599,
      "step": 620
    },
    {
      "epoch": 0.4200700116686114,
      "grad_norm": 0.5638642311096191,
      "learning_rate": 7.918133334330583e-05,
      "loss": 2.2756,
      "step": 630
    },
    {
      "epoch": 0.42673778963160525,
      "grad_norm": 0.538560688495636,
      "learning_rate": 7.8847375800736e-05,
      "loss": 2.2789,
      "step": 640
    },
    {
      "epoch": 0.4334055675945991,
      "grad_norm": 0.5349379777908325,
      "learning_rate": 7.851341825816616e-05,
      "loss": 2.3049,
      "step": 650
    },
    {
      "epoch": 0.4400733455575929,
      "grad_norm": 0.5196301937103271,
      "learning_rate": 7.817946071559634e-05,
      "loss": 2.281,
      "step": 660
    },
    {
      "epoch": 0.44674112352058676,
      "grad_norm": 0.6242773532867432,
      "learning_rate": 7.784550317302652e-05,
      "loss": 2.2194,
      "step": 670
    },
    {
      "epoch": 0.4534089014835806,
      "grad_norm": 0.5585293769836426,
      "learning_rate": 7.751154563045668e-05,
      "loss": 2.2802,
      "step": 680
    },
    {
      "epoch": 0.46007667944657443,
      "grad_norm": 0.5889605283737183,
      "learning_rate": 7.717758808788687e-05,
      "loss": 2.236,
      "step": 690
    },
    {
      "epoch": 0.46674445740956827,
      "grad_norm": 0.5777267813682556,
      "learning_rate": 7.684363054531705e-05,
      "loss": 2.2413,
      "step": 700
    },
    {
      "epoch": 0.4734122353725621,
      "grad_norm": 0.574440598487854,
      "learning_rate": 7.650967300274722e-05,
      "loss": 2.2942,
      "step": 710
    },
    {
      "epoch": 0.48008001333555594,
      "grad_norm": 0.5974252223968506,
      "learning_rate": 7.617571546017739e-05,
      "loss": 2.2061,
      "step": 720
    },
    {
      "epoch": 0.4867477912985498,
      "grad_norm": 0.5763422250747681,
      "learning_rate": 7.584175791760756e-05,
      "loss": 2.2713,
      "step": 730
    },
    {
      "epoch": 0.49341556926154356,
      "grad_norm": 0.6114822030067444,
      "learning_rate": 7.550780037503774e-05,
      "loss": 2.2573,
      "step": 740
    },
    {
      "epoch": 0.5000833472245374,
      "grad_norm": 0.5881882905960083,
      "learning_rate": 7.51738428324679e-05,
      "loss": 2.2782,
      "step": 750
    },
    {
      "epoch": 0.5067511251875313,
      "grad_norm": 0.6060173511505127,
      "learning_rate": 7.483988528989808e-05,
      "loss": 2.26,
      "step": 760
    },
    {
      "epoch": 0.5134189031505251,
      "grad_norm": 0.6410916447639465,
      "learning_rate": 7.450592774732825e-05,
      "loss": 2.1848,
      "step": 770
    },
    {
      "epoch": 0.520086681113519,
      "grad_norm": 0.626403272151947,
      "learning_rate": 7.417197020475843e-05,
      "loss": 2.2383,
      "step": 780
    },
    {
      "epoch": 0.5267544590765127,
      "grad_norm": 0.6070374846458435,
      "learning_rate": 7.38380126621886e-05,
      "loss": 2.2592,
      "step": 790
    },
    {
      "epoch": 0.5334222370395065,
      "grad_norm": 0.5682657957077026,
      "learning_rate": 7.350405511961878e-05,
      "loss": 2.2458,
      "step": 800
    },
    {
      "epoch": 0.5400900150025004,
      "grad_norm": 0.5400208234786987,
      "learning_rate": 7.317009757704895e-05,
      "loss": 2.3044,
      "step": 810
    },
    {
      "epoch": 0.5467577929654942,
      "grad_norm": 0.6004924178123474,
      "learning_rate": 7.283614003447912e-05,
      "loss": 2.2341,
      "step": 820
    },
    {
      "epoch": 0.5534255709284881,
      "grad_norm": 0.6856985092163086,
      "learning_rate": 7.25021824919093e-05,
      "loss": 2.2426,
      "step": 830
    },
    {
      "epoch": 0.5600933488914819,
      "grad_norm": 0.5984764695167542,
      "learning_rate": 7.216822494933947e-05,
      "loss": 2.2583,
      "step": 840
    },
    {
      "epoch": 0.5667611268544758,
      "grad_norm": 0.7410269379615784,
      "learning_rate": 7.183426740676964e-05,
      "loss": 2.2239,
      "step": 850
    },
    {
      "epoch": 0.5734289048174696,
      "grad_norm": 0.5749157667160034,
      "learning_rate": 7.150030986419981e-05,
      "loss": 2.2492,
      "step": 860
    },
    {
      "epoch": 0.5800966827804634,
      "grad_norm": 0.584314227104187,
      "learning_rate": 7.116635232162999e-05,
      "loss": 2.26,
      "step": 870
    },
    {
      "epoch": 0.5867644607434572,
      "grad_norm": 0.7645207643508911,
      "learning_rate": 7.083239477906017e-05,
      "loss": 2.1977,
      "step": 880
    },
    {
      "epoch": 0.5934322387064511,
      "grad_norm": 0.700303852558136,
      "learning_rate": 7.049843723649034e-05,
      "loss": 2.2389,
      "step": 890
    },
    {
      "epoch": 0.6001000166694449,
      "grad_norm": 0.657712996006012,
      "learning_rate": 7.016447969392052e-05,
      "loss": 2.2184,
      "step": 900
    },
    {
      "epoch": 0.6067677946324388,
      "grad_norm": 0.6807604432106018,
      "learning_rate": 6.983052215135068e-05,
      "loss": 2.1667,
      "step": 910
    },
    {
      "epoch": 0.6134355725954326,
      "grad_norm": 0.7645849585533142,
      "learning_rate": 6.949656460878086e-05,
      "loss": 2.2562,
      "step": 920
    },
    {
      "epoch": 0.6201033505584264,
      "grad_norm": 0.6480605602264404,
      "learning_rate": 6.916260706621103e-05,
      "loss": 2.2743,
      "step": 930
    },
    {
      "epoch": 0.6267711285214203,
      "grad_norm": 0.6778453588485718,
      "learning_rate": 6.882864952364121e-05,
      "loss": 2.2923,
      "step": 940
    },
    {
      "epoch": 0.633438906484414,
      "grad_norm": 0.7047722339630127,
      "learning_rate": 6.849469198107137e-05,
      "loss": 2.2069,
      "step": 950
    },
    {
      "epoch": 0.6401066844474079,
      "grad_norm": 0.683468222618103,
      "learning_rate": 6.816073443850155e-05,
      "loss": 2.2385,
      "step": 960
    },
    {
      "epoch": 0.6467744624104017,
      "grad_norm": 0.8051849007606506,
      "learning_rate": 6.782677689593173e-05,
      "loss": 2.2467,
      "step": 970
    },
    {
      "epoch": 0.6534422403733956,
      "grad_norm": 0.713133692741394,
      "learning_rate": 6.74928193533619e-05,
      "loss": 2.2039,
      "step": 980
    },
    {
      "epoch": 0.6601100183363894,
      "grad_norm": 0.8087161779403687,
      "learning_rate": 6.715886181079208e-05,
      "loss": 2.1915,
      "step": 990
    },
    {
      "epoch": 0.6667777962993833,
      "grad_norm": 0.7348162531852722,
      "learning_rate": 6.682490426822225e-05,
      "loss": 2.2815,
      "step": 1000
    },
    {
      "epoch": 0.6734455742623771,
      "grad_norm": 0.6532450914382935,
      "learning_rate": 6.649094672565242e-05,
      "loss": 2.1859,
      "step": 1010
    },
    {
      "epoch": 0.680113352225371,
      "grad_norm": 0.7468876242637634,
      "learning_rate": 6.61569891830826e-05,
      "loss": 2.1941,
      "step": 1020
    },
    {
      "epoch": 0.6867811301883647,
      "grad_norm": 0.7252943515777588,
      "learning_rate": 6.582303164051277e-05,
      "loss": 2.2631,
      "step": 1030
    },
    {
      "epoch": 0.6934489081513585,
      "grad_norm": 0.6649948358535767,
      "learning_rate": 6.548907409794293e-05,
      "loss": 2.1844,
      "step": 1040
    },
    {
      "epoch": 0.7001166861143524,
      "grad_norm": 0.7263763546943665,
      "learning_rate": 6.515511655537311e-05,
      "loss": 2.1651,
      "step": 1050
    },
    {
      "epoch": 0.7067844640773462,
      "grad_norm": 0.788378894329071,
      "learning_rate": 6.48211590128033e-05,
      "loss": 2.1625,
      "step": 1060
    },
    {
      "epoch": 0.7134522420403401,
      "grad_norm": 0.7759703397750854,
      "learning_rate": 6.448720147023348e-05,
      "loss": 2.2394,
      "step": 1070
    },
    {
      "epoch": 0.7201200200033339,
      "grad_norm": 0.7436508536338806,
      "learning_rate": 6.415324392766364e-05,
      "loss": 2.2127,
      "step": 1080
    },
    {
      "epoch": 0.7267877979663278,
      "grad_norm": 0.9141121506690979,
      "learning_rate": 6.381928638509381e-05,
      "loss": 2.2228,
      "step": 1090
    },
    {
      "epoch": 0.7334555759293215,
      "grad_norm": 0.7591886520385742,
      "learning_rate": 6.348532884252399e-05,
      "loss": 2.2012,
      "step": 1100
    },
    {
      "epoch": 0.7401233538923154,
      "grad_norm": 0.7387722730636597,
      "learning_rate": 6.315137129995415e-05,
      "loss": 2.1845,
      "step": 1110
    },
    {
      "epoch": 0.7467911318553092,
      "grad_norm": 0.8716336488723755,
      "learning_rate": 6.281741375738433e-05,
      "loss": 2.2741,
      "step": 1120
    },
    {
      "epoch": 0.7534589098183031,
      "grad_norm": 0.7162877917289734,
      "learning_rate": 6.24834562148145e-05,
      "loss": 2.2544,
      "step": 1130
    },
    {
      "epoch": 0.7601266877812969,
      "grad_norm": 0.8466161489486694,
      "learning_rate": 6.214949867224467e-05,
      "loss": 2.1761,
      "step": 1140
    },
    {
      "epoch": 0.7667944657442907,
      "grad_norm": 0.9309144616127014,
      "learning_rate": 6.181554112967486e-05,
      "loss": 2.1813,
      "step": 1150
    },
    {
      "epoch": 0.7734622437072846,
      "grad_norm": 0.8287764191627502,
      "learning_rate": 6.148158358710504e-05,
      "loss": 2.1807,
      "step": 1160
    },
    {
      "epoch": 0.7801300216702783,
      "grad_norm": 0.8537365198135376,
      "learning_rate": 6.11476260445352e-05,
      "loss": 2.1804,
      "step": 1170
    },
    {
      "epoch": 0.7867977996332722,
      "grad_norm": 0.8673957586288452,
      "learning_rate": 6.0813668501965375e-05,
      "loss": 2.1891,
      "step": 1180
    },
    {
      "epoch": 0.793465577596266,
      "grad_norm": 0.9245427250862122,
      "learning_rate": 6.047971095939555e-05,
      "loss": 2.2206,
      "step": 1190
    },
    {
      "epoch": 0.8001333555592599,
      "grad_norm": 0.8201044201850891,
      "learning_rate": 6.014575341682573e-05,
      "loss": 2.1519,
      "step": 1200
    },
    {
      "epoch": 0.8068011335222537,
      "grad_norm": 0.832247793674469,
      "learning_rate": 5.981179587425589e-05,
      "loss": 2.219,
      "step": 1210
    },
    {
      "epoch": 0.8134689114852476,
      "grad_norm": 0.8073312640190125,
      "learning_rate": 5.947783833168607e-05,
      "loss": 2.1488,
      "step": 1220
    },
    {
      "epoch": 0.8201366894482414,
      "grad_norm": 0.8235219120979309,
      "learning_rate": 5.914388078911625e-05,
      "loss": 2.2092,
      "step": 1230
    },
    {
      "epoch": 0.8268044674112353,
      "grad_norm": 0.9382472634315491,
      "learning_rate": 5.880992324654641e-05,
      "loss": 2.1656,
      "step": 1240
    },
    {
      "epoch": 0.833472245374229,
      "grad_norm": 0.7830002903938293,
      "learning_rate": 5.847596570397659e-05,
      "loss": 2.1917,
      "step": 1250
    },
    {
      "epoch": 0.8401400233372228,
      "grad_norm": 0.970643162727356,
      "learning_rate": 5.8142008161406765e-05,
      "loss": 2.2019,
      "step": 1260
    },
    {
      "epoch": 0.8468078013002167,
      "grad_norm": 1.1772961616516113,
      "learning_rate": 5.7808050618836935e-05,
      "loss": 2.1657,
      "step": 1270
    },
    {
      "epoch": 0.8534755792632105,
      "grad_norm": 0.8239079117774963,
      "learning_rate": 5.747409307626711e-05,
      "loss": 2.2099,
      "step": 1280
    },
    {
      "epoch": 0.8601433572262044,
      "grad_norm": 0.9235931038856506,
      "learning_rate": 5.714013553369729e-05,
      "loss": 2.2136,
      "step": 1290
    },
    {
      "epoch": 0.8668111351891982,
      "grad_norm": 1.0257972478866577,
      "learning_rate": 5.680617799112746e-05,
      "loss": 2.1656,
      "step": 1300
    },
    {
      "epoch": 0.8734789131521921,
      "grad_norm": 0.75919508934021,
      "learning_rate": 5.647222044855763e-05,
      "loss": 2.1828,
      "step": 1310
    },
    {
      "epoch": 0.8801466911151858,
      "grad_norm": 1.4252679347991943,
      "learning_rate": 5.613826290598781e-05,
      "loss": 2.1412,
      "step": 1320
    },
    {
      "epoch": 0.8868144690781797,
      "grad_norm": 1.4113682508468628,
      "learning_rate": 5.5804305363417986e-05,
      "loss": 2.1682,
      "step": 1330
    },
    {
      "epoch": 0.8934822470411735,
      "grad_norm": 0.9175013899803162,
      "learning_rate": 5.547034782084815e-05,
      "loss": 2.1426,
      "step": 1340
    },
    {
      "epoch": 0.9001500250041674,
      "grad_norm": 1.1304044723510742,
      "learning_rate": 5.5136390278278325e-05,
      "loss": 2.1497,
      "step": 1350
    },
    {
      "epoch": 0.9068178029671612,
      "grad_norm": 0.9830873012542725,
      "learning_rate": 5.480243273570851e-05,
      "loss": 2.142,
      "step": 1360
    },
    {
      "epoch": 0.913485580930155,
      "grad_norm": 0.9528536200523376,
      "learning_rate": 5.446847519313867e-05,
      "loss": 2.0885,
      "step": 1370
    },
    {
      "epoch": 0.9201533588931489,
      "grad_norm": 0.8071786761283875,
      "learning_rate": 5.413451765056885e-05,
      "loss": 2.1563,
      "step": 1380
    },
    {
      "epoch": 0.9268211368561426,
      "grad_norm": 0.8040830492973328,
      "learning_rate": 5.3800560107999023e-05,
      "loss": 2.2347,
      "step": 1390
    },
    {
      "epoch": 0.9334889148191365,
      "grad_norm": 1.1490782499313354,
      "learning_rate": 5.346660256542919e-05,
      "loss": 2.1682,
      "step": 1400
    },
    {
      "epoch": 0.9401566927821303,
      "grad_norm": 0.8764446973800659,
      "learning_rate": 5.313264502285937e-05,
      "loss": 2.1947,
      "step": 1410
    },
    {
      "epoch": 0.9468244707451242,
      "grad_norm": 0.8926452398300171,
      "learning_rate": 5.2798687480289546e-05,
      "loss": 2.1909,
      "step": 1420
    },
    {
      "epoch": 0.953492248708118,
      "grad_norm": 0.8790086507797241,
      "learning_rate": 5.246472993771971e-05,
      "loss": 2.2252,
      "step": 1430
    },
    {
      "epoch": 0.9601600266711119,
      "grad_norm": 1.0370049476623535,
      "learning_rate": 5.213077239514989e-05,
      "loss": 2.1793,
      "step": 1440
    },
    {
      "epoch": 0.9668278046341057,
      "grad_norm": 1.185847282409668,
      "learning_rate": 5.179681485258007e-05,
      "loss": 2.1088,
      "step": 1450
    },
    {
      "epoch": 0.9734955825970996,
      "grad_norm": 1.1151765584945679,
      "learning_rate": 5.1462857310010244e-05,
      "loss": 2.1441,
      "step": 1460
    },
    {
      "epoch": 0.9801633605600933,
      "grad_norm": 1.1173797845840454,
      "learning_rate": 5.112889976744041e-05,
      "loss": 2.1286,
      "step": 1470
    },
    {
      "epoch": 0.9868311385230871,
      "grad_norm": 0.9688489437103271,
      "learning_rate": 5.079494222487058e-05,
      "loss": 2.1586,
      "step": 1480
    },
    {
      "epoch": 0.993498916486081,
      "grad_norm": 0.9677185416221619,
      "learning_rate": 5.046098468230076e-05,
      "loss": 2.0996,
      "step": 1490
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.7346664667129517,
      "learning_rate": 5.012702713973093e-05,
      "loss": 2.2319,
      "step": 1500
    },
    {
      "epoch": 1.0,
      "eval_loss": 2.159543037414551,
      "eval_runtime": 938.9116,
      "eval_samples_per_second": 6.389,
      "eval_steps_per_second": 3.195,
      "step": 1500
    },
    {
      "epoch": 1.0066677779629938,
      "grad_norm": 1.464077353477478,
      "learning_rate": 4.9793069597161106e-05,
      "loss": 2.1335,
      "step": 1510
    },
    {
      "epoch": 1.0133355559259876,
      "grad_norm": 1.197239637374878,
      "learning_rate": 4.9459112054591275e-05,
      "loss": 2.1229,
      "step": 1520
    },
    {
      "epoch": 1.0200033338889816,
      "grad_norm": 0.9896669983863831,
      "learning_rate": 4.912515451202145e-05,
      "loss": 2.1157,
      "step": 1530
    },
    {
      "epoch": 1.0266711118519753,
      "grad_norm": 0.8790269494056702,
      "learning_rate": 4.879119696945163e-05,
      "loss": 2.155,
      "step": 1540
    },
    {
      "epoch": 1.0333388898149691,
      "grad_norm": 0.8540787696838379,
      "learning_rate": 4.8457239426881804e-05,
      "loss": 2.162,
      "step": 1550
    },
    {
      "epoch": 1.040006667777963,
      "grad_norm": 0.8842875361442566,
      "learning_rate": 4.8123281884311974e-05,
      "loss": 2.1325,
      "step": 1560
    },
    {
      "epoch": 1.046674445740957,
      "grad_norm": 1.0125696659088135,
      "learning_rate": 4.778932434174214e-05,
      "loss": 2.1288,
      "step": 1570
    },
    {
      "epoch": 1.0533422237039507,
      "grad_norm": 1.0740386247634888,
      "learning_rate": 4.745536679917232e-05,
      "loss": 2.1041,
      "step": 1580
    },
    {
      "epoch": 1.0600100016669445,
      "grad_norm": 1.2055261135101318,
      "learning_rate": 4.7121409256602496e-05,
      "loss": 2.1217,
      "step": 1590
    },
    {
      "epoch": 1.0666777796299383,
      "grad_norm": 1.309096097946167,
      "learning_rate": 4.678745171403267e-05,
      "loss": 2.139,
      "step": 1600
    },
    {
      "epoch": 1.073345557592932,
      "grad_norm": 1.300064206123352,
      "learning_rate": 4.645349417146284e-05,
      "loss": 2.1306,
      "step": 1610
    },
    {
      "epoch": 1.080013335555926,
      "grad_norm": 1.052742838859558,
      "learning_rate": 4.611953662889301e-05,
      "loss": 2.1595,
      "step": 1620
    },
    {
      "epoch": 1.0866811135189198,
      "grad_norm": 0.8521118760108948,
      "learning_rate": 4.5785579086323194e-05,
      "loss": 2.1634,
      "step": 1630
    },
    {
      "epoch": 1.0933488914819136,
      "grad_norm": 0.9675314426422119,
      "learning_rate": 4.5451621543753364e-05,
      "loss": 2.0887,
      "step": 1640
    },
    {
      "epoch": 1.1000166694449074,
      "grad_norm": 1.3841708898544312,
      "learning_rate": 4.5117664001183534e-05,
      "loss": 2.1359,
      "step": 1650
    },
    {
      "epoch": 1.1066844474079014,
      "grad_norm": 1.1434028148651123,
      "learning_rate": 4.478370645861371e-05,
      "loss": 2.0878,
      "step": 1660
    },
    {
      "epoch": 1.1133522253708952,
      "grad_norm": 0.9970536231994629,
      "learning_rate": 4.4449748916043886e-05,
      "loss": 2.069,
      "step": 1670
    },
    {
      "epoch": 1.120020003333889,
      "grad_norm": 1.1070455312728882,
      "learning_rate": 4.411579137347406e-05,
      "loss": 2.1138,
      "step": 1680
    },
    {
      "epoch": 1.1266877812968827,
      "grad_norm": 1.2278448343276978,
      "learning_rate": 4.378183383090423e-05,
      "loss": 2.1267,
      "step": 1690
    },
    {
      "epoch": 1.1333555592598765,
      "grad_norm": 1.5363261699676514,
      "learning_rate": 4.34478762883344e-05,
      "loss": 2.0496,
      "step": 1700
    },
    {
      "epoch": 1.1400233372228705,
      "grad_norm": 1.5080773830413818,
      "learning_rate": 4.311391874576458e-05,
      "loss": 2.0961,
      "step": 1710
    },
    {
      "epoch": 1.1466911151858643,
      "grad_norm": 1.6392210721969604,
      "learning_rate": 4.2779961203194754e-05,
      "loss": 2.0714,
      "step": 1720
    },
    {
      "epoch": 1.153358893148858,
      "grad_norm": 1.104085922241211,
      "learning_rate": 4.244600366062493e-05,
      "loss": 2.0662,
      "step": 1730
    },
    {
      "epoch": 1.160026671111852,
      "grad_norm": 1.184728741645813,
      "learning_rate": 4.21120461180551e-05,
      "loss": 2.1203,
      "step": 1740
    },
    {
      "epoch": 1.1666944490748459,
      "grad_norm": 0.8640111088752747,
      "learning_rate": 4.177808857548527e-05,
      "loss": 2.0824,
      "step": 1750
    },
    {
      "epoch": 1.1733622270378397,
      "grad_norm": 1.0031853914260864,
      "learning_rate": 4.1444131032915446e-05,
      "loss": 2.0764,
      "step": 1760
    },
    {
      "epoch": 1.1800300050008334,
      "grad_norm": 1.2499758005142212,
      "learning_rate": 4.111017349034562e-05,
      "loss": 2.1047,
      "step": 1770
    },
    {
      "epoch": 1.1866977829638272,
      "grad_norm": 2.494884967803955,
      "learning_rate": 4.077621594777579e-05,
      "loss": 2.0525,
      "step": 1780
    },
    {
      "epoch": 1.1933655609268212,
      "grad_norm": 2.1737277507781982,
      "learning_rate": 4.044225840520597e-05,
      "loss": 2.0714,
      "step": 1790
    },
    {
      "epoch": 1.200033338889815,
      "grad_norm": 1.0194613933563232,
      "learning_rate": 4.010830086263614e-05,
      "loss": 2.1165,
      "step": 1800
    },
    {
      "epoch": 1.2067011168528088,
      "grad_norm": 1.366461157798767,
      "learning_rate": 3.977434332006632e-05,
      "loss": 2.1106,
      "step": 1810
    },
    {
      "epoch": 1.2133688948158026,
      "grad_norm": 1.1622042655944824,
      "learning_rate": 3.944038577749649e-05,
      "loss": 2.0614,
      "step": 1820
    },
    {
      "epoch": 1.2200366727787966,
      "grad_norm": 1.4053601026535034,
      "learning_rate": 3.910642823492666e-05,
      "loss": 2.0983,
      "step": 1830
    },
    {
      "epoch": 1.2267044507417904,
      "grad_norm": 1.2605410814285278,
      "learning_rate": 3.8772470692356837e-05,
      "loss": 2.1194,
      "step": 1840
    },
    {
      "epoch": 1.2333722287047841,
      "grad_norm": 1.1246225833892822,
      "learning_rate": 3.843851314978701e-05,
      "loss": 2.068,
      "step": 1850
    },
    {
      "epoch": 1.240040006667778,
      "grad_norm": 1.2632023096084595,
      "learning_rate": 3.810455560721719e-05,
      "loss": 2.1044,
      "step": 1860
    },
    {
      "epoch": 1.2467077846307717,
      "grad_norm": 1.5678696632385254,
      "learning_rate": 3.777059806464736e-05,
      "loss": 2.0311,
      "step": 1870
    },
    {
      "epoch": 1.2533755625937657,
      "grad_norm": 1.1345771551132202,
      "learning_rate": 3.743664052207753e-05,
      "loss": 2.0917,
      "step": 1880
    },
    {
      "epoch": 1.2600433405567595,
      "grad_norm": 1.1972626447677612,
      "learning_rate": 3.7102682979507705e-05,
      "loss": 2.0782,
      "step": 1890
    },
    {
      "epoch": 1.2667111185197533,
      "grad_norm": 1.166019320487976,
      "learning_rate": 3.676872543693788e-05,
      "loss": 2.1209,
      "step": 1900
    },
    {
      "epoch": 1.273378896482747,
      "grad_norm": 1.1825485229492188,
      "learning_rate": 3.643476789436806e-05,
      "loss": 2.0872,
      "step": 1910
    },
    {
      "epoch": 1.280046674445741,
      "grad_norm": 1.2397246360778809,
      "learning_rate": 3.610081035179823e-05,
      "loss": 2.125,
      "step": 1920
    },
    {
      "epoch": 1.2867144524087348,
      "grad_norm": 1.6998971700668335,
      "learning_rate": 3.5766852809228396e-05,
      "loss": 2.081,
      "step": 1930
    },
    {
      "epoch": 1.2933822303717286,
      "grad_norm": 1.3073316812515259,
      "learning_rate": 3.543289526665857e-05,
      "loss": 2.0373,
      "step": 1940
    },
    {
      "epoch": 1.3000500083347224,
      "grad_norm": 1.9937657117843628,
      "learning_rate": 3.509893772408875e-05,
      "loss": 2.0215,
      "step": 1950
    },
    {
      "epoch": 1.3067177862977162,
      "grad_norm": 1.1796832084655762,
      "learning_rate": 3.476498018151892e-05,
      "loss": 2.0667,
      "step": 1960
    },
    {
      "epoch": 1.3133855642607102,
      "grad_norm": 1.5597312450408936,
      "learning_rate": 3.4431022638949095e-05,
      "loss": 2.0607,
      "step": 1970
    },
    {
      "epoch": 1.320053342223704,
      "grad_norm": 1.2384213209152222,
      "learning_rate": 3.4097065096379265e-05,
      "loss": 2.0887,
      "step": 1980
    },
    {
      "epoch": 1.3267211201866977,
      "grad_norm": 1.5370475053787231,
      "learning_rate": 3.376310755380944e-05,
      "loss": 2.0088,
      "step": 1990
    },
    {
      "epoch": 1.3333888981496917,
      "grad_norm": 1.117777705192566,
      "learning_rate": 3.342915001123962e-05,
      "loss": 2.1166,
      "step": 2000
    },
    {
      "epoch": 1.3400566761126855,
      "grad_norm": 1.2979565858840942,
      "learning_rate": 3.309519246866979e-05,
      "loss": 2.033,
      "step": 2010
    },
    {
      "epoch": 1.3467244540756793,
      "grad_norm": 1.357151746749878,
      "learning_rate": 3.276123492609996e-05,
      "loss": 2.0598,
      "step": 2020
    },
    {
      "epoch": 1.353392232038673,
      "grad_norm": 1.163809061050415,
      "learning_rate": 3.242727738353013e-05,
      "loss": 2.0038,
      "step": 2030
    },
    {
      "epoch": 1.3600600100016669,
      "grad_norm": 1.2176191806793213,
      "learning_rate": 3.2093319840960316e-05,
      "loss": 2.0592,
      "step": 2040
    },
    {
      "epoch": 1.3667277879646607,
      "grad_norm": 1.3788188695907593,
      "learning_rate": 3.1759362298390485e-05,
      "loss": 2.0049,
      "step": 2050
    },
    {
      "epoch": 1.3733955659276547,
      "grad_norm": 1.6295266151428223,
      "learning_rate": 3.1425404755820655e-05,
      "loss": 2.0237,
      "step": 2060
    },
    {
      "epoch": 1.3800633438906484,
      "grad_norm": 1.2646145820617676,
      "learning_rate": 3.109144721325083e-05,
      "loss": 2.1244,
      "step": 2070
    },
    {
      "epoch": 1.3867311218536422,
      "grad_norm": 1.5153913497924805,
      "learning_rate": 3.075748967068101e-05,
      "loss": 2.0762,
      "step": 2080
    },
    {
      "epoch": 1.3933988998166362,
      "grad_norm": 1.8348913192749023,
      "learning_rate": 3.0423532128111177e-05,
      "loss": 1.9828,
      "step": 2090
    },
    {
      "epoch": 1.40006667777963,
      "grad_norm": 1.3103893995285034,
      "learning_rate": 3.0089574585541353e-05,
      "loss": 2.1061,
      "step": 2100
    },
    {
      "epoch": 1.4067344557426238,
      "grad_norm": 0.9201390743255615,
      "learning_rate": 2.9755617042971523e-05,
      "loss": 2.1177,
      "step": 2110
    },
    {
      "epoch": 1.4134022337056176,
      "grad_norm": 1.5533711910247803,
      "learning_rate": 2.9421659500401703e-05,
      "loss": 2.0457,
      "step": 2120
    },
    {
      "epoch": 1.4200700116686114,
      "grad_norm": 1.335548758506775,
      "learning_rate": 2.9087701957831872e-05,
      "loss": 2.1467,
      "step": 2130
    },
    {
      "epoch": 1.4267377896316051,
      "grad_norm": 1.5822408199310303,
      "learning_rate": 2.8753744415262045e-05,
      "loss": 2.045,
      "step": 2140
    },
    {
      "epoch": 1.4334055675945991,
      "grad_norm": 1.480028510093689,
      "learning_rate": 2.841978687269222e-05,
      "loss": 2.0841,
      "step": 2150
    },
    {
      "epoch": 1.440073345557593,
      "grad_norm": 1.7150821685791016,
      "learning_rate": 2.8085829330122395e-05,
      "loss": 2.0264,
      "step": 2160
    },
    {
      "epoch": 1.4467411235205867,
      "grad_norm": 1.3385558128356934,
      "learning_rate": 2.775187178755257e-05,
      "loss": 2.1069,
      "step": 2170
    },
    {
      "epoch": 1.4534089014835807,
      "grad_norm": 1.1927381753921509,
      "learning_rate": 2.741791424498274e-05,
      "loss": 2.0528,
      "step": 2180
    },
    {
      "epoch": 1.4600766794465745,
      "grad_norm": 1.4984644651412964,
      "learning_rate": 2.7083956702412913e-05,
      "loss": 2.0224,
      "step": 2190
    },
    {
      "epoch": 1.4667444574095683,
      "grad_norm": 1.3691529035568237,
      "learning_rate": 2.674999915984309e-05,
      "loss": 2.1155,
      "step": 2200
    },
    {
      "epoch": 1.473412235372562,
      "grad_norm": 1.0814392566680908,
      "learning_rate": 2.6416041617273263e-05,
      "loss": 2.092,
      "step": 2210
    },
    {
      "epoch": 1.4800800133355558,
      "grad_norm": 0.9348087906837463,
      "learning_rate": 2.608208407470344e-05,
      "loss": 2.0148,
      "step": 2220
    },
    {
      "epoch": 1.4867477912985498,
      "grad_norm": 1.4463493824005127,
      "learning_rate": 2.5748126532133612e-05,
      "loss": 2.0601,
      "step": 2230
    },
    {
      "epoch": 1.4934155692615436,
      "grad_norm": 1.4662790298461914,
      "learning_rate": 2.541416898956378e-05,
      "loss": 2.0518,
      "step": 2240
    },
    {
      "epoch": 1.5000833472245374,
      "grad_norm": 1.1646685600280762,
      "learning_rate": 2.5080211446993958e-05,
      "loss": 2.1018,
      "step": 2250
    },
    {
      "epoch": 1.5067511251875314,
      "grad_norm": 1.2033945322036743,
      "learning_rate": 2.474625390442413e-05,
      "loss": 2.0215,
      "step": 2260
    },
    {
      "epoch": 1.5134189031505252,
      "grad_norm": 1.177703857421875,
      "learning_rate": 2.4412296361854304e-05,
      "loss": 2.1413,
      "step": 2270
    },
    {
      "epoch": 1.520086681113519,
      "grad_norm": 1.9119768142700195,
      "learning_rate": 2.407833881928448e-05,
      "loss": 2.0665,
      "step": 2280
    },
    {
      "epoch": 1.5267544590765127,
      "grad_norm": 1.487593173980713,
      "learning_rate": 2.374438127671465e-05,
      "loss": 2.064,
      "step": 2290
    },
    {
      "epoch": 1.5334222370395065,
      "grad_norm": 1.1635767221450806,
      "learning_rate": 2.3410423734144826e-05,
      "loss": 2.0683,
      "step": 2300
    },
    {
      "epoch": 1.5400900150025003,
      "grad_norm": 1.465643048286438,
      "learning_rate": 2.3076466191575e-05,
      "loss": 2.1234,
      "step": 2310
    },
    {
      "epoch": 1.546757792965494,
      "grad_norm": 1.248920202255249,
      "learning_rate": 2.2742508649005175e-05,
      "loss": 2.0962,
      "step": 2320
    },
    {
      "epoch": 1.553425570928488,
      "grad_norm": 1.0355405807495117,
      "learning_rate": 2.2408551106435348e-05,
      "loss": 2.1292,
      "step": 2330
    },
    {
      "epoch": 1.5600933488914819,
      "grad_norm": 1.8131937980651855,
      "learning_rate": 2.207459356386552e-05,
      "loss": 2.0822,
      "step": 2340
    },
    {
      "epoch": 1.5667611268544759,
      "grad_norm": 1.2921544313430786,
      "learning_rate": 2.1740636021295694e-05,
      "loss": 2.0183,
      "step": 2350
    },
    {
      "epoch": 1.5734289048174697,
      "grad_norm": 1.8514128923416138,
      "learning_rate": 2.1406678478725867e-05,
      "loss": 2.0392,
      "step": 2360
    },
    {
      "epoch": 1.5800966827804634,
      "grad_norm": 1.3353898525238037,
      "learning_rate": 2.1072720936156043e-05,
      "loss": 2.0077,
      "step": 2370
    },
    {
      "epoch": 1.5867644607434572,
      "grad_norm": 1.5843472480773926,
      "learning_rate": 2.0738763393586213e-05,
      "loss": 2.0287,
      "step": 2380
    },
    {
      "epoch": 1.593432238706451,
      "grad_norm": 2.5951414108276367,
      "learning_rate": 2.040480585101639e-05,
      "loss": 2.0648,
      "step": 2390
    },
    {
      "epoch": 1.6001000166694448,
      "grad_norm": 1.8264151811599731,
      "learning_rate": 2.0070848308446562e-05,
      "loss": 2.0568,
      "step": 2400
    },
    {
      "epoch": 1.6067677946324388,
      "grad_norm": 1.3652822971343994,
      "learning_rate": 1.973689076587674e-05,
      "loss": 1.9824,
      "step": 2410
    },
    {
      "epoch": 1.6134355725954326,
      "grad_norm": 1.7033504247665405,
      "learning_rate": 1.9402933223306908e-05,
      "loss": 1.9993,
      "step": 2420
    },
    {
      "epoch": 1.6201033505584264,
      "grad_norm": 1.4110755920410156,
      "learning_rate": 1.9068975680737084e-05,
      "loss": 2.0668,
      "step": 2430
    },
    {
      "epoch": 1.6267711285214204,
      "grad_norm": 1.0833896398544312,
      "learning_rate": 1.8735018138167257e-05,
      "loss": 2.13,
      "step": 2440
    },
    {
      "epoch": 1.6334389064844141,
      "grad_norm": 1.2303119897842407,
      "learning_rate": 1.840106059559743e-05,
      "loss": 2.0847,
      "step": 2450
    },
    {
      "epoch": 1.640106684447408,
      "grad_norm": 1.5742440223693848,
      "learning_rate": 1.8067103053027607e-05,
      "loss": 2.0756,
      "step": 2460
    },
    {
      "epoch": 1.6467744624104017,
      "grad_norm": 1.4103535413742065,
      "learning_rate": 1.7733145510457776e-05,
      "loss": 2.089,
      "step": 2470
    },
    {
      "epoch": 1.6534422403733955,
      "grad_norm": 1.0479984283447266,
      "learning_rate": 1.7399187967887953e-05,
      "loss": 2.0763,
      "step": 2480
    },
    {
      "epoch": 1.6601100183363893,
      "grad_norm": 1.3043466806411743,
      "learning_rate": 1.7065230425318125e-05,
      "loss": 1.9564,
      "step": 2490
    },
    {
      "epoch": 1.6667777962993833,
      "grad_norm": 1.223266839981079,
      "learning_rate": 1.67312728827483e-05,
      "loss": 2.0736,
      "step": 2500
    },
    {
      "epoch": 1.673445574262377,
      "grad_norm": 1.044224739074707,
      "learning_rate": 1.639731534017847e-05,
      "loss": 2.1009,
      "step": 2510
    },
    {
      "epoch": 1.680113352225371,
      "grad_norm": 1.7569773197174072,
      "learning_rate": 1.6063357797608644e-05,
      "loss": 2.0748,
      "step": 2520
    },
    {
      "epoch": 1.6867811301883648,
      "grad_norm": 2.2831454277038574,
      "learning_rate": 1.572940025503882e-05,
      "loss": 2.0171,
      "step": 2530
    },
    {
      "epoch": 1.6934489081513586,
      "grad_norm": 1.7041871547698975,
      "learning_rate": 1.5395442712468994e-05,
      "loss": 2.0414,
      "step": 2540
    },
    {
      "epoch": 1.7001166861143524,
      "grad_norm": 1.424553632736206,
      "learning_rate": 1.5061485169899168e-05,
      "loss": 2.1155,
      "step": 2550
    },
    {
      "epoch": 1.7067844640773462,
      "grad_norm": 1.330930471420288,
      "learning_rate": 1.472752762732934e-05,
      "loss": 2.0102,
      "step": 2560
    },
    {
      "epoch": 1.71345224204034,
      "grad_norm": 1.492753505706787,
      "learning_rate": 1.4393570084759514e-05,
      "loss": 2.0609,
      "step": 2570
    },
    {
      "epoch": 1.7201200200033337,
      "grad_norm": 1.5490660667419434,
      "learning_rate": 1.4059612542189689e-05,
      "loss": 2.085,
      "step": 2580
    },
    {
      "epoch": 1.7267877979663278,
      "grad_norm": 1.4197748899459839,
      "learning_rate": 1.3725654999619863e-05,
      "loss": 2.1227,
      "step": 2590
    },
    {
      "epoch": 1.7334555759293215,
      "grad_norm": 1.347549557685852,
      "learning_rate": 1.3391697457050035e-05,
      "loss": 2.0242,
      "step": 2600
    },
    {
      "epoch": 1.7401233538923155,
      "grad_norm": 1.2226680517196655,
      "learning_rate": 1.305773991448021e-05,
      "loss": 2.0332,
      "step": 2610
    },
    {
      "epoch": 1.7467911318553093,
      "grad_norm": 1.2072497606277466,
      "learning_rate": 1.2723782371910384e-05,
      "loss": 2.057,
      "step": 2620
    },
    {
      "epoch": 1.753458909818303,
      "grad_norm": 1.2527865171432495,
      "learning_rate": 1.2389824829340557e-05,
      "loss": 2.0413,
      "step": 2630
    },
    {
      "epoch": 1.7601266877812969,
      "grad_norm": 1.1325290203094482,
      "learning_rate": 1.205586728677073e-05,
      "loss": 1.9953,
      "step": 2640
    },
    {
      "epoch": 1.7667944657442907,
      "grad_norm": 1.3336542844772339,
      "learning_rate": 1.1721909744200905e-05,
      "loss": 1.9666,
      "step": 2650
    },
    {
      "epoch": 1.7734622437072844,
      "grad_norm": 1.4047008752822876,
      "learning_rate": 1.1387952201631077e-05,
      "loss": 2.0566,
      "step": 2660
    },
    {
      "epoch": 1.7801300216702782,
      "grad_norm": 1.3520723581314087,
      "learning_rate": 1.1053994659061252e-05,
      "loss": 2.0708,
      "step": 2670
    },
    {
      "epoch": 1.7867977996332722,
      "grad_norm": 1.3892930746078491,
      "learning_rate": 1.0720037116491425e-05,
      "loss": 1.9714,
      "step": 2680
    },
    {
      "epoch": 1.793465577596266,
      "grad_norm": 1.5486881732940674,
      "learning_rate": 1.03860795739216e-05,
      "loss": 1.9909,
      "step": 2690
    },
    {
      "epoch": 1.80013335555926,
      "grad_norm": 1.2617619037628174,
      "learning_rate": 1.0052122031351773e-05,
      "loss": 2.0679,
      "step": 2700
    },
    {
      "epoch": 1.8068011335222538,
      "grad_norm": 1.0261274576187134,
      "learning_rate": 9.718164488781947e-06,
      "loss": 2.1004,
      "step": 2710
    },
    {
      "epoch": 1.8134689114852476,
      "grad_norm": 2.1091420650482178,
      "learning_rate": 9.38420694621212e-06,
      "loss": 2.0405,
      "step": 2720
    },
    {
      "epoch": 1.8201366894482414,
      "grad_norm": 1.7274668216705322,
      "learning_rate": 9.050249403642293e-06,
      "loss": 1.9859,
      "step": 2730
    },
    {
      "epoch": 1.8268044674112351,
      "grad_norm": 1.355010747909546,
      "learning_rate": 8.716291861072466e-06,
      "loss": 2.0161,
      "step": 2740
    },
    {
      "epoch": 1.833472245374229,
      "grad_norm": 1.564488172531128,
      "learning_rate": 8.38233431850264e-06,
      "loss": 2.0473,
      "step": 2750
    },
    {
      "epoch": 1.8401400233372227,
      "grad_norm": 1.95427405834198,
      "learning_rate": 8.048376775932815e-06,
      "loss": 2.0394,
      "step": 2760
    },
    {
      "epoch": 1.8468078013002167,
      "grad_norm": 1.4169808626174927,
      "learning_rate": 7.714419233362988e-06,
      "loss": 2.0743,
      "step": 2770
    },
    {
      "epoch": 1.8534755792632105,
      "grad_norm": 1.7943731546401978,
      "learning_rate": 7.380461690793162e-06,
      "loss": 2.019,
      "step": 2780
    },
    {
      "epoch": 1.8601433572262045,
      "grad_norm": 1.2450991868972778,
      "learning_rate": 7.046504148223335e-06,
      "loss": 1.9447,
      "step": 2790
    },
    {
      "epoch": 1.8668111351891983,
      "grad_norm": 1.2668176889419556,
      "learning_rate": 6.71254660565351e-06,
      "loss": 2.0585,
      "step": 2800
    },
    {
      "epoch": 1.873478913152192,
      "grad_norm": 1.5765976905822754,
      "learning_rate": 6.378589063083683e-06,
      "loss": 2.0832,
      "step": 2810
    },
    {
      "epoch": 1.8801466911151858,
      "grad_norm": 1.2100327014923096,
      "learning_rate": 6.0446315205138565e-06,
      "loss": 2.0786,
      "step": 2820
    },
    {
      "epoch": 1.8868144690781796,
      "grad_norm": 2.020484447479248,
      "learning_rate": 5.71067397794403e-06,
      "loss": 2.0402,
      "step": 2830
    },
    {
      "epoch": 1.8934822470411734,
      "grad_norm": 1.2269434928894043,
      "learning_rate": 5.376716435374204e-06,
      "loss": 2.0132,
      "step": 2840
    },
    {
      "epoch": 1.9001500250041674,
      "grad_norm": 1.65056312084198,
      "learning_rate": 5.042758892804378e-06,
      "loss": 1.9775,
      "step": 2850
    },
    {
      "epoch": 1.9068178029671612,
      "grad_norm": 1.1073445081710815,
      "learning_rate": 4.708801350234552e-06,
      "loss": 2.0469,
      "step": 2860
    },
    {
      "epoch": 1.913485580930155,
      "grad_norm": 0.9993956089019775,
      "learning_rate": 4.3748438076647246e-06,
      "loss": 2.0116,
      "step": 2870
    },
    {
      "epoch": 1.920153358893149,
      "grad_norm": 1.0482655763626099,
      "learning_rate": 4.040886265094898e-06,
      "loss": 2.0076,
      "step": 2880
    },
    {
      "epoch": 1.9268211368561428,
      "grad_norm": 1.2106784582138062,
      "learning_rate": 3.706928722525072e-06,
      "loss": 2.0395,
      "step": 2890
    },
    {
      "epoch": 1.9334889148191365,
      "grad_norm": 1.1296108961105347,
      "learning_rate": 3.3729711799552455e-06,
      "loss": 2.0155,
      "step": 2900
    },
    {
      "epoch": 1.9401566927821303,
      "grad_norm": 1.3978283405303955,
      "learning_rate": 3.0390136373854197e-06,
      "loss": 2.0648,
      "step": 2910
    },
    {
      "epoch": 1.946824470745124,
      "grad_norm": 1.4175904989242554,
      "learning_rate": 2.705056094815593e-06,
      "loss": 1.9262,
      "step": 2920
    },
    {
      "epoch": 1.9534922487081179,
      "grad_norm": 1.1161831617355347,
      "learning_rate": 2.371098552245767e-06,
      "loss": 2.1202,
      "step": 2930
    },
    {
      "epoch": 1.9601600266711119,
      "grad_norm": 1.3139300346374512,
      "learning_rate": 2.0371410096759403e-06,
      "loss": 2.0099,
      "step": 2940
    },
    {
      "epoch": 1.9668278046341057,
      "grad_norm": 1.446398138999939,
      "learning_rate": 1.7031834671061145e-06,
      "loss": 2.044,
      "step": 2950
    },
    {
      "epoch": 1.9734955825970997,
      "grad_norm": 1.4943715333938599,
      "learning_rate": 1.369225924536288e-06,
      "loss": 2.0366,
      "step": 2960
    },
    {
      "epoch": 1.9801633605600935,
      "grad_norm": 1.2609846591949463,
      "learning_rate": 1.0352683819664617e-06,
      "loss": 2.0362,
      "step": 2970
    },
    {
      "epoch": 1.9868311385230872,
      "grad_norm": 1.4290804862976074,
      "learning_rate": 7.013108393966354e-07,
      "loss": 1.9701,
      "step": 2980
    },
    {
      "epoch": 1.993498916486081,
      "grad_norm": 1.0300006866455078,
      "learning_rate": 3.6735329682680893e-07,
      "loss": 1.9905,
      "step": 2990
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.6035137176513672,
      "learning_rate": 3.3395754256982634e-08,
      "loss": 2.0167,
      "step": 3000
    },
    {
      "epoch": 2.0,
      "eval_loss": 2.0527429580688477,
      "eval_runtime": 938.9514,
      "eval_samples_per_second": 6.389,
      "eval_steps_per_second": 3.195,
      "step": 3000
    }
  ],
  "logging_steps": 10,
  "max_steps": 3000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 500,
  "stateful_callbacks": {
    "EarlyStoppingCallback": {
      "args": {
        "early_stopping_patience": 2,
        "early_stopping_threshold": 0.0
      },
      "attributes": {
        "early_stopping_patience_counter": 0
      }
    },
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.0855199598588723e+18,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
