{
  "best_global_step": 3000,
  "best_metric": 1.988416314125061,
  "best_model_checkpoint": "json_outputs_all_data/fine-tune/optuna_output/optuna_trial_5/checkpoint-3000",
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 3000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.006667777962993832,
      "grad_norm": 0.2093830555677414,
      "learning_rate": 0.00012092491765904627,
      "loss": 2.6855,
      "step": 10
    },
    {
      "epoch": 0.013335555925987664,
      "grad_norm": 0.24591512978076935,
      "learning_rate": 0.00012052062171234268,
      "loss": 2.552,
      "step": 20
    },
    {
      "epoch": 0.020003333888981498,
      "grad_norm": 0.23667576909065247,
      "learning_rate": 0.00012011632576563907,
      "loss": 2.4891,
      "step": 30
    },
    {
      "epoch": 0.026671111851975328,
      "grad_norm": 0.22509033977985382,
      "learning_rate": 0.00011971202981893547,
      "loss": 2.4917,
      "step": 40
    },
    {
      "epoch": 0.03333888981496916,
      "grad_norm": 0.2330624908208847,
      "learning_rate": 0.00011930773387223188,
      "loss": 2.4286,
      "step": 50
    },
    {
      "epoch": 0.040006667777962995,
      "grad_norm": 0.2931148111820221,
      "learning_rate": 0.00011890343792552828,
      "loss": 2.411,
      "step": 60
    },
    {
      "epoch": 0.046674445740956826,
      "grad_norm": 0.2575039267539978,
      "learning_rate": 0.00011849914197882467,
      "loss": 2.3842,
      "step": 70
    },
    {
      "epoch": 0.053342223703950656,
      "grad_norm": 0.25062108039855957,
      "learning_rate": 0.00011809484603212108,
      "loss": 2.4355,
      "step": 80
    },
    {
      "epoch": 0.06001000166694449,
      "grad_norm": 0.28254467248916626,
      "learning_rate": 0.00011769055008541749,
      "loss": 2.408,
      "step": 90
    },
    {
      "epoch": 0.06667777962993832,
      "grad_norm": 0.3045024275779724,
      "learning_rate": 0.00011728625413871389,
      "loss": 2.392,
      "step": 100
    },
    {
      "epoch": 0.07334555759293215,
      "grad_norm": 0.30655381083488464,
      "learning_rate": 0.00011688195819201028,
      "loss": 2.3972,
      "step": 110
    },
    {
      "epoch": 0.08001333555592599,
      "grad_norm": 0.2984555959701538,
      "learning_rate": 0.00011647766224530669,
      "loss": 2.4117,
      "step": 120
    },
    {
      "epoch": 0.08668111351891981,
      "grad_norm": 0.2958756983280182,
      "learning_rate": 0.00011607336629860309,
      "loss": 2.3548,
      "step": 130
    },
    {
      "epoch": 0.09334889148191365,
      "grad_norm": 0.2838776111602783,
      "learning_rate": 0.0001156690703518995,
      "loss": 2.3862,
      "step": 140
    },
    {
      "epoch": 0.10001666944490749,
      "grad_norm": 0.321208655834198,
      "learning_rate": 0.00011526477440519589,
      "loss": 2.372,
      "step": 150
    },
    {
      "epoch": 0.10668444740790131,
      "grad_norm": 0.34337809681892395,
      "learning_rate": 0.00011486047845849229,
      "loss": 2.3497,
      "step": 160
    },
    {
      "epoch": 0.11335222537089515,
      "grad_norm": 0.3242899775505066,
      "learning_rate": 0.0001144561825117887,
      "loss": 2.3153,
      "step": 170
    },
    {
      "epoch": 0.12002000333388899,
      "grad_norm": 0.3389619290828705,
      "learning_rate": 0.0001140518865650851,
      "loss": 2.346,
      "step": 180
    },
    {
      "epoch": 0.12668778129688282,
      "grad_norm": 0.33825626969337463,
      "learning_rate": 0.0001136475906183815,
      "loss": 2.3405,
      "step": 190
    },
    {
      "epoch": 0.13335555925987663,
      "grad_norm": 0.3424461781978607,
      "learning_rate": 0.0001132432946716779,
      "loss": 2.3152,
      "step": 200
    },
    {
      "epoch": 0.14002333722287047,
      "grad_norm": 0.3422341048717499,
      "learning_rate": 0.00011283899872497431,
      "loss": 2.3515,
      "step": 210
    },
    {
      "epoch": 0.1466911151858643,
      "grad_norm": 0.3572887182235718,
      "learning_rate": 0.0001124347027782707,
      "loss": 2.3739,
      "step": 220
    },
    {
      "epoch": 0.15335889314885814,
      "grad_norm": 0.38726887106895447,
      "learning_rate": 0.0001120304068315671,
      "loss": 2.3052,
      "step": 230
    },
    {
      "epoch": 0.16002667111185198,
      "grad_norm": 0.3693453371524811,
      "learning_rate": 0.00011162611088486351,
      "loss": 2.3099,
      "step": 240
    },
    {
      "epoch": 0.16669444907484582,
      "grad_norm": 0.36612260341644287,
      "learning_rate": 0.00011122181493815992,
      "loss": 2.3178,
      "step": 250
    },
    {
      "epoch": 0.17336222703783963,
      "grad_norm": 0.4458692669868469,
      "learning_rate": 0.0001108175189914563,
      "loss": 2.3139,
      "step": 260
    },
    {
      "epoch": 0.18003000500083347,
      "grad_norm": 0.37378913164138794,
      "learning_rate": 0.00011041322304475271,
      "loss": 2.3804,
      "step": 270
    },
    {
      "epoch": 0.1866977829638273,
      "grad_norm": 0.4025785028934479,
      "learning_rate": 0.00011000892709804912,
      "loss": 2.4218,
      "step": 280
    },
    {
      "epoch": 0.19336556092682114,
      "grad_norm": 0.38467949628829956,
      "learning_rate": 0.0001096046311513455,
      "loss": 2.329,
      "step": 290
    },
    {
      "epoch": 0.20003333888981498,
      "grad_norm": 0.38975223898887634,
      "learning_rate": 0.00010920033520464191,
      "loss": 2.3379,
      "step": 300
    },
    {
      "epoch": 0.2067011168528088,
      "grad_norm": 0.3979068398475647,
      "learning_rate": 0.00010879603925793832,
      "loss": 2.3068,
      "step": 310
    },
    {
      "epoch": 0.21336889481580262,
      "grad_norm": 0.37467846274375916,
      "learning_rate": 0.00010839174331123473,
      "loss": 2.3382,
      "step": 320
    },
    {
      "epoch": 0.22003667277879646,
      "grad_norm": 0.4280910789966583,
      "learning_rate": 0.00010798744736453112,
      "loss": 2.2907,
      "step": 330
    },
    {
      "epoch": 0.2267044507417903,
      "grad_norm": 0.4198668897151947,
      "learning_rate": 0.00010758315141782753,
      "loss": 2.295,
      "step": 340
    },
    {
      "epoch": 0.23337222870478413,
      "grad_norm": 0.4552993178367615,
      "learning_rate": 0.00010717885547112393,
      "loss": 2.3076,
      "step": 350
    },
    {
      "epoch": 0.24004000666777797,
      "grad_norm": 0.41366705298423767,
      "learning_rate": 0.00010677455952442032,
      "loss": 2.3407,
      "step": 360
    },
    {
      "epoch": 0.24670778463077178,
      "grad_norm": 0.4173290431499481,
      "learning_rate": 0.00010637026357771673,
      "loss": 2.3644,
      "step": 370
    },
    {
      "epoch": 0.25337556259376565,
      "grad_norm": 0.4848112165927887,
      "learning_rate": 0.00010596596763101314,
      "loss": 2.3275,
      "step": 380
    },
    {
      "epoch": 0.2600433405567595,
      "grad_norm": 0.4614536762237549,
      "learning_rate": 0.00010556167168430953,
      "loss": 2.3012,
      "step": 390
    },
    {
      "epoch": 0.26671111851975327,
      "grad_norm": 0.43749067187309265,
      "learning_rate": 0.00010515737573760593,
      "loss": 2.3135,
      "step": 400
    },
    {
      "epoch": 0.2733788964827471,
      "grad_norm": 0.46917015314102173,
      "learning_rate": 0.00010475307979090234,
      "loss": 2.2795,
      "step": 410
    },
    {
      "epoch": 0.28004667444574094,
      "grad_norm": 0.44513434171676636,
      "learning_rate": 0.00010434878384419873,
      "loss": 2.2636,
      "step": 420
    },
    {
      "epoch": 0.2867144524087348,
      "grad_norm": 0.4312760829925537,
      "learning_rate": 0.00010394448789749514,
      "loss": 2.2713,
      "step": 430
    },
    {
      "epoch": 0.2933822303717286,
      "grad_norm": 0.4969259798526764,
      "learning_rate": 0.00010354019195079154,
      "loss": 2.3034,
      "step": 440
    },
    {
      "epoch": 0.30005000833472245,
      "grad_norm": 0.40693768858909607,
      "learning_rate": 0.00010313589600408795,
      "loss": 2.3218,
      "step": 450
    },
    {
      "epoch": 0.3067177862977163,
      "grad_norm": 0.4358574450016022,
      "learning_rate": 0.00010273160005738435,
      "loss": 2.3273,
      "step": 460
    },
    {
      "epoch": 0.3133855642607101,
      "grad_norm": 0.45799198746681213,
      "learning_rate": 0.00010232730411068074,
      "loss": 2.3383,
      "step": 470
    },
    {
      "epoch": 0.32005334222370396,
      "grad_norm": 0.43856632709503174,
      "learning_rate": 0.00010192300816397715,
      "loss": 2.2787,
      "step": 480
    },
    {
      "epoch": 0.3267211201866978,
      "grad_norm": 0.49462321400642395,
      "learning_rate": 0.00010151871221727355,
      "loss": 2.297,
      "step": 490
    },
    {
      "epoch": 0.33338889814969164,
      "grad_norm": 0.6005157232284546,
      "learning_rate": 0.00010111441627056996,
      "loss": 2.3242,
      "step": 500
    },
    {
      "epoch": 0.3400566761126855,
      "grad_norm": 0.5626174211502075,
      "learning_rate": 0.00010071012032386635,
      "loss": 2.3013,
      "step": 510
    },
    {
      "epoch": 0.34672445407567926,
      "grad_norm": 0.48163536190986633,
      "learning_rate": 0.00010030582437716275,
      "loss": 2.2639,
      "step": 520
    },
    {
      "epoch": 0.3533922320386731,
      "grad_norm": 0.5185046195983887,
      "learning_rate": 9.990152843045916e-05,
      "loss": 2.2987,
      "step": 530
    },
    {
      "epoch": 0.36006001000166693,
      "grad_norm": 0.5277281999588013,
      "learning_rate": 9.949723248375555e-05,
      "loss": 2.2359,
      "step": 540
    },
    {
      "epoch": 0.36672778796466077,
      "grad_norm": 0.5158942341804504,
      "learning_rate": 9.909293653705195e-05,
      "loss": 2.2487,
      "step": 550
    },
    {
      "epoch": 0.3733955659276546,
      "grad_norm": 0.491743803024292,
      "learning_rate": 9.868864059034836e-05,
      "loss": 2.1737,
      "step": 560
    },
    {
      "epoch": 0.38006334389064844,
      "grad_norm": 0.5595764517784119,
      "learning_rate": 9.828434464364477e-05,
      "loss": 2.2664,
      "step": 570
    },
    {
      "epoch": 0.3867311218536423,
      "grad_norm": 0.4844224750995636,
      "learning_rate": 9.788004869694116e-05,
      "loss": 2.2819,
      "step": 580
    },
    {
      "epoch": 0.3933988998166361,
      "grad_norm": 0.5312045216560364,
      "learning_rate": 9.747575275023756e-05,
      "loss": 2.275,
      "step": 590
    },
    {
      "epoch": 0.40006667777962995,
      "grad_norm": 0.5187171101570129,
      "learning_rate": 9.707145680353397e-05,
      "loss": 2.2502,
      "step": 600
    },
    {
      "epoch": 0.4067344557426238,
      "grad_norm": 0.528347373008728,
      "learning_rate": 9.666716085683038e-05,
      "loss": 2.2501,
      "step": 610
    },
    {
      "epoch": 0.4134022337056176,
      "grad_norm": 0.5433630347251892,
      "learning_rate": 9.626286491012676e-05,
      "loss": 2.2397,
      "step": 620
    },
    {
      "epoch": 0.4200700116686114,
      "grad_norm": 0.5888743996620178,
      "learning_rate": 9.585856896342317e-05,
      "loss": 2.2576,
      "step": 630
    },
    {
      "epoch": 0.42673778963160525,
      "grad_norm": 0.570504903793335,
      "learning_rate": 9.545427301671958e-05,
      "loss": 2.2567,
      "step": 640
    },
    {
      "epoch": 0.4334055675945991,
      "grad_norm": 0.5422021746635437,
      "learning_rate": 9.504997707001596e-05,
      "loss": 2.2843,
      "step": 650
    },
    {
      "epoch": 0.4400733455575929,
      "grad_norm": 0.5497398972511292,
      "learning_rate": 9.464568112331237e-05,
      "loss": 2.2619,
      "step": 660
    },
    {
      "epoch": 0.44674112352058676,
      "grad_norm": 0.6817405223846436,
      "learning_rate": 9.424138517660878e-05,
      "loss": 2.197,
      "step": 670
    },
    {
      "epoch": 0.4534089014835806,
      "grad_norm": 0.5548792481422424,
      "learning_rate": 9.383708922990518e-05,
      "loss": 2.2584,
      "step": 680
    },
    {
      "epoch": 0.46007667944657443,
      "grad_norm": 0.613423764705658,
      "learning_rate": 9.343279328320157e-05,
      "loss": 2.2113,
      "step": 690
    },
    {
      "epoch": 0.46674445740956827,
      "grad_norm": 0.6065059900283813,
      "learning_rate": 9.302849733649798e-05,
      "loss": 2.2168,
      "step": 700
    },
    {
      "epoch": 0.4734122353725621,
      "grad_norm": 0.6072501540184021,
      "learning_rate": 9.26242013897944e-05,
      "loss": 2.2715,
      "step": 710
    },
    {
      "epoch": 0.48008001333555594,
      "grad_norm": 0.6409238576889038,
      "learning_rate": 9.221990544309079e-05,
      "loss": 2.1812,
      "step": 720
    },
    {
      "epoch": 0.4867477912985498,
      "grad_norm": 0.5736690759658813,
      "learning_rate": 9.181560949638719e-05,
      "loss": 2.2503,
      "step": 730
    },
    {
      "epoch": 0.49341556926154356,
      "grad_norm": 0.6067655682563782,
      "learning_rate": 9.14113135496836e-05,
      "loss": 2.2355,
      "step": 740
    },
    {
      "epoch": 0.5000833472245374,
      "grad_norm": 0.5917676687240601,
      "learning_rate": 9.100701760297999e-05,
      "loss": 2.2487,
      "step": 750
    },
    {
      "epoch": 0.5067511251875313,
      "grad_norm": 0.5832530856132507,
      "learning_rate": 9.060272165627639e-05,
      "loss": 2.2389,
      "step": 760
    },
    {
      "epoch": 0.5134189031505251,
      "grad_norm": 0.8376033306121826,
      "learning_rate": 9.01984257095728e-05,
      "loss": 2.162,
      "step": 770
    },
    {
      "epoch": 0.520086681113519,
      "grad_norm": 0.5895106196403503,
      "learning_rate": 8.979412976286919e-05,
      "loss": 2.2133,
      "step": 780
    },
    {
      "epoch": 0.5267544590765127,
      "grad_norm": 0.5872392058372498,
      "learning_rate": 8.93898338161656e-05,
      "loss": 2.2344,
      "step": 790
    },
    {
      "epoch": 0.5334222370395065,
      "grad_norm": 0.5935719609260559,
      "learning_rate": 8.8985537869462e-05,
      "loss": 2.222,
      "step": 800
    },
    {
      "epoch": 0.5400900150025004,
      "grad_norm": 0.6193391680717468,
      "learning_rate": 8.85812419227584e-05,
      "loss": 2.2826,
      "step": 810
    },
    {
      "epoch": 0.5467577929654942,
      "grad_norm": 0.689495325088501,
      "learning_rate": 8.81769459760548e-05,
      "loss": 2.2012,
      "step": 820
    },
    {
      "epoch": 0.5534255709284881,
      "grad_norm": 0.8561007380485535,
      "learning_rate": 8.77726500293512e-05,
      "loss": 2.2138,
      "step": 830
    },
    {
      "epoch": 0.5600933488914819,
      "grad_norm": 0.6080737709999084,
      "learning_rate": 8.736835408264761e-05,
      "loss": 2.2317,
      "step": 840
    },
    {
      "epoch": 0.5667611268544758,
      "grad_norm": 0.7673758268356323,
      "learning_rate": 8.6964058135944e-05,
      "loss": 2.195,
      "step": 850
    },
    {
      "epoch": 0.5734289048174696,
      "grad_norm": 0.6842584013938904,
      "learning_rate": 8.655976218924042e-05,
      "loss": 2.2212,
      "step": 860
    },
    {
      "epoch": 0.5800966827804634,
      "grad_norm": 0.6673063635826111,
      "learning_rate": 8.615546624253681e-05,
      "loss": 2.2366,
      "step": 870
    },
    {
      "epoch": 0.5867644607434572,
      "grad_norm": 0.7431245446205139,
      "learning_rate": 8.575117029583321e-05,
      "loss": 2.1596,
      "step": 880
    },
    {
      "epoch": 0.5934322387064511,
      "grad_norm": 0.7449849247932434,
      "learning_rate": 8.534687434912962e-05,
      "loss": 2.2107,
      "step": 890
    },
    {
      "epoch": 0.6001000166694449,
      "grad_norm": 0.7017121911048889,
      "learning_rate": 8.494257840242603e-05,
      "loss": 2.1925,
      "step": 900
    },
    {
      "epoch": 0.6067677946324388,
      "grad_norm": 0.7490108609199524,
      "learning_rate": 8.453828245572241e-05,
      "loss": 2.1369,
      "step": 910
    },
    {
      "epoch": 0.6134355725954326,
      "grad_norm": 0.8694158792495728,
      "learning_rate": 8.413398650901882e-05,
      "loss": 2.2242,
      "step": 920
    },
    {
      "epoch": 0.6201033505584264,
      "grad_norm": 0.7030677199363708,
      "learning_rate": 8.372969056231523e-05,
      "loss": 2.2487,
      "step": 930
    },
    {
      "epoch": 0.6267711285214203,
      "grad_norm": 0.691906213760376,
      "learning_rate": 8.332539461561162e-05,
      "loss": 2.2617,
      "step": 940
    },
    {
      "epoch": 0.633438906484414,
      "grad_norm": 0.6974126100540161,
      "learning_rate": 8.292109866890802e-05,
      "loss": 2.1785,
      "step": 950
    },
    {
      "epoch": 0.6401066844474079,
      "grad_norm": 0.7106180191040039,
      "learning_rate": 8.251680272220443e-05,
      "loss": 2.2059,
      "step": 960
    },
    {
      "epoch": 0.6467744624104017,
      "grad_norm": 0.8122174739837646,
      "learning_rate": 8.211250677550084e-05,
      "loss": 2.2145,
      "step": 970
    },
    {
      "epoch": 0.6534422403733956,
      "grad_norm": 0.8419229984283447,
      "learning_rate": 8.170821082879722e-05,
      "loss": 2.1714,
      "step": 980
    },
    {
      "epoch": 0.6601100183363894,
      "grad_norm": 0.8692413568496704,
      "learning_rate": 8.130391488209363e-05,
      "loss": 2.1591,
      "step": 990
    },
    {
      "epoch": 0.6667777962993833,
      "grad_norm": 0.7418803572654724,
      "learning_rate": 8.089961893539004e-05,
      "loss": 2.2496,
      "step": 1000
    },
    {
      "epoch": 0.6734455742623771,
      "grad_norm": 0.7236425280570984,
      "learning_rate": 8.049532298868642e-05,
      "loss": 2.1443,
      "step": 1010
    },
    {
      "epoch": 0.680113352225371,
      "grad_norm": 0.958701491355896,
      "learning_rate": 8.009102704198283e-05,
      "loss": 2.1603,
      "step": 1020
    },
    {
      "epoch": 0.6867811301883647,
      "grad_norm": 0.8157709836959839,
      "learning_rate": 7.968673109527924e-05,
      "loss": 2.2311,
      "step": 1030
    },
    {
      "epoch": 0.6934489081513585,
      "grad_norm": 0.6909734010696411,
      "learning_rate": 7.928243514857564e-05,
      "loss": 2.153,
      "step": 1040
    },
    {
      "epoch": 0.7001166861143524,
      "grad_norm": 0.8140466213226318,
      "learning_rate": 7.887813920187203e-05,
      "loss": 2.1294,
      "step": 1050
    },
    {
      "epoch": 0.7067844640773462,
      "grad_norm": 0.806140124797821,
      "learning_rate": 7.847384325516844e-05,
      "loss": 2.1239,
      "step": 1060
    },
    {
      "epoch": 0.7134522420403401,
      "grad_norm": 1.3616944551467896,
      "learning_rate": 7.806954730846485e-05,
      "loss": 2.2079,
      "step": 1070
    },
    {
      "epoch": 0.7201200200033339,
      "grad_norm": 1.201428771018982,
      "learning_rate": 7.766525136176125e-05,
      "loss": 2.1766,
      "step": 1080
    },
    {
      "epoch": 0.7267877979663278,
      "grad_norm": 1.6733893156051636,
      "learning_rate": 7.726095541505764e-05,
      "loss": 2.1907,
      "step": 1090
    },
    {
      "epoch": 0.7334555759293215,
      "grad_norm": 0.9366385340690613,
      "learning_rate": 7.685665946835405e-05,
      "loss": 2.1687,
      "step": 1100
    },
    {
      "epoch": 0.7401233538923154,
      "grad_norm": 0.8652706742286682,
      "learning_rate": 7.645236352165045e-05,
      "loss": 2.1441,
      "step": 1110
    },
    {
      "epoch": 0.7467911318553092,
      "grad_norm": 0.82449871301651,
      "learning_rate": 7.604806757494685e-05,
      "loss": 2.2382,
      "step": 1120
    },
    {
      "epoch": 0.7534589098183031,
      "grad_norm": 0.9118805527687073,
      "learning_rate": 7.564377162824326e-05,
      "loss": 2.2128,
      "step": 1130
    },
    {
      "epoch": 0.7601266877812969,
      "grad_norm": 0.8517806529998779,
      "learning_rate": 7.523947568153965e-05,
      "loss": 2.1369,
      "step": 1140
    },
    {
      "epoch": 0.7667944657442907,
      "grad_norm": 1.1298155784606934,
      "learning_rate": 7.483517973483606e-05,
      "loss": 2.1392,
      "step": 1150
    },
    {
      "epoch": 0.7734622437072846,
      "grad_norm": 0.7850521802902222,
      "learning_rate": 7.443088378813246e-05,
      "loss": 2.1457,
      "step": 1160
    },
    {
      "epoch": 0.7801300216702783,
      "grad_norm": 0.8363320231437683,
      "learning_rate": 7.402658784142885e-05,
      "loss": 2.1394,
      "step": 1170
    },
    {
      "epoch": 0.7867977996332722,
      "grad_norm": 0.8720024228096008,
      "learning_rate": 7.362229189472526e-05,
      "loss": 2.1496,
      "step": 1180
    },
    {
      "epoch": 0.793465577596266,
      "grad_norm": 1.1643813848495483,
      "learning_rate": 7.321799594802167e-05,
      "loss": 2.1833,
      "step": 1190
    },
    {
      "epoch": 0.8001333555592599,
      "grad_norm": 0.8326148390769958,
      "learning_rate": 7.281370000131807e-05,
      "loss": 2.1104,
      "step": 1200
    },
    {
      "epoch": 0.8068011335222537,
      "grad_norm": 0.9552492499351501,
      "learning_rate": 7.240940405461446e-05,
      "loss": 2.182,
      "step": 1210
    },
    {
      "epoch": 0.8134689114852476,
      "grad_norm": 0.8139918446540833,
      "learning_rate": 7.200510810791087e-05,
      "loss": 2.1013,
      "step": 1220
    },
    {
      "epoch": 0.8201366894482414,
      "grad_norm": 0.895607054233551,
      "learning_rate": 7.160081216120727e-05,
      "loss": 2.176,
      "step": 1230
    },
    {
      "epoch": 0.8268044674112353,
      "grad_norm": 0.843768835067749,
      "learning_rate": 7.119651621450367e-05,
      "loss": 2.1196,
      "step": 1240
    },
    {
      "epoch": 0.833472245374229,
      "grad_norm": 0.9778002500534058,
      "learning_rate": 7.079222026780008e-05,
      "loss": 2.154,
      "step": 1250
    },
    {
      "epoch": 0.8401400233372228,
      "grad_norm": 1.4809037446975708,
      "learning_rate": 7.038792432109649e-05,
      "loss": 2.1645,
      "step": 1260
    },
    {
      "epoch": 0.8468078013002167,
      "grad_norm": 1.029752254486084,
      "learning_rate": 6.998362837439287e-05,
      "loss": 2.1198,
      "step": 1270
    },
    {
      "epoch": 0.8534755792632105,
      "grad_norm": 1.410400390625,
      "learning_rate": 6.957933242768928e-05,
      "loss": 2.1699,
      "step": 1280
    },
    {
      "epoch": 0.8601433572262044,
      "grad_norm": 1.0158380270004272,
      "learning_rate": 6.917503648098569e-05,
      "loss": 2.1717,
      "step": 1290
    },
    {
      "epoch": 0.8668111351891982,
      "grad_norm": 1.0208073854446411,
      "learning_rate": 6.877074053428207e-05,
      "loss": 2.1235,
      "step": 1300
    },
    {
      "epoch": 0.8734789131521921,
      "grad_norm": 0.85087651014328,
      "learning_rate": 6.836644458757848e-05,
      "loss": 2.1366,
      "step": 1310
    },
    {
      "epoch": 0.8801466911151858,
      "grad_norm": 1.902587890625,
      "learning_rate": 6.796214864087489e-05,
      "loss": 2.0942,
      "step": 1320
    },
    {
      "epoch": 0.8868144690781797,
      "grad_norm": 0.9681825041770935,
      "learning_rate": 6.75578526941713e-05,
      "loss": 2.1202,
      "step": 1330
    },
    {
      "epoch": 0.8934822470411735,
      "grad_norm": 0.8394220471382141,
      "learning_rate": 6.715355674746768e-05,
      "loss": 2.099,
      "step": 1340
    },
    {
      "epoch": 0.9001500250041674,
      "grad_norm": 1.0879976749420166,
      "learning_rate": 6.674926080076409e-05,
      "loss": 2.0977,
      "step": 1350
    },
    {
      "epoch": 0.9068178029671612,
      "grad_norm": 1.2068560123443604,
      "learning_rate": 6.63449648540605e-05,
      "loss": 2.1015,
      "step": 1360
    },
    {
      "epoch": 0.913485580930155,
      "grad_norm": 0.8754539489746094,
      "learning_rate": 6.59406689073569e-05,
      "loss": 2.0355,
      "step": 1370
    },
    {
      "epoch": 0.9201533588931489,
      "grad_norm": 1.0989328622817993,
      "learning_rate": 6.553637296065329e-05,
      "loss": 2.1175,
      "step": 1380
    },
    {
      "epoch": 0.9268211368561426,
      "grad_norm": 1.0421534776687622,
      "learning_rate": 6.51320770139497e-05,
      "loss": 2.1924,
      "step": 1390
    },
    {
      "epoch": 0.9334889148191365,
      "grad_norm": 1.1527001857757568,
      "learning_rate": 6.47277810672461e-05,
      "loss": 2.1221,
      "step": 1400
    },
    {
      "epoch": 0.9401566927821303,
      "grad_norm": 0.9124547243118286,
      "learning_rate": 6.432348512054249e-05,
      "loss": 2.1468,
      "step": 1410
    },
    {
      "epoch": 0.9468244707451242,
      "grad_norm": 1.0309356451034546,
      "learning_rate": 6.39191891738389e-05,
      "loss": 2.1368,
      "step": 1420
    },
    {
      "epoch": 0.953492248708118,
      "grad_norm": 0.8514421582221985,
      "learning_rate": 6.35148932271353e-05,
      "loss": 2.1875,
      "step": 1430
    },
    {
      "epoch": 0.9601600266711119,
      "grad_norm": 1.3703376054763794,
      "learning_rate": 6.311059728043171e-05,
      "loss": 2.1298,
      "step": 1440
    },
    {
      "epoch": 0.9668278046341057,
      "grad_norm": 1.4470198154449463,
      "learning_rate": 6.27063013337281e-05,
      "loss": 2.0643,
      "step": 1450
    },
    {
      "epoch": 0.9734955825970996,
      "grad_norm": 1.3656175136566162,
      "learning_rate": 6.230200538702451e-05,
      "loss": 2.082,
      "step": 1460
    },
    {
      "epoch": 0.9801633605600933,
      "grad_norm": 1.792046070098877,
      "learning_rate": 6.189770944032091e-05,
      "loss": 2.0825,
      "step": 1470
    },
    {
      "epoch": 0.9868311385230871,
      "grad_norm": 1.2632006406784058,
      "learning_rate": 6.14934134936173e-05,
      "loss": 2.1135,
      "step": 1480
    },
    {
      "epoch": 0.993498916486081,
      "grad_norm": 0.9737984538078308,
      "learning_rate": 6.108911754691371e-05,
      "loss": 2.0422,
      "step": 1490
    },
    {
      "epoch": 1.0,
      "grad_norm": 2.4618513584136963,
      "learning_rate": 6.068482160021011e-05,
      "loss": 2.1775,
      "step": 1500
    },
    {
      "epoch": 1.0,
      "eval_loss": 2.1095235347747803,
      "eval_runtime": 937.7539,
      "eval_samples_per_second": 6.397,
      "eval_steps_per_second": 3.199,
      "step": 1500
    },
    {
      "epoch": 1.0066677779629938,
      "grad_norm": 1.4909881353378296,
      "learning_rate": 6.0280525653506514e-05,
      "loss": 2.0778,
      "step": 1510
    },
    {
      "epoch": 1.0133355559259876,
      "grad_norm": 1.3106142282485962,
      "learning_rate": 5.9876229706802916e-05,
      "loss": 2.069,
      "step": 1520
    },
    {
      "epoch": 1.0200033338889816,
      "grad_norm": 1.2530401945114136,
      "learning_rate": 5.947193376009932e-05,
      "loss": 2.0692,
      "step": 1530
    },
    {
      "epoch": 1.0266711118519753,
      "grad_norm": 0.895697295665741,
      "learning_rate": 5.906763781339572e-05,
      "loss": 2.1069,
      "step": 1540
    },
    {
      "epoch": 1.0333388898149691,
      "grad_norm": 0.8684064149856567,
      "learning_rate": 5.8663341866692125e-05,
      "loss": 2.1186,
      "step": 1550
    },
    {
      "epoch": 1.040006667777963,
      "grad_norm": 0.928634762763977,
      "learning_rate": 5.825904591998853e-05,
      "loss": 2.0808,
      "step": 1560
    },
    {
      "epoch": 1.046674445740957,
      "grad_norm": 0.8906809091567993,
      "learning_rate": 5.785474997328492e-05,
      "loss": 2.0822,
      "step": 1570
    },
    {
      "epoch": 1.0533422237039507,
      "grad_norm": 1.4174622297286987,
      "learning_rate": 5.7450454026581326e-05,
      "loss": 2.0391,
      "step": 1580
    },
    {
      "epoch": 1.0600100016669445,
      "grad_norm": 1.6069215536117554,
      "learning_rate": 5.704615807987773e-05,
      "loss": 2.0539,
      "step": 1590
    },
    {
      "epoch": 1.0666777796299383,
      "grad_norm": 1.1896562576293945,
      "learning_rate": 5.664186213317413e-05,
      "loss": 2.0988,
      "step": 1600
    },
    {
      "epoch": 1.073345557592932,
      "grad_norm": 1.435322642326355,
      "learning_rate": 5.6237566186470534e-05,
      "loss": 2.0702,
      "step": 1610
    },
    {
      "epoch": 1.080013335555926,
      "grad_norm": 1.0728038549423218,
      "learning_rate": 5.583327023976693e-05,
      "loss": 2.1112,
      "step": 1620
    },
    {
      "epoch": 1.0866811135189198,
      "grad_norm": 0.8125749230384827,
      "learning_rate": 5.542897429306334e-05,
      "loss": 2.1119,
      "step": 1630
    },
    {
      "epoch": 1.0933488914819136,
      "grad_norm": 1.3628554344177246,
      "learning_rate": 5.5024678346359736e-05,
      "loss": 2.025,
      "step": 1640
    },
    {
      "epoch": 1.1000166694449074,
      "grad_norm": 1.2110050916671753,
      "learning_rate": 5.462038239965614e-05,
      "loss": 2.0804,
      "step": 1650
    },
    {
      "epoch": 1.1066844474079014,
      "grad_norm": 1.051485300064087,
      "learning_rate": 5.421608645295254e-05,
      "loss": 2.0311,
      "step": 1660
    },
    {
      "epoch": 1.1133522253708952,
      "grad_norm": 1.369602918624878,
      "learning_rate": 5.381179050624894e-05,
      "loss": 2.0112,
      "step": 1670
    },
    {
      "epoch": 1.120020003333889,
      "grad_norm": 1.349944829940796,
      "learning_rate": 5.340749455954535e-05,
      "loss": 2.0569,
      "step": 1680
    },
    {
      "epoch": 1.1266877812968827,
      "grad_norm": 1.360392451286316,
      "learning_rate": 5.300319861284174e-05,
      "loss": 2.072,
      "step": 1690
    },
    {
      "epoch": 1.1333555592598765,
      "grad_norm": 1.177550196647644,
      "learning_rate": 5.2598902666138146e-05,
      "loss": 1.9874,
      "step": 1700
    },
    {
      "epoch": 1.1400233372228705,
      "grad_norm": 1.1898560523986816,
      "learning_rate": 5.219460671943455e-05,
      "loss": 2.0425,
      "step": 1710
    },
    {
      "epoch": 1.1466911151858643,
      "grad_norm": 1.4103158712387085,
      "learning_rate": 5.179031077273095e-05,
      "loss": 2.0195,
      "step": 1720
    },
    {
      "epoch": 1.153358893148858,
      "grad_norm": 1.7392867803573608,
      "learning_rate": 5.1386014826027354e-05,
      "loss": 2.0003,
      "step": 1730
    },
    {
      "epoch": 1.160026671111852,
      "grad_norm": 2.1199779510498047,
      "learning_rate": 5.098171887932376e-05,
      "loss": 2.0693,
      "step": 1740
    },
    {
      "epoch": 1.1666944490748459,
      "grad_norm": 0.8278370499610901,
      "learning_rate": 5.057742293262015e-05,
      "loss": 2.0143,
      "step": 1750
    },
    {
      "epoch": 1.1733622270378397,
      "grad_norm": 1.0494632720947266,
      "learning_rate": 5.0173126985916556e-05,
      "loss": 2.0113,
      "step": 1760
    },
    {
      "epoch": 1.1800300050008334,
      "grad_norm": 1.415781021118164,
      "learning_rate": 4.976883103921296e-05,
      "loss": 2.0506,
      "step": 1770
    },
    {
      "epoch": 1.1866977829638272,
      "grad_norm": 2.582308292388916,
      "learning_rate": 4.9364535092509354e-05,
      "loss": 1.9866,
      "step": 1780
    },
    {
      "epoch": 1.1933655609268212,
      "grad_norm": 1.4069994688034058,
      "learning_rate": 4.8960239145805764e-05,
      "loss": 2.0088,
      "step": 1790
    },
    {
      "epoch": 1.200033338889815,
      "grad_norm": 1.0042204856872559,
      "learning_rate": 4.855594319910216e-05,
      "loss": 2.0563,
      "step": 1800
    },
    {
      "epoch": 1.2067011168528088,
      "grad_norm": 1.0636626482009888,
      "learning_rate": 4.815164725239857e-05,
      "loss": 2.0595,
      "step": 1810
    },
    {
      "epoch": 1.2133688948158026,
      "grad_norm": 0.9842692613601685,
      "learning_rate": 4.7747351305694965e-05,
      "loss": 1.9977,
      "step": 1820
    },
    {
      "epoch": 1.2200366727787966,
      "grad_norm": 1.3448387384414673,
      "learning_rate": 4.734305535899137e-05,
      "loss": 2.0394,
      "step": 1830
    },
    {
      "epoch": 1.2267044507417904,
      "grad_norm": 1.1182204484939575,
      "learning_rate": 4.693875941228777e-05,
      "loss": 2.055,
      "step": 1840
    },
    {
      "epoch": 1.2333722287047841,
      "grad_norm": 1.0453943014144897,
      "learning_rate": 4.653446346558417e-05,
      "loss": 2.0156,
      "step": 1850
    },
    {
      "epoch": 1.240040006667778,
      "grad_norm": 1.8166722059249878,
      "learning_rate": 4.6130167518880576e-05,
      "loss": 2.0466,
      "step": 1860
    },
    {
      "epoch": 1.2467077846307717,
      "grad_norm": 1.2169783115386963,
      "learning_rate": 4.572587157217697e-05,
      "loss": 1.9786,
      "step": 1870
    },
    {
      "epoch": 1.2533755625937657,
      "grad_norm": 1.2940819263458252,
      "learning_rate": 4.5321575625473375e-05,
      "loss": 2.0433,
      "step": 1880
    },
    {
      "epoch": 1.2600433405567595,
      "grad_norm": 1.148783564567566,
      "learning_rate": 4.491727967876978e-05,
      "loss": 2.0245,
      "step": 1890
    },
    {
      "epoch": 1.2667111185197533,
      "grad_norm": 1.2996083498001099,
      "learning_rate": 4.451298373206618e-05,
      "loss": 2.0713,
      "step": 1900
    },
    {
      "epoch": 1.273378896482747,
      "grad_norm": 1.192619800567627,
      "learning_rate": 4.4108687785362584e-05,
      "loss": 2.0386,
      "step": 1910
    },
    {
      "epoch": 1.280046674445741,
      "grad_norm": 1.2497912645339966,
      "learning_rate": 4.3704391838658986e-05,
      "loss": 2.0737,
      "step": 1920
    },
    {
      "epoch": 1.2867144524087348,
      "grad_norm": 1.4943749904632568,
      "learning_rate": 4.330009589195538e-05,
      "loss": 2.0278,
      "step": 1930
    },
    {
      "epoch": 1.2933822303717286,
      "grad_norm": 1.6610183715820312,
      "learning_rate": 4.289579994525179e-05,
      "loss": 1.9697,
      "step": 1940
    },
    {
      "epoch": 1.3000500083347224,
      "grad_norm": 1.6698343753814697,
      "learning_rate": 4.249150399854819e-05,
      "loss": 1.9621,
      "step": 1950
    },
    {
      "epoch": 1.3067177862977162,
      "grad_norm": 1.3778520822525024,
      "learning_rate": 4.2087208051844584e-05,
      "loss": 2.0063,
      "step": 1960
    },
    {
      "epoch": 1.3133855642607102,
      "grad_norm": 1.8421181440353394,
      "learning_rate": 4.168291210514099e-05,
      "loss": 1.9977,
      "step": 1970
    },
    {
      "epoch": 1.320053342223704,
      "grad_norm": 1.2913455963134766,
      "learning_rate": 4.127861615843739e-05,
      "loss": 2.0319,
      "step": 1980
    },
    {
      "epoch": 1.3267211201866977,
      "grad_norm": 1.2786320447921753,
      "learning_rate": 4.08743202117338e-05,
      "loss": 1.9375,
      "step": 1990
    },
    {
      "epoch": 1.3333888981496917,
      "grad_norm": 1.0734487771987915,
      "learning_rate": 4.0470024265030195e-05,
      "loss": 2.0616,
      "step": 2000
    },
    {
      "epoch": 1.3400566761126855,
      "grad_norm": 1.7379059791564941,
      "learning_rate": 4.00657283183266e-05,
      "loss": 1.9722,
      "step": 2010
    },
    {
      "epoch": 1.3467244540756793,
      "grad_norm": 1.6002721786499023,
      "learning_rate": 3.9661432371623e-05,
      "loss": 1.9875,
      "step": 2020
    },
    {
      "epoch": 1.353392232038673,
      "grad_norm": 1.2421293258666992,
      "learning_rate": 3.92571364249194e-05,
      "loss": 1.9435,
      "step": 2030
    },
    {
      "epoch": 1.3600600100016669,
      "grad_norm": 1.224599003791809,
      "learning_rate": 3.8852840478215806e-05,
      "loss": 2.0013,
      "step": 2040
    },
    {
      "epoch": 1.3667277879646607,
      "grad_norm": 1.4455770254135132,
      "learning_rate": 3.84485445315122e-05,
      "loss": 1.9246,
      "step": 2050
    },
    {
      "epoch": 1.3733955659276547,
      "grad_norm": 1.490064263343811,
      "learning_rate": 3.8044248584808605e-05,
      "loss": 1.9652,
      "step": 2060
    },
    {
      "epoch": 1.3800633438906484,
      "grad_norm": 1.395532488822937,
      "learning_rate": 3.763995263810501e-05,
      "loss": 2.0633,
      "step": 2070
    },
    {
      "epoch": 1.3867311218536422,
      "grad_norm": 1.5810506343841553,
      "learning_rate": 3.723565669140141e-05,
      "loss": 2.0212,
      "step": 2080
    },
    {
      "epoch": 1.3933988998166362,
      "grad_norm": 1.8590389490127563,
      "learning_rate": 3.6831360744697806e-05,
      "loss": 1.9184,
      "step": 2090
    },
    {
      "epoch": 1.40006667777963,
      "grad_norm": 1.1984410285949707,
      "learning_rate": 3.6427064797994216e-05,
      "loss": 2.0608,
      "step": 2100
    },
    {
      "epoch": 1.4067344557426238,
      "grad_norm": 0.8782965540885925,
      "learning_rate": 3.602276885129061e-05,
      "loss": 2.0626,
      "step": 2110
    },
    {
      "epoch": 1.4134022337056176,
      "grad_norm": 1.6682324409484863,
      "learning_rate": 3.561847290458702e-05,
      "loss": 1.9742,
      "step": 2120
    },
    {
      "epoch": 1.4200700116686114,
      "grad_norm": 1.058713674545288,
      "learning_rate": 3.521417695788342e-05,
      "loss": 2.0897,
      "step": 2130
    },
    {
      "epoch": 1.4267377896316051,
      "grad_norm": 1.5796010494232178,
      "learning_rate": 3.480988101117981e-05,
      "loss": 1.9738,
      "step": 2140
    },
    {
      "epoch": 1.4334055675945991,
      "grad_norm": 1.8124924898147583,
      "learning_rate": 3.440558506447622e-05,
      "loss": 2.0284,
      "step": 2150
    },
    {
      "epoch": 1.440073345557593,
      "grad_norm": 1.945090651512146,
      "learning_rate": 3.400128911777262e-05,
      "loss": 1.966,
      "step": 2160
    },
    {
      "epoch": 1.4467411235205867,
      "grad_norm": 1.411939263343811,
      "learning_rate": 3.359699317106903e-05,
      "loss": 2.053,
      "step": 2170
    },
    {
      "epoch": 1.4534089014835807,
      "grad_norm": 1.2812751531600952,
      "learning_rate": 3.3192697224365424e-05,
      "loss": 1.9875,
      "step": 2180
    },
    {
      "epoch": 1.4600766794465745,
      "grad_norm": 1.904516339302063,
      "learning_rate": 3.278840127766183e-05,
      "loss": 1.9596,
      "step": 2190
    },
    {
      "epoch": 1.4667444574095683,
      "grad_norm": 1.5729588270187378,
      "learning_rate": 3.238410533095823e-05,
      "loss": 2.0648,
      "step": 2200
    },
    {
      "epoch": 1.473412235372562,
      "grad_norm": 1.2120248079299927,
      "learning_rate": 3.197980938425463e-05,
      "loss": 2.0323,
      "step": 2210
    },
    {
      "epoch": 1.4800800133355558,
      "grad_norm": 1.0305837392807007,
      "learning_rate": 3.1575513437551035e-05,
      "loss": 1.957,
      "step": 2220
    },
    {
      "epoch": 1.4867477912985498,
      "grad_norm": 1.2716394662857056,
      "learning_rate": 3.117121749084743e-05,
      "loss": 2.0044,
      "step": 2230
    },
    {
      "epoch": 1.4934155692615436,
      "grad_norm": 1.2052922248840332,
      "learning_rate": 3.0766921544143834e-05,
      "loss": 1.9856,
      "step": 2240
    },
    {
      "epoch": 1.5000833472245374,
      "grad_norm": 1.506166934967041,
      "learning_rate": 3.036262559744024e-05,
      "loss": 2.045,
      "step": 2250
    },
    {
      "epoch": 1.5067511251875314,
      "grad_norm": 1.37185537815094,
      "learning_rate": 2.995832965073664e-05,
      "loss": 1.9545,
      "step": 2260
    },
    {
      "epoch": 1.5134189031505252,
      "grad_norm": 1.2156124114990234,
      "learning_rate": 2.955403370403304e-05,
      "loss": 2.0875,
      "step": 2270
    },
    {
      "epoch": 1.520086681113519,
      "grad_norm": 1.3551335334777832,
      "learning_rate": 2.9149737757329442e-05,
      "loss": 2.0049,
      "step": 2280
    },
    {
      "epoch": 1.5267544590765127,
      "grad_norm": 1.677872657775879,
      "learning_rate": 2.874544181062584e-05,
      "loss": 2.0058,
      "step": 2290
    },
    {
      "epoch": 1.5334222370395065,
      "grad_norm": 1.0846378803253174,
      "learning_rate": 2.8341145863922244e-05,
      "loss": 2.0148,
      "step": 2300
    },
    {
      "epoch": 1.5400900150025003,
      "grad_norm": 1.3677388429641724,
      "learning_rate": 2.7936849917218647e-05,
      "loss": 2.0748,
      "step": 2310
    },
    {
      "epoch": 1.546757792965494,
      "grad_norm": 0.9746821522712708,
      "learning_rate": 2.753255397051505e-05,
      "loss": 2.0412,
      "step": 2320
    },
    {
      "epoch": 1.553425570928488,
      "grad_norm": 1.2308521270751953,
      "learning_rate": 2.7128258023811452e-05,
      "loss": 2.0728,
      "step": 2330
    },
    {
      "epoch": 1.5600933488914819,
      "grad_norm": 1.9468672275543213,
      "learning_rate": 2.672396207710785e-05,
      "loss": 2.0199,
      "step": 2340
    },
    {
      "epoch": 1.5667611268544759,
      "grad_norm": 1.2598875761032104,
      "learning_rate": 2.6319666130404254e-05,
      "loss": 1.9441,
      "step": 2350
    },
    {
      "epoch": 1.5734289048174697,
      "grad_norm": 2.0374765396118164,
      "learning_rate": 2.5915370183700657e-05,
      "loss": 1.9689,
      "step": 2360
    },
    {
      "epoch": 1.5800966827804634,
      "grad_norm": 1.2825309038162231,
      "learning_rate": 2.5511074236997057e-05,
      "loss": 1.9442,
      "step": 2370
    },
    {
      "epoch": 1.5867644607434572,
      "grad_norm": 1.6727393865585327,
      "learning_rate": 2.5106778290293456e-05,
      "loss": 1.9593,
      "step": 2380
    },
    {
      "epoch": 1.593432238706451,
      "grad_norm": 1.956000566482544,
      "learning_rate": 2.470248234358986e-05,
      "loss": 2.0083,
      "step": 2390
    },
    {
      "epoch": 1.6001000166694448,
      "grad_norm": 1.7297418117523193,
      "learning_rate": 2.429818639688626e-05,
      "loss": 1.9922,
      "step": 2400
    },
    {
      "epoch": 1.6067677946324388,
      "grad_norm": 1.2849680185317993,
      "learning_rate": 2.3893890450182664e-05,
      "loss": 1.9166,
      "step": 2410
    },
    {
      "epoch": 1.6134355725954326,
      "grad_norm": 2.532897710800171,
      "learning_rate": 2.3489594503479064e-05,
      "loss": 1.9262,
      "step": 2420
    },
    {
      "epoch": 1.6201033505584264,
      "grad_norm": 1.8743062019348145,
      "learning_rate": 2.3085298556775466e-05,
      "loss": 2.0064,
      "step": 2430
    },
    {
      "epoch": 1.6267711285214204,
      "grad_norm": 1.0341933965682983,
      "learning_rate": 2.268100261007187e-05,
      "loss": 2.0757,
      "step": 2440
    },
    {
      "epoch": 1.6334389064844141,
      "grad_norm": 1.3900431394577026,
      "learning_rate": 2.2276706663368272e-05,
      "loss": 2.0307,
      "step": 2450
    },
    {
      "epoch": 1.640106684447408,
      "grad_norm": 2.163874626159668,
      "learning_rate": 2.187241071666467e-05,
      "loss": 2.0074,
      "step": 2460
    },
    {
      "epoch": 1.6467744624104017,
      "grad_norm": 1.5772180557250977,
      "learning_rate": 2.146811476996107e-05,
      "loss": 2.0286,
      "step": 2470
    },
    {
      "epoch": 1.6534422403733955,
      "grad_norm": 1.0564208030700684,
      "learning_rate": 2.1063818823257473e-05,
      "loss": 2.0112,
      "step": 2480
    },
    {
      "epoch": 1.6601100183363893,
      "grad_norm": 1.2785204648971558,
      "learning_rate": 2.0659522876553876e-05,
      "loss": 1.8876,
      "step": 2490
    },
    {
      "epoch": 1.6667777962993833,
      "grad_norm": 1.3052247762680054,
      "learning_rate": 2.025522692985028e-05,
      "loss": 2.0177,
      "step": 2500
    },
    {
      "epoch": 1.673445574262377,
      "grad_norm": 0.9796333312988281,
      "learning_rate": 1.985093098314668e-05,
      "loss": 2.0455,
      "step": 2510
    },
    {
      "epoch": 1.680113352225371,
      "grad_norm": 2.0782978534698486,
      "learning_rate": 1.944663503644308e-05,
      "loss": 2.0063,
      "step": 2520
    },
    {
      "epoch": 1.6867811301883648,
      "grad_norm": 1.928281307220459,
      "learning_rate": 1.9042339089739484e-05,
      "loss": 1.9525,
      "step": 2530
    },
    {
      "epoch": 1.6934489081513586,
      "grad_norm": 1.7256923913955688,
      "learning_rate": 1.8638043143035887e-05,
      "loss": 1.9848,
      "step": 2540
    },
    {
      "epoch": 1.7001166861143524,
      "grad_norm": 1.4999666213989258,
      "learning_rate": 1.823374719633229e-05,
      "loss": 2.0668,
      "step": 2550
    },
    {
      "epoch": 1.7067844640773462,
      "grad_norm": 1.4668720960617065,
      "learning_rate": 1.7829451249628685e-05,
      "loss": 1.945,
      "step": 2560
    },
    {
      "epoch": 1.71345224204034,
      "grad_norm": 1.5945442914962769,
      "learning_rate": 1.7425155302925088e-05,
      "loss": 1.9868,
      "step": 2570
    },
    {
      "epoch": 1.7201200200033337,
      "grad_norm": 1.6524344682693481,
      "learning_rate": 1.702085935622149e-05,
      "loss": 2.0166,
      "step": 2580
    },
    {
      "epoch": 1.7267877979663278,
      "grad_norm": 1.6092076301574707,
      "learning_rate": 1.6616563409517894e-05,
      "loss": 2.0582,
      "step": 2590
    },
    {
      "epoch": 1.7334555759293215,
      "grad_norm": 1.2924801111221313,
      "learning_rate": 1.6212267462814293e-05,
      "loss": 1.9686,
      "step": 2600
    },
    {
      "epoch": 1.7401233538923155,
      "grad_norm": 1.2745846509933472,
      "learning_rate": 1.5807971516110696e-05,
      "loss": 1.9653,
      "step": 2610
    },
    {
      "epoch": 1.7467911318553093,
      "grad_norm": 1.315247893333435,
      "learning_rate": 1.54036755694071e-05,
      "loss": 2.0001,
      "step": 2620
    },
    {
      "epoch": 1.753458909818303,
      "grad_norm": 1.2492048740386963,
      "learning_rate": 1.4999379622703498e-05,
      "loss": 1.9744,
      "step": 2630
    },
    {
      "epoch": 1.7601266877812969,
      "grad_norm": 1.19466233253479,
      "learning_rate": 1.45950836759999e-05,
      "loss": 1.9345,
      "step": 2640
    },
    {
      "epoch": 1.7667944657442907,
      "grad_norm": 1.333060383796692,
      "learning_rate": 1.4190787729296304e-05,
      "loss": 1.8941,
      "step": 2650
    },
    {
      "epoch": 1.7734622437072844,
      "grad_norm": 1.4730650186538696,
      "learning_rate": 1.3786491782592705e-05,
      "loss": 1.9946,
      "step": 2660
    },
    {
      "epoch": 1.7801300216702782,
      "grad_norm": 1.277207612991333,
      "learning_rate": 1.3382195835889106e-05,
      "loss": 2.0116,
      "step": 2670
    },
    {
      "epoch": 1.7867977996332722,
      "grad_norm": 1.473662257194519,
      "learning_rate": 1.2977899889185507e-05,
      "loss": 1.8887,
      "step": 2680
    },
    {
      "epoch": 1.793465577596266,
      "grad_norm": 1.4713455438613892,
      "learning_rate": 1.257360394248191e-05,
      "loss": 1.9257,
      "step": 2690
    },
    {
      "epoch": 1.80013335555926,
      "grad_norm": 1.191960334777832,
      "learning_rate": 1.216930799577831e-05,
      "loss": 2.0099,
      "step": 2700
    },
    {
      "epoch": 1.8068011335222538,
      "grad_norm": 0.9774153828620911,
      "learning_rate": 1.1765012049074713e-05,
      "loss": 2.0365,
      "step": 2710
    },
    {
      "epoch": 1.8134689114852476,
      "grad_norm": 1.5924683809280396,
      "learning_rate": 1.1360716102371113e-05,
      "loss": 1.9739,
      "step": 2720
    },
    {
      "epoch": 1.8201366894482414,
      "grad_norm": 1.5394401550292969,
      "learning_rate": 1.0956420155667516e-05,
      "loss": 1.9133,
      "step": 2730
    },
    {
      "epoch": 1.8268044674112351,
      "grad_norm": 1.4108328819274902,
      "learning_rate": 1.0552124208963917e-05,
      "loss": 1.951,
      "step": 2740
    },
    {
      "epoch": 1.833472245374229,
      "grad_norm": 1.5516003370285034,
      "learning_rate": 1.014782826226032e-05,
      "loss": 1.9831,
      "step": 2750
    },
    {
      "epoch": 1.8401400233372227,
      "grad_norm": 2.1821529865264893,
      "learning_rate": 9.743532315556722e-06,
      "loss": 1.9743,
      "step": 2760
    },
    {
      "epoch": 1.8468078013002167,
      "grad_norm": 1.470263123512268,
      "learning_rate": 9.339236368853122e-06,
      "loss": 2.0103,
      "step": 2770
    },
    {
      "epoch": 1.8534755792632105,
      "grad_norm": 1.8240854740142822,
      "learning_rate": 8.934940422149524e-06,
      "loss": 1.9534,
      "step": 2780
    },
    {
      "epoch": 1.8601433572262045,
      "grad_norm": 1.1542541980743408,
      "learning_rate": 8.530644475445925e-06,
      "loss": 1.865,
      "step": 2790
    },
    {
      "epoch": 1.8668111351891983,
      "grad_norm": 1.4329502582550049,
      "learning_rate": 8.126348528742328e-06,
      "loss": 1.9995,
      "step": 2800
    },
    {
      "epoch": 1.873478913152192,
      "grad_norm": 1.7247710227966309,
      "learning_rate": 7.722052582038727e-06,
      "loss": 2.0286,
      "step": 2810
    },
    {
      "epoch": 1.8801466911151858,
      "grad_norm": 1.211873173713684,
      "learning_rate": 7.317756635335131e-06,
      "loss": 2.0118,
      "step": 2820
    },
    {
      "epoch": 1.8868144690781796,
      "grad_norm": 1.9664956331253052,
      "learning_rate": 6.913460688631532e-06,
      "loss": 1.9731,
      "step": 2830
    },
    {
      "epoch": 1.8934822470411734,
      "grad_norm": 1.147250771522522,
      "learning_rate": 6.509164741927934e-06,
      "loss": 1.9555,
      "step": 2840
    },
    {
      "epoch": 1.9001500250041674,
      "grad_norm": 1.630248785018921,
      "learning_rate": 6.104868795224335e-06,
      "loss": 1.9105,
      "step": 2850
    },
    {
      "epoch": 1.9068178029671612,
      "grad_norm": 1.0712871551513672,
      "learning_rate": 5.700572848520737e-06,
      "loss": 1.9841,
      "step": 2860
    },
    {
      "epoch": 1.913485580930155,
      "grad_norm": 0.9385010004043579,
      "learning_rate": 5.296276901817138e-06,
      "loss": 1.9439,
      "step": 2870
    },
    {
      "epoch": 1.920153358893149,
      "grad_norm": 1.2709908485412598,
      "learning_rate": 4.89198095511354e-06,
      "loss": 1.9399,
      "step": 2880
    },
    {
      "epoch": 1.9268211368561428,
      "grad_norm": 1.4579389095306396,
      "learning_rate": 4.487685008409942e-06,
      "loss": 1.9688,
      "step": 2890
    },
    {
      "epoch": 1.9334889148191365,
      "grad_norm": 1.1111942529678345,
      "learning_rate": 4.083389061706343e-06,
      "loss": 1.9448,
      "step": 2900
    },
    {
      "epoch": 1.9401566927821303,
      "grad_norm": 1.5912578105926514,
      "learning_rate": 3.679093115002745e-06,
      "loss": 2.0086,
      "step": 2910
    },
    {
      "epoch": 1.946824470745124,
      "grad_norm": 1.374149203300476,
      "learning_rate": 3.274797168299147e-06,
      "loss": 1.8531,
      "step": 2920
    },
    {
      "epoch": 1.9534922487081179,
      "grad_norm": 1.07976496219635,
      "learning_rate": 2.8705012215955484e-06,
      "loss": 2.0632,
      "step": 2930
    },
    {
      "epoch": 1.9601600266711119,
      "grad_norm": 1.4398388862609863,
      "learning_rate": 2.46620527489195e-06,
      "loss": 1.9445,
      "step": 2940
    },
    {
      "epoch": 1.9668278046341057,
      "grad_norm": 1.4504287242889404,
      "learning_rate": 2.061909328188352e-06,
      "loss": 1.9818,
      "step": 2950
    },
    {
      "epoch": 1.9734955825970997,
      "grad_norm": 1.803045392036438,
      "learning_rate": 1.6576133814847533e-06,
      "loss": 1.9607,
      "step": 2960
    },
    {
      "epoch": 1.9801633605600935,
      "grad_norm": 1.176085114479065,
      "learning_rate": 1.2533174347811548e-06,
      "loss": 1.9699,
      "step": 2970
    },
    {
      "epoch": 1.9868311385230872,
      "grad_norm": 1.837335228919983,
      "learning_rate": 8.490214880775566e-07,
      "loss": 1.9018,
      "step": 2980
    },
    {
      "epoch": 1.993498916486081,
      "grad_norm": 1.020483136177063,
      "learning_rate": 4.447255413739582e-07,
      "loss": 1.9204,
      "step": 2990
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.7704625129699707,
      "learning_rate": 4.0429594670359836e-08,
      "loss": 1.946,
      "step": 3000
    },
    {
      "epoch": 2.0,
      "eval_loss": 1.988416314125061,
      "eval_runtime": 937.7962,
      "eval_samples_per_second": 6.397,
      "eval_steps_per_second": 3.199,
      "step": 3000
    }
  ],
  "logging_steps": 10,
  "max_steps": 3000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 500,
  "stateful_callbacks": {
    "EarlyStoppingCallback": {
      "args": {
        "early_stopping_patience": 2,
        "early_stopping_threshold": 0.0
      },
      "attributes": {
        "early_stopping_patience_counter": 0
      }
    },
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.0855199598588723e+18,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
