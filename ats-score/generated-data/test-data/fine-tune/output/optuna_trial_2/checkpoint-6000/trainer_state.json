{
  "best_global_step": 6000,
  "best_metric": 2.118459939956665,
  "best_model_checkpoint": "json_outputs_all_data/fine-tune/optuna_output/optuna_trial_2/checkpoint-6000",
  "epoch": 4.0,
  "eval_steps": 500,
  "global_step": 6000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.006667777962993832,
      "grad_norm": 0.3668053448200226,
      "learning_rate": 5.3341037243159054e-05,
      "loss": 2.7092,
      "step": 10
    },
    {
      "epoch": 0.013335555925987664,
      "grad_norm": 0.2968290448188782,
      "learning_rate": 5.3252001961497965e-05,
      "loss": 2.6045,
      "step": 20
    },
    {
      "epoch": 0.020003333888981498,
      "grad_norm": 0.30834025144577026,
      "learning_rate": 5.316296667983687e-05,
      "loss": 2.5575,
      "step": 30
    },
    {
      "epoch": 0.026671111851975328,
      "grad_norm": 0.3046305477619171,
      "learning_rate": 5.307393139817579e-05,
      "loss": 2.5741,
      "step": 40
    },
    {
      "epoch": 0.03333888981496916,
      "grad_norm": 0.30960750579833984,
      "learning_rate": 5.298489611651469e-05,
      "loss": 2.5082,
      "step": 50
    },
    {
      "epoch": 0.040006667777962995,
      "grad_norm": 0.34115689992904663,
      "learning_rate": 5.28958608348536e-05,
      "loss": 2.48,
      "step": 60
    },
    {
      "epoch": 0.046674445740956826,
      "grad_norm": 0.3372827172279358,
      "learning_rate": 5.2806825553192514e-05,
      "loss": 2.4455,
      "step": 70
    },
    {
      "epoch": 0.053342223703950656,
      "grad_norm": 0.3526439964771271,
      "learning_rate": 5.2717790271531425e-05,
      "loss": 2.4853,
      "step": 80
    },
    {
      "epoch": 0.06001000166694449,
      "grad_norm": 0.34579089283943176,
      "learning_rate": 5.262875498987033e-05,
      "loss": 2.4525,
      "step": 90
    },
    {
      "epoch": 0.06667777962993832,
      "grad_norm": 0.38845542073249817,
      "learning_rate": 5.253971970820925e-05,
      "loss": 2.4305,
      "step": 100
    },
    {
      "epoch": 0.07334555759293215,
      "grad_norm": 0.38927868008613586,
      "learning_rate": 5.245068442654815e-05,
      "loss": 2.434,
      "step": 110
    },
    {
      "epoch": 0.08001333555592599,
      "grad_norm": 0.38023629784584045,
      "learning_rate": 5.2361649144887064e-05,
      "loss": 2.4463,
      "step": 120
    },
    {
      "epoch": 0.08668111351891981,
      "grad_norm": 0.39833536744117737,
      "learning_rate": 5.2272613863225975e-05,
      "loss": 2.3894,
      "step": 130
    },
    {
      "epoch": 0.09334889148191365,
      "grad_norm": 0.3939032554626465,
      "learning_rate": 5.2183578581564886e-05,
      "loss": 2.4186,
      "step": 140
    },
    {
      "epoch": 0.10001666944490749,
      "grad_norm": 0.4805064797401428,
      "learning_rate": 5.209454329990379e-05,
      "loss": 2.4037,
      "step": 150
    },
    {
      "epoch": 0.10668444740790131,
      "grad_norm": 0.43818655610084534,
      "learning_rate": 5.20055080182427e-05,
      "loss": 2.3824,
      "step": 160
    },
    {
      "epoch": 0.11335222537089515,
      "grad_norm": 0.42560961842536926,
      "learning_rate": 5.191647273658161e-05,
      "loss": 2.3513,
      "step": 170
    },
    {
      "epoch": 0.12002000333388899,
      "grad_norm": 0.43975549936294556,
      "learning_rate": 5.182743745492052e-05,
      "loss": 2.3799,
      "step": 180
    },
    {
      "epoch": 0.12668778129688282,
      "grad_norm": 0.5175877809524536,
      "learning_rate": 5.1738402173259435e-05,
      "loss": 2.3739,
      "step": 190
    },
    {
      "epoch": 0.13335555925987663,
      "grad_norm": 0.4571895897388458,
      "learning_rate": 5.164936689159834e-05,
      "loss": 2.3491,
      "step": 200
    },
    {
      "epoch": 0.14002333722287047,
      "grad_norm": 0.4305247664451599,
      "learning_rate": 5.156033160993725e-05,
      "loss": 2.389,
      "step": 210
    },
    {
      "epoch": 0.1466911151858643,
      "grad_norm": 0.5003413558006287,
      "learning_rate": 5.147129632827616e-05,
      "loss": 2.4081,
      "step": 220
    },
    {
      "epoch": 0.15335889314885814,
      "grad_norm": 0.5146031975746155,
      "learning_rate": 5.1382261046615073e-05,
      "loss": 2.3391,
      "step": 230
    },
    {
      "epoch": 0.16002667111185198,
      "grad_norm": 0.5030456781387329,
      "learning_rate": 5.129322576495398e-05,
      "loss": 2.3468,
      "step": 240
    },
    {
      "epoch": 0.16669444907484582,
      "grad_norm": 0.4993475675582886,
      "learning_rate": 5.1204190483292896e-05,
      "loss": 2.3553,
      "step": 250
    },
    {
      "epoch": 0.17336222703783963,
      "grad_norm": 0.5384043455123901,
      "learning_rate": 5.11151552016318e-05,
      "loss": 2.3533,
      "step": 260
    },
    {
      "epoch": 0.18003000500083347,
      "grad_norm": 0.5235914587974548,
      "learning_rate": 5.102611991997072e-05,
      "loss": 2.4153,
      "step": 270
    },
    {
      "epoch": 0.1866977829638273,
      "grad_norm": 0.5718870162963867,
      "learning_rate": 5.093708463830962e-05,
      "loss": 2.4588,
      "step": 280
    },
    {
      "epoch": 0.19336556092682114,
      "grad_norm": 0.5026224851608276,
      "learning_rate": 5.0848049356648534e-05,
      "loss": 2.3645,
      "step": 290
    },
    {
      "epoch": 0.20003333888981498,
      "grad_norm": 0.5597790479660034,
      "learning_rate": 5.0759014074987445e-05,
      "loss": 2.3766,
      "step": 300
    },
    {
      "epoch": 0.2067011168528088,
      "grad_norm": 0.5370635986328125,
      "learning_rate": 5.066997879332635e-05,
      "loss": 2.3466,
      "step": 310
    },
    {
      "epoch": 0.21336889481580262,
      "grad_norm": 0.5074216723442078,
      "learning_rate": 5.058094351166526e-05,
      "loss": 2.3778,
      "step": 320
    },
    {
      "epoch": 0.22003667277879646,
      "grad_norm": 0.5584427714347839,
      "learning_rate": 5.049190823000417e-05,
      "loss": 2.3321,
      "step": 330
    },
    {
      "epoch": 0.2267044507417903,
      "grad_norm": 0.5839680433273315,
      "learning_rate": 5.0402872948343083e-05,
      "loss": 2.3376,
      "step": 340
    },
    {
      "epoch": 0.23337222870478413,
      "grad_norm": 0.5815520286560059,
      "learning_rate": 5.031383766668199e-05,
      "loss": 2.3546,
      "step": 350
    },
    {
      "epoch": 0.24004000666777797,
      "grad_norm": 0.5997860431671143,
      "learning_rate": 5.0224802385020906e-05,
      "loss": 2.3804,
      "step": 360
    },
    {
      "epoch": 0.24670778463077178,
      "grad_norm": 0.5845661163330078,
      "learning_rate": 5.013576710335981e-05,
      "loss": 2.4082,
      "step": 370
    },
    {
      "epoch": 0.25337556259376565,
      "grad_norm": 0.6416891813278198,
      "learning_rate": 5.004673182169872e-05,
      "loss": 2.3716,
      "step": 380
    },
    {
      "epoch": 0.2600433405567595,
      "grad_norm": 0.5781415700912476,
      "learning_rate": 4.995769654003763e-05,
      "loss": 2.352,
      "step": 390
    },
    {
      "epoch": 0.26671111851975327,
      "grad_norm": 0.5828028917312622,
      "learning_rate": 4.9868661258376544e-05,
      "loss": 2.358,
      "step": 400
    },
    {
      "epoch": 0.2733788964827471,
      "grad_norm": 0.6141397953033447,
      "learning_rate": 4.977962597671545e-05,
      "loss": 2.33,
      "step": 410
    },
    {
      "epoch": 0.28004667444574094,
      "grad_norm": 0.6156024932861328,
      "learning_rate": 4.969059069505436e-05,
      "loss": 2.3114,
      "step": 420
    },
    {
      "epoch": 0.2867144524087348,
      "grad_norm": 0.5925322771072388,
      "learning_rate": 4.960155541339327e-05,
      "loss": 2.3185,
      "step": 430
    },
    {
      "epoch": 0.2933822303717286,
      "grad_norm": 0.6456865668296814,
      "learning_rate": 4.9512520131732175e-05,
      "loss": 2.3568,
      "step": 440
    },
    {
      "epoch": 0.30005000833472245,
      "grad_norm": 0.574446976184845,
      "learning_rate": 4.9423484850071093e-05,
      "loss": 2.3691,
      "step": 450
    },
    {
      "epoch": 0.3067177862977163,
      "grad_norm": 0.6071268320083618,
      "learning_rate": 4.933444956841e-05,
      "loss": 2.3712,
      "step": 460
    },
    {
      "epoch": 0.3133855642607101,
      "grad_norm": 0.609879195690155,
      "learning_rate": 4.924541428674891e-05,
      "loss": 2.3865,
      "step": 470
    },
    {
      "epoch": 0.32005334222370396,
      "grad_norm": 0.632093608379364,
      "learning_rate": 4.915637900508782e-05,
      "loss": 2.3369,
      "step": 480
    },
    {
      "epoch": 0.3267211201866978,
      "grad_norm": 0.636239767074585,
      "learning_rate": 4.906734372342673e-05,
      "loss": 2.3524,
      "step": 490
    },
    {
      "epoch": 0.33338889814969164,
      "grad_norm": 0.6820055842399597,
      "learning_rate": 4.8978308441765636e-05,
      "loss": 2.3798,
      "step": 500
    },
    {
      "epoch": 0.3400566761126855,
      "grad_norm": 0.7606590390205383,
      "learning_rate": 4.8889273160104554e-05,
      "loss": 2.3568,
      "step": 510
    },
    {
      "epoch": 0.34672445407567926,
      "grad_norm": 0.635757565498352,
      "learning_rate": 4.880023787844346e-05,
      "loss": 2.3251,
      "step": 520
    },
    {
      "epoch": 0.3533922320386731,
      "grad_norm": 0.665002167224884,
      "learning_rate": 4.8711202596782376e-05,
      "loss": 2.3627,
      "step": 530
    },
    {
      "epoch": 0.36006001000166693,
      "grad_norm": 0.6834372282028198,
      "learning_rate": 4.862216731512128e-05,
      "loss": 2.2995,
      "step": 540
    },
    {
      "epoch": 0.36672778796466077,
      "grad_norm": 0.631553590297699,
      "learning_rate": 4.853313203346019e-05,
      "loss": 2.306,
      "step": 550
    },
    {
      "epoch": 0.3733955659276546,
      "grad_norm": 0.647770345211029,
      "learning_rate": 4.8444096751799103e-05,
      "loss": 2.2439,
      "step": 560
    },
    {
      "epoch": 0.38006334389064844,
      "grad_norm": 0.6448125243186951,
      "learning_rate": 4.835506147013801e-05,
      "loss": 2.3234,
      "step": 570
    },
    {
      "epoch": 0.3867311218536423,
      "grad_norm": 0.656695544719696,
      "learning_rate": 4.826602618847692e-05,
      "loss": 2.3398,
      "step": 580
    },
    {
      "epoch": 0.3933988998166361,
      "grad_norm": 0.739518940448761,
      "learning_rate": 4.817699090681583e-05,
      "loss": 2.3367,
      "step": 590
    },
    {
      "epoch": 0.40006667777962995,
      "grad_norm": 0.6440545320510864,
      "learning_rate": 4.808795562515474e-05,
      "loss": 2.328,
      "step": 600
    },
    {
      "epoch": 0.4067344557426238,
      "grad_norm": 0.7124055027961731,
      "learning_rate": 4.7998920343493646e-05,
      "loss": 2.3049,
      "step": 610
    },
    {
      "epoch": 0.4134022337056176,
      "grad_norm": 0.6988508701324463,
      "learning_rate": 4.7909885061832564e-05,
      "loss": 2.3108,
      "step": 620
    },
    {
      "epoch": 0.4200700116686114,
      "grad_norm": 0.7431545257568359,
      "learning_rate": 4.782084978017147e-05,
      "loss": 2.32,
      "step": 630
    },
    {
      "epoch": 0.42673778963160525,
      "grad_norm": 0.7721455693244934,
      "learning_rate": 4.773181449851038e-05,
      "loss": 2.3304,
      "step": 640
    },
    {
      "epoch": 0.4334055675945991,
      "grad_norm": 0.7886254787445068,
      "learning_rate": 4.764277921684929e-05,
      "loss": 2.3566,
      "step": 650
    },
    {
      "epoch": 0.4400733455575929,
      "grad_norm": 0.7010221481323242,
      "learning_rate": 4.75537439351882e-05,
      "loss": 2.3288,
      "step": 660
    },
    {
      "epoch": 0.44674112352058676,
      "grad_norm": 0.8081058859825134,
      "learning_rate": 4.7464708653527107e-05,
      "loss": 2.272,
      "step": 670
    },
    {
      "epoch": 0.4534089014835806,
      "grad_norm": 0.8462689518928528,
      "learning_rate": 4.7375673371866025e-05,
      "loss": 2.3318,
      "step": 680
    },
    {
      "epoch": 0.46007667944657443,
      "grad_norm": 0.7743210196495056,
      "learning_rate": 4.728663809020493e-05,
      "loss": 2.2919,
      "step": 690
    },
    {
      "epoch": 0.46674445740956827,
      "grad_norm": 0.742149293422699,
      "learning_rate": 4.719760280854384e-05,
      "loss": 2.2969,
      "step": 700
    },
    {
      "epoch": 0.4734122353725621,
      "grad_norm": 0.7544770836830139,
      "learning_rate": 4.710856752688275e-05,
      "loss": 2.3504,
      "step": 710
    },
    {
      "epoch": 0.48008001333555594,
      "grad_norm": 0.7495868802070618,
      "learning_rate": 4.7019532245221656e-05,
      "loss": 2.263,
      "step": 720
    },
    {
      "epoch": 0.4867477912985498,
      "grad_norm": 0.7333542108535767,
      "learning_rate": 4.693049696356057e-05,
      "loss": 2.322,
      "step": 730
    },
    {
      "epoch": 0.49341556926154356,
      "grad_norm": 0.8062014579772949,
      "learning_rate": 4.684146168189948e-05,
      "loss": 2.3093,
      "step": 740
    },
    {
      "epoch": 0.5000833472245374,
      "grad_norm": 0.7473324537277222,
      "learning_rate": 4.675242640023839e-05,
      "loss": 2.3412,
      "step": 750
    },
    {
      "epoch": 0.5067511251875313,
      "grad_norm": 0.7853045463562012,
      "learning_rate": 4.66633911185773e-05,
      "loss": 2.3082,
      "step": 760
    },
    {
      "epoch": 0.5134189031505251,
      "grad_norm": 0.7668623924255371,
      "learning_rate": 4.657435583691621e-05,
      "loss": 2.2401,
      "step": 770
    },
    {
      "epoch": 0.520086681113519,
      "grad_norm": 0.795496940612793,
      "learning_rate": 4.6485320555255117e-05,
      "loss": 2.2948,
      "step": 780
    },
    {
      "epoch": 0.5267544590765127,
      "grad_norm": 0.7445259690284729,
      "learning_rate": 4.6396285273594035e-05,
      "loss": 2.3196,
      "step": 790
    },
    {
      "epoch": 0.5334222370395065,
      "grad_norm": 0.7831029891967773,
      "learning_rate": 4.630724999193294e-05,
      "loss": 2.302,
      "step": 800
    },
    {
      "epoch": 0.5400900150025004,
      "grad_norm": 0.7264555096626282,
      "learning_rate": 4.621821471027185e-05,
      "loss": 2.3584,
      "step": 810
    },
    {
      "epoch": 0.5467577929654942,
      "grad_norm": 0.8191275596618652,
      "learning_rate": 4.612917942861076e-05,
      "loss": 2.3073,
      "step": 820
    },
    {
      "epoch": 0.5534255709284881,
      "grad_norm": 0.8094958662986755,
      "learning_rate": 4.604014414694967e-05,
      "loss": 2.3085,
      "step": 830
    },
    {
      "epoch": 0.5600933488914819,
      "grad_norm": 0.7669306993484497,
      "learning_rate": 4.595110886528858e-05,
      "loss": 2.3185,
      "step": 840
    },
    {
      "epoch": 0.5667611268544758,
      "grad_norm": 0.8828079104423523,
      "learning_rate": 4.586207358362749e-05,
      "loss": 2.2888,
      "step": 850
    },
    {
      "epoch": 0.5734289048174696,
      "grad_norm": 0.7503806352615356,
      "learning_rate": 4.57730383019664e-05,
      "loss": 2.3128,
      "step": 860
    },
    {
      "epoch": 0.5800966827804634,
      "grad_norm": 0.7462964653968811,
      "learning_rate": 4.5684003020305304e-05,
      "loss": 2.3147,
      "step": 870
    },
    {
      "epoch": 0.5867644607434572,
      "grad_norm": 0.8005026578903198,
      "learning_rate": 4.559496773864422e-05,
      "loss": 2.2789,
      "step": 880
    },
    {
      "epoch": 0.5934322387064511,
      "grad_norm": 0.9830164909362793,
      "learning_rate": 4.5505932456983127e-05,
      "loss": 2.3054,
      "step": 890
    },
    {
      "epoch": 0.6001000166694449,
      "grad_norm": 0.8767753839492798,
      "learning_rate": 4.541689717532204e-05,
      "loss": 2.2797,
      "step": 900
    },
    {
      "epoch": 0.6067677946324388,
      "grad_norm": 0.8151304721832275,
      "learning_rate": 4.532786189366095e-05,
      "loss": 2.228,
      "step": 910
    },
    {
      "epoch": 0.6134355725954326,
      "grad_norm": 0.8686376810073853,
      "learning_rate": 4.523882661199986e-05,
      "loss": 2.3243,
      "step": 920
    },
    {
      "epoch": 0.6201033505584264,
      "grad_norm": 0.8232620358467102,
      "learning_rate": 4.5149791330338765e-05,
      "loss": 2.3338,
      "step": 930
    },
    {
      "epoch": 0.6267711285214203,
      "grad_norm": 0.8725425601005554,
      "learning_rate": 4.506075604867768e-05,
      "loss": 2.3614,
      "step": 940
    },
    {
      "epoch": 0.633438906484414,
      "grad_norm": 0.8558036088943481,
      "learning_rate": 4.497172076701659e-05,
      "loss": 2.2731,
      "step": 950
    },
    {
      "epoch": 0.6401066844474079,
      "grad_norm": 0.8542634844779968,
      "learning_rate": 4.48826854853555e-05,
      "loss": 2.3079,
      "step": 960
    },
    {
      "epoch": 0.6467744624104017,
      "grad_norm": 0.8002933263778687,
      "learning_rate": 4.479365020369441e-05,
      "loss": 2.32,
      "step": 970
    },
    {
      "epoch": 0.6534422403733956,
      "grad_norm": 0.9112385511398315,
      "learning_rate": 4.4704614922033314e-05,
      "loss": 2.2752,
      "step": 980
    },
    {
      "epoch": 0.6601100183363894,
      "grad_norm": 1.029310703277588,
      "learning_rate": 4.4615579640372225e-05,
      "loss": 2.264,
      "step": 990
    },
    {
      "epoch": 0.6667777962993833,
      "grad_norm": 0.9456078410148621,
      "learning_rate": 4.4526544358711137e-05,
      "loss": 2.3498,
      "step": 1000
    },
    {
      "epoch": 0.6734455742623771,
      "grad_norm": 0.8346761465072632,
      "learning_rate": 4.443750907705005e-05,
      "loss": 2.2732,
      "step": 1010
    },
    {
      "epoch": 0.680113352225371,
      "grad_norm": 0.9506984949111938,
      "learning_rate": 4.434847379538896e-05,
      "loss": 2.2655,
      "step": 1020
    },
    {
      "epoch": 0.6867811301883647,
      "grad_norm": 0.9170476794242859,
      "learning_rate": 4.425943851372787e-05,
      "loss": 2.3354,
      "step": 1030
    },
    {
      "epoch": 0.6934489081513585,
      "grad_norm": 0.929556667804718,
      "learning_rate": 4.4170403232066775e-05,
      "loss": 2.2593,
      "step": 1040
    },
    {
      "epoch": 0.7001166861143524,
      "grad_norm": 0.8989338278770447,
      "learning_rate": 4.408136795040569e-05,
      "loss": 2.2465,
      "step": 1050
    },
    {
      "epoch": 0.7067844640773462,
      "grad_norm": 0.9126049280166626,
      "learning_rate": 4.39923326687446e-05,
      "loss": 2.2437,
      "step": 1060
    },
    {
      "epoch": 0.7134522420403401,
      "grad_norm": 0.9372772574424744,
      "learning_rate": 4.390329738708351e-05,
      "loss": 2.3135,
      "step": 1070
    },
    {
      "epoch": 0.7201200200033339,
      "grad_norm": 0.9747854471206665,
      "learning_rate": 4.381426210542242e-05,
      "loss": 2.2965,
      "step": 1080
    },
    {
      "epoch": 0.7267877979663278,
      "grad_norm": 0.8549670577049255,
      "learning_rate": 4.372522682376133e-05,
      "loss": 2.2975,
      "step": 1090
    },
    {
      "epoch": 0.7334555759293215,
      "grad_norm": 0.9189603328704834,
      "learning_rate": 4.3636191542100235e-05,
      "loss": 2.2786,
      "step": 1100
    },
    {
      "epoch": 0.7401233538923154,
      "grad_norm": 0.9407936334609985,
      "learning_rate": 4.3547156260439146e-05,
      "loss": 2.2739,
      "step": 1110
    },
    {
      "epoch": 0.7467911318553092,
      "grad_norm": 0.950289785861969,
      "learning_rate": 4.345812097877806e-05,
      "loss": 2.3493,
      "step": 1120
    },
    {
      "epoch": 0.7534589098183031,
      "grad_norm": 0.8145261406898499,
      "learning_rate": 4.336908569711696e-05,
      "loss": 2.3461,
      "step": 1130
    },
    {
      "epoch": 0.7601266877812969,
      "grad_norm": 0.9415910840034485,
      "learning_rate": 4.328005041545588e-05,
      "loss": 2.2635,
      "step": 1140
    },
    {
      "epoch": 0.7667944657442907,
      "grad_norm": 0.9753206968307495,
      "learning_rate": 4.3191015133794785e-05,
      "loss": 2.2719,
      "step": 1150
    },
    {
      "epoch": 0.7734622437072846,
      "grad_norm": 1.0162322521209717,
      "learning_rate": 4.3101979852133696e-05,
      "loss": 2.2598,
      "step": 1160
    },
    {
      "epoch": 0.7801300216702783,
      "grad_norm": 0.9497990012168884,
      "learning_rate": 4.301294457047261e-05,
      "loss": 2.2718,
      "step": 1170
    },
    {
      "epoch": 0.7867977996332722,
      "grad_norm": 1.065626621246338,
      "learning_rate": 4.292390928881152e-05,
      "loss": 2.272,
      "step": 1180
    },
    {
      "epoch": 0.793465577596266,
      "grad_norm": 1.0419020652770996,
      "learning_rate": 4.283487400715042e-05,
      "loss": 2.304,
      "step": 1190
    },
    {
      "epoch": 0.8001333555592599,
      "grad_norm": 0.9204309582710266,
      "learning_rate": 4.274583872548934e-05,
      "loss": 2.2436,
      "step": 1200
    },
    {
      "epoch": 0.8068011335222537,
      "grad_norm": 0.9880892634391785,
      "learning_rate": 4.2656803443828245e-05,
      "loss": 2.3046,
      "step": 1210
    },
    {
      "epoch": 0.8134689114852476,
      "grad_norm": 1.0185075998306274,
      "learning_rate": 4.2567768162167156e-05,
      "loss": 2.2459,
      "step": 1220
    },
    {
      "epoch": 0.8201366894482414,
      "grad_norm": 1.0539751052856445,
      "learning_rate": 4.247873288050607e-05,
      "loss": 2.2866,
      "step": 1230
    },
    {
      "epoch": 0.8268044674112353,
      "grad_norm": 1.034232258796692,
      "learning_rate": 4.238969759884498e-05,
      "loss": 2.263,
      "step": 1240
    },
    {
      "epoch": 0.833472245374229,
      "grad_norm": 0.920237123966217,
      "learning_rate": 4.230066231718388e-05,
      "loss": 2.2747,
      "step": 1250
    },
    {
      "epoch": 0.8401400233372228,
      "grad_norm": 0.9700085520744324,
      "learning_rate": 4.2211627035522795e-05,
      "loss": 2.2968,
      "step": 1260
    },
    {
      "epoch": 0.8468078013002167,
      "grad_norm": 1.0007107257843018,
      "learning_rate": 4.2122591753861706e-05,
      "loss": 2.2627,
      "step": 1270
    },
    {
      "epoch": 0.8534755792632105,
      "grad_norm": 0.9829798936843872,
      "learning_rate": 4.203355647220062e-05,
      "loss": 2.2994,
      "step": 1280
    },
    {
      "epoch": 0.8601433572262044,
      "grad_norm": 0.9982128143310547,
      "learning_rate": 4.194452119053953e-05,
      "loss": 2.3064,
      "step": 1290
    },
    {
      "epoch": 0.8668111351891982,
      "grad_norm": 1.057146430015564,
      "learning_rate": 4.185548590887843e-05,
      "loss": 2.2617,
      "step": 1300
    },
    {
      "epoch": 0.8734789131521921,
      "grad_norm": 0.9701072573661804,
      "learning_rate": 4.176645062721735e-05,
      "loss": 2.2835,
      "step": 1310
    },
    {
      "epoch": 0.8801466911151858,
      "grad_norm": 1.0356322526931763,
      "learning_rate": 4.1677415345556255e-05,
      "loss": 2.2418,
      "step": 1320
    },
    {
      "epoch": 0.8868144690781797,
      "grad_norm": 1.1608318090438843,
      "learning_rate": 4.1588380063895166e-05,
      "loss": 2.2691,
      "step": 1330
    },
    {
      "epoch": 0.8934822470411735,
      "grad_norm": 0.9885957837104797,
      "learning_rate": 4.149934478223408e-05,
      "loss": 2.2354,
      "step": 1340
    },
    {
      "epoch": 0.9001500250041674,
      "grad_norm": 1.0230844020843506,
      "learning_rate": 4.141030950057299e-05,
      "loss": 2.256,
      "step": 1350
    },
    {
      "epoch": 0.9068178029671612,
      "grad_norm": 0.9359952211380005,
      "learning_rate": 4.132127421891189e-05,
      "loss": 2.2345,
      "step": 1360
    },
    {
      "epoch": 0.913485580930155,
      "grad_norm": 1.0117270946502686,
      "learning_rate": 4.1232238937250805e-05,
      "loss": 2.1929,
      "step": 1370
    },
    {
      "epoch": 0.9201533588931489,
      "grad_norm": 0.9434916973114014,
      "learning_rate": 4.1143203655589716e-05,
      "loss": 2.2416,
      "step": 1380
    },
    {
      "epoch": 0.9268211368561426,
      "grad_norm": 1.0098069906234741,
      "learning_rate": 4.105416837392863e-05,
      "loss": 2.3258,
      "step": 1390
    },
    {
      "epoch": 0.9334889148191365,
      "grad_norm": 1.0728904008865356,
      "learning_rate": 4.096513309226754e-05,
      "loss": 2.2675,
      "step": 1400
    },
    {
      "epoch": 0.9401566927821303,
      "grad_norm": 0.9866170287132263,
      "learning_rate": 4.087609781060644e-05,
      "loss": 2.2899,
      "step": 1410
    },
    {
      "epoch": 0.9468244707451242,
      "grad_norm": 1.1052414178848267,
      "learning_rate": 4.0787062528945354e-05,
      "loss": 2.2952,
      "step": 1420
    },
    {
      "epoch": 0.953492248708118,
      "grad_norm": 1.0431840419769287,
      "learning_rate": 4.0698027247284265e-05,
      "loss": 2.3113,
      "step": 1430
    },
    {
      "epoch": 0.9601600266711119,
      "grad_norm": 1.255700945854187,
      "learning_rate": 4.0608991965623176e-05,
      "loss": 2.2807,
      "step": 1440
    },
    {
      "epoch": 0.9668278046341057,
      "grad_norm": 1.2429040670394897,
      "learning_rate": 4.051995668396208e-05,
      "loss": 2.2199,
      "step": 1450
    },
    {
      "epoch": 0.9734955825970996,
      "grad_norm": 1.151839017868042,
      "learning_rate": 4.0430921402301e-05,
      "loss": 2.2799,
      "step": 1460
    },
    {
      "epoch": 0.9801633605600933,
      "grad_norm": 1.1172826290130615,
      "learning_rate": 4.03418861206399e-05,
      "loss": 2.2372,
      "step": 1470
    },
    {
      "epoch": 0.9868311385230871,
      "grad_norm": 1.087565302848816,
      "learning_rate": 4.0252850838978815e-05,
      "loss": 2.2567,
      "step": 1480
    },
    {
      "epoch": 0.993498916486081,
      "grad_norm": 1.1423832178115845,
      "learning_rate": 4.0163815557317726e-05,
      "loss": 2.2143,
      "step": 1490
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.4028874635696411,
      "learning_rate": 4.007478027565664e-05,
      "loss": 2.3409,
      "step": 1500
    },
    {
      "epoch": 1.0,
      "eval_loss": 2.2669780254364014,
      "eval_runtime": 938.8088,
      "eval_samples_per_second": 6.39,
      "eval_steps_per_second": 3.196,
      "step": 1500
    },
    {
      "epoch": 1.0066677779629938,
      "grad_norm": 1.157787799835205,
      "learning_rate": 3.998574499399555e-05,
      "loss": 2.2551,
      "step": 1510
    },
    {
      "epoch": 1.0133355559259876,
      "grad_norm": 1.1098146438598633,
      "learning_rate": 3.989670971233445e-05,
      "loss": 2.2432,
      "step": 1520
    },
    {
      "epoch": 1.0200033338889816,
      "grad_norm": 1.0355814695358276,
      "learning_rate": 3.9807674430673364e-05,
      "loss": 2.2252,
      "step": 1530
    },
    {
      "epoch": 1.0266711118519753,
      "grad_norm": 1.0473134517669678,
      "learning_rate": 3.9718639149012275e-05,
      "loss": 2.2627,
      "step": 1540
    },
    {
      "epoch": 1.0333388898149691,
      "grad_norm": 1.0748933553695679,
      "learning_rate": 3.9629603867351186e-05,
      "loss": 2.2604,
      "step": 1550
    },
    {
      "epoch": 1.040006667777963,
      "grad_norm": 1.0725529193878174,
      "learning_rate": 3.954056858569009e-05,
      "loss": 2.2442,
      "step": 1560
    },
    {
      "epoch": 1.046674445740957,
      "grad_norm": 1.1124699115753174,
      "learning_rate": 3.945153330402901e-05,
      "loss": 2.2374,
      "step": 1570
    },
    {
      "epoch": 1.0533422237039507,
      "grad_norm": 1.024520754814148,
      "learning_rate": 3.936249802236791e-05,
      "loss": 2.2453,
      "step": 1580
    },
    {
      "epoch": 1.0600100016669445,
      "grad_norm": 1.2120156288146973,
      "learning_rate": 3.9273462740706825e-05,
      "loss": 2.2586,
      "step": 1590
    },
    {
      "epoch": 1.0666777796299383,
      "grad_norm": 1.1686595678329468,
      "learning_rate": 3.9184427459045736e-05,
      "loss": 2.2437,
      "step": 1600
    },
    {
      "epoch": 1.073345557592932,
      "grad_norm": 1.2523555755615234,
      "learning_rate": 3.909539217738465e-05,
      "loss": 2.2763,
      "step": 1610
    },
    {
      "epoch": 1.080013335555926,
      "grad_norm": 1.0915946960449219,
      "learning_rate": 3.900635689572355e-05,
      "loss": 2.272,
      "step": 1620
    },
    {
      "epoch": 1.0866811135189198,
      "grad_norm": 1.0639798641204834,
      "learning_rate": 3.891732161406247e-05,
      "loss": 2.2847,
      "step": 1630
    },
    {
      "epoch": 1.0933488914819136,
      "grad_norm": 1.2926877737045288,
      "learning_rate": 3.8828286332401374e-05,
      "loss": 2.2364,
      "step": 1640
    },
    {
      "epoch": 1.1000166694449074,
      "grad_norm": 1.0643587112426758,
      "learning_rate": 3.8739251050740285e-05,
      "loss": 2.2563,
      "step": 1650
    },
    {
      "epoch": 1.1066844474079014,
      "grad_norm": 1.1653385162353516,
      "learning_rate": 3.8650215769079196e-05,
      "loss": 2.2127,
      "step": 1660
    },
    {
      "epoch": 1.1133522253708952,
      "grad_norm": 1.0853559970855713,
      "learning_rate": 3.85611804874181e-05,
      "loss": 2.2003,
      "step": 1670
    },
    {
      "epoch": 1.120020003333889,
      "grad_norm": 1.2535525560379028,
      "learning_rate": 3.847214520575701e-05,
      "loss": 2.2376,
      "step": 1680
    },
    {
      "epoch": 1.1266877812968827,
      "grad_norm": 1.2532740831375122,
      "learning_rate": 3.838310992409592e-05,
      "loss": 2.2545,
      "step": 1690
    },
    {
      "epoch": 1.1333555592598765,
      "grad_norm": 1.0616313219070435,
      "learning_rate": 3.8294074642434834e-05,
      "loss": 2.1848,
      "step": 1700
    },
    {
      "epoch": 1.1400233372228705,
      "grad_norm": 1.2556405067443848,
      "learning_rate": 3.820503936077374e-05,
      "loss": 2.2345,
      "step": 1710
    },
    {
      "epoch": 1.1466911151858643,
      "grad_norm": 1.2646405696868896,
      "learning_rate": 3.811600407911266e-05,
      "loss": 2.1912,
      "step": 1720
    },
    {
      "epoch": 1.153358893148858,
      "grad_norm": 1.3023979663848877,
      "learning_rate": 3.802696879745156e-05,
      "loss": 2.2248,
      "step": 1730
    },
    {
      "epoch": 1.160026671111852,
      "grad_norm": 1.27669095993042,
      "learning_rate": 3.793793351579047e-05,
      "loss": 2.2437,
      "step": 1740
    },
    {
      "epoch": 1.1666944490748459,
      "grad_norm": 1.097092866897583,
      "learning_rate": 3.7848898234129384e-05,
      "loss": 2.2411,
      "step": 1750
    },
    {
      "epoch": 1.1733622270378397,
      "grad_norm": 1.3008553981781006,
      "learning_rate": 3.7759862952468295e-05,
      "loss": 2.2257,
      "step": 1760
    },
    {
      "epoch": 1.1800300050008334,
      "grad_norm": 1.737899899482727,
      "learning_rate": 3.7670827670807206e-05,
      "loss": 2.2371,
      "step": 1770
    },
    {
      "epoch": 1.1866977829638272,
      "grad_norm": 1.3983795642852783,
      "learning_rate": 3.758179238914612e-05,
      "loss": 2.2088,
      "step": 1780
    },
    {
      "epoch": 1.1933655609268212,
      "grad_norm": 1.3669697046279907,
      "learning_rate": 3.749275710748502e-05,
      "loss": 2.2268,
      "step": 1790
    },
    {
      "epoch": 1.200033338889815,
      "grad_norm": 1.2324970960617065,
      "learning_rate": 3.740372182582393e-05,
      "loss": 2.2506,
      "step": 1800
    },
    {
      "epoch": 1.2067011168528088,
      "grad_norm": 1.2459794282913208,
      "learning_rate": 3.7314686544162844e-05,
      "loss": 2.2338,
      "step": 1810
    },
    {
      "epoch": 1.2133688948158026,
      "grad_norm": 1.1473523378372192,
      "learning_rate": 3.722565126250175e-05,
      "loss": 2.2086,
      "step": 1820
    },
    {
      "epoch": 1.2200366727787966,
      "grad_norm": 1.3182601928710938,
      "learning_rate": 3.713661598084067e-05,
      "loss": 2.2327,
      "step": 1830
    },
    {
      "epoch": 1.2267044507417904,
      "grad_norm": 1.237039566040039,
      "learning_rate": 3.704758069917957e-05,
      "loss": 2.2582,
      "step": 1840
    },
    {
      "epoch": 1.2333722287047841,
      "grad_norm": 1.4964823722839355,
      "learning_rate": 3.695854541751848e-05,
      "loss": 2.1937,
      "step": 1850
    },
    {
      "epoch": 1.240040006667778,
      "grad_norm": 1.3010462522506714,
      "learning_rate": 3.6869510135857394e-05,
      "loss": 2.2319,
      "step": 1860
    },
    {
      "epoch": 1.2467077846307717,
      "grad_norm": 1.4199844598770142,
      "learning_rate": 3.6780474854196305e-05,
      "loss": 2.1576,
      "step": 1870
    },
    {
      "epoch": 1.2533755625937657,
      "grad_norm": 1.2236533164978027,
      "learning_rate": 3.669143957253521e-05,
      "loss": 2.2058,
      "step": 1880
    },
    {
      "epoch": 1.2600433405567595,
      "grad_norm": 1.2924941778182983,
      "learning_rate": 3.660240429087413e-05,
      "loss": 2.2179,
      "step": 1890
    },
    {
      "epoch": 1.2667111185197533,
      "grad_norm": 1.2203947305679321,
      "learning_rate": 3.651336900921303e-05,
      "loss": 2.2339,
      "step": 1900
    },
    {
      "epoch": 1.273378896482747,
      "grad_norm": 1.3385814428329468,
      "learning_rate": 3.642433372755194e-05,
      "loss": 2.2022,
      "step": 1910
    },
    {
      "epoch": 1.280046674445741,
      "grad_norm": 1.1488066911697388,
      "learning_rate": 3.6335298445890854e-05,
      "loss": 2.2396,
      "step": 1920
    },
    {
      "epoch": 1.2867144524087348,
      "grad_norm": 1.2760761976242065,
      "learning_rate": 3.624626316422976e-05,
      "loss": 2.209,
      "step": 1930
    },
    {
      "epoch": 1.2933822303717286,
      "grad_norm": 1.3189940452575684,
      "learning_rate": 3.615722788256867e-05,
      "loss": 2.2005,
      "step": 1940
    },
    {
      "epoch": 1.3000500083347224,
      "grad_norm": 1.348206639289856,
      "learning_rate": 3.606819260090758e-05,
      "loss": 2.1748,
      "step": 1950
    },
    {
      "epoch": 1.3067177862977162,
      "grad_norm": 1.3968970775604248,
      "learning_rate": 3.597915731924649e-05,
      "loss": 2.2164,
      "step": 1960
    },
    {
      "epoch": 1.3133855642607102,
      "grad_norm": 1.5049115419387817,
      "learning_rate": 3.58901220375854e-05,
      "loss": 2.209,
      "step": 1970
    },
    {
      "epoch": 1.320053342223704,
      "grad_norm": 1.3423033952713013,
      "learning_rate": 3.5801086755924315e-05,
      "loss": 2.2345,
      "step": 1980
    },
    {
      "epoch": 1.3267211201866977,
      "grad_norm": 1.6761394739151,
      "learning_rate": 3.571205147426322e-05,
      "loss": 2.1802,
      "step": 1990
    },
    {
      "epoch": 1.3333888981496917,
      "grad_norm": 1.2619661092758179,
      "learning_rate": 3.562301619260213e-05,
      "loss": 2.2431,
      "step": 2000
    },
    {
      "epoch": 1.3400566761126855,
      "grad_norm": 1.3956772089004517,
      "learning_rate": 3.553398091094104e-05,
      "loss": 2.1812,
      "step": 2010
    },
    {
      "epoch": 1.3467244540756793,
      "grad_norm": 1.4701508283615112,
      "learning_rate": 3.544494562927995e-05,
      "loss": 2.2322,
      "step": 2020
    },
    {
      "epoch": 1.353392232038673,
      "grad_norm": 1.2389534711837769,
      "learning_rate": 3.5355910347618864e-05,
      "loss": 2.1587,
      "step": 2030
    },
    {
      "epoch": 1.3600600100016669,
      "grad_norm": 1.2862752676010132,
      "learning_rate": 3.5266875065957776e-05,
      "loss": 2.2036,
      "step": 2040
    },
    {
      "epoch": 1.3667277879646607,
      "grad_norm": 1.591261863708496,
      "learning_rate": 3.517783978429668e-05,
      "loss": 2.1875,
      "step": 2050
    },
    {
      "epoch": 1.3733955659276547,
      "grad_norm": 1.4383615255355835,
      "learning_rate": 3.508880450263559e-05,
      "loss": 2.1722,
      "step": 2060
    },
    {
      "epoch": 1.3800633438906484,
      "grad_norm": 1.4457017183303833,
      "learning_rate": 3.49997692209745e-05,
      "loss": 2.2647,
      "step": 2070
    },
    {
      "epoch": 1.3867311218536422,
      "grad_norm": 1.3396270275115967,
      "learning_rate": 3.491073393931341e-05,
      "loss": 2.215,
      "step": 2080
    },
    {
      "epoch": 1.3933988998166362,
      "grad_norm": 1.7053180932998657,
      "learning_rate": 3.4821698657652325e-05,
      "loss": 2.143,
      "step": 2090
    },
    {
      "epoch": 1.40006667777963,
      "grad_norm": 1.2253950834274292,
      "learning_rate": 3.473266337599123e-05,
      "loss": 2.2154,
      "step": 2100
    },
    {
      "epoch": 1.4067344557426238,
      "grad_norm": 1.273927927017212,
      "learning_rate": 3.464362809433014e-05,
      "loss": 2.2529,
      "step": 2110
    },
    {
      "epoch": 1.4134022337056176,
      "grad_norm": 1.4773935079574585,
      "learning_rate": 3.455459281266905e-05,
      "loss": 2.201,
      "step": 2120
    },
    {
      "epoch": 1.4200700116686114,
      "grad_norm": 1.382313847541809,
      "learning_rate": 3.446555753100796e-05,
      "loss": 2.2719,
      "step": 2130
    },
    {
      "epoch": 1.4267377896316051,
      "grad_norm": 1.8021109104156494,
      "learning_rate": 3.437652224934687e-05,
      "loss": 2.2107,
      "step": 2140
    },
    {
      "epoch": 1.4334055675945991,
      "grad_norm": 1.9413121938705444,
      "learning_rate": 3.4287486967685786e-05,
      "loss": 2.216,
      "step": 2150
    },
    {
      "epoch": 1.440073345557593,
      "grad_norm": 1.3999685049057007,
      "learning_rate": 3.419845168602469e-05,
      "loss": 2.166,
      "step": 2160
    },
    {
      "epoch": 1.4467411235205867,
      "grad_norm": 1.3939377069473267,
      "learning_rate": 3.41094164043636e-05,
      "loss": 2.2279,
      "step": 2170
    },
    {
      "epoch": 1.4534089014835807,
      "grad_norm": 1.4257537126541138,
      "learning_rate": 3.402038112270251e-05,
      "loss": 2.2045,
      "step": 2180
    },
    {
      "epoch": 1.4600766794465745,
      "grad_norm": 1.6248606443405151,
      "learning_rate": 3.3931345841041424e-05,
      "loss": 2.1758,
      "step": 2190
    },
    {
      "epoch": 1.4667444574095683,
      "grad_norm": 1.3989661931991577,
      "learning_rate": 3.384231055938033e-05,
      "loss": 2.2389,
      "step": 2200
    },
    {
      "epoch": 1.473412235372562,
      "grad_norm": 1.4326664209365845,
      "learning_rate": 3.375327527771924e-05,
      "loss": 2.2352,
      "step": 2210
    },
    {
      "epoch": 1.4800800133355558,
      "grad_norm": 1.273605227470398,
      "learning_rate": 3.366423999605815e-05,
      "loss": 2.1575,
      "step": 2220
    },
    {
      "epoch": 1.4867477912985498,
      "grad_norm": 1.5440280437469482,
      "learning_rate": 3.3575204714397055e-05,
      "loss": 2.1959,
      "step": 2230
    },
    {
      "epoch": 1.4934155692615436,
      "grad_norm": 1.688827395439148,
      "learning_rate": 3.348616943273597e-05,
      "loss": 2.2121,
      "step": 2240
    },
    {
      "epoch": 1.5000833472245374,
      "grad_norm": 1.3085063695907593,
      "learning_rate": 3.339713415107488e-05,
      "loss": 2.2371,
      "step": 2250
    },
    {
      "epoch": 1.5067511251875314,
      "grad_norm": 1.7031376361846924,
      "learning_rate": 3.3308098869413796e-05,
      "loss": 2.183,
      "step": 2260
    },
    {
      "epoch": 1.5134189031505252,
      "grad_norm": 1.3975138664245605,
      "learning_rate": 3.32190635877527e-05,
      "loss": 2.2614,
      "step": 2270
    },
    {
      "epoch": 1.520086681113519,
      "grad_norm": 1.456390142440796,
      "learning_rate": 3.313002830609161e-05,
      "loss": 2.1992,
      "step": 2280
    },
    {
      "epoch": 1.5267544590765127,
      "grad_norm": 1.6264803409576416,
      "learning_rate": 3.304099302443052e-05,
      "loss": 2.2017,
      "step": 2290
    },
    {
      "epoch": 1.5334222370395065,
      "grad_norm": 1.5358010530471802,
      "learning_rate": 3.2951957742769434e-05,
      "loss": 2.2129,
      "step": 2300
    },
    {
      "epoch": 1.5400900150025003,
      "grad_norm": 1.575211524963379,
      "learning_rate": 3.286292246110834e-05,
      "loss": 2.2524,
      "step": 2310
    },
    {
      "epoch": 1.546757792965494,
      "grad_norm": 1.4442371129989624,
      "learning_rate": 3.277388717944725e-05,
      "loss": 2.2199,
      "step": 2320
    },
    {
      "epoch": 1.553425570928488,
      "grad_norm": 1.198960304260254,
      "learning_rate": 3.268485189778616e-05,
      "loss": 2.2589,
      "step": 2330
    },
    {
      "epoch": 1.5600933488914819,
      "grad_norm": 1.585135817527771,
      "learning_rate": 3.259581661612507e-05,
      "loss": 2.2297,
      "step": 2340
    },
    {
      "epoch": 1.5667611268544759,
      "grad_norm": 1.4755988121032715,
      "learning_rate": 3.250678133446398e-05,
      "loss": 2.1882,
      "step": 2350
    },
    {
      "epoch": 1.5734289048174697,
      "grad_norm": 1.744168758392334,
      "learning_rate": 3.241774605280289e-05,
      "loss": 2.1957,
      "step": 2360
    },
    {
      "epoch": 1.5800966827804634,
      "grad_norm": 1.5212740898132324,
      "learning_rate": 3.23287107711418e-05,
      "loss": 2.1523,
      "step": 2370
    },
    {
      "epoch": 1.5867644607434572,
      "grad_norm": 1.6343281269073486,
      "learning_rate": 3.223967548948071e-05,
      "loss": 2.1873,
      "step": 2380
    },
    {
      "epoch": 1.593432238706451,
      "grad_norm": 1.998313546180725,
      "learning_rate": 3.215064020781962e-05,
      "loss": 2.2047,
      "step": 2390
    },
    {
      "epoch": 1.6001000166694448,
      "grad_norm": 1.4695706367492676,
      "learning_rate": 3.2061604926158526e-05,
      "loss": 2.2091,
      "step": 2400
    },
    {
      "epoch": 1.6067677946324388,
      "grad_norm": 1.5309290885925293,
      "learning_rate": 3.1972569644497444e-05,
      "loss": 2.1508,
      "step": 2410
    },
    {
      "epoch": 1.6134355725954326,
      "grad_norm": 2.270703077316284,
      "learning_rate": 3.188353436283635e-05,
      "loss": 2.1615,
      "step": 2420
    },
    {
      "epoch": 1.6201033505584264,
      "grad_norm": 1.5790281295776367,
      "learning_rate": 3.179449908117526e-05,
      "loss": 2.2141,
      "step": 2430
    },
    {
      "epoch": 1.6267711285214204,
      "grad_norm": 1.4226871728897095,
      "learning_rate": 3.170546379951417e-05,
      "loss": 2.2697,
      "step": 2440
    },
    {
      "epoch": 1.6334389064844141,
      "grad_norm": 1.4785215854644775,
      "learning_rate": 3.161642851785308e-05,
      "loss": 2.221,
      "step": 2450
    },
    {
      "epoch": 1.640106684447408,
      "grad_norm": 1.6819030046463013,
      "learning_rate": 3.1527393236191986e-05,
      "loss": 2.2221,
      "step": 2460
    },
    {
      "epoch": 1.6467744624104017,
      "grad_norm": 1.5797755718231201,
      "learning_rate": 3.14383579545309e-05,
      "loss": 2.2287,
      "step": 2470
    },
    {
      "epoch": 1.6534422403733955,
      "grad_norm": 1.3484996557235718,
      "learning_rate": 3.134932267286981e-05,
      "loss": 2.2222,
      "step": 2480
    },
    {
      "epoch": 1.6601100183363893,
      "grad_norm": 1.6085755825042725,
      "learning_rate": 3.126028739120871e-05,
      "loss": 2.1396,
      "step": 2490
    },
    {
      "epoch": 1.6667777962993833,
      "grad_norm": 1.4024525880813599,
      "learning_rate": 3.117125210954763e-05,
      "loss": 2.2188,
      "step": 2500
    },
    {
      "epoch": 1.673445574262377,
      "grad_norm": 1.3631782531738281,
      "learning_rate": 3.1082216827886536e-05,
      "loss": 2.2258,
      "step": 2510
    },
    {
      "epoch": 1.680113352225371,
      "grad_norm": 1.7956678867340088,
      "learning_rate": 3.0993181546225454e-05,
      "loss": 2.2348,
      "step": 2520
    },
    {
      "epoch": 1.6867811301883648,
      "grad_norm": 1.6374469995498657,
      "learning_rate": 3.090414626456436e-05,
      "loss": 2.1679,
      "step": 2530
    },
    {
      "epoch": 1.6934489081513586,
      "grad_norm": 1.6611255407333374,
      "learning_rate": 3.081511098290327e-05,
      "loss": 2.1867,
      "step": 2540
    },
    {
      "epoch": 1.7001166861143524,
      "grad_norm": 1.677927017211914,
      "learning_rate": 3.072607570124218e-05,
      "loss": 2.2407,
      "step": 2550
    },
    {
      "epoch": 1.7067844640773462,
      "grad_norm": 1.6454694271087646,
      "learning_rate": 3.063704041958109e-05,
      "loss": 2.1796,
      "step": 2560
    },
    {
      "epoch": 1.71345224204034,
      "grad_norm": 1.7972248792648315,
      "learning_rate": 3.0548005137919996e-05,
      "loss": 2.2336,
      "step": 2570
    },
    {
      "epoch": 1.7201200200033337,
      "grad_norm": 1.7770646810531616,
      "learning_rate": 3.045896985625891e-05,
      "loss": 2.2467,
      "step": 2580
    },
    {
      "epoch": 1.7267877979663278,
      "grad_norm": 1.7927213907241821,
      "learning_rate": 3.036993457459782e-05,
      "loss": 2.2467,
      "step": 2590
    },
    {
      "epoch": 1.7334555759293215,
      "grad_norm": 1.6905797719955444,
      "learning_rate": 3.0280899292936727e-05,
      "loss": 2.1681,
      "step": 2600
    },
    {
      "epoch": 1.7401233538923155,
      "grad_norm": 1.458919882774353,
      "learning_rate": 3.019186401127564e-05,
      "loss": 2.2114,
      "step": 2610
    },
    {
      "epoch": 1.7467911318553093,
      "grad_norm": 1.444825291633606,
      "learning_rate": 3.010282872961455e-05,
      "loss": 2.2044,
      "step": 2620
    },
    {
      "epoch": 1.753458909818303,
      "grad_norm": 1.6599290370941162,
      "learning_rate": 3.0013793447953457e-05,
      "loss": 2.209,
      "step": 2630
    },
    {
      "epoch": 1.7601266877812969,
      "grad_norm": 1.456992506980896,
      "learning_rate": 2.992475816629237e-05,
      "loss": 2.1489,
      "step": 2640
    },
    {
      "epoch": 1.7667944657442907,
      "grad_norm": 1.6971617937088013,
      "learning_rate": 2.983572288463128e-05,
      "loss": 2.1415,
      "step": 2650
    },
    {
      "epoch": 1.7734622437072844,
      "grad_norm": 1.8882774114608765,
      "learning_rate": 2.9746687602970187e-05,
      "loss": 2.2013,
      "step": 2660
    },
    {
      "epoch": 1.7801300216702782,
      "grad_norm": 1.8394920825958252,
      "learning_rate": 2.96576523213091e-05,
      "loss": 2.2141,
      "step": 2670
    },
    {
      "epoch": 1.7867977996332722,
      "grad_norm": 1.799304723739624,
      "learning_rate": 2.9568617039648006e-05,
      "loss": 2.159,
      "step": 2680
    },
    {
      "epoch": 1.793465577596266,
      "grad_norm": 1.808755874633789,
      "learning_rate": 2.9479581757986914e-05,
      "loss": 2.1416,
      "step": 2690
    },
    {
      "epoch": 1.80013335555926,
      "grad_norm": 1.5168309211730957,
      "learning_rate": 2.939054647632583e-05,
      "loss": 2.2019,
      "step": 2700
    },
    {
      "epoch": 1.8068011335222538,
      "grad_norm": 1.425829291343689,
      "learning_rate": 2.9301511194664737e-05,
      "loss": 2.2498,
      "step": 2710
    },
    {
      "epoch": 1.8134689114852476,
      "grad_norm": 2.2849740982055664,
      "learning_rate": 2.9212475913003644e-05,
      "loss": 2.1923,
      "step": 2720
    },
    {
      "epoch": 1.8201366894482414,
      "grad_norm": 2.068972110748291,
      "learning_rate": 2.912344063134256e-05,
      "loss": 2.1516,
      "step": 2730
    },
    {
      "epoch": 1.8268044674112351,
      "grad_norm": 1.9211584329605103,
      "learning_rate": 2.9034405349681467e-05,
      "loss": 2.1851,
      "step": 2740
    },
    {
      "epoch": 1.833472245374229,
      "grad_norm": 2.164759397506714,
      "learning_rate": 2.8945370068020375e-05,
      "loss": 2.1965,
      "step": 2750
    },
    {
      "epoch": 1.8401400233372227,
      "grad_norm": 1.8798962831497192,
      "learning_rate": 2.885633478635929e-05,
      "loss": 2.1826,
      "step": 2760
    },
    {
      "epoch": 1.8468078013002167,
      "grad_norm": 1.5864267349243164,
      "learning_rate": 2.8767299504698197e-05,
      "loss": 2.2215,
      "step": 2770
    },
    {
      "epoch": 1.8534755792632105,
      "grad_norm": 1.9529054164886475,
      "learning_rate": 2.8678264223037112e-05,
      "loss": 2.1712,
      "step": 2780
    },
    {
      "epoch": 1.8601433572262045,
      "grad_norm": 1.7078752517700195,
      "learning_rate": 2.858922894137602e-05,
      "loss": 2.1356,
      "step": 2790
    },
    {
      "epoch": 1.8668111351891983,
      "grad_norm": 1.8765003681182861,
      "learning_rate": 2.8500193659714927e-05,
      "loss": 2.199,
      "step": 2800
    },
    {
      "epoch": 1.873478913152192,
      "grad_norm": 1.8571791648864746,
      "learning_rate": 2.841115837805384e-05,
      "loss": 2.2022,
      "step": 2810
    },
    {
      "epoch": 1.8801466911151858,
      "grad_norm": 1.5152251720428467,
      "learning_rate": 2.8322123096392747e-05,
      "loss": 2.217,
      "step": 2820
    },
    {
      "epoch": 1.8868144690781796,
      "grad_norm": 1.8603178262710571,
      "learning_rate": 2.8233087814731654e-05,
      "loss": 2.2025,
      "step": 2830
    },
    {
      "epoch": 1.8934822470411734,
      "grad_norm": 1.6323981285095215,
      "learning_rate": 2.814405253307057e-05,
      "loss": 2.1435,
      "step": 2840
    },
    {
      "epoch": 1.9001500250041674,
      "grad_norm": 2.03884220123291,
      "learning_rate": 2.8055017251409477e-05,
      "loss": 2.1526,
      "step": 2850
    },
    {
      "epoch": 1.9068178029671612,
      "grad_norm": 1.7488632202148438,
      "learning_rate": 2.7965981969748385e-05,
      "loss": 2.1857,
      "step": 2860
    },
    {
      "epoch": 1.913485580930155,
      "grad_norm": 1.6617600917816162,
      "learning_rate": 2.78769466880873e-05,
      "loss": 2.1638,
      "step": 2870
    },
    {
      "epoch": 1.920153358893149,
      "grad_norm": 1.4416327476501465,
      "learning_rate": 2.7787911406426207e-05,
      "loss": 2.1659,
      "step": 2880
    },
    {
      "epoch": 1.9268211368561428,
      "grad_norm": 1.7928160429000854,
      "learning_rate": 2.7698876124765115e-05,
      "loss": 2.1867,
      "step": 2890
    },
    {
      "epoch": 1.9334889148191365,
      "grad_norm": 1.5604350566864014,
      "learning_rate": 2.760984084310403e-05,
      "loss": 2.1628,
      "step": 2900
    },
    {
      "epoch": 1.9401566927821303,
      "grad_norm": 1.834774374961853,
      "learning_rate": 2.7520805561442937e-05,
      "loss": 2.1925,
      "step": 2910
    },
    {
      "epoch": 1.946824470745124,
      "grad_norm": 1.628785490989685,
      "learning_rate": 2.7431770279781845e-05,
      "loss": 2.1075,
      "step": 2920
    },
    {
      "epoch": 1.9534922487081179,
      "grad_norm": 1.5539000034332275,
      "learning_rate": 2.734273499812076e-05,
      "loss": 2.2452,
      "step": 2930
    },
    {
      "epoch": 1.9601600266711119,
      "grad_norm": 1.6458584070205688,
      "learning_rate": 2.7253699716459668e-05,
      "loss": 2.1541,
      "step": 2940
    },
    {
      "epoch": 1.9668278046341057,
      "grad_norm": 2.189310312271118,
      "learning_rate": 2.7164664434798576e-05,
      "loss": 2.1867,
      "step": 2950
    },
    {
      "epoch": 1.9734955825970997,
      "grad_norm": 3.2792131900787354,
      "learning_rate": 2.7075629153137487e-05,
      "loss": 2.1872,
      "step": 2960
    },
    {
      "epoch": 1.9801633605600935,
      "grad_norm": 2.019238233566284,
      "learning_rate": 2.6986593871476395e-05,
      "loss": 2.1784,
      "step": 2970
    },
    {
      "epoch": 1.9868311385230872,
      "grad_norm": 1.6982150077819824,
      "learning_rate": 2.6897558589815302e-05,
      "loss": 2.1248,
      "step": 2980
    },
    {
      "epoch": 1.993498916486081,
      "grad_norm": 1.5424810647964478,
      "learning_rate": 2.6808523308154217e-05,
      "loss": 2.143,
      "step": 2990
    },
    {
      "epoch": 2.0,
      "grad_norm": 2.372469425201416,
      "learning_rate": 2.6719488026493125e-05,
      "loss": 2.1658,
      "step": 3000
    },
    {
      "epoch": 2.0,
      "eval_loss": 2.188035011291504,
      "eval_runtime": 939.3971,
      "eval_samples_per_second": 6.386,
      "eval_steps_per_second": 3.194,
      "step": 3000
    },
    {
      "epoch": 2.006667777962994,
      "grad_norm": 1.9255990982055664,
      "learning_rate": 2.6630452744832036e-05,
      "loss": 2.1551,
      "step": 3010
    },
    {
      "epoch": 2.0133355559259876,
      "grad_norm": 1.981541633605957,
      "learning_rate": 2.6541417463170947e-05,
      "loss": 2.1464,
      "step": 3020
    },
    {
      "epoch": 2.0200033338889813,
      "grad_norm": 2.10602068901062,
      "learning_rate": 2.6452382181509855e-05,
      "loss": 2.1772,
      "step": 3030
    },
    {
      "epoch": 2.026671111851975,
      "grad_norm": 2.9281082153320312,
      "learning_rate": 2.6363346899848766e-05,
      "loss": 2.1708,
      "step": 3040
    },
    {
      "epoch": 2.0333388898149694,
      "grad_norm": 1.9071269035339355,
      "learning_rate": 2.6274311618187678e-05,
      "loss": 2.1381,
      "step": 3050
    },
    {
      "epoch": 2.040006667777963,
      "grad_norm": 1.6338697671890259,
      "learning_rate": 2.6185276336526586e-05,
      "loss": 2.146,
      "step": 3060
    },
    {
      "epoch": 2.046674445740957,
      "grad_norm": 2.0579991340637207,
      "learning_rate": 2.6096241054865497e-05,
      "loss": 2.1246,
      "step": 3070
    },
    {
      "epoch": 2.0533422237039507,
      "grad_norm": 2.3000741004943848,
      "learning_rate": 2.6007205773204408e-05,
      "loss": 2.1843,
      "step": 3080
    },
    {
      "epoch": 2.0600100016669445,
      "grad_norm": 2.02638578414917,
      "learning_rate": 2.5918170491543316e-05,
      "loss": 2.1664,
      "step": 3090
    },
    {
      "epoch": 2.0666777796299383,
      "grad_norm": 2.0794782638549805,
      "learning_rate": 2.5829135209882224e-05,
      "loss": 2.1883,
      "step": 3100
    },
    {
      "epoch": 2.073345557592932,
      "grad_norm": 1.869168996810913,
      "learning_rate": 2.5740099928221135e-05,
      "loss": 2.1938,
      "step": 3110
    },
    {
      "epoch": 2.080013335555926,
      "grad_norm": 2.551143169403076,
      "learning_rate": 2.5651064646560046e-05,
      "loss": 2.1673,
      "step": 3120
    },
    {
      "epoch": 2.0866811135189196,
      "grad_norm": 2.300164222717285,
      "learning_rate": 2.5562029364898954e-05,
      "loss": 2.063,
      "step": 3130
    },
    {
      "epoch": 2.093348891481914,
      "grad_norm": 2.349815845489502,
      "learning_rate": 2.5472994083237865e-05,
      "loss": 2.1602,
      "step": 3140
    },
    {
      "epoch": 2.1000166694449076,
      "grad_norm": 1.7666813135147095,
      "learning_rate": 2.5383958801576776e-05,
      "loss": 2.1678,
      "step": 3150
    },
    {
      "epoch": 2.1066844474079014,
      "grad_norm": 2.8656935691833496,
      "learning_rate": 2.5294923519915684e-05,
      "loss": 2.1381,
      "step": 3160
    },
    {
      "epoch": 2.113352225370895,
      "grad_norm": 1.66599440574646,
      "learning_rate": 2.5205888238254595e-05,
      "loss": 2.1422,
      "step": 3170
    },
    {
      "epoch": 2.120020003333889,
      "grad_norm": 1.7330068349838257,
      "learning_rate": 2.5116852956593507e-05,
      "loss": 2.106,
      "step": 3180
    },
    {
      "epoch": 2.1266877812968827,
      "grad_norm": 2.5626885890960693,
      "learning_rate": 2.5027817674932418e-05,
      "loss": 2.1273,
      "step": 3190
    },
    {
      "epoch": 2.1333555592598765,
      "grad_norm": 2.3004891872406006,
      "learning_rate": 2.4938782393271326e-05,
      "loss": 2.124,
      "step": 3200
    },
    {
      "epoch": 2.1400233372228703,
      "grad_norm": 1.908067226409912,
      "learning_rate": 2.4849747111610237e-05,
      "loss": 2.1273,
      "step": 3210
    },
    {
      "epoch": 2.146691115185864,
      "grad_norm": 1.9202094078063965,
      "learning_rate": 2.4760711829949145e-05,
      "loss": 2.1311,
      "step": 3220
    },
    {
      "epoch": 2.1533588931488583,
      "grad_norm": 1.7010750770568848,
      "learning_rate": 2.4671676548288053e-05,
      "loss": 2.1803,
      "step": 3230
    },
    {
      "epoch": 2.160026671111852,
      "grad_norm": 1.96953547000885,
      "learning_rate": 2.4582641266626964e-05,
      "loss": 2.1662,
      "step": 3240
    },
    {
      "epoch": 2.166694449074846,
      "grad_norm": 1.945749044418335,
      "learning_rate": 2.4493605984965875e-05,
      "loss": 2.1389,
      "step": 3250
    },
    {
      "epoch": 2.1733622270378397,
      "grad_norm": 2.1389365196228027,
      "learning_rate": 2.4404570703304783e-05,
      "loss": 2.2066,
      "step": 3260
    },
    {
      "epoch": 2.1800300050008334,
      "grad_norm": 1.6795276403427124,
      "learning_rate": 2.4315535421643694e-05,
      "loss": 2.145,
      "step": 3270
    },
    {
      "epoch": 2.186697782963827,
      "grad_norm": 1.825835943222046,
      "learning_rate": 2.4226500139982605e-05,
      "loss": 2.1689,
      "step": 3280
    },
    {
      "epoch": 2.193365560926821,
      "grad_norm": 2.161410331726074,
      "learning_rate": 2.4137464858321513e-05,
      "loss": 2.1606,
      "step": 3290
    },
    {
      "epoch": 2.200033338889815,
      "grad_norm": 2.0709474086761475,
      "learning_rate": 2.4048429576660425e-05,
      "loss": 2.1711,
      "step": 3300
    },
    {
      "epoch": 2.206701116852809,
      "grad_norm": 2.1694600582122803,
      "learning_rate": 2.3959394294999336e-05,
      "loss": 2.183,
      "step": 3310
    },
    {
      "epoch": 2.213368894815803,
      "grad_norm": 2.1888561248779297,
      "learning_rate": 2.3870359013338247e-05,
      "loss": 2.1734,
      "step": 3320
    },
    {
      "epoch": 2.2200366727787966,
      "grad_norm": 3.587113618850708,
      "learning_rate": 2.3781323731677155e-05,
      "loss": 2.1144,
      "step": 3330
    },
    {
      "epoch": 2.2267044507417904,
      "grad_norm": 2.184375047683716,
      "learning_rate": 2.3692288450016066e-05,
      "loss": 2.0834,
      "step": 3340
    },
    {
      "epoch": 2.233372228704784,
      "grad_norm": 2.1231160163879395,
      "learning_rate": 2.3603253168354974e-05,
      "loss": 2.1501,
      "step": 3350
    },
    {
      "epoch": 2.240040006667778,
      "grad_norm": 2.247408866882324,
      "learning_rate": 2.3514217886693885e-05,
      "loss": 2.185,
      "step": 3360
    },
    {
      "epoch": 2.2467077846307717,
      "grad_norm": 2.5361621379852295,
      "learning_rate": 2.3425182605032793e-05,
      "loss": 2.1488,
      "step": 3370
    },
    {
      "epoch": 2.2533755625937655,
      "grad_norm": 2.093362331390381,
      "learning_rate": 2.3336147323371704e-05,
      "loss": 2.1609,
      "step": 3380
    },
    {
      "epoch": 2.2600433405567593,
      "grad_norm": 1.7714159488677979,
      "learning_rate": 2.3247112041710612e-05,
      "loss": 2.1414,
      "step": 3390
    },
    {
      "epoch": 2.266711118519753,
      "grad_norm": 3.211226463317871,
      "learning_rate": 2.3158076760049523e-05,
      "loss": 2.1555,
      "step": 3400
    },
    {
      "epoch": 2.2733788964827473,
      "grad_norm": 1.7183207273483276,
      "learning_rate": 2.3069041478388435e-05,
      "loss": 2.2168,
      "step": 3410
    },
    {
      "epoch": 2.280046674445741,
      "grad_norm": 2.1618404388427734,
      "learning_rate": 2.2980006196727342e-05,
      "loss": 2.1613,
      "step": 3420
    },
    {
      "epoch": 2.286714452408735,
      "grad_norm": 2.1197047233581543,
      "learning_rate": 2.2890970915066254e-05,
      "loss": 2.2314,
      "step": 3430
    },
    {
      "epoch": 2.2933822303717286,
      "grad_norm": 1.7946052551269531,
      "learning_rate": 2.2801935633405165e-05,
      "loss": 2.1809,
      "step": 3440
    },
    {
      "epoch": 2.3000500083347224,
      "grad_norm": 2.1214540004730225,
      "learning_rate": 2.2712900351744076e-05,
      "loss": 2.1652,
      "step": 3450
    },
    {
      "epoch": 2.306717786297716,
      "grad_norm": 2.119075059890747,
      "learning_rate": 2.2623865070082984e-05,
      "loss": 2.1397,
      "step": 3460
    },
    {
      "epoch": 2.31338556426071,
      "grad_norm": 4.844588279724121,
      "learning_rate": 2.2534829788421895e-05,
      "loss": 2.2313,
      "step": 3470
    },
    {
      "epoch": 2.320053342223704,
      "grad_norm": 2.127519130706787,
      "learning_rate": 2.2445794506760806e-05,
      "loss": 2.1143,
      "step": 3480
    },
    {
      "epoch": 2.326721120186698,
      "grad_norm": 2.044248580932617,
      "learning_rate": 2.2356759225099714e-05,
      "loss": 2.2076,
      "step": 3490
    },
    {
      "epoch": 2.3333888981496917,
      "grad_norm": 2.631972312927246,
      "learning_rate": 2.2267723943438622e-05,
      "loss": 2.1761,
      "step": 3500
    },
    {
      "epoch": 2.3400566761126855,
      "grad_norm": 3.0298664569854736,
      "learning_rate": 2.2178688661777533e-05,
      "loss": 2.1439,
      "step": 3510
    },
    {
      "epoch": 2.3467244540756793,
      "grad_norm": 2.579242706298828,
      "learning_rate": 2.208965338011644e-05,
      "loss": 2.1174,
      "step": 3520
    },
    {
      "epoch": 2.353392232038673,
      "grad_norm": 2.0333664417266846,
      "learning_rate": 2.2000618098455352e-05,
      "loss": 2.1915,
      "step": 3530
    },
    {
      "epoch": 2.360060010001667,
      "grad_norm": 2.664890766143799,
      "learning_rate": 2.1911582816794264e-05,
      "loss": 2.1546,
      "step": 3540
    },
    {
      "epoch": 2.3667277879646607,
      "grad_norm": 2.430971145629883,
      "learning_rate": 2.182254753513317e-05,
      "loss": 2.1439,
      "step": 3550
    },
    {
      "epoch": 2.3733955659276544,
      "grad_norm": 2.0492401123046875,
      "learning_rate": 2.1733512253472083e-05,
      "loss": 2.141,
      "step": 3560
    },
    {
      "epoch": 2.380063343890648,
      "grad_norm": 2.6440885066986084,
      "learning_rate": 2.1644476971810994e-05,
      "loss": 2.102,
      "step": 3570
    },
    {
      "epoch": 2.3867311218536424,
      "grad_norm": 1.8140525817871094,
      "learning_rate": 2.1555441690149905e-05,
      "loss": 2.1394,
      "step": 3580
    },
    {
      "epoch": 2.3933988998166362,
      "grad_norm": 1.9638854265213013,
      "learning_rate": 2.1466406408488813e-05,
      "loss": 2.1595,
      "step": 3590
    },
    {
      "epoch": 2.40006667777963,
      "grad_norm": 2.321112632751465,
      "learning_rate": 2.1377371126827724e-05,
      "loss": 2.0669,
      "step": 3600
    },
    {
      "epoch": 2.406734455742624,
      "grad_norm": 2.351421356201172,
      "learning_rate": 2.1288335845166635e-05,
      "loss": 2.1284,
      "step": 3610
    },
    {
      "epoch": 2.4134022337056176,
      "grad_norm": 4.046978950500488,
      "learning_rate": 2.1199300563505543e-05,
      "loss": 2.146,
      "step": 3620
    },
    {
      "epoch": 2.4200700116686114,
      "grad_norm": 2.7112319469451904,
      "learning_rate": 2.111026528184445e-05,
      "loss": 2.1822,
      "step": 3630
    },
    {
      "epoch": 2.426737789631605,
      "grad_norm": 2.7413978576660156,
      "learning_rate": 2.1021230000183362e-05,
      "loss": 2.1936,
      "step": 3640
    },
    {
      "epoch": 2.433405567594599,
      "grad_norm": 2.257240056991577,
      "learning_rate": 2.093219471852227e-05,
      "loss": 2.0899,
      "step": 3650
    },
    {
      "epoch": 2.440073345557593,
      "grad_norm": 1.6664777994155884,
      "learning_rate": 2.084315943686118e-05,
      "loss": 2.1492,
      "step": 3660
    },
    {
      "epoch": 2.446741123520587,
      "grad_norm": 2.1591477394104004,
      "learning_rate": 2.0754124155200093e-05,
      "loss": 2.1754,
      "step": 3670
    },
    {
      "epoch": 2.4534089014835807,
      "grad_norm": 1.7469899654388428,
      "learning_rate": 2.0665088873539e-05,
      "loss": 2.1757,
      "step": 3680
    },
    {
      "epoch": 2.4600766794465745,
      "grad_norm": 1.913163185119629,
      "learning_rate": 2.057605359187791e-05,
      "loss": 2.1382,
      "step": 3690
    },
    {
      "epoch": 2.4667444574095683,
      "grad_norm": 2.78798770904541,
      "learning_rate": 2.0487018310216823e-05,
      "loss": 2.1223,
      "step": 3700
    },
    {
      "epoch": 2.473412235372562,
      "grad_norm": 1.850332260131836,
      "learning_rate": 2.0397983028555734e-05,
      "loss": 2.1817,
      "step": 3710
    },
    {
      "epoch": 2.480080013335556,
      "grad_norm": 2.6501166820526123,
      "learning_rate": 2.0308947746894642e-05,
      "loss": 2.139,
      "step": 3720
    },
    {
      "epoch": 2.4867477912985496,
      "grad_norm": 2.442096710205078,
      "learning_rate": 2.0219912465233553e-05,
      "loss": 2.1578,
      "step": 3730
    },
    {
      "epoch": 2.4934155692615434,
      "grad_norm": 3.144435405731201,
      "learning_rate": 2.0130877183572464e-05,
      "loss": 2.1508,
      "step": 3740
    },
    {
      "epoch": 2.500083347224537,
      "grad_norm": 2.4610729217529297,
      "learning_rate": 2.0041841901911372e-05,
      "loss": 2.1169,
      "step": 3750
    },
    {
      "epoch": 2.5067511251875314,
      "grad_norm": 2.4419145584106445,
      "learning_rate": 1.9952806620250283e-05,
      "loss": 2.1165,
      "step": 3760
    },
    {
      "epoch": 2.513418903150525,
      "grad_norm": 2.3662428855895996,
      "learning_rate": 1.986377133858919e-05,
      "loss": 2.148,
      "step": 3770
    },
    {
      "epoch": 2.520086681113519,
      "grad_norm": 2.031733751296997,
      "learning_rate": 1.97747360569281e-05,
      "loss": 2.1154,
      "step": 3780
    },
    {
      "epoch": 2.5267544590765127,
      "grad_norm": 2.6138486862182617,
      "learning_rate": 1.968570077526701e-05,
      "loss": 2.1878,
      "step": 3790
    },
    {
      "epoch": 2.5334222370395065,
      "grad_norm": 2.7713589668273926,
      "learning_rate": 1.959666549360592e-05,
      "loss": 2.1383,
      "step": 3800
    },
    {
      "epoch": 2.5400900150025003,
      "grad_norm": 1.5630453824996948,
      "learning_rate": 1.950763021194483e-05,
      "loss": 2.1551,
      "step": 3810
    },
    {
      "epoch": 2.546757792965494,
      "grad_norm": 2.4921534061431885,
      "learning_rate": 1.941859493028374e-05,
      "loss": 2.1496,
      "step": 3820
    },
    {
      "epoch": 2.5534255709284883,
      "grad_norm": 2.6589252948760986,
      "learning_rate": 1.9329559648622652e-05,
      "loss": 2.1533,
      "step": 3830
    },
    {
      "epoch": 2.560093348891482,
      "grad_norm": 2.4741644859313965,
      "learning_rate": 1.9240524366961563e-05,
      "loss": 2.1502,
      "step": 3840
    },
    {
      "epoch": 2.566761126854476,
      "grad_norm": 2.62388014793396,
      "learning_rate": 1.915148908530047e-05,
      "loss": 2.1287,
      "step": 3850
    },
    {
      "epoch": 2.5734289048174697,
      "grad_norm": 2.546900510787964,
      "learning_rate": 1.9062453803639382e-05,
      "loss": 2.0909,
      "step": 3860
    },
    {
      "epoch": 2.5800966827804634,
      "grad_norm": 3.302018165588379,
      "learning_rate": 1.8973418521978293e-05,
      "loss": 2.1768,
      "step": 3870
    },
    {
      "epoch": 2.5867644607434572,
      "grad_norm": 1.7780513763427734,
      "learning_rate": 1.88843832403172e-05,
      "loss": 2.1439,
      "step": 3880
    },
    {
      "epoch": 2.593432238706451,
      "grad_norm": 3.0757339000701904,
      "learning_rate": 1.8795347958656113e-05,
      "loss": 2.1213,
      "step": 3890
    },
    {
      "epoch": 2.600100016669445,
      "grad_norm": 3.4334299564361572,
      "learning_rate": 1.870631267699502e-05,
      "loss": 2.0827,
      "step": 3900
    },
    {
      "epoch": 2.6067677946324386,
      "grad_norm": 2.482170343399048,
      "learning_rate": 1.8617277395333928e-05,
      "loss": 2.1084,
      "step": 3910
    },
    {
      "epoch": 2.6134355725954324,
      "grad_norm": 2.6080682277679443,
      "learning_rate": 1.852824211367284e-05,
      "loss": 2.1662,
      "step": 3920
    },
    {
      "epoch": 2.620103350558426,
      "grad_norm": 1.8362993001937866,
      "learning_rate": 1.843920683201175e-05,
      "loss": 2.1304,
      "step": 3930
    },
    {
      "epoch": 2.6267711285214204,
      "grad_norm": 2.1971633434295654,
      "learning_rate": 1.8350171550350662e-05,
      "loss": 2.1583,
      "step": 3940
    },
    {
      "epoch": 2.633438906484414,
      "grad_norm": 3.0807113647460938,
      "learning_rate": 1.826113626868957e-05,
      "loss": 2.1271,
      "step": 3950
    },
    {
      "epoch": 2.640106684447408,
      "grad_norm": 2.1237146854400635,
      "learning_rate": 1.817210098702848e-05,
      "loss": 2.1112,
      "step": 3960
    },
    {
      "epoch": 2.6467744624104017,
      "grad_norm": 2.5836124420166016,
      "learning_rate": 1.8083065705367392e-05,
      "loss": 2.0883,
      "step": 3970
    },
    {
      "epoch": 2.6534422403733955,
      "grad_norm": 1.9585498571395874,
      "learning_rate": 1.79940304237063e-05,
      "loss": 2.1023,
      "step": 3980
    },
    {
      "epoch": 2.6601100183363893,
      "grad_norm": 2.1693296432495117,
      "learning_rate": 1.790499514204521e-05,
      "loss": 2.073,
      "step": 3990
    },
    {
      "epoch": 2.6667777962993835,
      "grad_norm": 2.6279730796813965,
      "learning_rate": 1.7815959860384123e-05,
      "loss": 2.1189,
      "step": 4000
    },
    {
      "epoch": 2.6734455742623773,
      "grad_norm": 2.5279183387756348,
      "learning_rate": 1.772692457872303e-05,
      "loss": 2.1226,
      "step": 4010
    },
    {
      "epoch": 2.680113352225371,
      "grad_norm": 1.9541783332824707,
      "learning_rate": 1.763788929706194e-05,
      "loss": 2.0909,
      "step": 4020
    },
    {
      "epoch": 2.686781130188365,
      "grad_norm": 2.811208724975586,
      "learning_rate": 1.7548854015400853e-05,
      "loss": 2.1725,
      "step": 4030
    },
    {
      "epoch": 2.6934489081513586,
      "grad_norm": 3.478929042816162,
      "learning_rate": 1.745981873373976e-05,
      "loss": 2.1785,
      "step": 4040
    },
    {
      "epoch": 2.7001166861143524,
      "grad_norm": 2.480602502822876,
      "learning_rate": 1.737078345207867e-05,
      "loss": 2.1877,
      "step": 4050
    },
    {
      "epoch": 2.706784464077346,
      "grad_norm": 2.397202491760254,
      "learning_rate": 1.728174817041758e-05,
      "loss": 2.1369,
      "step": 4060
    },
    {
      "epoch": 2.71345224204034,
      "grad_norm": 3.5191056728363037,
      "learning_rate": 1.719271288875649e-05,
      "loss": 2.148,
      "step": 4070
    },
    {
      "epoch": 2.7201200200033337,
      "grad_norm": 2.593616247177124,
      "learning_rate": 1.71036776070954e-05,
      "loss": 2.1395,
      "step": 4080
    },
    {
      "epoch": 2.7267877979663275,
      "grad_norm": 1.9030652046203613,
      "learning_rate": 1.701464232543431e-05,
      "loss": 2.0942,
      "step": 4090
    },
    {
      "epoch": 2.7334555759293213,
      "grad_norm": 2.3775901794433594,
      "learning_rate": 1.692560704377322e-05,
      "loss": 2.1103,
      "step": 4100
    },
    {
      "epoch": 2.7401233538923155,
      "grad_norm": 3.1724648475646973,
      "learning_rate": 1.683657176211213e-05,
      "loss": 2.1189,
      "step": 4110
    },
    {
      "epoch": 2.7467911318553093,
      "grad_norm": 2.6759724617004395,
      "learning_rate": 1.674753648045104e-05,
      "loss": 2.1312,
      "step": 4120
    },
    {
      "epoch": 2.753458909818303,
      "grad_norm": 2.301874876022339,
      "learning_rate": 1.665850119878995e-05,
      "loss": 2.0956,
      "step": 4130
    },
    {
      "epoch": 2.760126687781297,
      "grad_norm": 1.865451455116272,
      "learning_rate": 1.656946591712886e-05,
      "loss": 2.1027,
      "step": 4140
    },
    {
      "epoch": 2.7667944657442907,
      "grad_norm": 4.077884674072266,
      "learning_rate": 1.648043063546777e-05,
      "loss": 2.0933,
      "step": 4150
    },
    {
      "epoch": 2.7734622437072844,
      "grad_norm": 2.4048209190368652,
      "learning_rate": 1.6391395353806682e-05,
      "loss": 2.1021,
      "step": 4160
    },
    {
      "epoch": 2.7801300216702782,
      "grad_norm": 2.4160540103912354,
      "learning_rate": 1.630236007214559e-05,
      "loss": 2.1322,
      "step": 4170
    },
    {
      "epoch": 2.7867977996332725,
      "grad_norm": 2.7040929794311523,
      "learning_rate": 1.6213324790484498e-05,
      "loss": 2.1552,
      "step": 4180
    },
    {
      "epoch": 2.7934655775962662,
      "grad_norm": 3.821690320968628,
      "learning_rate": 1.612428950882341e-05,
      "loss": 2.1015,
      "step": 4190
    },
    {
      "epoch": 2.80013335555926,
      "grad_norm": 2.319894552230835,
      "learning_rate": 1.603525422716232e-05,
      "loss": 2.0801,
      "step": 4200
    },
    {
      "epoch": 2.806801133522254,
      "grad_norm": 2.620816707611084,
      "learning_rate": 1.5946218945501228e-05,
      "loss": 2.1508,
      "step": 4210
    },
    {
      "epoch": 2.8134689114852476,
      "grad_norm": 4.058202743530273,
      "learning_rate": 1.585718366384014e-05,
      "loss": 2.1122,
      "step": 4220
    },
    {
      "epoch": 2.8201366894482414,
      "grad_norm": 2.9048233032226562,
      "learning_rate": 1.576814838217905e-05,
      "loss": 2.1147,
      "step": 4230
    },
    {
      "epoch": 2.826804467411235,
      "grad_norm": 3.970208168029785,
      "learning_rate": 1.5679113100517958e-05,
      "loss": 2.078,
      "step": 4240
    },
    {
      "epoch": 2.833472245374229,
      "grad_norm": 2.1849827766418457,
      "learning_rate": 1.559007781885687e-05,
      "loss": 2.0621,
      "step": 4250
    },
    {
      "epoch": 2.8401400233372227,
      "grad_norm": 2.643521785736084,
      "learning_rate": 1.550104253719578e-05,
      "loss": 2.0602,
      "step": 4260
    },
    {
      "epoch": 2.8468078013002165,
      "grad_norm": 3.1120858192443848,
      "learning_rate": 1.541200725553469e-05,
      "loss": 2.1258,
      "step": 4270
    },
    {
      "epoch": 2.8534755792632103,
      "grad_norm": 3.0604162216186523,
      "learning_rate": 1.53229719738736e-05,
      "loss": 2.0477,
      "step": 4280
    },
    {
      "epoch": 2.8601433572262045,
      "grad_norm": 2.0270495414733887,
      "learning_rate": 1.523393669221251e-05,
      "loss": 2.0533,
      "step": 4290
    },
    {
      "epoch": 2.8668111351891983,
      "grad_norm": 3.630703926086426,
      "learning_rate": 1.5144901410551417e-05,
      "loss": 2.1246,
      "step": 4300
    },
    {
      "epoch": 2.873478913152192,
      "grad_norm": 2.4935598373413086,
      "learning_rate": 1.5055866128890328e-05,
      "loss": 2.1623,
      "step": 4310
    },
    {
      "epoch": 2.880146691115186,
      "grad_norm": 2.438961982727051,
      "learning_rate": 1.496683084722924e-05,
      "loss": 2.113,
      "step": 4320
    },
    {
      "epoch": 2.8868144690781796,
      "grad_norm": 2.071054220199585,
      "learning_rate": 1.487779556556815e-05,
      "loss": 2.0969,
      "step": 4330
    },
    {
      "epoch": 2.8934822470411734,
      "grad_norm": 3.0303683280944824,
      "learning_rate": 1.4788760283907059e-05,
      "loss": 2.1001,
      "step": 4340
    },
    {
      "epoch": 2.9001500250041676,
      "grad_norm": 2.359844207763672,
      "learning_rate": 1.4699725002245968e-05,
      "loss": 2.065,
      "step": 4350
    },
    {
      "epoch": 2.9068178029671614,
      "grad_norm": 2.742842197418213,
      "learning_rate": 1.461068972058488e-05,
      "loss": 2.1099,
      "step": 4360
    },
    {
      "epoch": 2.913485580930155,
      "grad_norm": 2.6607706546783447,
      "learning_rate": 1.4521654438923787e-05,
      "loss": 2.1185,
      "step": 4370
    },
    {
      "epoch": 2.920153358893149,
      "grad_norm": 2.3679275512695312,
      "learning_rate": 1.4432619157262698e-05,
      "loss": 2.1097,
      "step": 4380
    },
    {
      "epoch": 2.9268211368561428,
      "grad_norm": 1.68625807762146,
      "learning_rate": 1.434358387560161e-05,
      "loss": 2.1209,
      "step": 4390
    },
    {
      "epoch": 2.9334889148191365,
      "grad_norm": 1.9240446090698242,
      "learning_rate": 1.4254548593940517e-05,
      "loss": 2.0731,
      "step": 4400
    },
    {
      "epoch": 2.9401566927821303,
      "grad_norm": 2.2135465145111084,
      "learning_rate": 1.4165513312279429e-05,
      "loss": 2.1643,
      "step": 4410
    },
    {
      "epoch": 2.946824470745124,
      "grad_norm": 2.1026487350463867,
      "learning_rate": 1.4076478030618338e-05,
      "loss": 2.1092,
      "step": 4420
    },
    {
      "epoch": 2.953492248708118,
      "grad_norm": 3.1771810054779053,
      "learning_rate": 1.3987442748957246e-05,
      "loss": 2.1688,
      "step": 4430
    },
    {
      "epoch": 2.9601600266711117,
      "grad_norm": 3.4878711700439453,
      "learning_rate": 1.3898407467296157e-05,
      "loss": 2.1058,
      "step": 4440
    },
    {
      "epoch": 2.9668278046341054,
      "grad_norm": 2.9518308639526367,
      "learning_rate": 1.3809372185635069e-05,
      "loss": 2.175,
      "step": 4450
    },
    {
      "epoch": 2.9734955825970997,
      "grad_norm": 2.887970447540283,
      "learning_rate": 1.372033690397398e-05,
      "loss": 2.1417,
      "step": 4460
    },
    {
      "epoch": 2.9801633605600935,
      "grad_norm": 4.869598388671875,
      "learning_rate": 1.3631301622312888e-05,
      "loss": 2.0972,
      "step": 4470
    },
    {
      "epoch": 2.9868311385230872,
      "grad_norm": 2.991687297821045,
      "learning_rate": 1.3542266340651797e-05,
      "loss": 2.1259,
      "step": 4480
    },
    {
      "epoch": 2.993498916486081,
      "grad_norm": 3.700082778930664,
      "learning_rate": 1.3453231058990708e-05,
      "loss": 2.1225,
      "step": 4490
    },
    {
      "epoch": 3.0,
      "grad_norm": 2.4385995864868164,
      "learning_rate": 1.3364195777329616e-05,
      "loss": 2.1941,
      "step": 4500
    },
    {
      "epoch": 3.0,
      "eval_loss": 2.137887716293335,
      "eval_runtime": 939.9018,
      "eval_samples_per_second": 6.383,
      "eval_steps_per_second": 3.192,
      "step": 4500
    },
    {
      "epoch": 3.006667777962994,
      "grad_norm": 2.82104754447937,
      "learning_rate": 1.3275160495668527e-05,
      "loss": 2.0978,
      "step": 4510
    },
    {
      "epoch": 3.0133355559259876,
      "grad_norm": 2.611823558807373,
      "learning_rate": 1.3186125214007437e-05,
      "loss": 2.1307,
      "step": 4520
    },
    {
      "epoch": 3.0200033338889813,
      "grad_norm": 2.1636977195739746,
      "learning_rate": 1.3097089932346348e-05,
      "loss": 2.1576,
      "step": 4530
    },
    {
      "epoch": 3.026671111851975,
      "grad_norm": 2.314177989959717,
      "learning_rate": 1.3008054650685258e-05,
      "loss": 2.0826,
      "step": 4540
    },
    {
      "epoch": 3.0333388898149694,
      "grad_norm": 3.580280065536499,
      "learning_rate": 1.2919019369024167e-05,
      "loss": 2.1036,
      "step": 4550
    },
    {
      "epoch": 3.040006667777963,
      "grad_norm": 2.7206108570098877,
      "learning_rate": 1.2829984087363077e-05,
      "loss": 2.0707,
      "step": 4560
    },
    {
      "epoch": 3.046674445740957,
      "grad_norm": 2.3329570293426514,
      "learning_rate": 1.2740948805701986e-05,
      "loss": 2.1362,
      "step": 4570
    },
    {
      "epoch": 3.0533422237039507,
      "grad_norm": 2.35219407081604,
      "learning_rate": 1.2651913524040898e-05,
      "loss": 2.086,
      "step": 4580
    },
    {
      "epoch": 3.0600100016669445,
      "grad_norm": 2.3149352073669434,
      "learning_rate": 1.2562878242379807e-05,
      "loss": 2.1154,
      "step": 4590
    },
    {
      "epoch": 3.0666777796299383,
      "grad_norm": 2.127692461013794,
      "learning_rate": 1.2473842960718718e-05,
      "loss": 2.1355,
      "step": 4600
    },
    {
      "epoch": 3.073345557592932,
      "grad_norm": 3.7391180992126465,
      "learning_rate": 1.2384807679057628e-05,
      "loss": 2.0923,
      "step": 4610
    },
    {
      "epoch": 3.080013335555926,
      "grad_norm": 3.0876946449279785,
      "learning_rate": 1.2295772397396536e-05,
      "loss": 2.1133,
      "step": 4620
    },
    {
      "epoch": 3.0866811135189196,
      "grad_norm": 2.3827338218688965,
      "learning_rate": 1.2206737115735447e-05,
      "loss": 2.1199,
      "step": 4630
    },
    {
      "epoch": 3.093348891481914,
      "grad_norm": 4.214344501495361,
      "learning_rate": 1.2117701834074356e-05,
      "loss": 2.1282,
      "step": 4640
    },
    {
      "epoch": 3.1000166694449076,
      "grad_norm": 2.148970127105713,
      "learning_rate": 1.2028666552413266e-05,
      "loss": 2.0842,
      "step": 4650
    },
    {
      "epoch": 3.1066844474079014,
      "grad_norm": 2.6289780139923096,
      "learning_rate": 1.1939631270752177e-05,
      "loss": 2.1163,
      "step": 4660
    },
    {
      "epoch": 3.113352225370895,
      "grad_norm": 3.322946786880493,
      "learning_rate": 1.1850595989091087e-05,
      "loss": 2.0404,
      "step": 4670
    },
    {
      "epoch": 3.120020003333889,
      "grad_norm": 2.589848756790161,
      "learning_rate": 1.1761560707429998e-05,
      "loss": 2.1146,
      "step": 4680
    },
    {
      "epoch": 3.1266877812968827,
      "grad_norm": 3.1393516063690186,
      "learning_rate": 1.1672525425768906e-05,
      "loss": 2.1042,
      "step": 4690
    },
    {
      "epoch": 3.1333555592598765,
      "grad_norm": 2.3010425567626953,
      "learning_rate": 1.1583490144107815e-05,
      "loss": 2.0974,
      "step": 4700
    },
    {
      "epoch": 3.1400233372228703,
      "grad_norm": 2.6821701526641846,
      "learning_rate": 1.1494454862446727e-05,
      "loss": 2.0461,
      "step": 4710
    },
    {
      "epoch": 3.146691115185864,
      "grad_norm": 2.7372231483459473,
      "learning_rate": 1.1405419580785636e-05,
      "loss": 2.1144,
      "step": 4720
    },
    {
      "epoch": 3.1533588931488583,
      "grad_norm": 2.900892972946167,
      "learning_rate": 1.1316384299124547e-05,
      "loss": 2.0603,
      "step": 4730
    },
    {
      "epoch": 3.160026671111852,
      "grad_norm": 2.995285987854004,
      "learning_rate": 1.1227349017463457e-05,
      "loss": 2.1246,
      "step": 4740
    },
    {
      "epoch": 3.166694449074846,
      "grad_norm": 3.047183036804199,
      "learning_rate": 1.1138313735802366e-05,
      "loss": 2.074,
      "step": 4750
    },
    {
      "epoch": 3.1733622270378397,
      "grad_norm": 2.2603206634521484,
      "learning_rate": 1.1049278454141276e-05,
      "loss": 2.2085,
      "step": 4760
    },
    {
      "epoch": 3.1800300050008334,
      "grad_norm": 3.551567554473877,
      "learning_rate": 1.0960243172480186e-05,
      "loss": 2.079,
      "step": 4770
    },
    {
      "epoch": 3.186697782963827,
      "grad_norm": 2.3721163272857666,
      "learning_rate": 1.0871207890819095e-05,
      "loss": 2.1556,
      "step": 4780
    },
    {
      "epoch": 3.193365560926821,
      "grad_norm": 2.615527391433716,
      "learning_rate": 1.0782172609158006e-05,
      "loss": 2.1223,
      "step": 4790
    },
    {
      "epoch": 3.200033338889815,
      "grad_norm": 2.779432535171509,
      "learning_rate": 1.0693137327496916e-05,
      "loss": 2.1192,
      "step": 4800
    },
    {
      "epoch": 3.206701116852809,
      "grad_norm": 3.203052282333374,
      "learning_rate": 1.0604102045835827e-05,
      "loss": 2.1048,
      "step": 4810
    },
    {
      "epoch": 3.213368894815803,
      "grad_norm": 2.242713451385498,
      "learning_rate": 1.0515066764174737e-05,
      "loss": 2.149,
      "step": 4820
    },
    {
      "epoch": 3.2200366727787966,
      "grad_norm": 2.6459007263183594,
      "learning_rate": 1.0426031482513644e-05,
      "loss": 2.1301,
      "step": 4830
    },
    {
      "epoch": 3.2267044507417904,
      "grad_norm": 5.821287631988525,
      "learning_rate": 1.0336996200852556e-05,
      "loss": 2.1227,
      "step": 4840
    },
    {
      "epoch": 3.233372228704784,
      "grad_norm": 3.350205183029175,
      "learning_rate": 1.0247960919191465e-05,
      "loss": 2.0745,
      "step": 4850
    },
    {
      "epoch": 3.240040006667778,
      "grad_norm": 2.7555301189422607,
      "learning_rate": 1.0158925637530376e-05,
      "loss": 2.1686,
      "step": 4860
    },
    {
      "epoch": 3.2467077846307717,
      "grad_norm": 3.5695533752441406,
      "learning_rate": 1.0069890355869286e-05,
      "loss": 2.031,
      "step": 4870
    },
    {
      "epoch": 3.2533755625937655,
      "grad_norm": 1.9340201616287231,
      "learning_rate": 9.980855074208196e-06,
      "loss": 2.1847,
      "step": 4880
    },
    {
      "epoch": 3.2600433405567593,
      "grad_norm": 2.0357062816619873,
      "learning_rate": 9.891819792547105e-06,
      "loss": 2.1054,
      "step": 4890
    },
    {
      "epoch": 3.266711118519753,
      "grad_norm": 3.0391340255737305,
      "learning_rate": 9.802784510886015e-06,
      "loss": 2.1132,
      "step": 4900
    },
    {
      "epoch": 3.2733788964827473,
      "grad_norm": 3.023822784423828,
      "learning_rate": 9.713749229224926e-06,
      "loss": 2.1584,
      "step": 4910
    },
    {
      "epoch": 3.280046674445741,
      "grad_norm": 2.3213350772857666,
      "learning_rate": 9.624713947563835e-06,
      "loss": 2.0658,
      "step": 4920
    },
    {
      "epoch": 3.286714452408735,
      "grad_norm": 2.981919050216675,
      "learning_rate": 9.535678665902745e-06,
      "loss": 2.1283,
      "step": 4930
    },
    {
      "epoch": 3.2933822303717286,
      "grad_norm": 2.618710994720459,
      "learning_rate": 9.446643384241656e-06,
      "loss": 2.1778,
      "step": 4940
    },
    {
      "epoch": 3.3000500083347224,
      "grad_norm": 3.293581247329712,
      "learning_rate": 9.357608102580566e-06,
      "loss": 2.0849,
      "step": 4950
    },
    {
      "epoch": 3.306717786297716,
      "grad_norm": 1.9511706829071045,
      "learning_rate": 9.268572820919475e-06,
      "loss": 2.1509,
      "step": 4960
    },
    {
      "epoch": 3.31338556426071,
      "grad_norm": 2.192610502243042,
      "learning_rate": 9.179537539258385e-06,
      "loss": 2.1156,
      "step": 4970
    },
    {
      "epoch": 3.320053342223704,
      "grad_norm": 2.5075807571411133,
      "learning_rate": 9.090502257597294e-06,
      "loss": 2.1251,
      "step": 4980
    },
    {
      "epoch": 3.326721120186698,
      "grad_norm": 4.129232406616211,
      "learning_rate": 9.001466975936205e-06,
      "loss": 2.1197,
      "step": 4990
    },
    {
      "epoch": 3.3333888981496917,
      "grad_norm": 2.7485978603363037,
      "learning_rate": 8.912431694275115e-06,
      "loss": 2.1331,
      "step": 5000
    },
    {
      "epoch": 3.3400566761126855,
      "grad_norm": 2.294002056121826,
      "learning_rate": 8.823396412614025e-06,
      "loss": 2.1561,
      "step": 5010
    },
    {
      "epoch": 3.3467244540756793,
      "grad_norm": 2.9407920837402344,
      "learning_rate": 8.734361130952936e-06,
      "loss": 2.0535,
      "step": 5020
    },
    {
      "epoch": 3.353392232038673,
      "grad_norm": 2.694106101989746,
      "learning_rate": 8.645325849291844e-06,
      "loss": 2.0761,
      "step": 5030
    },
    {
      "epoch": 3.360060010001667,
      "grad_norm": 2.5251429080963135,
      "learning_rate": 8.556290567630755e-06,
      "loss": 2.1077,
      "step": 5040
    },
    {
      "epoch": 3.3667277879646607,
      "grad_norm": 2.641932487487793,
      "learning_rate": 8.467255285969664e-06,
      "loss": 2.0601,
      "step": 5050
    },
    {
      "epoch": 3.3733955659276544,
      "grad_norm": 2.376054048538208,
      "learning_rate": 8.378220004308574e-06,
      "loss": 2.0784,
      "step": 5060
    },
    {
      "epoch": 3.380063343890648,
      "grad_norm": 2.9334959983825684,
      "learning_rate": 8.289184722647485e-06,
      "loss": 2.1679,
      "step": 5070
    },
    {
      "epoch": 3.3867311218536424,
      "grad_norm": 2.378960371017456,
      "learning_rate": 8.200149440986395e-06,
      "loss": 2.0757,
      "step": 5080
    },
    {
      "epoch": 3.3933988998166362,
      "grad_norm": 2.6701552867889404,
      "learning_rate": 8.111114159325304e-06,
      "loss": 2.0718,
      "step": 5090
    },
    {
      "epoch": 3.40006667777963,
      "grad_norm": 3.272407054901123,
      "learning_rate": 8.022078877664214e-06,
      "loss": 2.1493,
      "step": 5100
    },
    {
      "epoch": 3.406734455742624,
      "grad_norm": 3.9564621448516846,
      "learning_rate": 7.933043596003123e-06,
      "loss": 2.0814,
      "step": 5110
    },
    {
      "epoch": 3.4134022337056176,
      "grad_norm": 2.7357020378112793,
      "learning_rate": 7.844008314342035e-06,
      "loss": 2.0902,
      "step": 5120
    },
    {
      "epoch": 3.4200700116686114,
      "grad_norm": 3.2984330654144287,
      "learning_rate": 7.754973032680944e-06,
      "loss": 2.0305,
      "step": 5130
    },
    {
      "epoch": 3.426737789631605,
      "grad_norm": 2.7376368045806885,
      "learning_rate": 7.665937751019854e-06,
      "loss": 2.0245,
      "step": 5140
    },
    {
      "epoch": 3.433405567594599,
      "grad_norm": 2.5877952575683594,
      "learning_rate": 7.576902469358764e-06,
      "loss": 2.1368,
      "step": 5150
    },
    {
      "epoch": 3.440073345557593,
      "grad_norm": 2.6336472034454346,
      "learning_rate": 7.4878671876976735e-06,
      "loss": 2.1284,
      "step": 5160
    },
    {
      "epoch": 3.446741123520587,
      "grad_norm": 2.7509355545043945,
      "learning_rate": 7.398831906036585e-06,
      "loss": 2.1304,
      "step": 5170
    },
    {
      "epoch": 3.4534089014835807,
      "grad_norm": 3.564265251159668,
      "learning_rate": 7.3097966243754934e-06,
      "loss": 2.1016,
      "step": 5180
    },
    {
      "epoch": 3.4600766794465745,
      "grad_norm": 2.938389778137207,
      "learning_rate": 7.220761342714403e-06,
      "loss": 2.0696,
      "step": 5190
    },
    {
      "epoch": 3.4667444574095683,
      "grad_norm": 3.3204357624053955,
      "learning_rate": 7.131726061053314e-06,
      "loss": 2.1251,
      "step": 5200
    },
    {
      "epoch": 3.473412235372562,
      "grad_norm": 3.4510421752929688,
      "learning_rate": 7.042690779392224e-06,
      "loss": 2.12,
      "step": 5210
    },
    {
      "epoch": 3.480080013335556,
      "grad_norm": 3.9434142112731934,
      "learning_rate": 6.953655497731134e-06,
      "loss": 2.0815,
      "step": 5220
    },
    {
      "epoch": 3.4867477912985496,
      "grad_norm": 2.829571485519409,
      "learning_rate": 6.864620216070044e-06,
      "loss": 2.0962,
      "step": 5230
    },
    {
      "epoch": 3.4934155692615434,
      "grad_norm": 3.069261312484741,
      "learning_rate": 6.775584934408953e-06,
      "loss": 2.1581,
      "step": 5240
    },
    {
      "epoch": 3.500083347224537,
      "grad_norm": 4.5194573402404785,
      "learning_rate": 6.6865496527478636e-06,
      "loss": 2.0656,
      "step": 5250
    },
    {
      "epoch": 3.5067511251875314,
      "grad_norm": 2.3337907791137695,
      "learning_rate": 6.597514371086773e-06,
      "loss": 2.133,
      "step": 5260
    },
    {
      "epoch": 3.513418903150525,
      "grad_norm": 3.703376293182373,
      "learning_rate": 6.5084790894256835e-06,
      "loss": 2.1299,
      "step": 5270
    },
    {
      "epoch": 3.520086681113519,
      "grad_norm": 3.0950264930725098,
      "learning_rate": 6.419443807764594e-06,
      "loss": 2.0493,
      "step": 5280
    },
    {
      "epoch": 3.5267544590765127,
      "grad_norm": 2.3126585483551025,
      "learning_rate": 6.3304085261035026e-06,
      "loss": 2.0849,
      "step": 5290
    },
    {
      "epoch": 3.5334222370395065,
      "grad_norm": 3.1920034885406494,
      "learning_rate": 6.241373244442413e-06,
      "loss": 2.0606,
      "step": 5300
    },
    {
      "epoch": 3.5400900150025003,
      "grad_norm": 2.6280176639556885,
      "learning_rate": 6.152337962781323e-06,
      "loss": 2.0935,
      "step": 5310
    },
    {
      "epoch": 3.546757792965494,
      "grad_norm": 2.02789568901062,
      "learning_rate": 6.063302681120233e-06,
      "loss": 2.1744,
      "step": 5320
    },
    {
      "epoch": 3.5534255709284883,
      "grad_norm": 3.9026196002960205,
      "learning_rate": 5.974267399459143e-06,
      "loss": 2.0564,
      "step": 5330
    },
    {
      "epoch": 3.560093348891482,
      "grad_norm": 3.020888328552246,
      "learning_rate": 5.885232117798053e-06,
      "loss": 2.1037,
      "step": 5340
    },
    {
      "epoch": 3.566761126854476,
      "grad_norm": 4.155600547790527,
      "learning_rate": 5.796196836136963e-06,
      "loss": 2.1187,
      "step": 5350
    },
    {
      "epoch": 3.5734289048174697,
      "grad_norm": 3.9842069149017334,
      "learning_rate": 5.707161554475873e-06,
      "loss": 2.0508,
      "step": 5360
    },
    {
      "epoch": 3.5800966827804634,
      "grad_norm": 3.4710817337036133,
      "learning_rate": 5.618126272814783e-06,
      "loss": 2.0418,
      "step": 5370
    },
    {
      "epoch": 3.5867644607434572,
      "grad_norm": 3.1506946086883545,
      "learning_rate": 5.529090991153693e-06,
      "loss": 2.1612,
      "step": 5380
    },
    {
      "epoch": 3.593432238706451,
      "grad_norm": 2.7605302333831787,
      "learning_rate": 5.440055709492602e-06,
      "loss": 2.1473,
      "step": 5390
    },
    {
      "epoch": 3.600100016669445,
      "grad_norm": 2.7386224269866943,
      "learning_rate": 5.3510204278315125e-06,
      "loss": 2.0735,
      "step": 5400
    },
    {
      "epoch": 3.6067677946324386,
      "grad_norm": 2.298863172531128,
      "learning_rate": 5.261985146170423e-06,
      "loss": 2.0918,
      "step": 5410
    },
    {
      "epoch": 3.6134355725954324,
      "grad_norm": 2.2472848892211914,
      "learning_rate": 5.1729498645093324e-06,
      "loss": 2.1247,
      "step": 5420
    },
    {
      "epoch": 3.620103350558426,
      "grad_norm": 1.8018101453781128,
      "learning_rate": 5.083914582848242e-06,
      "loss": 2.0847,
      "step": 5430
    },
    {
      "epoch": 3.6267711285214204,
      "grad_norm": 2.1240322589874268,
      "learning_rate": 4.994879301187152e-06,
      "loss": 2.0524,
      "step": 5440
    },
    {
      "epoch": 3.633438906484414,
      "grad_norm": 2.5235671997070312,
      "learning_rate": 4.905844019526063e-06,
      "loss": 2.1272,
      "step": 5450
    },
    {
      "epoch": 3.640106684447408,
      "grad_norm": 2.472947597503662,
      "learning_rate": 4.816808737864972e-06,
      "loss": 2.0549,
      "step": 5460
    },
    {
      "epoch": 3.6467744624104017,
      "grad_norm": 3.2230916023254395,
      "learning_rate": 4.727773456203882e-06,
      "loss": 2.0584,
      "step": 5470
    },
    {
      "epoch": 3.6534422403733955,
      "grad_norm": 2.6986212730407715,
      "learning_rate": 4.638738174542792e-06,
      "loss": 2.0892,
      "step": 5480
    },
    {
      "epoch": 3.6601100183363893,
      "grad_norm": 3.163283348083496,
      "learning_rate": 4.549702892881702e-06,
      "loss": 2.1142,
      "step": 5490
    },
    {
      "epoch": 3.6667777962993835,
      "grad_norm": 2.380983352661133,
      "learning_rate": 4.460667611220612e-06,
      "loss": 2.1156,
      "step": 5500
    },
    {
      "epoch": 3.6734455742623773,
      "grad_norm": 3.028928756713867,
      "learning_rate": 4.371632329559522e-06,
      "loss": 2.0718,
      "step": 5510
    },
    {
      "epoch": 3.680113352225371,
      "grad_norm": 3.3257462978363037,
      "learning_rate": 4.282597047898432e-06,
      "loss": 2.109,
      "step": 5520
    },
    {
      "epoch": 3.686781130188365,
      "grad_norm": 2.842747211456299,
      "learning_rate": 4.1935617662373416e-06,
      "loss": 2.0828,
      "step": 5530
    },
    {
      "epoch": 3.6934489081513586,
      "grad_norm": 3.079669952392578,
      "learning_rate": 4.104526484576252e-06,
      "loss": 2.1192,
      "step": 5540
    },
    {
      "epoch": 3.7001166861143524,
      "grad_norm": 3.8474204540252686,
      "learning_rate": 4.015491202915162e-06,
      "loss": 2.1035,
      "step": 5550
    },
    {
      "epoch": 3.706784464077346,
      "grad_norm": 2.2632129192352295,
      "learning_rate": 3.926455921254071e-06,
      "loss": 2.0454,
      "step": 5560
    },
    {
      "epoch": 3.71345224204034,
      "grad_norm": 3.383279323577881,
      "learning_rate": 3.837420639592981e-06,
      "loss": 2.0182,
      "step": 5570
    },
    {
      "epoch": 3.7201200200033337,
      "grad_norm": 2.3185832500457764,
      "learning_rate": 3.7483853579318914e-06,
      "loss": 2.1043,
      "step": 5580
    },
    {
      "epoch": 3.7267877979663275,
      "grad_norm": 2.1578593254089355,
      "learning_rate": 3.6593500762708017e-06,
      "loss": 2.1103,
      "step": 5590
    },
    {
      "epoch": 3.7334555759293213,
      "grad_norm": 2.800044536590576,
      "learning_rate": 3.570314794609711e-06,
      "loss": 2.136,
      "step": 5600
    },
    {
      "epoch": 3.7401233538923155,
      "grad_norm": 1.9151839017868042,
      "learning_rate": 3.4812795129486212e-06,
      "loss": 2.0131,
      "step": 5610
    },
    {
      "epoch": 3.7467911318553093,
      "grad_norm": 2.2291667461395264,
      "learning_rate": 3.392244231287531e-06,
      "loss": 2.0902,
      "step": 5620
    },
    {
      "epoch": 3.753458909818303,
      "grad_norm": 3.1756577491760254,
      "learning_rate": 3.303208949626441e-06,
      "loss": 2.0663,
      "step": 5630
    },
    {
      "epoch": 3.760126687781297,
      "grad_norm": 2.91892671585083,
      "learning_rate": 3.214173667965351e-06,
      "loss": 2.079,
      "step": 5640
    },
    {
      "epoch": 3.7667944657442907,
      "grad_norm": 2.7478201389312744,
      "learning_rate": 3.125138386304261e-06,
      "loss": 2.0953,
      "step": 5650
    },
    {
      "epoch": 3.7734622437072844,
      "grad_norm": 2.6219215393066406,
      "learning_rate": 3.036103104643171e-06,
      "loss": 2.1096,
      "step": 5660
    },
    {
      "epoch": 3.7801300216702782,
      "grad_norm": 2.403529167175293,
      "learning_rate": 2.947067822982081e-06,
      "loss": 2.0944,
      "step": 5670
    },
    {
      "epoch": 3.7867977996332725,
      "grad_norm": 2.747131586074829,
      "learning_rate": 2.858032541320991e-06,
      "loss": 2.0492,
      "step": 5680
    },
    {
      "epoch": 3.7934655775962662,
      "grad_norm": 3.0520036220550537,
      "learning_rate": 2.768997259659901e-06,
      "loss": 2.1159,
      "step": 5690
    },
    {
      "epoch": 3.80013335555926,
      "grad_norm": 2.1349449157714844,
      "learning_rate": 2.6799619779988104e-06,
      "loss": 2.0903,
      "step": 5700
    },
    {
      "epoch": 3.806801133522254,
      "grad_norm": 2.637943744659424,
      "learning_rate": 2.590926696337721e-06,
      "loss": 2.1232,
      "step": 5710
    },
    {
      "epoch": 3.8134689114852476,
      "grad_norm": 2.963502883911133,
      "learning_rate": 2.5018914146766304e-06,
      "loss": 2.123,
      "step": 5720
    },
    {
      "epoch": 3.8201366894482414,
      "grad_norm": 3.286292791366577,
      "learning_rate": 2.4128561330155407e-06,
      "loss": 2.0988,
      "step": 5730
    },
    {
      "epoch": 3.826804467411235,
      "grad_norm": 2.2997357845306396,
      "learning_rate": 2.3238208513544503e-06,
      "loss": 2.0313,
      "step": 5740
    },
    {
      "epoch": 3.833472245374229,
      "grad_norm": 3.9768998622894287,
      "learning_rate": 2.2347855696933607e-06,
      "loss": 2.1071,
      "step": 5750
    },
    {
      "epoch": 3.8401400233372227,
      "grad_norm": 1.9219865798950195,
      "learning_rate": 2.1457502880322706e-06,
      "loss": 2.0626,
      "step": 5760
    },
    {
      "epoch": 3.8468078013002165,
      "grad_norm": 3.3437037467956543,
      "learning_rate": 2.05671500637118e-06,
      "loss": 2.1196,
      "step": 5770
    },
    {
      "epoch": 3.8534755792632103,
      "grad_norm": 3.3440725803375244,
      "learning_rate": 1.9676797247100905e-06,
      "loss": 2.0652,
      "step": 5780
    },
    {
      "epoch": 3.8601433572262045,
      "grad_norm": 2.213064193725586,
      "learning_rate": 1.878644443049e-06,
      "loss": 2.1008,
      "step": 5790
    },
    {
      "epoch": 3.8668111351891983,
      "grad_norm": 2.8244564533233643,
      "learning_rate": 1.7896091613879102e-06,
      "loss": 2.102,
      "step": 5800
    },
    {
      "epoch": 3.873478913152192,
      "grad_norm": 2.5708227157592773,
      "learning_rate": 1.70057387972682e-06,
      "loss": 2.0855,
      "step": 5810
    },
    {
      "epoch": 3.880146691115186,
      "grad_norm": 2.682436466217041,
      "learning_rate": 1.6115385980657302e-06,
      "loss": 2.0836,
      "step": 5820
    },
    {
      "epoch": 3.8868144690781796,
      "grad_norm": 3.8542673587799072,
      "learning_rate": 1.5225033164046401e-06,
      "loss": 2.0388,
      "step": 5830
    },
    {
      "epoch": 3.8934822470411734,
      "grad_norm": 2.596640110015869,
      "learning_rate": 1.4334680347435499e-06,
      "loss": 2.0615,
      "step": 5840
    },
    {
      "epoch": 3.9001500250041676,
      "grad_norm": 2.196345090866089,
      "learning_rate": 1.3444327530824598e-06,
      "loss": 2.0909,
      "step": 5850
    },
    {
      "epoch": 3.9068178029671614,
      "grad_norm": 2.532259702682495,
      "learning_rate": 1.2553974714213698e-06,
      "loss": 2.1101,
      "step": 5860
    },
    {
      "epoch": 3.913485580930155,
      "grad_norm": 2.438037633895874,
      "learning_rate": 1.1663621897602797e-06,
      "loss": 2.1363,
      "step": 5870
    },
    {
      "epoch": 3.920153358893149,
      "grad_norm": 2.4325203895568848,
      "learning_rate": 1.0773269080991897e-06,
      "loss": 2.1281,
      "step": 5880
    },
    {
      "epoch": 3.9268211368561428,
      "grad_norm": 2.8991057872772217,
      "learning_rate": 9.882916264380997e-07,
      "loss": 2.0942,
      "step": 5890
    },
    {
      "epoch": 3.9334889148191365,
      "grad_norm": 2.5660176277160645,
      "learning_rate": 8.992563447770095e-07,
      "loss": 2.1122,
      "step": 5900
    },
    {
      "epoch": 3.9401566927821303,
      "grad_norm": 3.0861740112304688,
      "learning_rate": 8.102210631159196e-07,
      "loss": 2.0571,
      "step": 5910
    },
    {
      "epoch": 3.946824470745124,
      "grad_norm": 3.178783655166626,
      "learning_rate": 7.211857814548294e-07,
      "loss": 2.076,
      "step": 5920
    },
    {
      "epoch": 3.953492248708118,
      "grad_norm": 3.197464942932129,
      "learning_rate": 6.321504997937394e-07,
      "loss": 2.0488,
      "step": 5930
    },
    {
      "epoch": 3.9601600266711117,
      "grad_norm": 2.4673874378204346,
      "learning_rate": 5.431152181326494e-07,
      "loss": 2.0967,
      "step": 5940
    },
    {
      "epoch": 3.9668278046341054,
      "grad_norm": 2.3412349224090576,
      "learning_rate": 4.540799364715593e-07,
      "loss": 2.0904,
      "step": 5950
    },
    {
      "epoch": 3.9734955825970997,
      "grad_norm": 3.189786911010742,
      "learning_rate": 3.650446548104693e-07,
      "loss": 2.1188,
      "step": 5960
    },
    {
      "epoch": 3.9801633605600935,
      "grad_norm": 2.453059196472168,
      "learning_rate": 2.760093731493792e-07,
      "loss": 2.1168,
      "step": 5970
    },
    {
      "epoch": 3.9868311385230872,
      "grad_norm": 2.29476261138916,
      "learning_rate": 1.869740914882891e-07,
      "loss": 2.1164,
      "step": 5980
    },
    {
      "epoch": 3.993498916486081,
      "grad_norm": 2.9995062351226807,
      "learning_rate": 9.793880982719906e-08,
      "loss": 2.0657,
      "step": 5990
    },
    {
      "epoch": 4.0,
      "grad_norm": 4.2090911865234375,
      "learning_rate": 8.903528166109006e-09,
      "loss": 2.1383,
      "step": 6000
    },
    {
      "epoch": 4.0,
      "eval_loss": 2.118459939956665,
      "eval_runtime": 939.6465,
      "eval_samples_per_second": 6.384,
      "eval_steps_per_second": 3.193,
      "step": 6000
    }
  ],
  "logging_steps": 10,
  "max_steps": 6000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 4,
  "save_steps": 500,
  "stateful_callbacks": {
    "EarlyStoppingCallback": {
      "args": {
        "early_stopping_patience": 2,
        "early_stopping_threshold": 0.0
      },
      "attributes": {
        "early_stopping_patience_counter": 0
      }
    },
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 4.1702959716551885e+18,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
