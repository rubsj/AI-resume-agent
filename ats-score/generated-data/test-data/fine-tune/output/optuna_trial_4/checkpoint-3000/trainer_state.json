{
  "best_global_step": 3000,
  "best_metric": 1.2789655923843384,
  "best_model_checkpoint": "json_outputs_all_data/fine-tune/optuna_output/optuna_trial_4/checkpoint-3000",
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 3000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.006667777962993832,
      "grad_norm": 0.2365945279598236,
      "learning_rate": 0.0003275215611436091,
      "loss": 2.6177,
      "step": 10
    },
    {
      "epoch": 0.013335555925987664,
      "grad_norm": 0.2642991244792938,
      "learning_rate": 0.00032642653753563986,
      "loss": 2.4381,
      "step": 20
    },
    {
      "epoch": 0.020003333888981498,
      "grad_norm": 0.22344626486301422,
      "learning_rate": 0.00032533151392767054,
      "loss": 2.3897,
      "step": 30
    },
    {
      "epoch": 0.026671111851975328,
      "grad_norm": 0.23025962710380554,
      "learning_rate": 0.0003242364903197013,
      "loss": 2.4114,
      "step": 40
    },
    {
      "epoch": 0.03333888981496916,
      "grad_norm": 0.22683946788311005,
      "learning_rate": 0.000323141466711732,
      "loss": 2.3672,
      "step": 50
    },
    {
      "epoch": 0.040006667777962995,
      "grad_norm": 0.2546841502189636,
      "learning_rate": 0.0003220464431037627,
      "loss": 2.3564,
      "step": 60
    },
    {
      "epoch": 0.046674445740956826,
      "grad_norm": 0.22788166999816895,
      "learning_rate": 0.00032095141949579345,
      "loss": 2.3252,
      "step": 70
    },
    {
      "epoch": 0.053342223703950656,
      "grad_norm": 0.2440686970949173,
      "learning_rate": 0.0003198563958878242,
      "loss": 2.3825,
      "step": 80
    },
    {
      "epoch": 0.06001000166694449,
      "grad_norm": 0.24982251226902008,
      "learning_rate": 0.0003187613722798549,
      "loss": 2.3492,
      "step": 90
    },
    {
      "epoch": 0.06667777962993832,
      "grad_norm": 0.28681787848472595,
      "learning_rate": 0.00031766634867188567,
      "loss": 2.3288,
      "step": 100
    },
    {
      "epoch": 0.07334555759293215,
      "grad_norm": 0.2692966163158417,
      "learning_rate": 0.0003165713250639164,
      "loss": 2.337,
      "step": 110
    },
    {
      "epoch": 0.08001333555592599,
      "grad_norm": 0.27725884318351746,
      "learning_rate": 0.00031547630145594715,
      "loss": 2.3446,
      "step": 120
    },
    {
      "epoch": 0.08668111351891981,
      "grad_norm": 0.27531012892723083,
      "learning_rate": 0.00031438127784797783,
      "loss": 2.2875,
      "step": 130
    },
    {
      "epoch": 0.09334889148191365,
      "grad_norm": 0.26499709486961365,
      "learning_rate": 0.00031328625424000857,
      "loss": 2.3145,
      "step": 140
    },
    {
      "epoch": 0.10001666944490749,
      "grad_norm": 0.27835142612457275,
      "learning_rate": 0.0003121912306320393,
      "loss": 2.3013,
      "step": 150
    },
    {
      "epoch": 0.10668444740790131,
      "grad_norm": 0.31318894028663635,
      "learning_rate": 0.00031109620702407,
      "loss": 2.2669,
      "step": 160
    },
    {
      "epoch": 0.11335222537089515,
      "grad_norm": 0.3009483218193054,
      "learning_rate": 0.00031000118341610074,
      "loss": 2.2187,
      "step": 170
    },
    {
      "epoch": 0.12002000333388899,
      "grad_norm": 0.3123930096626282,
      "learning_rate": 0.0003089061598081315,
      "loss": 2.2569,
      "step": 180
    },
    {
      "epoch": 0.12668778129688282,
      "grad_norm": 0.31474605202674866,
      "learning_rate": 0.0003078111362001622,
      "loss": 2.2423,
      "step": 190
    },
    {
      "epoch": 0.13335555925987663,
      "grad_norm": 0.36106598377227783,
      "learning_rate": 0.00030671611259219296,
      "loss": 2.2074,
      "step": 200
    },
    {
      "epoch": 0.14002333722287047,
      "grad_norm": 0.3505327105522156,
      "learning_rate": 0.0003056210889842237,
      "loss": 2.2302,
      "step": 210
    },
    {
      "epoch": 0.1466911151858643,
      "grad_norm": 0.3713533282279968,
      "learning_rate": 0.00030452606537625444,
      "loss": 2.2566,
      "step": 220
    },
    {
      "epoch": 0.15335889314885814,
      "grad_norm": 0.3540310859680176,
      "learning_rate": 0.0003034310417682851,
      "loss": 2.1928,
      "step": 230
    },
    {
      "epoch": 0.16002667111185198,
      "grad_norm": 0.33150914311408997,
      "learning_rate": 0.00030233601816031586,
      "loss": 2.1728,
      "step": 240
    },
    {
      "epoch": 0.16669444907484582,
      "grad_norm": 0.3670956790447235,
      "learning_rate": 0.0003012409945523466,
      "loss": 2.1689,
      "step": 250
    },
    {
      "epoch": 0.17336222703783963,
      "grad_norm": 0.44656533002853394,
      "learning_rate": 0.0003001459709443773,
      "loss": 2.1584,
      "step": 260
    },
    {
      "epoch": 0.18003000500083347,
      "grad_norm": 0.37128934264183044,
      "learning_rate": 0.00029905094733640803,
      "loss": 2.2429,
      "step": 270
    },
    {
      "epoch": 0.1866977829638273,
      "grad_norm": 0.3763819932937622,
      "learning_rate": 0.00029795592372843877,
      "loss": 2.269,
      "step": 280
    },
    {
      "epoch": 0.19336556092682114,
      "grad_norm": 0.45954206585884094,
      "learning_rate": 0.0002968609001204695,
      "loss": 2.1817,
      "step": 290
    },
    {
      "epoch": 0.20003333888981498,
      "grad_norm": 0.4155758023262024,
      "learning_rate": 0.00029576587651250025,
      "loss": 2.165,
      "step": 300
    },
    {
      "epoch": 0.2067011168528088,
      "grad_norm": 0.44666290283203125,
      "learning_rate": 0.000294670852904531,
      "loss": 2.1376,
      "step": 310
    },
    {
      "epoch": 0.21336889481580262,
      "grad_norm": 0.447454035282135,
      "learning_rate": 0.0002935758292965617,
      "loss": 2.1517,
      "step": 320
    },
    {
      "epoch": 0.22003667277879646,
      "grad_norm": 0.4801075756549835,
      "learning_rate": 0.0002924808056885924,
      "loss": 2.1028,
      "step": 330
    },
    {
      "epoch": 0.2267044507417903,
      "grad_norm": 0.5410224199295044,
      "learning_rate": 0.00029138578208062315,
      "loss": 2.0996,
      "step": 340
    },
    {
      "epoch": 0.23337222870478413,
      "grad_norm": 0.5308963656425476,
      "learning_rate": 0.0002902907584726539,
      "loss": 2.0886,
      "step": 350
    },
    {
      "epoch": 0.24004000666777797,
      "grad_norm": 0.496465802192688,
      "learning_rate": 0.0002891957348646846,
      "loss": 2.1561,
      "step": 360
    },
    {
      "epoch": 0.24670778463077178,
      "grad_norm": 0.48753610253334045,
      "learning_rate": 0.0002881007112567153,
      "loss": 2.1452,
      "step": 370
    },
    {
      "epoch": 0.25337556259376565,
      "grad_norm": 0.5732950568199158,
      "learning_rate": 0.00028700568764874606,
      "loss": 2.1206,
      "step": 380
    },
    {
      "epoch": 0.2600433405567595,
      "grad_norm": 0.4859045445919037,
      "learning_rate": 0.00028591066404077674,
      "loss": 2.0512,
      "step": 390
    },
    {
      "epoch": 0.26671111851975327,
      "grad_norm": 0.7010806202888489,
      "learning_rate": 0.0002848156404328075,
      "loss": 2.0803,
      "step": 400
    },
    {
      "epoch": 0.2733788964827471,
      "grad_norm": 0.5854932069778442,
      "learning_rate": 0.0002837206168248382,
      "loss": 2.0178,
      "step": 410
    },
    {
      "epoch": 0.28004667444574094,
      "grad_norm": 0.660979151725769,
      "learning_rate": 0.00028262559321686896,
      "loss": 2.0189,
      "step": 420
    },
    {
      "epoch": 0.2867144524087348,
      "grad_norm": 0.9455897212028503,
      "learning_rate": 0.0002815305696088997,
      "loss": 2.0213,
      "step": 430
    },
    {
      "epoch": 0.2933822303717286,
      "grad_norm": 0.7254356741905212,
      "learning_rate": 0.00028043554600093044,
      "loss": 2.0301,
      "step": 440
    },
    {
      "epoch": 0.30005000833472245,
      "grad_norm": 0.7033002376556396,
      "learning_rate": 0.0002793405223929612,
      "loss": 2.0871,
      "step": 450
    },
    {
      "epoch": 0.3067177862977163,
      "grad_norm": 0.7176424860954285,
      "learning_rate": 0.00027824549878499187,
      "loss": 2.0895,
      "step": 460
    },
    {
      "epoch": 0.3133855642607101,
      "grad_norm": 0.595197856426239,
      "learning_rate": 0.0002771504751770226,
      "loss": 2.0791,
      "step": 470
    },
    {
      "epoch": 0.32005334222370396,
      "grad_norm": 0.611312985420227,
      "learning_rate": 0.00027605545156905335,
      "loss": 1.9551,
      "step": 480
    },
    {
      "epoch": 0.3267211201866978,
      "grad_norm": 0.911441445350647,
      "learning_rate": 0.00027496042796108403,
      "loss": 1.989,
      "step": 490
    },
    {
      "epoch": 0.33338889814969164,
      "grad_norm": 0.6284927129745483,
      "learning_rate": 0.0002738654043531148,
      "loss": 2.0039,
      "step": 500
    },
    {
      "epoch": 0.3400566761126855,
      "grad_norm": 0.47365307807922363,
      "learning_rate": 0.0002727703807451455,
      "loss": 2.0103,
      "step": 510
    },
    {
      "epoch": 0.34672445407567926,
      "grad_norm": 0.7076330184936523,
      "learning_rate": 0.00027167535713717625,
      "loss": 1.9271,
      "step": 520
    },
    {
      "epoch": 0.3533922320386731,
      "grad_norm": 0.7214084267616272,
      "learning_rate": 0.000270580333529207,
      "loss": 1.9274,
      "step": 530
    },
    {
      "epoch": 0.36006001000166693,
      "grad_norm": 0.7175093293190002,
      "learning_rate": 0.00026948530992123773,
      "loss": 1.8699,
      "step": 540
    },
    {
      "epoch": 0.36672778796466077,
      "grad_norm": 0.5620095729827881,
      "learning_rate": 0.0002683902863132684,
      "loss": 1.9217,
      "step": 550
    },
    {
      "epoch": 0.3733955659276546,
      "grad_norm": 0.6877134442329407,
      "learning_rate": 0.00026729526270529916,
      "loss": 1.7958,
      "step": 560
    },
    {
      "epoch": 0.38006334389064844,
      "grad_norm": 0.819949209690094,
      "learning_rate": 0.0002662002390973299,
      "loss": 1.9495,
      "step": 570
    },
    {
      "epoch": 0.3867311218536423,
      "grad_norm": 0.6293943524360657,
      "learning_rate": 0.00026510521548936064,
      "loss": 1.9632,
      "step": 580
    },
    {
      "epoch": 0.3933988998166361,
      "grad_norm": 0.5444856882095337,
      "learning_rate": 0.0002640101918813913,
      "loss": 1.9483,
      "step": 590
    },
    {
      "epoch": 0.40006667777962995,
      "grad_norm": 0.6622467041015625,
      "learning_rate": 0.00026291516827342206,
      "loss": 1.7998,
      "step": 600
    },
    {
      "epoch": 0.4067344557426238,
      "grad_norm": 0.6453842520713806,
      "learning_rate": 0.0002618201446654528,
      "loss": 1.9357,
      "step": 610
    },
    {
      "epoch": 0.4134022337056176,
      "grad_norm": 0.8984797596931458,
      "learning_rate": 0.00026072512105748354,
      "loss": 1.804,
      "step": 620
    },
    {
      "epoch": 0.4200700116686114,
      "grad_norm": 0.8958271741867065,
      "learning_rate": 0.0002596300974495143,
      "loss": 1.888,
      "step": 630
    },
    {
      "epoch": 0.42673778963160525,
      "grad_norm": 0.9828309416770935,
      "learning_rate": 0.000258535073841545,
      "loss": 1.8245,
      "step": 640
    },
    {
      "epoch": 0.4334055675945991,
      "grad_norm": 0.7945621013641357,
      "learning_rate": 0.0002574400502335757,
      "loss": 1.843,
      "step": 650
    },
    {
      "epoch": 0.4400733455575929,
      "grad_norm": 0.789608895778656,
      "learning_rate": 0.00025634502662560645,
      "loss": 1.8956,
      "step": 660
    },
    {
      "epoch": 0.44674112352058676,
      "grad_norm": 1.033450722694397,
      "learning_rate": 0.0002552500030176372,
      "loss": 1.7857,
      "step": 670
    },
    {
      "epoch": 0.4534089014835806,
      "grad_norm": 0.7769325971603394,
      "learning_rate": 0.0002541549794096679,
      "loss": 1.8696,
      "step": 680
    },
    {
      "epoch": 0.46007667944657443,
      "grad_norm": 0.9568365216255188,
      "learning_rate": 0.0002530599558016986,
      "loss": 1.7569,
      "step": 690
    },
    {
      "epoch": 0.46674445740956827,
      "grad_norm": 1.132836937904358,
      "learning_rate": 0.00025196493219372936,
      "loss": 1.7512,
      "step": 700
    },
    {
      "epoch": 0.4734122353725621,
      "grad_norm": 1.274942398071289,
      "learning_rate": 0.0002508699085857601,
      "loss": 1.7839,
      "step": 710
    },
    {
      "epoch": 0.48008001333555594,
      "grad_norm": 0.7886455059051514,
      "learning_rate": 0.00024977488497779084,
      "loss": 1.7217,
      "step": 720
    },
    {
      "epoch": 0.4867477912985498,
      "grad_norm": 0.8360062837600708,
      "learning_rate": 0.0002486798613698216,
      "loss": 1.8827,
      "step": 730
    },
    {
      "epoch": 0.49341556926154356,
      "grad_norm": 1.4555649757385254,
      "learning_rate": 0.0002475848377618523,
      "loss": 1.8247,
      "step": 740
    },
    {
      "epoch": 0.5000833472245374,
      "grad_norm": 0.7853316068649292,
      "learning_rate": 0.000246489814153883,
      "loss": 1.738,
      "step": 750
    },
    {
      "epoch": 0.5067511251875313,
      "grad_norm": 0.7470027208328247,
      "learning_rate": 0.00024539479054591374,
      "loss": 1.8284,
      "step": 760
    },
    {
      "epoch": 0.5134189031505251,
      "grad_norm": 0.8418090343475342,
      "learning_rate": 0.0002442997669379445,
      "loss": 1.7295,
      "step": 770
    },
    {
      "epoch": 0.520086681113519,
      "grad_norm": 0.8717788457870483,
      "learning_rate": 0.0002432047433299752,
      "loss": 1.7512,
      "step": 780
    },
    {
      "epoch": 0.5267544590765127,
      "grad_norm": 0.779685378074646,
      "learning_rate": 0.00024210971972200593,
      "loss": 1.7731,
      "step": 790
    },
    {
      "epoch": 0.5334222370395065,
      "grad_norm": 1.291025161743164,
      "learning_rate": 0.00024101469611403667,
      "loss": 1.7835,
      "step": 800
    },
    {
      "epoch": 0.5400900150025004,
      "grad_norm": 0.7706059217453003,
      "learning_rate": 0.00023991967250606736,
      "loss": 1.8961,
      "step": 810
    },
    {
      "epoch": 0.5467577929654942,
      "grad_norm": 0.7780072093009949,
      "learning_rate": 0.0002388246488980981,
      "loss": 1.6471,
      "step": 820
    },
    {
      "epoch": 0.5534255709284881,
      "grad_norm": 0.6551146507263184,
      "learning_rate": 0.00023772962529012884,
      "loss": 1.7292,
      "step": 830
    },
    {
      "epoch": 0.5600933488914819,
      "grad_norm": 0.9838706254959106,
      "learning_rate": 0.00023663460168215958,
      "loss": 1.7616,
      "step": 840
    },
    {
      "epoch": 0.5667611268544758,
      "grad_norm": 0.9297866821289062,
      "learning_rate": 0.0002355395780741903,
      "loss": 1.7154,
      "step": 850
    },
    {
      "epoch": 0.5734289048174696,
      "grad_norm": 1.0330044031143188,
      "learning_rate": 0.00023444455446622103,
      "loss": 1.7241,
      "step": 860
    },
    {
      "epoch": 0.5800966827804634,
      "grad_norm": 0.8431087732315063,
      "learning_rate": 0.00023334953085825177,
      "loss": 1.8059,
      "step": 870
    },
    {
      "epoch": 0.5867644607434572,
      "grad_norm": 1.0262666940689087,
      "learning_rate": 0.00023225450725028246,
      "loss": 1.5095,
      "step": 880
    },
    {
      "epoch": 0.5934322387064511,
      "grad_norm": 0.6882549524307251,
      "learning_rate": 0.0002311594836423132,
      "loss": 1.7156,
      "step": 890
    },
    {
      "epoch": 0.6001000166694449,
      "grad_norm": 0.7464706897735596,
      "learning_rate": 0.00023006446003434394,
      "loss": 1.7136,
      "step": 900
    },
    {
      "epoch": 0.6067677946324388,
      "grad_norm": 0.8487297892570496,
      "learning_rate": 0.00022896943642637465,
      "loss": 1.6994,
      "step": 910
    },
    {
      "epoch": 0.6134355725954326,
      "grad_norm": 0.9807471036911011,
      "learning_rate": 0.0002278744128184054,
      "loss": 1.7529,
      "step": 920
    },
    {
      "epoch": 0.6201033505584264,
      "grad_norm": 0.9035906195640564,
      "learning_rate": 0.00022677938921043613,
      "loss": 1.7757,
      "step": 930
    },
    {
      "epoch": 0.6267711285214203,
      "grad_norm": 1.0267268419265747,
      "learning_rate": 0.00022568436560246687,
      "loss": 1.6915,
      "step": 940
    },
    {
      "epoch": 0.633438906484414,
      "grad_norm": 0.9362131953239441,
      "learning_rate": 0.00022458934199449758,
      "loss": 1.7158,
      "step": 950
    },
    {
      "epoch": 0.6401066844474079,
      "grad_norm": 0.9072103500366211,
      "learning_rate": 0.00022349431838652832,
      "loss": 1.7058,
      "step": 960
    },
    {
      "epoch": 0.6467744624104017,
      "grad_norm": 1.0249100923538208,
      "learning_rate": 0.00022239929477855906,
      "loss": 1.6864,
      "step": 970
    },
    {
      "epoch": 0.6534422403733956,
      "grad_norm": 1.2252609729766846,
      "learning_rate": 0.00022130427117058975,
      "loss": 1.6201,
      "step": 980
    },
    {
      "epoch": 0.6601100183363894,
      "grad_norm": 0.8415032029151917,
      "learning_rate": 0.0002202092475626205,
      "loss": 1.621,
      "step": 990
    },
    {
      "epoch": 0.6667777962993833,
      "grad_norm": 1.1598174571990967,
      "learning_rate": 0.00021911422395465123,
      "loss": 1.7455,
      "step": 1000
    },
    {
      "epoch": 0.6734455742623771,
      "grad_norm": 1.0636833906173706,
      "learning_rate": 0.00021801920034668194,
      "loss": 1.4964,
      "step": 1010
    },
    {
      "epoch": 0.680113352225371,
      "grad_norm": 0.9405612349510193,
      "learning_rate": 0.00021692417673871268,
      "loss": 1.6539,
      "step": 1020
    },
    {
      "epoch": 0.6867811301883647,
      "grad_norm": 0.9872044324874878,
      "learning_rate": 0.00021582915313074342,
      "loss": 1.6603,
      "step": 1030
    },
    {
      "epoch": 0.6934489081513585,
      "grad_norm": 0.877098023891449,
      "learning_rate": 0.00021473412952277413,
      "loss": 1.6023,
      "step": 1040
    },
    {
      "epoch": 0.7001166861143524,
      "grad_norm": 0.808498203754425,
      "learning_rate": 0.00021363910591480487,
      "loss": 1.5485,
      "step": 1050
    },
    {
      "epoch": 0.7067844640773462,
      "grad_norm": 1.0939313173294067,
      "learning_rate": 0.0002125440823068356,
      "loss": 1.5224,
      "step": 1060
    },
    {
      "epoch": 0.7134522420403401,
      "grad_norm": 0.8380913138389587,
      "learning_rate": 0.00021144905869886635,
      "loss": 1.6897,
      "step": 1070
    },
    {
      "epoch": 0.7201200200033339,
      "grad_norm": 1.350712537765503,
      "learning_rate": 0.00021035403509089704,
      "loss": 1.5867,
      "step": 1080
    },
    {
      "epoch": 0.7267877979663278,
      "grad_norm": 0.7000626921653748,
      "learning_rate": 0.00020925901148292778,
      "loss": 1.6357,
      "step": 1090
    },
    {
      "epoch": 0.7334555759293215,
      "grad_norm": 0.8607426285743713,
      "learning_rate": 0.00020816398787495852,
      "loss": 1.5921,
      "step": 1100
    },
    {
      "epoch": 0.7401233538923154,
      "grad_norm": 1.0080475807189941,
      "learning_rate": 0.00020706896426698923,
      "loss": 1.5511,
      "step": 1110
    },
    {
      "epoch": 0.7467911318553092,
      "grad_norm": 1.3415251970291138,
      "learning_rate": 0.00020597394065901997,
      "loss": 1.7157,
      "step": 1120
    },
    {
      "epoch": 0.7534589098183031,
      "grad_norm": 1.1041979789733887,
      "learning_rate": 0.0002048789170510507,
      "loss": 1.5895,
      "step": 1130
    },
    {
      "epoch": 0.7601266877812969,
      "grad_norm": 1.0156821012496948,
      "learning_rate": 0.0002037838934430814,
      "loss": 1.5595,
      "step": 1140
    },
    {
      "epoch": 0.7667944657442907,
      "grad_norm": 1.092016577720642,
      "learning_rate": 0.00020268886983511214,
      "loss": 1.5562,
      "step": 1150
    },
    {
      "epoch": 0.7734622437072846,
      "grad_norm": 0.8313906788825989,
      "learning_rate": 0.00020159384622714288,
      "loss": 1.6201,
      "step": 1160
    },
    {
      "epoch": 0.7801300216702783,
      "grad_norm": 1.0406640768051147,
      "learning_rate": 0.0002004988226191736,
      "loss": 1.5376,
      "step": 1170
    },
    {
      "epoch": 0.7867977996332722,
      "grad_norm": 0.8267114758491516,
      "learning_rate": 0.00019940379901120433,
      "loss": 1.624,
      "step": 1180
    },
    {
      "epoch": 0.793465577596266,
      "grad_norm": 1.078299880027771,
      "learning_rate": 0.00019830877540323507,
      "loss": 1.6179,
      "step": 1190
    },
    {
      "epoch": 0.8001333555592599,
      "grad_norm": 0.9592360258102417,
      "learning_rate": 0.0001972137517952658,
      "loss": 1.5572,
      "step": 1200
    },
    {
      "epoch": 0.8068011335222537,
      "grad_norm": 0.9808937907218933,
      "learning_rate": 0.00019611872818729652,
      "loss": 1.6508,
      "step": 1210
    },
    {
      "epoch": 0.8134689114852476,
      "grad_norm": 1.0693460702896118,
      "learning_rate": 0.00019502370457932726,
      "loss": 1.4669,
      "step": 1220
    },
    {
      "epoch": 0.8201366894482414,
      "grad_norm": 0.9904124736785889,
      "learning_rate": 0.000193928680971358,
      "loss": 1.6616,
      "step": 1230
    },
    {
      "epoch": 0.8268044674112353,
      "grad_norm": 1.0319327116012573,
      "learning_rate": 0.0001928336573633887,
      "loss": 1.5151,
      "step": 1240
    },
    {
      "epoch": 0.833472245374229,
      "grad_norm": 0.8702383041381836,
      "learning_rate": 0.00019173863375541943,
      "loss": 1.6246,
      "step": 1250
    },
    {
      "epoch": 0.8401400233372228,
      "grad_norm": 1.055994987487793,
      "learning_rate": 0.00019064361014745017,
      "loss": 1.5941,
      "step": 1260
    },
    {
      "epoch": 0.8468078013002167,
      "grad_norm": 1.0014089345932007,
      "learning_rate": 0.00018954858653948088,
      "loss": 1.4442,
      "step": 1270
    },
    {
      "epoch": 0.8534755792632105,
      "grad_norm": 0.9701205492019653,
      "learning_rate": 0.00018845356293151162,
      "loss": 1.5679,
      "step": 1280
    },
    {
      "epoch": 0.8601433572262044,
      "grad_norm": 1.1218522787094116,
      "learning_rate": 0.00018735853932354236,
      "loss": 1.5797,
      "step": 1290
    },
    {
      "epoch": 0.8668111351891982,
      "grad_norm": 0.9281911849975586,
      "learning_rate": 0.00018626351571557307,
      "loss": 1.5,
      "step": 1300
    },
    {
      "epoch": 0.8734789131521921,
      "grad_norm": 0.9144324660301208,
      "learning_rate": 0.0001851684921076038,
      "loss": 1.522,
      "step": 1310
    },
    {
      "epoch": 0.8801466911151858,
      "grad_norm": 1.0375608205795288,
      "learning_rate": 0.00018407346849963455,
      "loss": 1.5491,
      "step": 1320
    },
    {
      "epoch": 0.8868144690781797,
      "grad_norm": 1.2301338911056519,
      "learning_rate": 0.0001829784448916653,
      "loss": 1.5144,
      "step": 1330
    },
    {
      "epoch": 0.8934822470411735,
      "grad_norm": 0.7351090908050537,
      "learning_rate": 0.00018188342128369598,
      "loss": 1.5648,
      "step": 1340
    },
    {
      "epoch": 0.9001500250041674,
      "grad_norm": 0.9540753364562988,
      "learning_rate": 0.00018078839767572672,
      "loss": 1.5081,
      "step": 1350
    },
    {
      "epoch": 0.9068178029671612,
      "grad_norm": 0.910214364528656,
      "learning_rate": 0.00017969337406775746,
      "loss": 1.571,
      "step": 1360
    },
    {
      "epoch": 0.913485580930155,
      "grad_norm": 0.7877411842346191,
      "learning_rate": 0.00017859835045978817,
      "loss": 1.4196,
      "step": 1370
    },
    {
      "epoch": 0.9201533588931489,
      "grad_norm": 0.9003491997718811,
      "learning_rate": 0.0001775033268518189,
      "loss": 1.5703,
      "step": 1380
    },
    {
      "epoch": 0.9268211368561426,
      "grad_norm": 0.958170473575592,
      "learning_rate": 0.00017640830324384965,
      "loss": 1.6905,
      "step": 1390
    },
    {
      "epoch": 0.9334889148191365,
      "grad_norm": 1.0989577770233154,
      "learning_rate": 0.00017531327963588036,
      "loss": 1.5592,
      "step": 1400
    },
    {
      "epoch": 0.9401566927821303,
      "grad_norm": 1.184006690979004,
      "learning_rate": 0.0001742182560279111,
      "loss": 1.5885,
      "step": 1410
    },
    {
      "epoch": 0.9468244707451242,
      "grad_norm": 0.9371858835220337,
      "learning_rate": 0.00017312323241994182,
      "loss": 1.5251,
      "step": 1420
    },
    {
      "epoch": 0.953492248708118,
      "grad_norm": 1.0852700471878052,
      "learning_rate": 0.00017202820881197253,
      "loss": 1.6552,
      "step": 1430
    },
    {
      "epoch": 0.9601600266711119,
      "grad_norm": 1.1742899417877197,
      "learning_rate": 0.00017093318520400327,
      "loss": 1.5164,
      "step": 1440
    },
    {
      "epoch": 0.9668278046341057,
      "grad_norm": 1.0343601703643799,
      "learning_rate": 0.000169838161596034,
      "loss": 1.4602,
      "step": 1450
    },
    {
      "epoch": 0.9734955825970996,
      "grad_norm": 1.0104619264602661,
      "learning_rate": 0.00016874313798806475,
      "loss": 1.4307,
      "step": 1460
    },
    {
      "epoch": 0.9801633605600933,
      "grad_norm": 1.0695899724960327,
      "learning_rate": 0.00016764811438009546,
      "loss": 1.4955,
      "step": 1470
    },
    {
      "epoch": 0.9868311385230871,
      "grad_norm": 1.1315890550613403,
      "learning_rate": 0.0001665530907721262,
      "loss": 1.5007,
      "step": 1480
    },
    {
      "epoch": 0.993498916486081,
      "grad_norm": 1.0786445140838623,
      "learning_rate": 0.00016545806716415694,
      "loss": 1.4758,
      "step": 1490
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.3309346437454224,
      "learning_rate": 0.00016436304355618763,
      "loss": 1.5633,
      "step": 1500
    },
    {
      "epoch": 1.0,
      "eval_loss": 1.477697491645813,
      "eval_runtime": 938.3934,
      "eval_samples_per_second": 6.393,
      "eval_steps_per_second": 3.197,
      "step": 1500
    },
    {
      "epoch": 1.0066677779629938,
      "grad_norm": 1.0143388509750366,
      "learning_rate": 0.00016326801994821837,
      "loss": 1.3841,
      "step": 1510
    },
    {
      "epoch": 1.0133355559259876,
      "grad_norm": 0.9151965975761414,
      "learning_rate": 0.0001621729963402491,
      "loss": 1.467,
      "step": 1520
    },
    {
      "epoch": 1.0200033338889816,
      "grad_norm": 1.517952561378479,
      "learning_rate": 0.00016107797273227985,
      "loss": 1.5307,
      "step": 1530
    },
    {
      "epoch": 1.0266711118519753,
      "grad_norm": 0.8724790215492249,
      "learning_rate": 0.00015998294912431056,
      "loss": 1.5441,
      "step": 1540
    },
    {
      "epoch": 1.0333388898149691,
      "grad_norm": 0.9838696122169495,
      "learning_rate": 0.0001588879255163413,
      "loss": 1.5414,
      "step": 1550
    },
    {
      "epoch": 1.040006667777963,
      "grad_norm": 0.9538003206253052,
      "learning_rate": 0.000157792901908372,
      "loss": 1.4558,
      "step": 1560
    },
    {
      "epoch": 1.046674445740957,
      "grad_norm": 0.8483902812004089,
      "learning_rate": 0.00015669787830040275,
      "loss": 1.5346,
      "step": 1570
    },
    {
      "epoch": 1.0533422237039507,
      "grad_norm": 1.0977576971054077,
      "learning_rate": 0.0001556028546924335,
      "loss": 1.3975,
      "step": 1580
    },
    {
      "epoch": 1.0600100016669445,
      "grad_norm": 1.036187767982483,
      "learning_rate": 0.0001545078310844642,
      "loss": 1.3515,
      "step": 1590
    },
    {
      "epoch": 1.0666777796299383,
      "grad_norm": 0.9255346655845642,
      "learning_rate": 0.00015341280747649494,
      "loss": 1.5194,
      "step": 1600
    },
    {
      "epoch": 1.073345557592932,
      "grad_norm": 0.9278059005737305,
      "learning_rate": 0.00015231778386852566,
      "loss": 1.347,
      "step": 1610
    },
    {
      "epoch": 1.080013335555926,
      "grad_norm": 1.0491018295288086,
      "learning_rate": 0.0001512227602605564,
      "loss": 1.5135,
      "step": 1620
    },
    {
      "epoch": 1.0866811135189198,
      "grad_norm": 0.8945335745811462,
      "learning_rate": 0.00015012773665258714,
      "loss": 1.4777,
      "step": 1630
    },
    {
      "epoch": 1.0933488914819136,
      "grad_norm": 0.9904834628105164,
      "learning_rate": 0.00014903271304461785,
      "loss": 1.4061,
      "step": 1640
    },
    {
      "epoch": 1.1000166694449074,
      "grad_norm": 0.9821330904960632,
      "learning_rate": 0.00014793768943664856,
      "loss": 1.4586,
      "step": 1650
    },
    {
      "epoch": 1.1066844474079014,
      "grad_norm": 1.091017723083496,
      "learning_rate": 0.0001468426658286793,
      "loss": 1.4132,
      "step": 1660
    },
    {
      "epoch": 1.1133522253708952,
      "grad_norm": 1.0092015266418457,
      "learning_rate": 0.00014574764222071004,
      "loss": 1.3732,
      "step": 1670
    },
    {
      "epoch": 1.120020003333889,
      "grad_norm": 0.9503981471061707,
      "learning_rate": 0.00014465261861274078,
      "loss": 1.4457,
      "step": 1680
    },
    {
      "epoch": 1.1266877812968827,
      "grad_norm": 1.0111970901489258,
      "learning_rate": 0.0001435575950047715,
      "loss": 1.3786,
      "step": 1690
    },
    {
      "epoch": 1.1333555592598765,
      "grad_norm": 1.1769182682037354,
      "learning_rate": 0.0001424625713968022,
      "loss": 1.3221,
      "step": 1700
    },
    {
      "epoch": 1.1400233372228705,
      "grad_norm": 1.1997735500335693,
      "learning_rate": 0.00014136754778883295,
      "loss": 1.3488,
      "step": 1710
    },
    {
      "epoch": 1.1466911151858643,
      "grad_norm": 1.2010055780410767,
      "learning_rate": 0.00014027252418086366,
      "loss": 1.4263,
      "step": 1720
    },
    {
      "epoch": 1.153358893148858,
      "grad_norm": 1.1767289638519287,
      "learning_rate": 0.0001391775005728944,
      "loss": 1.2968,
      "step": 1730
    },
    {
      "epoch": 1.160026671111852,
      "grad_norm": 1.061316967010498,
      "learning_rate": 0.00013808247696492514,
      "loss": 1.4914,
      "step": 1740
    },
    {
      "epoch": 1.1666944490748459,
      "grad_norm": 1.1855494976043701,
      "learning_rate": 0.00013698745335695585,
      "loss": 1.2487,
      "step": 1750
    },
    {
      "epoch": 1.1733622270378397,
      "grad_norm": 1.076444149017334,
      "learning_rate": 0.0001358924297489866,
      "loss": 1.3597,
      "step": 1760
    },
    {
      "epoch": 1.1800300050008334,
      "grad_norm": 0.8391638398170471,
      "learning_rate": 0.0001347974061410173,
      "loss": 1.4044,
      "step": 1770
    },
    {
      "epoch": 1.1866977829638272,
      "grad_norm": 1.2782994508743286,
      "learning_rate": 0.00013370238253304805,
      "loss": 1.3436,
      "step": 1780
    },
    {
      "epoch": 1.1933655609268212,
      "grad_norm": 1.165113091468811,
      "learning_rate": 0.00013260735892507879,
      "loss": 1.2929,
      "step": 1790
    },
    {
      "epoch": 1.200033338889815,
      "grad_norm": 0.9811070561408997,
      "learning_rate": 0.0001315123353171095,
      "loss": 1.4146,
      "step": 1800
    },
    {
      "epoch": 1.2067011168528088,
      "grad_norm": 0.911916971206665,
      "learning_rate": 0.00013041731170914024,
      "loss": 1.4202,
      "step": 1810
    },
    {
      "epoch": 1.2133688948158026,
      "grad_norm": 0.9627054333686829,
      "learning_rate": 0.00012932228810117095,
      "loss": 1.3041,
      "step": 1820
    },
    {
      "epoch": 1.2200366727787966,
      "grad_norm": 0.923786461353302,
      "learning_rate": 0.0001282272644932017,
      "loss": 1.4221,
      "step": 1830
    },
    {
      "epoch": 1.2267044507417904,
      "grad_norm": 1.2359371185302734,
      "learning_rate": 0.00012713224088523243,
      "loss": 1.4146,
      "step": 1840
    },
    {
      "epoch": 1.2333722287047841,
      "grad_norm": 1.454553484916687,
      "learning_rate": 0.00012603721727726314,
      "loss": 1.3541,
      "step": 1850
    },
    {
      "epoch": 1.240040006667778,
      "grad_norm": 1.3504236936569214,
      "learning_rate": 0.00012494219366929388,
      "loss": 1.383,
      "step": 1860
    },
    {
      "epoch": 1.2467077846307717,
      "grad_norm": 1.0372910499572754,
      "learning_rate": 0.0001238471700613246,
      "loss": 1.3878,
      "step": 1870
    },
    {
      "epoch": 1.2533755625937657,
      "grad_norm": 1.1873918771743774,
      "learning_rate": 0.00012275214645335534,
      "loss": 1.4129,
      "step": 1880
    },
    {
      "epoch": 1.2600433405567595,
      "grad_norm": 1.034285068511963,
      "learning_rate": 0.00012165712284538606,
      "loss": 1.4132,
      "step": 1890
    },
    {
      "epoch": 1.2667111185197533,
      "grad_norm": 0.9670440554618835,
      "learning_rate": 0.00012056209923741679,
      "loss": 1.4648,
      "step": 1900
    },
    {
      "epoch": 1.273378896482747,
      "grad_norm": 1.1538009643554688,
      "learning_rate": 0.00011946707562944753,
      "loss": 1.4082,
      "step": 1910
    },
    {
      "epoch": 1.280046674445741,
      "grad_norm": 1.1140427589416504,
      "learning_rate": 0.00011837205202147826,
      "loss": 1.4565,
      "step": 1920
    },
    {
      "epoch": 1.2867144524087348,
      "grad_norm": 1.056024193763733,
      "learning_rate": 0.00011727702841350897,
      "loss": 1.4634,
      "step": 1930
    },
    {
      "epoch": 1.2933822303717286,
      "grad_norm": 1.086655616760254,
      "learning_rate": 0.00011618200480553971,
      "loss": 1.2271,
      "step": 1940
    },
    {
      "epoch": 1.3000500083347224,
      "grad_norm": 1.2214082479476929,
      "learning_rate": 0.00011508698119757043,
      "loss": 1.2862,
      "step": 1950
    },
    {
      "epoch": 1.3067177862977162,
      "grad_norm": 1.1619426012039185,
      "learning_rate": 0.00011399195758960115,
      "loss": 1.3946,
      "step": 1960
    },
    {
      "epoch": 1.3133855642607102,
      "grad_norm": 1.02764093875885,
      "learning_rate": 0.00011289693398163189,
      "loss": 1.2743,
      "step": 1970
    },
    {
      "epoch": 1.320053342223704,
      "grad_norm": 0.9369844198226929,
      "learning_rate": 0.00011180191037366261,
      "loss": 1.4467,
      "step": 1980
    },
    {
      "epoch": 1.3267211201866977,
      "grad_norm": 0.9526063203811646,
      "learning_rate": 0.00011070688676569335,
      "loss": 1.263,
      "step": 1990
    },
    {
      "epoch": 1.3333888981496917,
      "grad_norm": 1.2778414487838745,
      "learning_rate": 0.00010961186315772408,
      "loss": 1.354,
      "step": 2000
    },
    {
      "epoch": 1.3400566761126855,
      "grad_norm": 1.0935505628585815,
      "learning_rate": 0.00010851683954975479,
      "loss": 1.3544,
      "step": 2010
    },
    {
      "epoch": 1.3467244540756793,
      "grad_norm": 1.2971476316452026,
      "learning_rate": 0.00010742181594178553,
      "loss": 1.2441,
      "step": 2020
    },
    {
      "epoch": 1.353392232038673,
      "grad_norm": 0.9148457646369934,
      "learning_rate": 0.00010632679233381626,
      "loss": 1.3183,
      "step": 2030
    },
    {
      "epoch": 1.3600600100016669,
      "grad_norm": 0.9007148146629333,
      "learning_rate": 0.000105231768725847,
      "loss": 1.4074,
      "step": 2040
    },
    {
      "epoch": 1.3667277879646607,
      "grad_norm": 1.224690318107605,
      "learning_rate": 0.00010413674511787772,
      "loss": 1.1837,
      "step": 2050
    },
    {
      "epoch": 1.3733955659276547,
      "grad_norm": 1.018024206161499,
      "learning_rate": 0.00010304172150990844,
      "loss": 1.2945,
      "step": 2060
    },
    {
      "epoch": 1.3800633438906484,
      "grad_norm": 1.1403318643569946,
      "learning_rate": 0.00010194669790193918,
      "loss": 1.3535,
      "step": 2070
    },
    {
      "epoch": 1.3867311218536422,
      "grad_norm": 0.9532329440116882,
      "learning_rate": 0.0001008516742939699,
      "loss": 1.3805,
      "step": 2080
    },
    {
      "epoch": 1.3933988998166362,
      "grad_norm": 0.9327608942985535,
      "learning_rate": 9.975665068600062e-05,
      "loss": 1.2879,
      "step": 2090
    },
    {
      "epoch": 1.40006667777963,
      "grad_norm": 1.049912452697754,
      "learning_rate": 9.866162707803136e-05,
      "loss": 1.4269,
      "step": 2100
    },
    {
      "epoch": 1.4067344557426238,
      "grad_norm": 1.1286088228225708,
      "learning_rate": 9.756660347006208e-05,
      "loss": 1.3673,
      "step": 2110
    },
    {
      "epoch": 1.4134022337056176,
      "grad_norm": 1.5969793796539307,
      "learning_rate": 9.647157986209282e-05,
      "loss": 1.2602,
      "step": 2120
    },
    {
      "epoch": 1.4200700116686114,
      "grad_norm": 0.949741542339325,
      "learning_rate": 9.537655625412355e-05,
      "loss": 1.3959,
      "step": 2130
    },
    {
      "epoch": 1.4267377896316051,
      "grad_norm": 1.1393873691558838,
      "learning_rate": 9.428153264615426e-05,
      "loss": 1.3129,
      "step": 2140
    },
    {
      "epoch": 1.4334055675945991,
      "grad_norm": 1.165794849395752,
      "learning_rate": 9.3186509038185e-05,
      "loss": 1.3772,
      "step": 2150
    },
    {
      "epoch": 1.440073345557593,
      "grad_norm": 1.1384891271591187,
      "learning_rate": 9.209148543021573e-05,
      "loss": 1.3338,
      "step": 2160
    },
    {
      "epoch": 1.4467411235205867,
      "grad_norm": 1.0166438817977905,
      "learning_rate": 9.099646182224647e-05,
      "loss": 1.4372,
      "step": 2170
    },
    {
      "epoch": 1.4534089014835807,
      "grad_norm": 1.0894237756729126,
      "learning_rate": 8.99014382142772e-05,
      "loss": 1.2265,
      "step": 2180
    },
    {
      "epoch": 1.4600766794465745,
      "grad_norm": 1.2060139179229736,
      "learning_rate": 8.880641460630791e-05,
      "loss": 1.2589,
      "step": 2190
    },
    {
      "epoch": 1.4667444574095683,
      "grad_norm": 0.9493557810783386,
      "learning_rate": 8.771139099833865e-05,
      "loss": 1.5177,
      "step": 2200
    },
    {
      "epoch": 1.473412235372562,
      "grad_norm": 1.228654384613037,
      "learning_rate": 8.661636739036937e-05,
      "loss": 1.3703,
      "step": 2210
    },
    {
      "epoch": 1.4800800133355558,
      "grad_norm": 1.0284937620162964,
      "learning_rate": 8.552134378240011e-05,
      "loss": 1.2655,
      "step": 2220
    },
    {
      "epoch": 1.4867477912985498,
      "grad_norm": 1.0114820003509521,
      "learning_rate": 8.442632017443084e-05,
      "loss": 1.3593,
      "step": 2230
    },
    {
      "epoch": 1.4934155692615436,
      "grad_norm": 0.9141907691955566,
      "learning_rate": 8.333129656646155e-05,
      "loss": 1.3152,
      "step": 2240
    },
    {
      "epoch": 1.5000833472245374,
      "grad_norm": 0.959983229637146,
      "learning_rate": 8.223627295849229e-05,
      "loss": 1.3802,
      "step": 2250
    },
    {
      "epoch": 1.5067511251875314,
      "grad_norm": 0.9626503586769104,
      "learning_rate": 8.114124935052302e-05,
      "loss": 1.3074,
      "step": 2260
    },
    {
      "epoch": 1.5134189031505252,
      "grad_norm": 1.2857861518859863,
      "learning_rate": 8.004622574255375e-05,
      "loss": 1.4232,
      "step": 2270
    },
    {
      "epoch": 1.520086681113519,
      "grad_norm": 1.1132835149765015,
      "learning_rate": 7.895120213458447e-05,
      "loss": 1.264,
      "step": 2280
    },
    {
      "epoch": 1.5267544590765127,
      "grad_norm": 1.1615244150161743,
      "learning_rate": 7.78561785266152e-05,
      "loss": 1.3534,
      "step": 2290
    },
    {
      "epoch": 1.5334222370395065,
      "grad_norm": 1.062968373298645,
      "learning_rate": 7.676115491864592e-05,
      "loss": 1.331,
      "step": 2300
    },
    {
      "epoch": 1.5400900150025003,
      "grad_norm": 1.0517151355743408,
      "learning_rate": 7.566613131067666e-05,
      "loss": 1.4329,
      "step": 2310
    },
    {
      "epoch": 1.546757792965494,
      "grad_norm": 1.2428632974624634,
      "learning_rate": 7.457110770270739e-05,
      "loss": 1.4589,
      "step": 2320
    },
    {
      "epoch": 1.553425570928488,
      "grad_norm": 0.9916052222251892,
      "learning_rate": 7.347608409473812e-05,
      "loss": 1.4312,
      "step": 2330
    },
    {
      "epoch": 1.5600933488914819,
      "grad_norm": 1.0296847820281982,
      "learning_rate": 7.238106048676884e-05,
      "loss": 1.3721,
      "step": 2340
    },
    {
      "epoch": 1.5667611268544759,
      "grad_norm": 1.0973352193832397,
      "learning_rate": 7.128603687879957e-05,
      "loss": 1.1575,
      "step": 2350
    },
    {
      "epoch": 1.5734289048174697,
      "grad_norm": 1.1556836366653442,
      "learning_rate": 7.019101327083031e-05,
      "loss": 1.2523,
      "step": 2360
    },
    {
      "epoch": 1.5800966827804634,
      "grad_norm": 0.9928479790687561,
      "learning_rate": 6.909598966286104e-05,
      "loss": 1.2339,
      "step": 2370
    },
    {
      "epoch": 1.5867644607434572,
      "grad_norm": 1.1566845178604126,
      "learning_rate": 6.800096605489176e-05,
      "loss": 1.227,
      "step": 2380
    },
    {
      "epoch": 1.593432238706451,
      "grad_norm": 1.148449182510376,
      "learning_rate": 6.690594244692249e-05,
      "loss": 1.3484,
      "step": 2390
    },
    {
      "epoch": 1.6001000166694448,
      "grad_norm": 1.1149173974990845,
      "learning_rate": 6.581091883895321e-05,
      "loss": 1.3042,
      "step": 2400
    },
    {
      "epoch": 1.6067677946324388,
      "grad_norm": 0.9513399600982666,
      "learning_rate": 6.471589523098394e-05,
      "loss": 1.2594,
      "step": 2410
    },
    {
      "epoch": 1.6134355725954326,
      "grad_norm": 1.1178479194641113,
      "learning_rate": 6.362087162301467e-05,
      "loss": 1.2499,
      "step": 2420
    },
    {
      "epoch": 1.6201033505584264,
      "grad_norm": 1.1255996227264404,
      "learning_rate": 6.25258480150454e-05,
      "loss": 1.3739,
      "step": 2430
    },
    {
      "epoch": 1.6267711285214204,
      "grad_norm": 0.8564472198486328,
      "learning_rate": 6.143082440707613e-05,
      "loss": 1.3816,
      "step": 2440
    },
    {
      "epoch": 1.6334389064844141,
      "grad_norm": 0.9265642762184143,
      "learning_rate": 6.033580079910686e-05,
      "loss": 1.444,
      "step": 2450
    },
    {
      "epoch": 1.640106684447408,
      "grad_norm": 0.8915114402770996,
      "learning_rate": 5.924077719113759e-05,
      "loss": 1.2988,
      "step": 2460
    },
    {
      "epoch": 1.6467744624104017,
      "grad_norm": 0.9406484365463257,
      "learning_rate": 5.814575358316831e-05,
      "loss": 1.3337,
      "step": 2470
    },
    {
      "epoch": 1.6534422403733955,
      "grad_norm": 0.9783940315246582,
      "learning_rate": 5.7050729975199046e-05,
      "loss": 1.3696,
      "step": 2480
    },
    {
      "epoch": 1.6601100183363893,
      "grad_norm": 0.9977773427963257,
      "learning_rate": 5.595570636722977e-05,
      "loss": 1.193,
      "step": 2490
    },
    {
      "epoch": 1.6667777962993833,
      "grad_norm": 0.9164002537727356,
      "learning_rate": 5.4860682759260505e-05,
      "loss": 1.383,
      "step": 2500
    },
    {
      "epoch": 1.673445574262377,
      "grad_norm": 0.895294725894928,
      "learning_rate": 5.3765659151291225e-05,
      "loss": 1.3681,
      "step": 2510
    },
    {
      "epoch": 1.680113352225371,
      "grad_norm": 1.0275071859359741,
      "learning_rate": 5.267063554332196e-05,
      "loss": 1.3633,
      "step": 2520
    },
    {
      "epoch": 1.6867811301883648,
      "grad_norm": 1.0997161865234375,
      "learning_rate": 5.1575611935352684e-05,
      "loss": 1.2759,
      "step": 2530
    },
    {
      "epoch": 1.6934489081513586,
      "grad_norm": 0.9609471559524536,
      "learning_rate": 5.048058832738342e-05,
      "loss": 1.281,
      "step": 2540
    },
    {
      "epoch": 1.7001166861143524,
      "grad_norm": 1.1261390447616577,
      "learning_rate": 4.938556471941415e-05,
      "loss": 1.4353,
      "step": 2550
    },
    {
      "epoch": 1.7067844640773462,
      "grad_norm": 1.056107759475708,
      "learning_rate": 4.829054111144487e-05,
      "loss": 1.171,
      "step": 2560
    },
    {
      "epoch": 1.71345224204034,
      "grad_norm": 1.1906152963638306,
      "learning_rate": 4.7195517503475603e-05,
      "loss": 1.2669,
      "step": 2570
    },
    {
      "epoch": 1.7201200200033337,
      "grad_norm": 0.8771300911903381,
      "learning_rate": 4.610049389550633e-05,
      "loss": 1.3515,
      "step": 2580
    },
    {
      "epoch": 1.7267877979663278,
      "grad_norm": 1.1197104454040527,
      "learning_rate": 4.500547028753706e-05,
      "loss": 1.3692,
      "step": 2590
    },
    {
      "epoch": 1.7334555759293215,
      "grad_norm": 0.9254445433616638,
      "learning_rate": 4.391044667956778e-05,
      "loss": 1.2612,
      "step": 2600
    },
    {
      "epoch": 1.7401233538923155,
      "grad_norm": 1.0177791118621826,
      "learning_rate": 4.2815423071598516e-05,
      "loss": 1.269,
      "step": 2610
    },
    {
      "epoch": 1.7467911318553093,
      "grad_norm": 1.052605390548706,
      "learning_rate": 4.172039946362924e-05,
      "loss": 1.3458,
      "step": 2620
    },
    {
      "epoch": 1.753458909818303,
      "grad_norm": 0.9492025971412659,
      "learning_rate": 4.062537585565997e-05,
      "loss": 1.2481,
      "step": 2630
    },
    {
      "epoch": 1.7601266877812969,
      "grad_norm": 0.8648191690444946,
      "learning_rate": 3.95303522476907e-05,
      "loss": 1.2604,
      "step": 2640
    },
    {
      "epoch": 1.7667944657442907,
      "grad_norm": 1.0330696105957031,
      "learning_rate": 3.843532863972143e-05,
      "loss": 1.2173,
      "step": 2650
    },
    {
      "epoch": 1.7734622437072844,
      "grad_norm": 1.0769561529159546,
      "learning_rate": 3.7340305031752154e-05,
      "loss": 1.2865,
      "step": 2660
    },
    {
      "epoch": 1.7801300216702782,
      "grad_norm": 1.154012680053711,
      "learning_rate": 3.624528142378289e-05,
      "loss": 1.445,
      "step": 2670
    },
    {
      "epoch": 1.7867977996332722,
      "grad_norm": 0.7577117085456848,
      "learning_rate": 3.5150257815813614e-05,
      "loss": 1.1786,
      "step": 2680
    },
    {
      "epoch": 1.793465577596266,
      "grad_norm": 1.1459170579910278,
      "learning_rate": 3.405523420784435e-05,
      "loss": 1.2755,
      "step": 2690
    },
    {
      "epoch": 1.80013335555926,
      "grad_norm": 1.0360689163208008,
      "learning_rate": 3.296021059987507e-05,
      "loss": 1.2834,
      "step": 2700
    },
    {
      "epoch": 1.8068011335222538,
      "grad_norm": 1.0309133529663086,
      "learning_rate": 3.18651869919058e-05,
      "loss": 1.3407,
      "step": 2710
    },
    {
      "epoch": 1.8134689114852476,
      "grad_norm": 0.9453358054161072,
      "learning_rate": 3.0770163383936526e-05,
      "loss": 1.3068,
      "step": 2720
    },
    {
      "epoch": 1.8201366894482414,
      "grad_norm": 0.9696240425109863,
      "learning_rate": 2.967513977596726e-05,
      "loss": 1.2007,
      "step": 2730
    },
    {
      "epoch": 1.8268044674112351,
      "grad_norm": 0.9463112950325012,
      "learning_rate": 2.8580116167997985e-05,
      "loss": 1.2754,
      "step": 2740
    },
    {
      "epoch": 1.833472245374229,
      "grad_norm": 1.2692524194717407,
      "learning_rate": 2.7485092560028715e-05,
      "loss": 1.2912,
      "step": 2750
    },
    {
      "epoch": 1.8401400233372227,
      "grad_norm": 0.9069505333900452,
      "learning_rate": 2.6390068952059445e-05,
      "loss": 1.3394,
      "step": 2760
    },
    {
      "epoch": 1.8468078013002167,
      "grad_norm": 1.1452518701553345,
      "learning_rate": 2.529504534409017e-05,
      "loss": 1.3174,
      "step": 2770
    },
    {
      "epoch": 1.8534755792632105,
      "grad_norm": 1.000296950340271,
      "learning_rate": 2.42000217361209e-05,
      "loss": 1.2605,
      "step": 2780
    },
    {
      "epoch": 1.8601433572262045,
      "grad_norm": 1.033894419670105,
      "learning_rate": 2.3104998128151627e-05,
      "loss": 1.0773,
      "step": 2790
    },
    {
      "epoch": 1.8668111351891983,
      "grad_norm": 1.1695852279663086,
      "learning_rate": 2.2009974520182357e-05,
      "loss": 1.2718,
      "step": 2800
    },
    {
      "epoch": 1.873478913152192,
      "grad_norm": 1.1237270832061768,
      "learning_rate": 2.0914950912213083e-05,
      "loss": 1.3415,
      "step": 2810
    },
    {
      "epoch": 1.8801466911151858,
      "grad_norm": 1.3278158903121948,
      "learning_rate": 1.9819927304243813e-05,
      "loss": 1.2724,
      "step": 2820
    },
    {
      "epoch": 1.8868144690781796,
      "grad_norm": 1.0758094787597656,
      "learning_rate": 1.8724903696274543e-05,
      "loss": 1.2841,
      "step": 2830
    },
    {
      "epoch": 1.8934822470411734,
      "grad_norm": 1.1032332181930542,
      "learning_rate": 1.7629880088305273e-05,
      "loss": 1.2702,
      "step": 2840
    },
    {
      "epoch": 1.9001500250041674,
      "grad_norm": 1.096437692642212,
      "learning_rate": 1.6534856480336e-05,
      "loss": 1.2135,
      "step": 2850
    },
    {
      "epoch": 1.9068178029671612,
      "grad_norm": 0.8255008459091187,
      "learning_rate": 1.543983287236673e-05,
      "loss": 1.3193,
      "step": 2860
    },
    {
      "epoch": 1.913485580930155,
      "grad_norm": 0.9289690852165222,
      "learning_rate": 1.4344809264397457e-05,
      "loss": 1.3185,
      "step": 2870
    },
    {
      "epoch": 1.920153358893149,
      "grad_norm": 0.8011090159416199,
      "learning_rate": 1.3249785656428185e-05,
      "loss": 1.26,
      "step": 2880
    },
    {
      "epoch": 1.9268211368561428,
      "grad_norm": 0.9766982793807983,
      "learning_rate": 1.2154762048458913e-05,
      "loss": 1.2338,
      "step": 2890
    },
    {
      "epoch": 1.9334889148191365,
      "grad_norm": 1.0009205341339111,
      "learning_rate": 1.105973844048964e-05,
      "loss": 1.2695,
      "step": 2900
    },
    {
      "epoch": 1.9401566927821303,
      "grad_norm": 1.0122109651565552,
      "learning_rate": 9.96471483252037e-06,
      "loss": 1.3338,
      "step": 2910
    },
    {
      "epoch": 1.946824470745124,
      "grad_norm": 1.0244556665420532,
      "learning_rate": 8.869691224551099e-06,
      "loss": 1.1963,
      "step": 2920
    },
    {
      "epoch": 1.9534922487081179,
      "grad_norm": 1.0457642078399658,
      "learning_rate": 7.774667616581827e-06,
      "loss": 1.3515,
      "step": 2930
    },
    {
      "epoch": 1.9601600266711119,
      "grad_norm": 1.193676233291626,
      "learning_rate": 6.6796440086125555e-06,
      "loss": 1.2322,
      "step": 2940
    },
    {
      "epoch": 1.9668278046341057,
      "grad_norm": 1.0501303672790527,
      "learning_rate": 5.584620400643285e-06,
      "loss": 1.324,
      "step": 2950
    },
    {
      "epoch": 1.9734955825970997,
      "grad_norm": 1.0488941669464111,
      "learning_rate": 4.489596792674013e-06,
      "loss": 1.1783,
      "step": 2960
    },
    {
      "epoch": 1.9801633605600935,
      "grad_norm": 0.9460679292678833,
      "learning_rate": 3.3945731847047414e-06,
      "loss": 1.3053,
      "step": 2970
    },
    {
      "epoch": 1.9868311385230872,
      "grad_norm": 0.9263679385185242,
      "learning_rate": 2.2995495767354703e-06,
      "loss": 1.1536,
      "step": 2980
    },
    {
      "epoch": 1.993498916486081,
      "grad_norm": 0.9658956527709961,
      "learning_rate": 1.2045259687661985e-06,
      "loss": 1.2047,
      "step": 2990
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.9439611434936523,
      "learning_rate": 1.0950236079692714e-07,
      "loss": 1.2674,
      "step": 3000
    },
    {
      "epoch": 2.0,
      "eval_loss": 1.2789655923843384,
      "eval_runtime": 938.1072,
      "eval_samples_per_second": 6.395,
      "eval_steps_per_second": 3.198,
      "step": 3000
    }
  ],
  "logging_steps": 10,
  "max_steps": 3000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 500,
  "stateful_callbacks": {
    "EarlyStoppingCallback": {
      "args": {
        "early_stopping_patience": 2,
        "early_stopping_threshold": 0.0
      },
      "attributes": {
        "early_stopping_patience_counter": 0
      }
    },
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.0862639079214285e+18,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
