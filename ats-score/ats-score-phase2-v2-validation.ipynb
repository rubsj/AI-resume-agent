{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec64c0b3",
   "metadata": {},
   "source": [
    "# Global utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7577143f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_running_in_colab():\n",
    "    try:\n",
    "        import google.colab  # Only available in Colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb849be",
   "metadata": {},
   "source": [
    "# Phase 1: First Steps Notebook ‚Äî Data Ingestion + Minimal Parsing\n",
    "1. Setup and Install Dependencies\n",
    "2. Load Resume and JD datasets\n",
    "3. Minimal Parsing into JSON Structure\n",
    "4. Save structured JSON for Phase 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dd9696",
   "metadata": {},
   "source": [
    "## Setup and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4abca3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install kaggle kagglehub pandas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb756b8f",
   "metadata": {},
   "source": [
    "## Util Classes and methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43abc8ac",
   "metadata": {},
   "source": [
    "### Configurations  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e827a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# üõ† CONFIGURATION\n",
    "# ==============================\n",
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "class Config:\n",
    "    DATASET_DOWNLOAD_DIR = \"datasets\"\n",
    "    JSON_OUTPUT_DIR = \"json_outputs\"\n",
    "    AUTO_CLEANUP = True\n",
    "\n",
    "    @staticmethod\n",
    "    def setup_kaggle_credentials():\n",
    "        kaggle_path = os.path.expanduser('~/.kaggle/kaggle.json')\n",
    "        if not os.path.exists(kaggle_path):\n",
    "            from google.colab import files\n",
    "            print(\"üìÇ Upload kaggle.json file...\")\n",
    "            uploaded = files.upload()\n",
    "            os.makedirs(os.path.dirname(kaggle_path), exist_ok=True)\n",
    "            for filename in uploaded.keys():\n",
    "                shutil.move(filename, kaggle_path)\n",
    "            os.chmod(kaggle_path, 0o600)\n",
    "            print(f\"‚úÖ Kaggle credentials setup at {kaggle_path}\")\n",
    "        else:\n",
    "            print(f\"‚úÖ Kaggle credentials already exist at {kaggle_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a537a5",
   "metadata": {},
   "source": [
    "### Downloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89480102",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================\n",
    "# DOWNLOADER\n",
    "# ==============================\n",
    "class DatasetDownloader:\n",
    "    @staticmethod\n",
    "    def download_and_extract(dataset_path: str) -> tuple[str, str]:\n",
    "        os.makedirs(Config.DATASET_DOWNLOAD_DIR, exist_ok=True)\n",
    "        dataset_slug = dataset_path.split(\"/\")[-1]\n",
    "        extract_folder_path = os.path.join(Config.DATASET_DOWNLOAD_DIR, dataset_slug)\n",
    "        zip_filename = f\"{dataset_slug}.zip\"\n",
    "        zip_path = os.path.join(Config.DATASET_DOWNLOAD_DIR, zip_filename)\n",
    "\n",
    "        if os.path.exists(extract_folder_path) and any(Path(extract_folder_path).rglob(\"*.csv\")):\n",
    "            print(f\"‚ö° Dataset folder already exists at '{extract_folder_path}', skipping download and extraction.\")\n",
    "            return extract_folder_path, zip_filename\n",
    "\n",
    "        print(f\"‚¨áÔ∏è Downloading dataset: {dataset_path} ...\")\n",
    "        !kaggle datasets download -d {dataset_path} -p {Config.DATASET_DOWNLOAD_DIR}\n",
    "\n",
    "        if not os.path.exists(zip_path):\n",
    "            raise FileNotFoundError(f\"‚ùå Zip file '{zip_filename}' not found after download!\")\n",
    "\n",
    "        os.makedirs(extract_folder_path, exist_ok=True)\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_folder_path)\n",
    "\n",
    "        print(f\"‚úÖ Downloaded and extracted to '{extract_folder_path}'.\")\n",
    "        return extract_folder_path, zip_filename\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1170df00",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c06119",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================\n",
    "# LOADER\n",
    "# ==============================\n",
    "class DatasetLoader:\n",
    "    @staticmethod\n",
    "    def load_csv(dataset_folder: str, target_csv_name: str) -> pd.DataFrame:\n",
    "        print(f\"üîç Searching for '{target_csv_name}' inside {dataset_folder}...\")\n",
    "        if not os.path.exists(dataset_folder):\n",
    "            raise FileNotFoundError(f\"‚ùå Dataset folder '{dataset_folder}' does not exist!\")\n",
    "\n",
    "        for root, _, files in os.walk(dataset_folder):\n",
    "            for file in files:\n",
    "                if file.lower() == target_csv_name.lower():\n",
    "                    csv_path = os.path.join(root, file)\n",
    "                    df = pd.read_csv(csv_path)\n",
    "                    print(f\"‚úÖ Loaded CSV with shape {df.shape}\")\n",
    "                    return df\n",
    "\n",
    "        raise FileNotFoundError(f\"‚ùå CSV file '{target_csv_name}' not found inside extracted dataset!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a80a5a",
   "metadata": {},
   "source": [
    "### Data Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08247c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================\n",
    "# PROCESSOR\n",
    "# ==============================\n",
    "class DatasetProcessor:\n",
    "    @staticmethod\n",
    "    def filter_fields(df: pd.DataFrame, allowed_fields: List[str]) -> pd.DataFrame:\n",
    "        missing_fields = [field for field in allowed_fields if field not in df.columns]\n",
    "        if missing_fields:\n",
    "            raise ValueError(f\"‚ùå Fields {missing_fields} not found in dataset!\")\n",
    "\n",
    "        filtered_df = df[allowed_fields]\n",
    "        print(f\"‚úÖ Filtered columns: {list(filtered_df.columns)}\")\n",
    "        return filtered_df\n",
    "\n",
    "    @staticmethod\n",
    "    def save_to_json(df: pd.DataFrame, output_json_name: str):\n",
    "        os.makedirs(Config.JSON_OUTPUT_DIR, exist_ok=True)\n",
    "        output_path = os.path.join(Config.JSON_OUTPUT_DIR, output_json_name)\n",
    "\n",
    "        if os.path.exists(output_path):\n",
    "            os.remove(output_path)\n",
    "            print(f\"üóëÔ∏è Existing JSON '{output_path}' deleted.\")\n",
    "\n",
    "        df.to_json(output_path, orient='records', lines=True, force_ascii=False)\n",
    "        print(f\"‚úÖ Data saved to JSON at '{output_path}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fc4fb6",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7245255",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ==============================\n",
    "# CLEANER\n",
    "# ==============================\n",
    "class Cleaner:\n",
    "    @staticmethod\n",
    "    def cleanup_dataset_artifacts(extracted_folder_path: str, zip_filename: str):\n",
    "        if os.path.exists(extracted_folder_path):\n",
    "            shutil.rmtree(extracted_folder_path)\n",
    "            print(f\"üßπ Folder '{extracted_folder_path}' has been deleted successfully.\")\n",
    "\n",
    "        zip_path = os.path.join(Config.DATASET_DOWNLOAD_DIR, zip_filename)\n",
    "        if os.path.exists(zip_path):\n",
    "            os.remove(zip_path)\n",
    "            print(f\"üóëÔ∏è Zip file '{zip_path}' has been deleted successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a2525b",
   "metadata": {},
   "source": [
    "### Hybrid Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d85253",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ==============================\n",
    "# HYBRID LOADER\n",
    "# ==============================\n",
    "try:\n",
    "    import kagglehub\n",
    "    from kagglehub import KaggleDatasetAdapter\n",
    "except ImportError:\n",
    "    kagglehub = None\n",
    "\n",
    "class HybridDatasetLoader:\n",
    "    @staticmethod\n",
    "    def load_dataset(dataset_path: str, file_name: str) -> pd.DataFrame:\n",
    "        if kagglehub:\n",
    "            try:\n",
    "                print(f\"üì• Trying KaggleHub for {dataset_path}...\")\n",
    "                df = kagglehub.dataset_load(KaggleDatasetAdapter.PANDAS, dataset_path, file_name)\n",
    "                print(f\"‚úÖ Loaded using KaggleHub: shape = {df.shape}\")\n",
    "                return df\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è KaggleHub failed: {e}\\nFalling back to ZIP-based loader.\")\n",
    "\n",
    "        extracted_folder, _ = DatasetDownloader.download_and_extract(dataset_path)\n",
    "        return DatasetLoader.load_csv(extracted_folder, file_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e917e9d6",
   "metadata": {},
   "source": [
    "### Main flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766a42bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================\n",
    "# MAIN FLOW\n",
    "# ==============================\n",
    "def process_dataset(dataset_path: str, target_csv_name: str, allowed_fields: List[str], output_json_name: str):\n",
    "    df = HybridDatasetLoader.load_dataset(dataset_path, target_csv_name)\n",
    "    filtered_df = DatasetProcessor.filter_fields(df, allowed_fields)\n",
    "    DatasetProcessor.save_to_json(filtered_df, output_json_name)\n",
    "\n",
    "    dataset_slug = dataset_path.split(\"/\")[-1]\n",
    "    extracted_folder = os.path.join(Config.DATASET_DOWNLOAD_DIR, dataset_slug)\n",
    "    zip_filename = f\"{dataset_slug}.zip\"\n",
    "    if Config.AUTO_CLEANUP:\n",
    "        Cleaner.cleanup_dataset_artifacts(extracted_folder, zip_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60577088",
   "metadata": {},
   "source": [
    "## Login and do the processing of Resume and JD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bb5022",
   "metadata": {},
   "outputs": [],
   "source": [
    "Config.setup_kaggle_credentials()\n",
    "# Process Resume Dataset\n",
    "process_dataset(\n",
    "    dataset_path=\"snehaanbhawal/resume-dataset\",\n",
    "    target_csv_name=\"Resume.csv\",\n",
    "    allowed_fields=[\"Category\", \"Resume_str\"],\n",
    "    output_json_name=\"parsed_resumes.json\"\n",
    ")\n",
    "\n",
    "# Process Job Postings Dataset\n",
    "process_dataset(\n",
    "    dataset_path=\"arshkon/linkedin-job-postings\",\n",
    "    target_csv_name=\"postings.csv\",\n",
    "    allowed_fields=[\"title\", \"company_name\", \"location\", \"description\", \"skills_desc\", \"job_id\" , \"formatted_experience_level\", \"formatted_work_type\"],\n",
    "    output_json_name=\"parsed_jds.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74195950",
   "metadata": {},
   "source": [
    "# Phase 2 -\tParse resume/JD into JSON structured scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27168f09",
   "metadata": {},
   "source": [
    "##  Install Dependencies  & Login to Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381adace",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q transformers accelerate sentencepiece pydantic huggingface_hub\n",
    "\n",
    "#%pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu121\n",
    "#%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "%pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128\n",
    "%pip install -U bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d970ff92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# Set your token here securely or prompt for it in Colab\n",
    "# Recommended: store in Colab secrets or environment variable\n",
    "HF_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "\n",
    "if not HF_TOKEN:\n",
    "    if is_running_in_colab():\n",
    "        # If running in Colab, use the Colab secrets\n",
    "        try:\n",
    "            from google.colab import userdata\n",
    "            HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "            if not HF_TOKEN:\n",
    "                raise ValueError(\"‚ö†Ô∏è Hugging Face token not found in Colab secrets.\")\n",
    "            print(\"üîë Hugging Face token found in Colab secrets.\")\n",
    "        except ImportError:\n",
    "            print(\"‚ö†Ô∏è Unable to authenticate in Colab. Please set your Hugging Face token manually.\")\n",
    "    else:\n",
    "        # Prompt for token if not set in environment\n",
    "        print(\"üîë Please enter your Hugging Face token:\")\n",
    "        # For Colab or local prompt input\n",
    "        HF_TOKEN = input(\"üîë Enter your Hugging Face token: \").strip()\n",
    "\n",
    "login(token=HF_TOKEN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cae340",
   "metadata": {},
   "source": [
    "## Mount Google Drive (Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f485a41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_running_in_colab():\n",
    "   from google.colab import drive\n",
    "   drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e277a8c7",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd88e640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from typing import List, Optional\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "import torch\n",
    "from pydantic import BaseModel, ValidationError\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95271ea9",
   "metadata": {},
   "source": [
    "##  Load Mistral-7B-Instruct with Fallback to Quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af05c12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "\n",
    "def get_available_gpu_memory_gb():\n",
    "    \"\"\"Returns available GPU memory (in GB) using nvidia-smi. Returns 0 if GPU not available.\"\"\"\n",
    "    try:\n",
    "        output = subprocess.check_output(\n",
    "            [\"nvidia-smi\", \"--query-gpu=memory.free\", \"--format=csv,nounits,noheader\"],\n",
    "            encoding=\"utf-8\"\n",
    "        )\n",
    "        # First GPU (assume single GPU setup)\n",
    "        free_mem_mb = int(output.strip().split(\"\\n\")[0])\n",
    "        return free_mem_mb / 1024  # Convert MB to GB\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not fetch GPU memory: {str(e)}\")\n",
    "        return 0.0\n",
    "\n",
    "def load_mistral_pipeline_dynamic(model_name=\"mistralai/Mistral-7B-Instruct-v0.1\", hf_token=None):\n",
    "    print(\"üîç Detecting system resources...\")\n",
    "    has_cuda = torch.cuda.is_available()\n",
    "    free_vram_gb = get_available_gpu_memory_gb() if has_cuda else 0\n",
    "\n",
    "    print(f\"üß† CUDA available: {has_cuda}\")\n",
    "    print(f\"üìä Free GPU memory: {free_vram_gb:.2f} GB\")\n",
    "\n",
    "    device_map = {\"\": 0} if has_cuda else \"cpu\"\n",
    "    load_quantized = (not has_cuda) or (free_vram_gb < 14)  # ~14 GB is a safe threshold for Mistral-7B FP16\n",
    "\n",
    "    print(\"üîß Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        token=hf_token,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        if load_quantized:\n",
    "            print(\"‚öôÔ∏è Using 8-bit quantized model (low VRAM or CPU fallback)...\")\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                token=hf_token,\n",
    "                device_map=device_map,\n",
    "                quantization_config=BitsAndBytesConfig(load_in_8bit=True),\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            print(\"‚úÖ Loaded 8-bit model.\")\n",
    "        else:\n",
    "            print(\"‚öôÔ∏è Using full precision FP16 model...\")\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                token=hf_token,\n",
    "                device_map=device_map,\n",
    "                torch_dtype=torch.float16,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            print(\"‚úÖ Loaded full precision model.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading model: {str(e)}\")\n",
    "        raise RuntimeError(\"Model load failed.\")\n",
    "\n",
    "    print(\"üéØ Model on device:\", next(model.parameters()).device)\n",
    "    return pipeline(\"text-generation\", model=model, tokenizer=tokenizer, batch_size=2)\n",
    "\n",
    "llm_pipeline = load_mistral_pipeline_dynamic(hf_token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4157bab3",
   "metadata": {},
   "source": [
    "## Define Pydantic Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e1425f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Education(BaseModel):\n",
    "    degree: str\n",
    "    field: str\n",
    "    institution: str\n",
    "    year: str\n",
    "\n",
    "class Experience(BaseModel):\n",
    "    job_title: str\n",
    "    company: str\n",
    "    duration: str\n",
    "    description: str\n",
    "\n",
    "class ResumeSchema(BaseModel):\n",
    "    basics: dict\n",
    "    education: List[Education]\n",
    "    experience: List[Experience]\n",
    "    skills: List[str]\n",
    "    certifications: List[str]\n",
    "    projects: List[str]\n",
    "\n",
    "class JobDescriptionSchema(BaseModel):\n",
    "    title: str\n",
    "    summary: str\n",
    "    required_experience_years: float\n",
    "    preferred_degrees: List[str]\n",
    "    required_skills: List[str]\n",
    "    certifications: List[str]\n",
    "    soft_skills: List[str]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc3114a",
   "metadata": {},
   "source": [
    "##  Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fd3ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESUME_PROMPT_TEMPLATE = \"\"\"Extract the structured resume JSON from the raw resume below:\n",
    "--------------------\n",
    "{text}\n",
    "--------------------\n",
    "The output should match this schema (no extra fields):\n",
    "{schema}\n",
    "Return a valid JSON object only.\n",
    "\"\"\"\n",
    "\n",
    "JD_PROMPT_TEMPLATE = \"\"\"Extract structured job description JSON from the raw JD below:\n",
    "--------------------\n",
    "{text}\n",
    "--------------------\n",
    "The output should match this schema:\n",
    "{schema}\n",
    "Return a valid JSON object only.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec7d2c6",
   "metadata": {},
   "source": [
    "## Inference Function -JSON Extraction  with Retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b1aea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_structured_json(text, prompt_template, schema_dict, max_new_tokens=512, retries=2):\n",
    "    schema_str = json.dumps(schema_dict, indent=2)\n",
    "    prompt = prompt_template.format(text=text.strip()[:1500], schema=schema_str)\n",
    "    attempt = 0\n",
    "\n",
    "    while attempt <= retries:\n",
    "        try:\n",
    "            response = llm_pipeline(prompt, max_new_tokens=max_new_tokens, do_sample=False)[0][\"generated_text\"]\n",
    "            json_start = response.find(\"{\")\n",
    "            return json.loads(response[json_start:])\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Attempt {attempt + 1} failed: {str(e)}\")\n",
    "            attempt += 1\n",
    "            time.sleep(1)\n",
    "\n",
    "    return {\"raw_output\": \"failed\"}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a69c78",
   "metadata": {},
   "source": [
    "##  Normalize in Batches with Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c001b8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from datetime import datetime\n",
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "def validate_entry(entry, is_resume):\n",
    "    try:\n",
    "        if is_resume:\n",
    "            ResumeSchema.parse_obj(entry)\n",
    "        else:\n",
    "            JobDescriptionSchema.parse_obj(entry)\n",
    "        return True\n",
    "    except ValidationError as ve:\n",
    "        return False\n",
    "\n",
    "def normalize_and_save(\n",
    "    input_filename,\n",
    "    output_filename_prefix,\n",
    "    is_resume=True,\n",
    "    output_dir=Path(\"json_outputs\"),\n",
    "    google_drive_sync=True,\n",
    "    drive_subdir=\"AI-Resume-Agent\"\n",
    "):\n",
    "    # === Setup Metadata ===\n",
    "    batch_id = uuid.uuid4().hex[:6]\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    valid_file = f\"{output_filename_prefix}_{timestamp}_{batch_id}.json\"\n",
    "    invalid_file = f\"invalid_{output_filename_prefix}_{timestamp}_{batch_id}.json\"\n",
    "    meta_file = f\"run_metadata_{output_filename_prefix}_{timestamp}_{batch_id}.json\"\n",
    "\n",
    "    input_path = output_dir / input_filename\n",
    "    valid_output_path = output_dir / valid_file\n",
    "    invalid_output_path = output_dir / invalid_file\n",
    "    metadata_path = output_dir / meta_file\n",
    "\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw_data = [json.loads(line) for line in f.readlines() if line.strip()]\n",
    "\n",
    "    results, invalids = [], []\n",
    "    prompt_template = RESUME_PROMPT_TEMPLATE if is_resume else JD_PROMPT_TEMPLATE\n",
    "    schema_ref = ResumeSchema.schema() if is_resume else JobDescriptionSchema.schema()\n",
    "\n",
    "    print(f\"‚è≥ Processing {len(raw_data)} records...\")\n",
    "    for record in tqdm(raw_data):\n",
    "        text = record.get(\"Resume_str\" if is_resume else \"description\", \"\")\n",
    "        parsed = extract_structured_json(\n",
    "            text=text,\n",
    "            prompt_template=prompt_template,\n",
    "            schema_dict=schema_ref\n",
    "        )\n",
    "        if validate_entry(parsed, is_resume):\n",
    "            results.append(parsed)\n",
    "        else:\n",
    "            invalids.append({\"input\": text, \"output\": parsed})\n",
    "\n",
    "    # === Save Results ===\n",
    "    with open(valid_output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "    with open(invalid_output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(invalids, f, indent=2)\n",
    "\n",
    "    # === Metadata ===\n",
    "    meta = {\n",
    "        \"batch_id\": batch_id,\n",
    "        \"timestamp\": timestamp,\n",
    "        \"input_file\": input_filename,\n",
    "        \"output_valid_file\": valid_file,\n",
    "        \"output_invalid_file\": invalid_file,\n",
    "        \"record_count\": len(raw_data),\n",
    "        \"valid_count\": len(results),\n",
    "        \"invalid_count\": len(invalids),\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"quantized\": not any(p.dtype == torch.float16 for p in llm_pipeline.model.parameters()),\n",
    "        \"device\": str(next(llm_pipeline.model.parameters()).device)\n",
    "    }\n",
    "\n",
    "    with open(metadata_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "\n",
    "    print(f\"‚úÖ Saved {len(results)} valid ‚Üí {valid_output_path}\")\n",
    "    print(f\"‚ö†Ô∏è Saved {len(invalids)} invalid ‚Üí {invalid_output_path}\")\n",
    "    print(f\"üìÑ Metadata ‚Üí {metadata_path}\")\n",
    "\n",
    "    # === Google Drive Sync (Colab only) ===\n",
    "    if google_drive_sync and is_running_in_colab():\n",
    "        drive_base = Path(\"/content/drive/MyDrive\") / drive_subdir\n",
    "        drive_base.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        for file in [valid_output_path, invalid_output_path, metadata_path]:\n",
    "            if file.exists():\n",
    "                dest = drive_base / file.name\n",
    "                file.replace(dest)\n",
    "                print(f\"üìÇ Synced to Google Drive: {dest}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48f65d6",
   "metadata": {},
   "source": [
    "## Run Phase 2 End-to-End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43c3cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_phase2_structured_normalization():\n",
    "    normalize_and_save(\"parsed_resumes.json\", \"normalized_resumes.json\", is_resume=True)\n",
    "    normalize_and_save(\"parsed_jds.json\", \"normalized_jds.json\", is_resume=False)\n",
    "\n",
    "run_phase2_structured_normalization()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
